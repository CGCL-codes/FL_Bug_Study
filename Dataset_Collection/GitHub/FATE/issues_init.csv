url,label,title,all_text,comments,created_time,updated_time,closed_time
https://github.com/FederatedAI/FATE/issues/4107,[],"use standalone_fate:1.8.0,can not upload data","use standalone_fate:1.8.0,can not upload data**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
export version=1.8.0
docker pull federatedai/standalone_fate:${version}
docker run -d --name standalone_fate -p 8080:8080 federatedai/standalone_fate:${version}
docker exec -it $(docker ps -aqf ""name=standalone_fate"") bash
vi aaa
`
{
    ""file"": ""examples/data/breast_hetero_guest.csv"",
    ""id_delimiter"": "","",
    ""head"": 1,
    ""partition"": 1,
    ""namespace"": ""experiment"",
    ""table_name"": ""breast_hetero_guest"",
    ""storage_engine"": ""EGGROLL"",
    ""use_local_data"": 0
}
`
flow data upload -c aaaa
```
{
    ""data"": {
        ""board_url"": ""http://127.0.0.1:8080/index.html#/dashboard?job_id=202207150852574621610&role=local&party_id=0"",
        ""code"": 0,
        ""dsl_path"": ""/data/projects/fate/fateflow/jobs/202207150852574621610/job_dsl.json"",
        ""job_id"": ""202207150852574621610"",
        ""logs_directory"": ""/data/projects/fate/fateflow/logs/202207150852574621610"",
        ""message"": ""success"",
        ""model_info"": {
            ""model_id"": ""local-0#model"",
            ""model_version"": ""202207150852574621610""
        },
        ""namespace"": ""experiment"",
        ""pipeline_dsl_path"": ""/data/projects/fate/fateflow/jobs/202207150852574621610/pipeline_dsl.json"",
        ""runtime_conf_on_party_path"": ""/data/projects/fate/fateflow/jobs/202207150852574621610/local/0/job_runtime_on_party_conf.json"",
        ""runtime_conf_path"": ""/data/projects/fate/fateflow/jobs/202207150852574621610/job_runtime_conf.json"",
        ""table_name"": ""breast_hetero_guest"",
        ""train_runtime_conf_path"": ""/data/projects/fate/fateflow/jobs/202207150852574621610/train_runtime_conf.json""
    },
    ""jobId"": ""202207150852574621610"",
    ""retcode"": 0,
    ""retmsg"": ""success""
}
```
flow table info -n experiment -t breast_hetero_guest
```
{
    ""data"": {
        ""address"": null,
        ""count"": 0,
        ""enable"": true,
        ""exist"": 0,
        ""namespace"": ""experiment"",
        ""origin"": null,
        ""partition"": null,
        ""schema"": null,
        ""table_name"": ""breast_hetero_guest""
    },
    ""retcode"": 0,
    ""retmsg"": ""success""
}
```
cd /data/projects/fate/fateflow/logs/202207150852574621610
cat fate_flow_schedule_error.log
```
[ERROR] [2022-07-15 08:53:09,113] [202207150852574621610] [1431:139914187827008] - [_session.get_session_from_record] [line:397]: No module named 'eggroll'
```

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
使用fateflow/examples/upload/upload_guest.json这个文件可以创建成功。
看到了一个引擎""storage_type"": ""LMDB""，但是我在文档里面没看到这种引擎。
https://fate-flow.readthedocs.io/en/latest/zh/fate_flow_data_access/> 使用fateflow/examples/upload/upload_guest.json这个文件可以创建成功。 看到了一个引擎""storage_type"": ""LMDB""，但是我在文档里面没看到这种引擎。 https://fate-flow.readthedocs.io/en/latest/zh/fate_flow_data_access/

单机版不是使用的EGGROLL，用默认配置上传即可@mgqa34 的确是这样，社区的文档有点不太全。",3,2022-07-15 09:04:35,2022-07-19 06:16:50,2022-07-19 06:16:50
https://github.com/FederatedAI/FATE/issues/4093,[],FATE1.8 运行10万级数据逻辑回归报错,"FATE1.8 运行10万级数据逻辑回归报错用kubefate的docker方式安装了FATE1.8版本，运行10万*500特征逻辑回归报错，前端页面无报错信息，具体报错内容如下。
Guest日志
![image](https://user-images.githubusercontent.com/62459458/178182421-80fb2c1c-b821-4601-af36-a3e20b006ee2.png)
Host日志
![image](https://user-images.githubusercontent.com/62459458/178182443-7de3a07c-fd5f-44b0-9e9c-9d9b2a2fdb59.png)
页面信息如下：
![image](https://user-images.githubusercontent.com/62459458/178182455-6b1567d9-e541-4145-86b9-659e3ec03b6c.png)
![image](https://user-images.githubusercontent.com/62459458/178182469-65bb6c78-aaea-4fa2-b7c4-00c2644a2118.png)
看起来这是三方逻辑回归？ 是的话麻烦检查下arbiter日志是否有错误补充arbiter日志，日志中没有报错，而是直接终止了。
Arbiter日志
![image](https://user-images.githubusercontent.com/62459458/178190988-f401ec57-254b-4f95-8cee-29c57c9147b3.png)
![image](https://user-images.githubusercontent.com/62459458/178191003-dde4cb0e-f0cd-4c9b-beb3-2ad3aeba6fb0.png)


> 看起来这是三方逻辑回归？ 是的话麻烦检查下arbiter日志是否有错误

> 补充arbiter日志，日志中没有报错，而是直接终止了。 Arbiter日志 ![image](https://user-images.githubusercontent.com/62459458/178190988-f401ec57-254b-4f95-8cee-29c57c9147b3.png) ![image](https://user-images.githubusercontent.com/62459458/178191003-dde4cb0e-f0cd-4c9b-beb3-2ad3aeba6fb0.png)
> 
> > 看起来这是三方逻辑回归？ 是的话麻烦检查下arbiter日志是否有错误

看了一下图片，任务运行超过72小时了，默认任务超市时间是72小时，可以在job_parameters::common::timeout字段设置下，单位是秒> > 补充arbiter日志，日志中没有报错，而是直接终止了。 Arbiter日志 ![image](https://user-images.githubusercontent.com/62459458/178190988-f401ec57-254b-4f95-8cee-29c57c9147b3.png) ![image](https://user-images.githubusercontent.com/62459458/178191003-dde4cb0e-f0cd-4c9b-beb3-2ad3aeba6fb0.png)
> > > 看起来这是三方逻辑回归？ 是的话麻烦检查下arbiter日志是否有错误
> 
> 看了一下图片，任务运行超过72小时了，默认任务超市时间是72小时，可以在job_parameters::common::timeout字段设置下，单位是秒

好的，我试试，谢谢",4,2022-07-11 03:20:43,2022-07-11 07:34:35,2022-07-11 07:34:35
https://github.com/FederatedAI/FATE/issues/4079,[],Paillier并没有被调用在纵向Secureboost中.,"Paillier并没有被调用在纵向Secureboost中.我运行纵向secureboost时将test_secureboost_train_binary_conf.json中的HE算法改为:  ""encrypt_param"": { ""method"": ""Paillier"", ""key_length"": 3072}，但是我发现在secureprotol下的paillier并没有被调用！！！首先，我承认因为我将密钥改为了3072，感觉似乎是变慢了，这符合预期，但说实话并没有变慢多少，这让我很疑惑，但确实变慢了，同时我确定secureprotol下的paillier并没有被调用，因为如果被调用了会有我打印的日志。因此，我在配置文件将HE修改为Paillier的操作其实是无效的。但是，在FATE Board的参数查看中，明确能看到我使用的是Paillier-3072，但其实根本没有被调用。

还有就是，因为我修改秘钥长度而让训练速度变慢是事实，我想秘钥长度的修改是或多或少生效了的，也许还是调用的原方案，而不是调用的Paillier，但我认为这应该在FATE Board中指明Paillier并没有生效。

最后，我想问一下开发者，如果我想修改HE的并发量，我该怎么做？比如，我不希望它是并发的，而是串行执行的。同时我进行补充，如果运行纵向LR，那么Paillier是被调用了的，而且能看到我的打印日志，我想我应该是没有错的，请开发者查实一下。我找到秘钥生成的地方了，确定Paillier的秘钥确实是被生成了，但是整个训练过程中完全没有用到加解密？这是否合理呢？这里使用最新1.8的单机版进行测试，key_length=3072，在hetero_decision_tree_guest.py的377行加入了这行debug，它打印出梯度加密后的结果：
![image](https://user-images.githubusercontent.com/10768970/177459532-141ddda0-d645-4f55-b762-8b268e4b4bf9.png)
日志中的结果:
![企业微信截图_16570768962722](https://user-images.githubusercontent.com/10768970/177459598-0190d9c4-5a48-42e3-bb14-7a27d3cce9b1.png)

Paillier在Secureboost中是一定生效的，不存在不生效的情况，如果你因为修改代码，导致Paillier不工作，那么你应该检查你的代码；或者检查debug log位置是否正确@talkingwallace 非常感谢您的解答，我已经找到加密的地方了，我本以为不管是LR还是Secureboost都是调用的fate_paillier下的加解密算法，结果在Secureboost中调用的是encrypt_mode.py下的加密算法。

但是我还有一个问题，如果您方便的话能不能解答一下？在hetero_decision_tree_guest.py中我没有找到解密的调用，因为我调用find_best_split_guest_and_host函数的need_decrypt的bool值一直为False。我不知道解密在哪里发生的？是写在了其他地方吗？解密的操作在splitter.py下的374行哈，传入了paillier encrypter并对g, h进行了解密@talkingwallace 好的，非常感谢！好的 不客气",7,2022-07-05 12:42:36,2022-07-06 09:01:07,2022-07-06 08:58:24
https://github.com/FederatedAI/FATE/issues/4078,[],社区要加油啊，隔壁蚂蚁刚刚开源了他们的隐私计算框架 隐语,"社区要加油啊，隔壁蚂蚁刚刚开源了他们的隐私计算框架 隐语**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
Let's work hard together to make privacy preserving better~.  By the way, it will be much better to discuss technical problems when opening an issue.",1,2022-07-05 09:38:35,2022-07-05 11:29:54,2022-07-05 11:29:54
https://github.com/FederatedAI/FATE/issues/4075,[],Fate_1.8.0 跑secureboost_cross_validation_binary预测任务报错,"Fate_1.8.0 跑secureboost_cross_validation_binary预测任务报错**Describe the bug**
A clear and concise description of what the bug is.

1. secureboost_cross_validation_binary预测任务报错


**To Reproduce**
Steps to reproduce the behavior:
1. 使用给定模版：test_secureboost_cross_validation_binary_conf.json、test_secureboost_cross_validation_dsl.json跑正常训练任务，使用跑完的模型跑预测任务，通过`flow model get-predict-dsl ` `flow model get-predict-conf` 下载预测所需的dsl、conf文件来跑预测任务，通过 `flow job submit -d -c`来执行提交预测任务，预测任务在hetero_secure_boost_0节点报错
2. 报错信息如下
`[INFO] [2022-07-01 08:24:06,641] [202207010823023086500] [64728:139764175443776] - [task_executor._run_] [line:145]: task input dsl {'data': {'test_data': ['intersection_0.data']}, 'model': ['pipeline.hetero_secure_boost_0.model']}
[INFO] [2022-07-01 08:24:06,641] [202207010823023086500] [64728:139764175443776] - [tracker_client.get_output_data_info] [line:265]: Request read job 202207010823023086500 task 202207010823023086500_hetero_secure_boost_0 0 on guest 9999 data data info
[INFO] [2022-07-01 08:24:06,657] [202207010823023086500] [64728:139764175443776] - [task_executor.get_task_run_args] [line:339]: load computing table use 4
[INFO] [2022-07-01 08:24:06,714] [202207010823023086500] [64728:139764175443776] - [task_executor._run_] [line:156]: task input args {'data': {'intersection_0': {'test_data': [<fate_arch.computing.eggroll._table.Table object at 0x7f1d10fa8c88>]}}, 'model': {'hetero_secure_boost_0': {}}}
[INFO] [2022-07-01 08:24:06,716] [202207010823023086500] [64728:139764175443776] - [task_executor._run_] [line:194]: profile logging is disabled
[WARNING] [2022-07-01 08:24:06,719] [202207010823023086500] [64728:139764175443776] - [base_param._warn_to_deprecate_param] [line:375]: boosting_param's validation_freqs will be deprecated in future release; please use callback_param's 'validation_freqs' instead.
[ERROR] [2022-07-01 08:24:06,720] [202207010823023086500] [64728:139764175443776] - [task_executor._run_] [line:243]: Train_data should be configured in cross-validate task or stepwise task
Traceback (most recent call last):
  File ""/data/projects/fate/fateflow/python/fate_flow/worker/task_executor.py"", line 195, in _run_
    cpn_output = run_object.run(cpn_input)
  File ""/data/projects/fate/fate/python/federatedml/model_base.py"", line 236, in run
    self._run(cpn_input=cpn_input)
  File ""/data/projects/fate/fate/python/federatedml/model_base.py"", line 299, in _run
    datasets=cpn_input.datasets, models=cpn_input.models, cpn=self
  File ""/data/projects/fate/fate/python/federatedml/util/component_properties.py"", line 299, in extract_running_rules
    datasets, cpn
  File ""/data/projects/fate/fate/python/federatedml/util/component_properties.py"", line 248, in extract_input_data
    self._abnormal_dsl_config_detect()
  File ""/data/projects/fate/fate/python/federatedml/util/component_properties.py"", line 178, in _abnormal_dsl_config_detect
  File ""/data/projects/fate/fate/python/federatedml/util/component_properties.py"", line 178, in _abnormal_dsl_config_detect
    ""Train_data should be configured in cross-validate ""
    ""Train_data should be configured in cross-validate ""
federatedml.util.component_properties.DSLConfigError: Train_data should be configured in cross-validate task or stepwise task
[INFO] [2022-07-01 08:24:06,721] [202207010823023086500] [64728:139764175443776] - [task_base_worker.report_task_info_to_driver] [line:124]: report TaskExecutor 202207010823023086500_hetero_secure_boost_0 0 guest 9999 to driver:
{'job_id': '202207010823023086500', 'component_name': 'hetero_secure_boost_0', 'task_id': '202207010823023086500_hetero_secure_boost_0', 'task_version': '0', 'role': 'guest', 'party_id': '9999', 'run_ip': 'xxx.xxx.xxx.xxx', 'run_pid': 64728, 'party_status': 'failed', 'src_fate_ver': '1.8.0', 'src_role': '', 'src_party_id': '', 'end_time': 1656663846721, 'elapsed': 3408}
[INFO] [2022-07-01 08:24:06,721] [202207010823023086500] [64728:139764175443776] - [control_client.report_task] [line:41]: request update job 202207010823023086500 task 202207010823023086500_hetero_secure_boost_0 0 on guest 9999
[INFO] [2022-07-01 08:24:06,763] [202207010823023086500] [64728:139764175443776] - [task_executor._run_] [line:254]: finish hetero_secure_boost_0 202207010823023086500_hetero_secure_boost_0 0 on guest 9999 with failed
[INFO] [2022-07-01 08:24:06,764] [202207010823023086500] [64728:139764175443776] - [base_worker.run] [line:123]: worker TaskExecutor, process role: ProcessRole.WORKER, pid: 64728, elapsed: 4025 ms`

**Expected behavior**
A clear and concise description of what you expected to happen.
1. 正常应该可以顺利跑完预测任务
**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - Browser [e.g. chrome,]
 - 使用Docker部署
 `
  images 
  federatedai/client:1.8.0-release   
  federatedai/fateboard:1.8.0-release
  federatedai/python:1.8.0-release   
  federatedai/eggroll:1.8.0-release  
  federatedai/eggroll:1.8.0-release  
  federatedai/eggroll:1.8.0-release  
  mysql:8.0.28
`

![image](https://user-images.githubusercontent.com/10768970/177113939-f0561e2b-60f7-45f6-a377-8ff917ed190b.png)
看样子是数据配置有问题 可以看看DSL吗> ![image](https://user-images.githubusercontent.com/10768970/177113939-f0561e2b-60f7-45f6-a377-8ff917ed190b.png) 看样子是数据配置有问题 可以看看DSL吗
dsl如下
`{
    ""components"": {
        ""reader_0"": {
            ""module"": ""Reader"",
            ""output"": {
                ""data"": [
                    ""data""
                ]
            },
            ""CodePath"": ""Reader""
        },
        ""data_transform_0"": {
            ""module"": ""DataTransform"",
            ""input"": {
                ""model"": [
                    ""pipeline.data_transform_0.model""
                ],
                ""data"": {
                    ""data"": [
                        ""reader_0.data""
                    ]
                }
            },
            ""output"": {
                ""data"": [
                    ""data""
                ]
            },
            ""CodePath"": ""DataTransform""
        },
        ""intersection_0"": {
            ""module"": ""Intersection"",
            ""output"": {
                ""data"": [
                    ""data""
                ]
            },
            ""input"": {
                ""data"": {
                    ""data"": [
                        ""data_transform_0.data""
                    ]
                }
            },
            ""CodePath"": ""IntersectGuest""
        },
        ""hetero_secure_boost_0"": {
            ""module"": ""HeteroSecureBoost"",
            ""input"": {
                ""model"": [
                    ""pipeline.hetero_secure_boost_0.model""
                ],
                ""data"": {
                    ""test_data"": [
                        ""intersection_0.data""
                    ]
                }
            },
            ""output"": {
                ""data"": [
                    ""data""
                ]
            },
            ""CodePath"": ""HeteroSecureBoostingTreeGuest""
        }
    }
}`Cross validation does not support predict, it only evaluates data performance and will not generate model outputs.好的，了解了",4,2022-07-04 02:30:07,2022-07-06 08:58:38,2022-07-06 08:58:38
https://github.com/FederatedAI/FATE/issues/4074,[],fate on spark （rabbitmq）Create job faild,"fate on spark （rabbitmq）Create job faildHadoop、spark是现有集群 。跑run_toy_example报错。Create job faild。

<img width=""936"" alt=""问题"" src=""https://user-images.githubusercontent.com/69965169/176812100-aa7003b2-1078-423b-9f58-f8943baeed16.png"">
Nginx镜像有问题，使用正确的镜像就不会出现这种问题。",1,2022-07-01 02:37:16,2022-07-12 06:28:58,2022-07-12 06:27:51
https://github.com/FederatedAI/FATE/issues/4072,[],编码中遇到错误,"编码中遇到错误在单机尝试的时候，遇到报错，IP not configured，请问这是什么问题，怎么修改，
![IMG_2281(20220630-142605)](https://user-images.githubusercontent.com/36072695/176607489-ed92bdeb-65ac-466a-ae0c-97e9afa60bdd.PNG)
Please initialize the pipeline first before running examples.这个问题我解决了，但是还有一个问题就是9999 is not in list，这个该怎么办？",2,2022-06-30 06:27:29,2022-06-30 08:45:04,2022-06-30 08:45:04
https://github.com/FederatedAI/FATE/issues/4066,[],different prediction result from same prediction data ,"different prediction result from same prediction data **Describe the bug**
host and guest have same prediction data but get different prediction result
model: homo secure boosting 

**To Reproduce**
environment: docker deploy cluster
training: HomoSecureBoost

prediction data: 
    host: {sid: 127, x1:  1882} 
    guest: {sid: 127, x1:  1882} 

prediction result:
    host: 1315.755862
    guest: 803.990449

**Expected behavior**
prediction result from host and guest are supposed to be the same

**Screenshots**
<img width=""255"" alt=""1656466106047"" src=""https://user-images.githubusercontent.com/7354609/176331524-2933b033-e4e0-4c22-983b-933b60f6fff8.png"">
<img width=""960"" alt=""1656465926646"" src=""https://user-images.githubusercontent.com/7354609/176331157-55a2fe89-a731-4049-b123-08b332213a7c.png"">
<img width=""769"" alt=""1656466001396"" src=""https://user-images.githubusercontent.com/7354609/176331295-ed0db23d-6be1-4fb5-9f33-1a065538b7dc.png"">
<img width=""773"" alt=""1656466042191"" src=""https://user-images.githubusercontent.com/7354609/176331356-1b4de053-7b69-48d1-ad5c-42a21e6caf46.png"">
<img width=""770"" alt=""1656466074551"" src=""https://user-images.githubusercontent.com/7354609/176331445-c3237627-30e7-4cf2-8a40-f13827121fad.png"">



for the config info:

training:
```python
    pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, host=host, arbiter=arbiter)
    
    reader_0 = Reader(name=""reader_0"")
    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)
    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)
    
    data_transform_0 = DataTransform(name=""data_transform_0"")
    data_transform_0.get_party_instance(
        role='guest',
        party_id=guest).component_param(
        with_label=True,
        output_format=""dense"",
        label_type=""float"")
    data_transform_0.get_party_instance(
        role='host',
        party_id=host).component_param(
        with_label=True,
        output_format=""dense"",
        label_type=""float"")
    

    homo_secureboost_0 = HomoSecureBoost(name=""homo_secureboost_0"",
                                         num_trees=50,
                                         task_type='regression',
                                         objective_param={""objective"": ""lse""},
                                         tree_param={
                                             ""max_depth"": 50
                                         },
                                         validation_freqs=1
                                         )


    pipeline.add_component(reader_0)
    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))
    pipeline.add_component(homo_secureboost_0, data=Data(train_data=data_transform_0.output.data))
```

prediction:
```python
    pipeline = PipeLine.load_model_from_file(model)
    pipeline.deploy_component([pipeline.data_transform_0, pipeline.homo_secureboost_0])

    predict_pipeline = PipeLine()  
    
    reader_1 = Reader(name=""reader_1"")
    reader_1.get_party_instance(role='guest', party_id=guest).component_param(table=guest_predict_data)
    reader_1.get_party_instance(role='host', party_id=host).component_param(table=host_predict_data)

    predict_pipeline.add_component(reader_1)
    predict_pipeline.add_component(pipeline, data=Data(predict_input={pipeline.data_transform_0.input.data: 
reader_1.output.data}))
    predict_pipeline.predict()
```

parts of training data:
sid,x1,y
70,621,1151
71,1151,807
72,807,741
73,741,1429
74,1429,688
75,688,801
76,801,931
77,931,1343
78,1343,691
79,691,1564
80,1564,1459
81,1459,1458
82,1458,1201
.....Please give the version of fate.  Moreover, it will be much better if full dataset is given.**host training data：**
sid,x1,y
70,621,1151
71,1151,807
72,807,741
73,741,1429
74,1429,688
75,688,801
76,801,931
77,931,1343
78,1343,691
79,691,1564
80,1564,1459
81,1459,1458
82,1458,1201
83,1201,1403
84,1403,1527
85,1527,902
86,902,1371
87,1371,1697
88,1697,1579
89,1579,1168
90,1168,1545
91,1545,1104
92,1104,1144
93,1144,1484
94,1484,1318
95,1318,883
96,883,1344
97,1344,1769
98,1769,1195
99,1195,933
100,933,1606
101,1606,1810
102,1810,1162
103,1162,1069
104,1069,1204
105,1204,993
106,993,1126
107,1126,1234
108,1234,1318
109,1318,1704
110,1704,1381
111,1381,1706
112,1706,1020
113,1020,1675
114,1675,1869
115,1869,1199
116,1199,1182
117,1182,1355
118,1355,1668
119,1668,1364
120,1364,1731
121,1731,1765
122,1765,1707
123,1707,1710
124,1710,1738
125,1738,1443
126,1443,1882


**guest training data：**
sid,x1,y
0,462,339
1,339,368
2,368,621
3,621,527
4,527,389
5,389,999
6,999,134
7,134,299
8,299,328
9,328,894
10,894,879
11,879,254
12,254,984
13,984,988
14,988,769
15,769,639
16,639,1003
17,1003,514
18,514,988
19,988,327
20,327,491
21,491,390
22,390,576
23,576,476
24,476,733
25,733,436
26,436,933
27,933,540
28,540,584
29,584,596
30,596,1072
31,1072,970
32,970,385
33,385,562
34,562,530
35,530,784
36,784,1000
37,1000,1022
38,1022,1280
39,1280,1249
40,1249,1289
41,1289,487
42,487,784
43,784,635
44,635,1150
45,1150,561
46,561,548
47,548,1270
48,1270,775
49,775,911
50,911,1320
51,1320,1013
52,1013,1037
53,1037,1327
54,1327,1229
55,1229,656
56,656,1018
57,1018,1337
58,1337,1253
59,1253,1284
60,1284,904
61,904,1288
62,1288,1459
63,1459,1041
64,1041,1269
65,1269,1486
66,1486,1055
67,1055,1143
68,1143,816
69,816,621Hi！ Could you please provide the model files or the origin dataset, if possible? So that we can reproduce the problem locally. Thanks.> Please give the version of fate. Moreover, it will be much better if full dataset is given.

thank you for your reply, the version I used is 1.7.2 docker compose deployment, and the full dataset is attached above. thanks.Hi! This problem is caused by init-score initialization. In the regression task, init scores are computed by averaging labels, so guest/host models have different init scores. This causes the difference in predicted results. 
Thanks for your feedback, we ll fix this in the following version.",6,2022-06-29 01:32:21,2022-07-18 02:45:48,2022-07-18 02:45:48
https://github.com/FederatedAI/FATE/issues/4057,[],单机部署后如何使用在线预测功能？,"单机部署后如何使用在线预测功能？场景：基于 [官方部署文档](https://fate.readthedocs.io/en/latest/zh/deploy/standalone-deploy/) 单机部署了环境，然后进行模型训练后，想使用在线预测功能。

问题：由于 `standalone-deploy` 只部署了 `FATE-Flow` 和 `FATE-Board` 两个服务，缺少 `FATE-Serving` 服务，无法使用 **在线预测功能**，请问在单机环境下如何部署 `FATE-Serving` 服务，有没有相关文档。Standalone does not support publish model to Fate-Serving, use cluster deploy instead. ",1,2022-06-24 01:37:20,2022-06-29 06:15:14,2022-06-29 06:15:14
https://github.com/FederatedAI/FATE/issues/4049,[],使用pipeline建立nn模型时报错,"使用pipeline建立nn模型时报错![image](https://user-images.githubusercontent.com/33534276/174574713-66a08c94-297a-46b7-9b36-b62f9ecaf902.png)
报错代码位置:
![image](https://user-images.githubusercontent.com/33534276/174574507-bcf6e5b2-2d4b-4cb9-8d7b-55637fbbcb14.png)
示例中的keras.layers.core.Dense与tensorflow.python.keras.engine.base_layer.Layer
isinstance总为false
Give more details like  ""executable code"", do you modify something? Examples in fate are always tested before release. tensorflow版本影响，回退至2.3.0，不报错",2,2022-06-20 09:50:27,2022-06-21 09:24:50,2022-06-21 09:24:49
https://github.com/FederatedAI/FATE/issues/4044,[],spark-rabbitmq timeout or pulsar long time issue for large-scale dataset,"spark-rabbitmq timeout or pulsar long time issue for large-scale datasetsmall data works fine, only large data will have timeout issue
![图片](https://user-images.githubusercontent.com/19666925/174069597-7914293c-565a-40ca-8741-747668be13f1.png)

for rabbitmq, 
after 15 minutes, intersect_0 task will report error like: connections refused
![图片](https://user-images.githubusercontent.com/19666925/174069711-4162b553-cc99-4640-a5dd-6b7dbfd24ea4.png)

for pulsar, 

the ""remote host_ids_process to Guest"" step will last more than 10 hours
![图片](https://user-images.githubusercontent.com/19666925/174069895-0d1a0d57-b6e8-4fef-971d-116bd38cc4f3.png)


thanks 
majorinupdated
for rabbitmq, it stop at 8000w size
for pulsar, it stop at 2000w size

![图片](https://user-images.githubusercontent.com/19666925/178423929-b43f6acb-68de-4a1c-8554-a97e74d19e2c.png)

the error still the same as described above.

75E means 75 Executors",1,2022-06-16 12:30:54,2022-07-12 06:33:56,2022-06-23 03:31:25
https://github.com/FederatedAI/FATE/issues/4043,[],spark resource cannot adjust successfully ,"spark resource cannot adjust successfully in Job_parameters executor-memory can only set as 4G，executor-cores is not working

![config](https://user-images.githubusercontent.com/19666925/174069034-def0d5c7-7fa8-4050-bc77-fe48832f5bab.png)

![2c4g](https://user-images.githubusercontent.com/19666925/174069137-0747b9ca-6651-4678-8f14-e949f5555970.png)


if adjust executor-memory as 32G, cpu will change to 0, then reader_0 task will not work.

![32g](https://user-images.githubusercontent.com/19666925/174069231-c9223026-3152-4576-a3f9-bbb2bb8bb54d.png)

thanks
majorin kubefate resource config problem, have fixed",1,2022-06-16 12:26:17,2022-06-23 03:31:11,2022-06-23 03:31:11
https://github.com/FederatedAI/FATE/issues/4038,[],secure information retrieval 单个id隐匿查询,"secure information retrieval 单个id隐匿查询fate当前版本是将guest和host方所有交集id的x0,x3值查询出来，如果guest和host方交集只有一个id,对应的x0,x3值查询不出来，当前隐匿查询不支持单个id的查询是吗？如果想支持单个id的隐匿查询，需要修改什么地方？已经解决",2,2022-06-15 02:28:15,2022-06-15 10:56:28,2022-06-15 10:56:28
https://github.com/FederatedAI/FATE/issues/4031,[],我在使用homo_nn自定义模型训练mnist时，出现这个了错误，‘NoneType' object has no attribute 'count',"我在使用homo_nn自定义模型训练mnist时，出现这个了错误，‘NoneType' object has no attribute 'count'![image](https://user-images.githubusercontent.com/105529434/173223122-588d6dd8-ca17-423a-8d70-ad3ed3e0209a.png)
![image](https://user-images.githubusercontent.com/105529434/173223218-a20f58c0-a3f7-4403-9482-df1ff1392dcb.png)
What's the version of fate? The error means homo_nn's input data is NoneMy fate version is 1.4。standlone deployed&nbsp;


高露洁丶
***@***.***



&nbsp;




------------------&nbsp;原始邮件&nbsp;------------------
发件人: ***@***.***&gt;; 
发送时间: 2022年6月13日(星期一) 下午2:43
收件人: ***@***.***&gt;; 
抄送: ***@***.***&gt;; ***@***.***&gt;; 
主题: Re: [FederatedAI/FATE] 我在使用homo_nn自定义模型训练mnist时，出现这个了错误，‘NoneType&#39; object has no attribute &#39;count&#39; (Issue #4031)





 
What's the version of fate? The error means homo_nn's input data is None
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you authored the thread.Message ID: ***@***.***&gt;Check the consistence of dataio's output and homo_nn's input in dsl json file please.Thanks for your reply. I'll check it
&nbsp;


高露洁丶
***@***.***



&nbsp;




------------------&nbsp;原始邮件&nbsp;------------------
发件人: ***@***.***&gt;; 
发送时间: 2022年6月13日(星期一) 下午3:25
收件人: ***@***.***&gt;; 
抄送: ***@***.***&gt;; ***@***.***&gt;; 
主题: Re: [FederatedAI/FATE] 我在使用homo_nn自定义模型训练mnist时，出现这个了错误，‘NoneType&#39; object has no attribute &#39;count&#39; (Issue #4031)





 
Check the consistence of dataio's output and homo_nn's input in dsl json file please.
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you authored the thread.Message ID: ***@***.***&gt;{
&nbsp; ""components"": {
&nbsp; &nbsp; &nbsp; ""dataio_0"": {
&nbsp; &nbsp; &nbsp; ""module"": ""DataIO"",
&nbsp; &nbsp; &nbsp; ""input"": {
&nbsp; &nbsp; &nbsp; &nbsp; ""data"": {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ""data"": [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ""args.train_data""
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ]
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; &nbsp; ""output"": {
&nbsp; &nbsp; &nbsp; &nbsp; ""data"": [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ""train""
&nbsp; &nbsp; &nbsp; &nbsp; ],
&nbsp; &nbsp; &nbsp; &nbsp; ""model"": [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ""dataio""
&nbsp; &nbsp; &nbsp; &nbsp; ]
&nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; },
&nbsp; &nbsp; ""homo_nn_0"": {
&nbsp; &nbsp; &nbsp; ""module"": ""HomoNN"",
&nbsp; &nbsp; &nbsp; ""input"": {
&nbsp; &nbsp; &nbsp; &nbsp; ""data"": {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ""train_data"": [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ""dataio_0.train""
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ]
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; &nbsp; ""output"": {
&nbsp; &nbsp; &nbsp; &nbsp; ""data"": [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ""train""
&nbsp; &nbsp; &nbsp; &nbsp; ],
&nbsp; &nbsp; &nbsp; &nbsp; ""model"": [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ""homo_nn""
&nbsp; &nbsp; &nbsp; &nbsp; ]
&nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; },
	""evaluation_0"":{
	 ""module"": ""Evaluation"",
	 ""input"": {
		""data"" :{
			""data"": [""homo_nn_0.train""]
	 }			
	}
&nbsp; }
&nbsp; }}
this is my dsl file&nbsp;
&nbsp;


高露洁丶
***@***.***



&nbsp;




------------------&nbsp;原始邮件&nbsp;------------------
发件人: ***@***.***&gt;; 
发送时间: 2022年6月13日(星期一) 下午3:25
收件人: ***@***.***&gt;; 
抄送: ***@***.***&gt;; ***@***.***&gt;; 
主题: Re: [FederatedAI/FATE] 我在使用homo_nn自定义模型训练mnist时，出现这个了错误，‘NoneType&#39; object has no attribute &#39;count&#39; (Issue #4031)





 
Check the consistence of dataio's output and homo_nn's input in dsl json file please.
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you authored the thread.Message ID: ***@***.***&gt;{
  ""components"": {
       ""dataio_0"": {
      ""module"": ""DataIO"",
      ""input"": {
        ""data"": {
          ""data"": [
            ""args.train_data""
          ]
        }
      },
      ""output"": {
        ""data"": [
          ""data""
        ],
        ""model"": [
          ""dataio""
        ]
      }
    },
    ""homo_nn_0"": {
      ""module"": ""HomoNN"",
      ""input"": {
        ""data"": {
          ""train_data"": [
            ""dataio_0.train""
          ]
        }
      },
      ""output"": {
        ""data"": [
          ""train""
        ],
        ""model"": [
          ""homo_nn""
        ]
      }
    },
	""evaluation_0"":{
	 ""module"": ""Evaluation"",
	 ""input"": {
		""data"" :{
			""data"": [""homo_nn_0.train""]
	 }			
	}
  }
  }}
this is my dsl file.> { ""components"": { ""dataio_0"": { ""module"": ""DataIO"", ""input"": { ""data"": { ""data"": [ ""args.train_data"" ] } }, ""output"": { ""data"": [ ""data"" ], ""model"": [ ""dataio"" ] } }, ""homo_nn_0"": { ""module"": ""HomoNN"", ""input"": { ""data"": { ""train_data"": [ ""dataio_0.train"" ] } }, ""output"": { ""data"": [ ""train"" ], ""model"": [ ""homo_nn"" ] } }, ""evaluation_0"":{ ""module"": ""Evaluation"", ""input"": { ""data"" :{ ""data"": [""homo_nn_0.train""] } } } }} this is my dsl file.

input data of homo_nn should be ""dataio_0.data"" instead of ""dataio_0.train""oh,thanks
&nbsp;


高露洁丶
***@***.***



&nbsp;




------------------&nbsp;原始邮件&nbsp;------------------
发件人: ***@***.***&gt;; 
发送时间: 2022年6月15日(星期三) 下午2:20
收件人: ***@***.***&gt;; 
抄送: ***@***.***&gt;; ***@***.***&gt;; 
主题: Re: [FederatedAI/FATE] 我在使用homo_nn自定义模型训练mnist时，出现这个了错误，‘NoneType&#39; object has no attribute &#39;count&#39; (Issue #4031)





  
{ ""components"": { ""dataio_0"": { ""module"": ""DataIO"", ""input"": { ""data"": { ""data"": [ ""args.train_data"" ] } }, ""output"": { ""data"": [ ""data"" ], ""model"": [ ""dataio"" ] } }, ""homo_nn_0"": { ""module"": ""HomoNN"", ""input"": { ""data"": { ""train_data"": [ ""dataio_0.train"" ] } }, ""output"": { ""data"": [ ""train"" ], ""model"": [ ""homo_nn"" ] } }, ""evaluation_0"":{ ""module"": ""Evaluation"", ""input"": { ""data"" :{ ""data"": [""homo_nn_0.train""] } } } }} this is my dsl file.
  
input data of homo_nn should be ""dataio_0.train"" instead of ""dataio_0.data""
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you authored the thread.Message ID: ***@***.***&gt;",8,2022-06-12 07:57:18,2022-07-01 02:07:21,2022-07-01 02:07:21
https://github.com/FederatedAI/FATE/issues/4021,[],我用homo_nn自定义模型训练mnist，在确定json文件格式正确的情况下，提交任务依然报错，这个错误还会是什么原因造成的呢？,"我用homo_nn自定义模型训练mnist，在确定json文件格式正确的情况下，提交任务依然报错，这个错误还会是什么原因造成的呢？![2801d791b262d6e965bc26c39382bf5](https://user-images.githubusercontent.com/105529434/172270601-6b92ba9a-1d35-4781-b78e-33b47563bd90.png)
![8df6dcd4fb7797752db3958b651c767](https://user-images.githubusercontent.com/105529434/172270622-2fee9d66-0ad4-4567-850d-1a5cdfa59a83.png)
dsl文件的问题，一直在检查conf文件……",1,2022-06-07 00:27:40,2022-06-07 07:59:34,2022-06-07 07:58:37
https://github.com/FederatedAI/FATE/issues/4006,[],FederatedAI / FATE这个仓库default分支是master，FederatedAI / FATE-Flow仓库default分支是main，现在github不是因为种族歧视问题已经禁止master/slave分支命名了吗？为啥还会出现呢？FATE具有一定影响力，这个会不会有问题？,"FederatedAI / FATE这个仓库default分支是master，FederatedAI / FATE-Flow仓库default分支是main，现在github不是因为种族歧视问题已经禁止master/slave分支命名了吗？为啥还会出现呢？FATE具有一定影响力，这个会不会有问题？FederatedAI / FATE这个仓库default分支是master，FederatedAI / FATE-Flow仓库default分支是main，现在github不是因为种族歧视问题已经禁止master/slave分支命名了吗？为啥还会出现呢？FATE具有一定影响力，这个会不会有问题？@jat001 @dylan-fan @hainingzhang @zhihuiwan @MiKKiYang  我个人觉得没有这个必要，修改主分支名还可能破坏已有的一些第三方链接， 事实上有大量的更有影响力的项目都继续保持了主分支的命名， 比如tensorflow。 
默认以master为主分支的命名方式有非常久的历史了，本身也不是为了冒犯某些群体，我们是技术开源项目，我想我们应该更关注技术本身。> @MiKKiYang 我个人觉得没有这个必要，修改主分支名还可能破坏已有的一些第三方链接， 事实上有大量的更有影响力的项目都继续保持了主分支的命名， 比如tensorflow。 默认以master为主分支的命名方式有非常久的历史了，本身也不是为了冒犯某些群体，我们是技术开源项目，我想我们应该更关注技术本身。

好的好的、了解了、tks~ @weiwee",3,2022-05-25 14:03:50,2022-06-05 16:31:45,2022-06-05 16:31:45
https://github.com/FederatedAI/FATE/issues/4001,[],关于1.8.0版本secure_information_retrieval组件的使用问题,"关于1.8.0版本secure_information_retrieval组件的使用问题**Describe the bug**
1.8.0 版本secure_information_retrieval组件使用，dsl v2方式，example中案例包含Reader，DataTransform，SecureInformationRetrieval三个组件，可以执行成功
1.5.0版本中secure_information_retrieval组件使用，dsl v1方式，example中案例包含DataIO，SecureInformationRetrieval，两个组件，可以执行成功
问题在于，1.8.0 版本secure_information_retrieval组件使用，dsl v2方式，配置Reader和SecureInformationRetrieval失败报错，请问1.8.0版本后，如果要用到secure_information_retrieval组件，就必须要配置DataTransform组件吗


**To Reproduce**
执行命令：flow job submit -c test_secure_information_retrieval_conf.json -d test_secure_information_retrieval_dsl.json
1.8.0 secure_information_retrieval dsl配置文件
test_secure_information_retrieval_conf.json
```
{
    ""dsl_version"": 2,
    ""initiator"": {
        ""role"": ""guest"",
        ""party_id"": 10000
    },
    ""role"": {
        ""host"": [
            9999
        ],
        ""guest"": [
            10000
        ]
    },
    ""component_parameters"": {
        ""role"": {
            ""guest"": {
                ""0"": {
                    ""reader_0"": {
                        ""table"": {
                            ""name"": ""breast_hetero_guest"",
                            ""namespace"": ""experiment""
                        }
                    }
                }
            },
            ""host"": {
                ""0"": {
                    ""reader_0"": {
                        ""table"": {
                            ""name"": ""breast_hetero_host"",
                            ""namespace"": ""experiment""
                        }
                    }
                }
            }
        },
        ""common"": {
            ""secure_information_retrieval_0"": {
                ""security_level"": 0.5,
                ""oblivious_transfer_protocol"": ""OT_Hauck"",
                ""commutative_encryption"": ""CommutativeEncryptionPohligHellman"",
                ""non_committing_encryption"": ""aes"",
                ""dh_params"": {
                    ""key_length"": 1024
                },
                ""raw_retrieval"": false,
                ""target_cols"": [
                    ""x10"",
                    ""x11""
                ]
            }
        }
    }
}
```

test_secure_information_retrieval_dsl.json
```
{
    ""components"": {
        ""reader_0"": {
            ""module"": ""Reader"",
            ""output"": {
                ""data"": [
                    ""data""
                ]
            }
        },
        ""secure_information_retrieval_0"": {
            ""module"": ""SecureInformationRetrieval"",
            ""input"": {
                ""data"": {
                    ""data"": [
                        ""reader_0.data""
                    ]
                }
            },
            ""output"": {
                ""data"": [
                    ""data""
                ],
                ""model"": [
                    ""model""
                ]
            }
        }
    }
}
```
**报错日志**
host方
```
  File ""/data/projects/fate/fateflow/python/fate_flow/worker/task_executor.py"", line 195, in _run_
    cpn_output = run_object.run(cpn_input)
  File ""/data/projects/fate/fate/python/federatedml/model_base.py"", line 236, in run
    self._run(cpn_input=cpn_input)
  File ""/data/projects/fate/fate/python/federatedml/model_base.py"", line 311, in _run
    this_data_output = func(*real_param)
  File ""/data/projects/fate/fate/python/federatedml/secure_information_retrieval/secure_information_retrieval_host.py"", line 98, in fit
    self.need_label)      # List[(Ei, val)]
  File ""/data/projects/fate/fate/python/federatedml/secure_information_retrieval/secure_information_retrieval_host.py"", line 229, in _restore_value
lambda v, u:
  File ""/data/projects/fate/fate/python/fate_arch/common/profile.py"", line 318, in _fn
    rtn = func(*args, **kwargs)
  File ""/data/projects/fate/fate/python/fate_arch/computing/eggroll/_table.py"", line 117, in join
    return Table(self._rp.join(other._rp, func=func))
  File ""/data/projects/fate/eggroll/python/eggroll/core/aspects.py"", line 30, in wrapper
    result = func(*args, **kwargs)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/roll_pair.py"", line 1114, in join
    task_results = self._run_job(job=job)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/roll_pair.py"", line 475, in _run_job
    results.append(future.result())
  File ""/opt/rh/rh-python36/root/usr/lib64/python3.6/concurrent/futures/_base.py"", line 432, in result
    return self.__get_result()
  File ""/opt/rh/rh-python36/root/usr/lib64/python3.6/concurrent/futures/_base.py"", line 384, in __get_result
    raise self._exception
  File ""/data/projects/fate/eggroll/python/eggroll/core/datastructure/threadpool.py"", line 51, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/data/projects/fate/eggroll/python/eggroll/core/client.py"", line 97, in sync_send
    raise CommandCallError(command_uri, endpoint, e)
eggroll.core.client.CommandCallError: ('Failed to call command: CommandURI(_uri=v1/egg-pair/runTask) to endpoint: nodemanager:46412, caused by: ', <_Rendezvous of RPC that terminated with:
	status = StatusCode.UNKNOWN
	details = ""Exception calling application: 
==== detail start, at 20220525.032430.311 ====
Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 158, in _run_binary
    output_writebatch)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 418, in merge_join_wrapper
    right_value_serdes.deserialize(v_right_bytes))))
  File ""/data/projects/fate/fate/python/federatedml/secure_information_retrieval/secure_information_retrieval_host.py"", line 232, in <lambda>
    need_label))
  File ""/data/projects/fate/fate/python/federatedml/secure_information_retrieval/secure_information_retrieval_host.py"", line 169, in extract_value
    features = [instance.features[i] for i in target_indexes]
  File ""/data/projects/fate/fate/python/federatedml/secure_information_retrieval/secure_information_retrieval_host.py"", line 169, in <listcomp>
    features = [instance.features[i] for i in target_indexes]
AttributeError: 'str' object has no attribute 'features'
```







> **Describe the bug** 1.8.0 版本secure_information_retrieval组件使用，dsl v2方式，example中案例包含Reader，DataTransform，SecureInformationRetrieval三个组件，可以执行成功 1.5.0版本中secure_information_retrieval组件使用，dsl v1方式，example中案例包含DataIO，SecureInformationRetrieval，两个组件，可以执行成功 问题在于，1.8.0 版本secure_information_retrieval组件使用，dsl v2方式，配置Reader和SecureInformationRetrieval失败报错，请问1.8.0版本后，如果要用到secure_information_retrieval组件，就必须要配置DataTransform组件吗
> 
> **To Reproduce** 执行命令：flow job submit -c test_secure_information_retrieval_conf.json -d test_secure_information_retrieval_dsl.json 1.8.0 secure_information_retrieval dsl配置文件 test_secure_information_retrieval_conf.json
> 
> ```
> {
>     ""dsl_version"": 2,
>     ""initiator"": {
>         ""role"": ""guest"",
>         ""party_id"": 10000
>     },
>     ""role"": {
>         ""host"": [
>             9999
>         ],
>         ""guest"": [
>             10000
>         ]
>     },
>     ""component_parameters"": {
>         ""role"": {
>             ""guest"": {
>                 ""0"": {
>                     ""reader_0"": {
>                         ""table"": {
>                             ""name"": ""breast_hetero_guest"",
>                             ""namespace"": ""experiment""
>                         }
>                     }
>                 }
>             },
>             ""host"": {
>                 ""0"": {
>                     ""reader_0"": {
>                         ""table"": {
>                             ""name"": ""breast_hetero_host"",
>                             ""namespace"": ""experiment""
>                         }
>                     }
>                 }
>             }
>         },
>         ""common"": {
>             ""secure_information_retrieval_0"": {
>                 ""security_level"": 0.5,
>                 ""oblivious_transfer_protocol"": ""OT_Hauck"",
>                 ""commutative_encryption"": ""CommutativeEncryptionPohligHellman"",
>                 ""non_committing_encryption"": ""aes"",
>                 ""dh_params"": {
>                     ""key_length"": 1024
>                 },
>                 ""raw_retrieval"": false,
>                 ""target_cols"": [
>                     ""x10"",
>                     ""x11""
>                 ]
>             }
>         }
>     }
> }
> ```
> 
> test_secure_information_retrieval_dsl.json
> 
> ```
> {
>     ""components"": {
>         ""reader_0"": {
>             ""module"": ""Reader"",
>             ""output"": {
>                 ""data"": [
>                     ""data""
>                 ]
>             }
>         },
>         ""secure_information_retrieval_0"": {
>             ""module"": ""SecureInformationRetrieval"",
>             ""input"": {
>                 ""data"": {
>                     ""data"": [
>                         ""reader_0.data""
>                     ]
>                 }
>             },
>             ""output"": {
>                 ""data"": [
>                     ""data""
>                 ],
>                 ""model"": [
>                     ""model""
>                 ]
>             }
>         }
>     }
> }
> ```
> 
> **报错日志** host方
> 
> ```
>   File ""/data/projects/fate/fateflow/python/fate_flow/worker/task_executor.py"", line 195, in _run_
>     cpn_output = run_object.run(cpn_input)
>   File ""/data/projects/fate/fate/python/federatedml/model_base.py"", line 236, in run
>     self._run(cpn_input=cpn_input)
>   File ""/data/projects/fate/fate/python/federatedml/model_base.py"", line 311, in _run
>     this_data_output = func(*real_param)
>   File ""/data/projects/fate/fate/python/federatedml/secure_information_retrieval/secure_information_retrieval_host.py"", line 98, in fit
>     self.need_label)      # List[(Ei, val)]
>   File ""/data/projects/fate/fate/python/federatedml/secure_information_retrieval/secure_information_retrieval_host.py"", line 229, in _restore_value
> lambda v, u:
>   File ""/data/projects/fate/fate/python/fate_arch/common/profile.py"", line 318, in _fn
>     rtn = func(*args, **kwargs)
>   File ""/data/projects/fate/fate/python/fate_arch/computing/eggroll/_table.py"", line 117, in join
>     return Table(self._rp.join(other._rp, func=func))
>   File ""/data/projects/fate/eggroll/python/eggroll/core/aspects.py"", line 30, in wrapper
>     result = func(*args, **kwargs)
>   File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/roll_pair.py"", line 1114, in join
>     task_results = self._run_job(job=job)
>   File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/roll_pair.py"", line 475, in _run_job
>     results.append(future.result())
>   File ""/opt/rh/rh-python36/root/usr/lib64/python3.6/concurrent/futures/_base.py"", line 432, in result
>     return self.__get_result()
>   File ""/opt/rh/rh-python36/root/usr/lib64/python3.6/concurrent/futures/_base.py"", line 384, in __get_result
>     raise self._exception
>   File ""/data/projects/fate/eggroll/python/eggroll/core/datastructure/threadpool.py"", line 51, in run
>     result = self.fn(*self.args, **self.kwargs)
>   File ""/data/projects/fate/eggroll/python/eggroll/core/client.py"", line 97, in sync_send
>     raise CommandCallError(command_uri, endpoint, e)
> eggroll.core.client.CommandCallError: ('Failed to call command: CommandURI(_uri=v1/egg-pair/runTask) to endpoint: nodemanager:46412, caused by: ', <_Rendezvous of RPC that terminated with:
> 	status = StatusCode.UNKNOWN
> 	details = ""Exception calling application: 
> ==== detail start, at 20220525.032430.311 ====
> Traceback (most recent call last):
>   File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 158, in _run_binary
>     output_writebatch)
>   File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 418, in merge_join_wrapper
>     right_value_serdes.deserialize(v_right_bytes))))
>   File ""/data/projects/fate/fate/python/federatedml/secure_information_retrieval/secure_information_retrieval_host.py"", line 232, in <lambda>
>     need_label))
>   File ""/data/projects/fate/fate/python/federatedml/secure_information_retrieval/secure_information_retrieval_host.py"", line 169, in extract_value
>     features = [instance.features[i] for i in target_indexes]
>   File ""/data/projects/fate/fate/python/federatedml/secure_information_retrieval/secure_information_retrieval_host.py"", line 169, in <listcomp>
>     features = [instance.features[i] for i in target_indexes]
> AttributeError: 'str' object has no attribute 'features'
> ```

需要的，DataTransform其实是DataIO的升级版组件，已不再维护DataIO，用Reader->DataTransform(DataIO)->SIR都可以> 

了解了，谢谢",2,2022-05-25 06:23:23,2022-05-26 01:00:37,2022-05-26 01:00:37
https://github.com/FederatedAI/FATE/issues/4000,[],"Failed to run demo, SecretShare MPC Protocol(SPDZ) demo.","Failed to run demo, SecretShare MPC Protocol(SPDZ) demo.Session.init_federation property 'service_conf' is required in version 1.7.2.
![1653446938(1)](https://user-images.githubusercontent.com/85269941/170169391-98423588-cca5-44cc-8572-026e4d096b0e.jpg)
I don't know what to fill in this value.
I tried two ways:
1.Fill in the IP and port of the other party.
service_conf={""host"":""other ip"",""port"" :9370}
2.Fill in my own IP and port.
service_conf={""host"":""my ip"",""port"" :9370}

Both failed!

> Session.init_federation property 'service_conf' is required in version 1.7.2. ![1653446938(1)](https://user-images.githubusercontent.com/85269941/170169391-98423588-cca5-44cc-8572-026e4d096b0e.jpg) I don't know what to fill in this value. I tried two ways: 1.Fill in the IP and port of the other party. service_conf={""host"":""other ip"",""port"" :9370} 2.Fill in my own IP and port. service_conf={""host"":""my ip"",""port"" :9370}
> 
> Both failed!

Please give the error message of the second type. After waiting for a few minutes, the program is wrong.
The code:
![1653468030(1)](https://user-images.githubusercontent.com/85269941/170222039-35e64207-5567-4243-a287-39e0064a459b.jpg)
![image](https://user-images.githubusercontent.com/85269941/170222256-74ad96ce-c527-4fd9-8cb7-a51a58a8d70a.png)

error message:
![image](https://user-images.githubusercontent.com/85269941/170224401-be79421e-cb01-47fe-9a3c-c1d205f1c566.png)
![1653469313(1)](https://user-images.githubusercontent.com/85269941/170224864-b3029cf3-e0a3-4479-8b28-c4cb5fbcdebe.jpg)
> After waiting for a few minutes, the program is wrong. The code: ![1653468030(1)](https://user-images.githubusercontent.com/85269941/170222039-35e64207-5567-4243-a287-39e0064a459b.jpg) ![image](https://user-images.githubusercontent.com/85269941/170222256-74ad96ce-c527-4fd9-8cb7-a51a58a8d70a.png)
> 
> error message: ![image](https://user-images.githubusercontent.com/85269941/170224401-be79421e-cb01-47fe-9a3c-c1d205f1c566.png) ![1653469313(1)](https://user-images.githubusercontent.com/85269941/170224864-b3029cf3-e0a3-4479-8b28-c4cb5fbcdebe.jpg)

modify the code in both guest and host site:
a. choose a common federation id (this should be same in both site)
b. 
host_site: 
    session_id = ""_"".join([federation_id, ""host"", str(host_party_id)])
    in your image, host_party_id=10001
guest_site:
    session_id = ""_"".join([federation_id, ""guest"", str(guest_party_id)])
    in your image, guest_party_id=10000

then both site should init federation and computing engine like the following:
s.init_computing(session_id)
s.init_federation(federation_id,
                             runtime_conf=..., service_conf=...)


Please have a try and tell us if it works or not.
Hi
I tried, but it was the same error.
Modified code:
![image](https://user-images.githubusercontent.com/85269941/170453909-fc10460c-2e35-418f-afde-d9d6f7699831.png)
![1653555198(1)](https://user-images.githubusercontent.com/85269941/170454620-3bd911a3-532c-4f0a-974a-2fe32fe66233.jpg)

The same error message:
![image](https://user-images.githubusercontent.com/85269941/170454741-472ca243-b604-4e34-8928-8072b620a4db.png)

guest: 
<img width=""835"" alt=""image"" src=""https://user-images.githubusercontent.com/13565267/170920312-d0144fde-f9cc-4b4a-b234-52f0915b4b67.png"">

host:
<img width=""1040"" alt=""image"" src=""https://user-images.githubusercontent.com/13565267/170920368-cc9b9e33-dd59-441f-a25d-28744ac2bb9d.png"">

I use the above config and run spdz sucessfully
Thank you very much. The problem has been solved. 
Can I submit the demo I ran successfully and update the MD document？> Thank you very much. The problem has been solved. Can I submit the demo I ran successfully and update the MD document？

Sure~  It will be very great and helpful.",7,2022-05-25 03:32:46,2022-06-17 03:07:37,2022-06-17 03:07:37
https://github.com/FederatedAI/FATE/issues/3996,[],Fate1.5 运行项目 pipeline-upload.py 脚本，出错,"Fate1.5 运行项目 pipeline-upload.py 脚本，出错 UPLOADING:||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.00%
2022-05-23 16:30:02.024 | ERROR    | __main__:main:54 - An error has been caught in function 'main', process 'MainProcess' (15176), thread 'MainThread' (10680):
Traceback (most recent call last):

  File ""G:\ProgramData\Miniconda3\envs\FL\lib\site-packages\pipeline\utils\invoker\job_submitter.py"", line 105, in upload_data
    raise ValueError

ValueError


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""F:/study/联邦学习/源码/FATE/examples/pipeline/upload\pipeline-upload.py"", line 58, in <module>
    main()
    └ <function main at 0x0000023A4E8A2F28>

> File ""F:/study/联邦学习/源码/FATE/examples/pipeline/upload\pipeline-upload.py"", line 54, in main
    pipeline_upload.upload(drop=1)
    │               └ <function PipeLine.upload at 0x0000023A6128F1E0>
    └ <pipeline.backend.pipeline.PipeLine object at 0x0000023A6127B940>

  File ""G:\ProgramData\Miniconda3\envs\FL\lib\site-packages\pipeline\backend\pipeline.py"", line 508, in upload
    self._train_job_id, detail_info = self._job_invoker.upload_data(upload_conf, int(drop))
    │    │                            │    │            │           │                └ 1
    │    │                            │    │            │           └ {'file': 'F:/study/FL/data/breast_hetero_guest.csv', 'table_name': 'breast_hetero_guest', 'namespace': 'experiment', 'head': ...
    │    │                            │    │            └ <function JobInvoker.upload_data at 0x0000023A612809D8>
    │    │                            │    └ <pipeline.utils.invoker.job_submitter.JobInvoker object at 0x0000023A6127B898>
    │    │                            └ <pipeline.backend.pipeline.PipeLine object at 0x0000023A6127B940>
    │    └ None
    └ <pipeline.backend.pipeline.PipeLine object at 0x0000023A6127B940>

  File ""G:\ProgramData\Miniconda3\envs\FL\lib\site-packages\pipeline\utils\invoker\job_submitter.py"", line 113, in upload_data
    raise ValueError(""job submit failed, err msg: {}"".format(result))
                                                             └ {'retcode': 100, 'retmsg': 'required parameters are missing: work_mode,namespace,table_name,head,partition;'}

ValueError: job submit failed, err msg: {'retcode': 100, 'retmsg': 'required parameters are missing: work_mode,namespace,table_name,head,partition;'}
Traceback (most recent call last):
  File ""G:\ProgramData\Miniconda3\envs\FL\lib\site-packages\pipeline\utils\invoker\job_submitter.py"", line 105, in upload_data
    raise ValueError
ValueError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""F:/study/联邦学习/源码/FATE/examples/pipeline/upload/pipeline-upload.py"", line 58, in <module>
    main()
  File ""F:/study/联邦学习/源码/FATE/examples/pipeline/upload/pipeline-upload.py"", line 54, in main
    pipeline_upload.upload(drop=1)
  File ""G:\ProgramData\Miniconda3\envs\FL\lib\site-packages\loguru\_logger.py"", line 1220, in catch_wrapper
    return function(*args, **kwargs)
  File ""G:\ProgramData\Miniconda3\envs\FL\lib\site-packages\pipeline\backend\pipeline.py"", line 508, in upload
    self._train_job_id, detail_info = self._job_invoker.upload_data(upload_conf, int(drop))
  File ""G:\ProgramData\Miniconda3\envs\FL\lib\site-packages\pipeline\utils\invoker\job_submitter.py"", line 113, in upload_data
    raise ValueError(""job submit failed, err msg: {}"".format(result))
ValueError: job submit failed, err msg: {'retcode': 100, 'retmsg': 'required parameters are missing: work_mode,namespace,table_name,head,partition;'}

Process finished with exit code 1
可以附上Pipeline的代码，这些参数看1.5的examples都是有的，是否使用了更高版本的examples但fate版本是旧的呢？def main(config=""../../config.yaml"", namespace=""""):
    # obtain config
    if isinstance(config, str):
        config = load_job_config(config)
    parties = config.parties
    guest = parties.guest[0]
    data_base = config.data_base_dir

    # partition for data storage
    partition = 4

    # table name and namespace, used in FATE job configuration
    dense_data = {""name"": ""breast_hetero_guest"", ""namespace"": f""experiment{namespace}""}
    tag_data = {""name"": ""tag_value_1"", ""namespace"": f""experiment{namespace}""}

    pipeline_upload = PipeLine().set_initiator(role=""guest"", party_id=guest).set_roles(guest=guest)

    # add upload data info
    # path to csv file(s) to be uploaded
    pipeline_upload.add_upload_data(file=os.path.join(data_base, ""breast_hetero_guest.csv""),
                                    table_name=dense_data[""name""],             # table name
                                    namespace=dense_data[""namespace""],         # namespace
                                    head=1, partition=partition,               # data info
                                    id_delimiter="","")

    pipeline_upload.add_upload_data(file=os.path.join(data_base, ""tag_value_1000_140.csv""),
                                    table_name=tag_data[""name""],
                                    namespace=tag_data[""namespace""],
                                    head=0, partition=partition,
                                    id_delimiter="","")
    # upload both data
    pipeline_upload.upload(drop=1)


if __name__ == ""__main__"":
    main()docker最新是1.8？> def main(config=""../../config.yaml"", namespace=""""): # obtain config if isinstance(config, str): config = load_job_config(config) parties = config.parties guest = parties.guest[0] data_base = config.data_base_dir
> 
> ```
> # partition for data storage
> partition = 4
> 
> # table name and namespace, used in FATE job configuration
> dense_data = {""name"": ""breast_hetero_guest"", ""namespace"": f""experiment{namespace}""}
> tag_data = {""name"": ""tag_value_1"", ""namespace"": f""experiment{namespace}""}
> 
> pipeline_upload = PipeLine().set_initiator(role=""guest"", party_id=guest).set_roles(guest=guest)
> 
> # add upload data info
> # path to csv file(s) to be uploaded
> pipeline_upload.add_upload_data(file=os.path.join(data_base, ""breast_hetero_guest.csv""),
>                                 table_name=dense_data[""name""],             # table name
>                                 namespace=dense_data[""namespace""],         # namespace
>                                 head=1, partition=partition,               # data info
>                                 id_delimiter="","")
> 
> pipeline_upload.add_upload_data(file=os.path.join(data_base, ""tag_value_1000_140.csv""),
>                                 table_name=tag_data[""name""],
>                                 namespace=tag_data[""namespace""],
>                                 head=0, partition=partition,
>                                 id_delimiter="","")
> # upload both data
> pipeline_upload.upload(drop=1)
> ```
> 
> if **name** == ""**main**"": main()

这个examples不是1.5.x的，可以在tags下找对应版本的examples使用",4,2022-05-23 08:30:20,2022-05-24 04:23:06,2022-05-24 04:23:06
https://github.com/FederatedAI/FATE/issues/3984,[],Security Risk for RSA Intersection Cardinality: Intersection items exposed. ,"Security Risk for RSA Intersection Cardinality: Intersection items exposed. **Describe the bug**
Specific Intersection items are exposed on the Guest Side for Intersection Cardinality, consider the definition of PSI-CA protocol, this is not considered secure.
  

> Private [set intersection] cardinality (PSI-CA) allows two parties, the sender and receiver, to compute the cardinality of the intersection, without revealing anything more to the other party.

Check the 310th line of rsa_intersect_guest.py:
`intersect_ids_list = [ids.map(lambda k, v: (v[0], 1)) for ids in intersect_ids_list]   `
in which v[0] is actually the private id and I can print out these private ids in log。

I checked the run_cardinality function. It seems that oher than the Bloom Filters to filter the private intersection. Other parts basically stays the same as original PSI.  

Please verify this security risk or is it a design by intention.
Hi, 
In current version of FATE, cardinality calculation is a process of estimation with BloomFilter, and so the resulted cardinality count is not exact, neither are the `intersect_ids` in the intermediate process. On the other hand, ’cardinality_only’ mode is designed to allow Host(often data source with large amount of ids) and Guest quickly estimate their intersection count without revealing Guest’s ids to Host. This situation is most common in industrial applications, where compared to Host, whose database covers huge amount of users, Guest holds a limited but indicative number of ids. In such cases, Guest’s id collection need to be held unknown to Host, and so security of intersection id set against Host takes priority.

As for the mentioned scenario where neither participants can gain more information than cardinality, we arranged plans to implement DH-based PSI-CA in the upcoming 1.9 version, you may follow this ISSUE #3856 for any updates.Thanks for the clarification. @nemirorox",2,2022-05-19 03:08:15,2022-05-23 03:04:26,2022-05-21 07:29:16
https://github.com/FederatedAI/FATE/issues/3981,[],刚提交任务报错，dataio_0未成功……,"刚提交任务报错，dataio_0未成功……![59a53c2e795e0a76354387e1efe5dea](https://user-images.githubusercontent.com/105529434/168832470-264743b4-ae41-4031-b91c-5621834d589c.png)
![ebfe9341a39892e6fa3b45352b45ea2](https://user-images.githubusercontent.com/105529434/168832523-08d2b353-b770-47a0-91d4-5227421acf22.png)
![image](https://user-images.githubusercontent.com/105529434/168936420-c8d79f0d-860a-46bc-8d06-9c2c071ead12.png)
确认数据每一行的列数量和header是否一致，报错是使用header中y所在的列去解析每一行的标签的时候越界了谢谢回复，请问如何确定数据每行的列数量以及header中的列数量呢……我这里的数据集是breast.csv,这里的列数量是指excel打开之后每行有32列的32么


------------------&nbsp;原始邮件&nbsp;------------------
发件人:                                                                                                                        ""FederatedAI/FATE""                                                                                    ***@***.***&gt;;
发送时间:&nbsp;2022年5月18日(星期三) 中午12:50
***@***.***&gt;;
***@***.******@***.***&gt;;
主题:&nbsp;Re: [FederatedAI/FATE] 刚提交任务报错，dataio_0未成功…… (Issue #3981)





 
确认数据每一行的列数量和header是否一致，报错是使用header中y所在的列去解析每一行的标签的时候越界了
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you authored the thread.Message ID: ***@***.***&gt;> 确认数据每一行的列数量和header是否一致，报错是使用header中y所在的列去解析每一行的标签的时候越界了

谢谢回复，请问应该如何确定数据每行的列数量以及header中列数量呢……我这里的数据集是breast.csv，这里的列数量是指excel打开之后每行有32列的32么重新上传数据集，配置数据上传文件时，重新命名table_name，namespace（注意不能与之前名字重复）。再次提交任务就🆗了，我之前多次对同一个table_name，namespace上传数据，可能是这个原因导致这个错误。",5,2022-05-17 14:16:08,2022-05-20 08:06:48,2022-05-20 08:06:48
https://github.com/FederatedAI/FATE/issues/3977,[],fateboard日志显示正常，但提示system error，dashboard无反应,"fateboard日志显示正常，但提示system error，dashboard无反应![260cffc6fc45b9131b4e3256965990f](https://user-images.githubusercontent.com/105529434/168771783-53d33b6a-0d23-466d-98d6-2264617a3fec.png)
确认fate_flow服务是否还在，另一种可能是是系统在运行比较大的任务，吃满系统资源进入fateboard容器找到error日志发现是配置问题，修改之后就可以可以正常显示了。
![`G3}M}{6)I%@X8`LUP~UFHC](https://user-images.githubusercontent.com/105529434/168945510-43662de8-3728-426f-af3c-a06030b3ea43.png)
您好，请问一下，您是怎么解决的，我自己装了sqlite，他会报缺少表不胜感激> 不胜感激
我部署的fate单机版1.4，不需要自己装sqlite",5,2022-05-17 08:54:55,2022-07-01 02:06:16,2022-05-18 02:56:32
https://github.com/FederatedAI/FATE/issues/3974,[],Does sample-weight support SecureBoost for weight training in Fate1.7.2?,"Does sample-weight support SecureBoost for weight training in Fate1.7.2?Does sample-weight support SecureBoost for weight training in Fate1.7.2? I used sample-weight in SecureBoost to handle sample imbalance. Why doesn't it work?Thanks for your feedback, I ll check this problem and reply you in these days.Hi~ I saw your msg in the wechat-group, please notice that Homo-SecureBoost does not support sample weights",2,2022-05-14 08:00:14,2022-05-23 09:27:39,2022-05-23 09:27:39
https://github.com/FederatedAI/FATE/issues/3962,[],fateboard 1.8无法登录,"fateboard 1.8无法登录# New Algorithm
更新fate V1.8之后无法登录fateboard

## Short Description
更新fate1.8以后，无法登录fateboard
1. 默认不设置情况下，无法使用admin/admin登录
2. yaml配置admin/admin（容器内部配置文件修改成功），依然无法登录

## 配置与截图
```
server.port=8080
fateflow.url=http://fateflow:9380
fateflow.http_app_key=
fateflow.http_secret_key=
spring.http.encoding.charset=UTF-8
spring.http.encoding.enabled=true
server.tomcat.uri-encoding=UTF-8
fateboard.front_end.cors=false
fateboard.front_end.url=http://localhost:8028
server.tomcat.max-threads=1000
server.tomcat.max-connections=20000
spring.servlet.multipart.max-file-size=10MB
spring.servlet.multipart.max-request-size=100MB
server.compression.enabled=true
server.compression.mime-types=application/json,application/xml,text/html,text/xml,text/plain
server.board.login.username=admin
server.board.login.password=admin
server.servlet.session.timeout=4h
server.servlet.session.cookie.max-age=4h
management.endpoints.web.exposure.exclude=*
```
![fateerror](https://user-images.githubusercontent.com/7101412/167103154-13264771-58f4-4287-aac3-d21391c1691a.png)

你修改后，重启服务了吗
先确保清除了缓存，1.8更新登录这块的代码，如果有浏览器缓存的话是无法登录的> 先确保清除了缓存，1.8更新登录这块的代码，如果有浏览器缓存的话是无法登录的

老师您好。我已经重启过了服务。同时尝试清理缓存、使用无痕模式。均无法登录。
请问还有没有别的地方需要修改的？或者有没有详细日志，我去截取提供出来，便于分析打开浏览器的控制台（一般快捷键是F12），切换到network一栏，输完账号密码点击登录时会有一个请求，点开这个请求截个图还有一个可能的问题是，容器内的时间跟系统时间不一样，在浏览器控制台输入`Date()`，在容器里输入`date`，看下两边时间是否一致> 打开浏览器的控制台（一般快捷键是F12），切换到network一栏，输完账号密码点击登录时会有一个请求，点开这个请求截个图
请求头+url
![fate_error1](https://user-images.githubusercontent.com/7101412/167332735-384b5a4e-d3dd-4130-b822-d9de3b98d8c1.png)

请求体
![fate_error2](https://user-images.githubusercontent.com/7101412/167332666-a3b7e17f-902d-4fab-8658-2cdbf2d8975e.png)
1e307e4.png)
响应体
![fate_error3](https://user-images.githubusercontent.com/7101412/167332675-0d7980f1-ca99-42ca-beac-ec857c821cf4.png)



> 还有一个可能的问题是，容器内的时间跟系统时间不一样，在浏览器控制台输入`Date()`，在容器里输入`date`，看下两边时间是否一致

确定是时间问题，修改时间后可正常运行。谢谢老师不客气，那我关掉issue了> 不客气，那我关掉issue了

老师，我还有一个问题。新版本上传数据的代码放哪了？我们有针对1.6上传数据进行二次开发，但1.8的找不到了",9,2022-05-06 09:13:35,2022-05-09 03:41:59,2022-05-09 03:02:27
https://github.com/FederatedAI/FATE/issues/3959,[],fate 集群版安装后，toy单边案例测试不通过,"fate 集群版安装后，toy单边案例测试不通过安装的是fate原生版集群。在虚拟机内存14g，集群部署完毕后，服务正常启动，fateboard启动正常，测试test toy 单边案例，一直在 wating，然后就超时。日志查看不出错误信息。有没有大佬遇到这种问题的，帮忙指点一下。是单台机器还是两个party_id？这里的waiting指的是任务已经提交上去了但job状态waiting，还是说任务提交不上去，一直卡住到超时？> 是单台机器还是两个party_id？这里的waiting指的是任务已经提交上去了但job状态waiting，还是说任务提交不上去，一直卡住到超时？

单台机器部署的单party，测试的是自带的flow test toy -gid 9999 -hid 9999 案例，任务应该是已经提交上去了，在fateboard是有这个jobid的，下边是部分job日志
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
toy test job 202205060956109945590 is waiting
check job status timeout
auto check log failed, please check /data/projects/fate/logs/toy/job_202205060956109945590_log
怀疑是资源申请的问题，总核数<4，toy里面默认使用的资源配置是2核/party，所以需要的总资源是4核。可以看下fateflow/$jobid/下面的schedule日志，确认下guest\host是不是有一方资源申请成功，一边没成功

或者跑toy的时候--task-cores 1看下能否成功（注意先停掉已有的waiting任务）> 怀疑是资源申请的问题，总核数<4，toy里面默认使用的资源配置是2核/party，所以需要的总资源是4核。可以看下fateflow/$jobid/下面的schedule日志，确认下guest\host是不是有一方资源申请成功，一边没成功
> 
> 或者跑toy的时候--task-cores 1看下能否成功（注意先停掉已有的waiting任务）
> > 怀疑是资源申请的问题，总核数<4，toy里面默认使用的资源配置是2核/party，所以需要的总资源是4核。可以看下fateflow/$jobid/下面的schedule日志，确认下guest\host是不是有一方资源申请成功，一边没成功
> > 或者跑toy的时候--task-cores 1看下能否成功（注意先停掉已有的waiting任务）

是否还有其他问题呢？ 没的话该issue将会关闭> > > 怀疑是资源申请的问题，总核数<4，toy里面默认使用的资源配置是2核/party，所以需要的总资源是4核。可以看下fateflow/$jobid/下面的schedule日志，确认下guest\host是不是有一方资源申请成功，一边没成功
> > > 或者跑toy的时候--task-cores 1看下能否成功（注意先停掉已有的waiting任务）
> 
> 是否还有其他问题呢？ 没的话该issue将会关闭

已经解决，谢谢。我现在关闭您好，我也遇到了这个问题，fate 集群版安装后，toy单边案例测试不通过，
![6a47e761e71206ab6a95823cab9a76b](https://user-images.githubusercontent.com/49178688/179195007-b2b0029d-b90a-4f0f-bd34-8417abe0708c.png)
根据提示打开日志显示
![96b89eade4df4d141139e986c0f40a9](https://user-images.githubusercontent.com/49178688/179195062-41a28724-e7b0-45e1-a159-ab7b1f74388e.png)
看schedule日志里任务也提交成功了
![b25f46a0c9c017dee6a578dd2093880](https://user-images.githubusercontent.com/49178688/179195119-d0d3cefd-3b0a-48bd-9f92-3a680ee0b630.png)


> 您好，我也遇到了这个问题，fate 集群版安装后，toy单边案例测试不通过， ![6a47e761e71206ab6a95823cab9a76b](https://user-images.githubusercontent.com/49178688/179195007-b2b0029d-b90a-4f0f-bd34-8417abe0708c.png) 根据提示打开日志显示 ![96b89eade4df4d141139e986c0f40a9](https://user-images.githubusercontent.com/49178688/179195062-41a28724-e7b0-45e1-a159-ab7b1f74388e.png) 看schedule日志里任务也提交成功了 ![b25f46a0c9c017dee6a578dd2093880](https://user-images.githubusercontent.com/49178688/179195119-d0d3cefd-3b0a-48bd-9f92-3a680ee0b630.png)

可能是资源问题，我把虚拟机核数和内存扩大之后问题就解决了，参考上边其他人的回复",8,2022-05-05 09:46:36,2022-07-15 13:20:37,2022-05-17 09:36:04
https://github.com/FederatedAI/FATE/issues/3955,[],单机版的1.7.0安装包的网址改变了吗,"单机版的1.7.0安装包的网址改变了吗**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
Please refer to fate-v1.7.0' s [standalone deployment document](https://github.com/FederatedAI/FATE/tree/v1.7.0/deploy/standalone-deploy) wget https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/fate/${version}/release/standalone_fate_docker_image_${version}_release.tar.gz
--2022-05-01 01:02:11--  https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/fate/1.7.0/release/standalone_fate_docker_image_1.7.0_release.tar.gz
Resolving webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com (webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com)... 112.49.57.176, 112.49.57.186, 112.49.57.185, ...
Connecting to webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com (webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com)|112.49.57.176|:443... connected.
HTTP request sent, awaiting response... 404 Not Found
2022-05-01 01:02:11 ERROR 404: Not Found.
用windows打开网址也是错误的Please  read the document carefully, the link is https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/fate/1.7.0/release/standalone_fate_docker_image_1.7.0_release.tar, without "".gz"" in the endthanks",5,2022-05-01 03:53:22,2022-05-05 03:25:42,2022-05-05 03:25:42
https://github.com/FederatedAI/FATE/issues/3953,['bug'],exclusive_data_type 貌似没有生效,"exclusive_data_type 貌似没有生效FATE 1.7.0版本，定义Transform指定了exclusive_data_type ，如下所示：
data_transform_0= DataTransform(name=""data_transform_0"")
data_transform_0.get_party_instance(role='guest', party_id=guest)\
        .component_param(with_label=True, label_name=""y"", output_format=""dense"", exclusive_data_type={'CODE_GENDER':'str'}) 
其中，CODE_GENDER列为包含M和F的字符串。
执行时报错，错误信息为：
### ValueError: could not convert string to float: 'M'
991
992
During handling of the above exception, another exception occurred:
993
994
Traceback (most recent call last):
995
  File ""./fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
996
    return func(*args, **kw)
997
  File ""./fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 414, in run_task
998
    self._run_binary(merge_join_wrapper, task)
999
  File ""./fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 161, in _run_binary
1000
    raise EnvironmentError(""exec task:{} error"".format(task), e)
1001
OSError: [Errno exec task:<ErTask(id=202204291747444663880_data_transform_0_1_guest_211-py-job-20220429.174952.276598_join-task-0, name=join, inputs=[<ErPartition(id=0, store_locator=<ErStoreLocator(id=0, store_type=IN_MEMORY, namespace=202204291747444663880_data_transform_0_1_guest_211, name=783a1f68c7a111ec8dfa5254004cb936_20220429.174950.283606.1, path=, total_partitions=4, partitioner=, serdes=) at 0x7f7e322e7828>, processor=<ErProcessor(id=3561, server_node_id=2, name=, processor_type=egg_pair, status=RUNNING, command_endpoint=<ErEndpoint(host=xxx.xxx, port=34053) at 0x7f7e322e7860>, transfer_endpoint=<ErEndpoint(host=xxx.xxx, port=35859) at 0x7f7e322e7ac8>, pid=7690, options=[{}], tag=) at 0x7f7e322e7b00>, rank_in_node=-1) at 0x7f7e322e7780>, <ErPartition(id=0, store_locator=<ErStoreLocator(id=0, store_type=IN_MEMORY, namespace=202204291747444663880_data_transform_0_1_guest_211, name=783a1f68c7a111ec8dfa5254004cb936_20220429.174952.039342.2, path=, total_partitions=4, partitioner=, serdes=) at 0x7f7e322e7940>, processor=<ErProcessor(id=3561, server_node_id=2, name=, processor_type=egg_pair, status=RUNNING, command_endpoint=<ErEndpoint(host=xxx.xxx, port=34053) at 0x7f7e322e7978>, transfer_endpoint=<ErEndpoint(host=xxx.xxx, port=35859) at 0x7f7e322e7198>, pid=7690, options=[{}], tag=) at 0x7f7e322e7400>, rank_in_node=-1) at 0x7f7e322e73c8>], outputs=[<ErPartition(id=0, store_locator=<ErStoreLocator(id=0, store_type=IN_MEMORY, namespace=202204291747444663880_data_transform_0_1_guest_211, name=783a1f68c7a111ec8dfa5254004cb936_20220429.174952.276732.1, path=, total_partitions=4, partitioner=, serdes=) at 0x7f7e322e7a58>, processor=<ErProcessor(id=3561, server_node_id=2, name=, processor_type=egg_pair, status=RUNNING, command_endpoint=<ErEndpoint(host=xxx.xxx, port=34053) at 0x7f7e322e7a90>, transfer_endpoint=<ErEndpoint(host=xxx.xxx, port=35859) at 0x7f7e322e7cf8>, pid=7690, options=[{}], tag=) at 0x7f7e322e7fd0>, rank_in_node=-1) at 0x7f7e322e79b0>], job=<ErJob(id=202204291747444663880_data_transform_0_1_guest_211-py-job-20220429.174952.276598_join, name=join, inputs=[<ErStore(store_locator=<ErStoreLocator(id=0, store_type=IN_MEMORY, namespace=202204291747444663880_data_transform_0_1_guest_211, name=783a1f68c7a111ec8dfa5254004cb936_20220429.174950.283606.1, path=, total_partitions=4, partitioner=, serdes=) at 0x7f7e322e7748>, partitions=[***, len=4], options=[{}]) at 0x7f7e322e7240>, <ErStore(store_locator=<ErStoreLocator(id=0, store_type=IN_MEMORY, namespace=202204291747444663880_data_transform_0_1_guest_211, name=783a1f68c7a111ec8dfa5254004cb936_20220429.174952.039342.2, path=, total_partitions=4, partitioner=, serdes=) at 0x7f7e322e7710>, partitions=[***, len=4], options=[{}]) at 0x7f7e1008cb00>], outputs=[<ErStore(store_locator=<ErStoreLocator(id=0, store_type=IN_MEMORY, namespace=202204291747444663880_data_transform_0_1_guest_211, name=783a1f68c7a111ec8dfa5254004cb936_20220429.174952.276732.1, path=, total_partitions=4, partitioner=, serdes=) at 0x7f7e1008cef0>, partitions=[***, len=4], options=[{}]) at 0x7f7e1008ce10>], functors=[1], options={}) at 0x7f7e322e7a20>) at 0x7f7e322e7d30> error] could not convert string to float: 'M'
1033
1034
During handling of the above exception, another exception occurred:
1035
1036
Traceback (most recent call last):
1037
  File ""./fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
1038
    return func(*args, **kw)
1039
  File ""./fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 414, in run_task
1040
    self._run_binary(merge_join_wrapper, task)
1041
  File ""./fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 161, in _run_binary
1042
    raise EnvironmentError(""exec task:{} error"".format(task), e)
Please change {'CODE_GENDER':'str'} to {'code_gender':'str'}, and test again. This may be a bug because column names are changing to lowercases, but names in exclusive_data_type  are not. Ok, tks!",2,2022-04-29 09:51:49,2022-05-05 02:01:43,2022-05-05 02:01:42
https://github.com/FederatedAI/FATE/issues/3952,[],Question about Hetero-LR on guest side sum operation,"Question about Hetero-LR on guest side sum operationWhen I looked into the code, I felt confused about the following question.
![image](https://user-images.githubusercontent.com/39375136/165746145-750857ec-6731-4eb5-b87b-ee70255d7fbf.png)
Fore_gradient just added the host_forward. But host_forward was encrypted and fore_gradient is plaintext, it was different from the paper. I wondered if I had a wrong understanding.

Thanks a lot if u can give some suggestions.The code was substracted from class Guest in /federatedml/optim/gradient/hetero_linear_model_gradient.py .I remember that they just overwrite the function of  `+`  operation between Paillier-Encrypted number and plaintext.",2,2022-04-28 11:55:51,2022-05-10 05:50:18,2022-05-10 05:50:18
https://github.com/FederatedAI/FATE/issues/3944,['bug'],FATE Standalone1.8  Unit tests error,"FATE Standalone1.8  Unit tests error **Describe the bug**
FATE Standalone1.8,   Unit tests error  ( fate_test unittest federatedml --yes) 
reference link is  https://github.com/FederatedAI/FATE/tree/master/deploy/standalone-deploy

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

Please execute ""source bin/init_env.sh"" first.",1,2022-04-26 15:48:48,2022-05-24 04:23:37,2022-05-24 04:23:37
https://github.com/FederatedAI/FATE/issues/3940,[],update job status does not take effect,"update job status does not take effect执行**两方**的纵向逻辑回归，在Schedule Log里面报如下错误，但是仍然能够成功执行。不过，执行**三方**的纵向逻辑回归时就卡在HeteroLR: hetero_lr_0处失败，没有其他报错日志，也是只有如下的调度日志报错。
使用的时Ansible部署的Exchange模式下的FATE1.7.0,算法代码也是很简单的reader、transform、intersection和heteroLR，请问有遇到过这个问题的么？


1
[ERROR] [2022-04-26 10:38:17,694] [202204261038075892820] [2313:140654841952000] - [federated_scheduler.task_command] [line:246]: failed to execute federated task reader_0 command(status/running) detail: 
2
{'guest': {'211': {'retcode': 10, 'retmsg': 'update job status does not take effect'}}, 'host': {'212': {'retcode': 0, 'retmsg': 'success'}}}
3
[ERROR] [2022-04-26 10:38:48,420] [202204261038075892820] [2313:140654841952000] - [federated_scheduler.task_command] [line:246]: failed to execute federated task data_transform_0 command(status/running) detail: 
4
{'guest': {'211': {'retcode': 10, 'retmsg': 'update job status does not take effect'}}, 'host': {'212': {'retcode': 0, 'retmsg': 'success'}}}
5
[ERROR] [2022-04-26 10:39:16,409] [202204261038075892820] [2313:140654841952000] - [federated_scheduler.task_command] [line:246]: failed to execute federated task intersection_0 command(status/running) detail: 
6
{'guest': {'211': {'retcode': 10, 'retmsg': 'update job status does not take effect'}}, 'host': {'212': {'retcode': 0, 'retmsg': 'success'}}}
7
[ERROR] [2022-04-26 10:39:57,652] [202204261038075892820] [2313:140654841952000] - [federated_scheduler.task_command] [line:246]: failed to execute federated task hetero_lr_0 command(status/running) detail: 
8
{'arbiter': {'212': {'retcode': 0, 'retmsg': 'success'}}, 'guest': {'211': {'retcode': 10, 'retmsg': 'update job status does not take effect'}}, 'host': {'212': {'retcode': 0, 'retmsg': 'success'}}}
9
[ERROR] [2022-04-26 11:02:42,247] [202204261038075892820] [2313:140654841952000] - [federated_scheduler.task_command] [line:246]: failed to execute federated task evaluation_0 command(status/running) detail: 
10
{'arbiter': {'212': {'retcode': 0, 'retmsg': 'success'}}, 'guest': {'211': {'retcode': 10, 'retmsg': 'update job status does not take effect'}}, 'host': {'212': {'retcode': 0, 'retmsg': 'success'}}}

查看Host的调度日志，会出现如下错误：
1312
Traceback (most recent call last):
1313
  File ""./fate/fateflow/python/fate_flow/manager/worker_manager.py"", line 293, in kill_task_all_workers
1314
    cls.kill_worker(worker_info)
1315
  File ""./fate/fateflow/python/fate_flow/manager/worker_manager.py"", line 303, in kill_worker
1316
    process = psutil.Process(worker_info.f_run_pid)
1317
  File ""./fate/common/python/venv/lib/python3.6/site-packages/psutil/__init__.py"", line 326, in __init__
1318
    self._init(pid)
1319
  File ""./fate/common/python/venv/lib/python3.6/site-packages/psutil/__init__.py"", line 367, in _init
1320
    raise NoSuchProcess(pid, None, msg)
1321
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 31067
1322
[INFO] [2022-04-26 12:10:45,490] [202204261206089786280] [8584:140317630121728] - [worker_manager.kill_task_all_workers] [line:299]: task 202204261206089786280_reader_0 0 kill all workers successfully on host 213
这些调度日志其实是warn,在后面版本我们有优化。请问下“逻辑回归时就卡在HeteroLR“的现象具体是怎样的？task状态一直处于running吗> 这些调度日志其实是warn,在后面版本我们有优化。请问下“逻辑回归时就卡在HeteroLR“的现象具体是怎样的？task状态一直处于running吗

具体现象就是：
1、同样的三方数据，我如果任选两方用HeteroLR，可以执行成功，也出行上面的warn日志；如果三方联合训练，在HeteroLR组件这里iter=0的时候就task就失败了，日志里面依然只有那些，看不到错误信息。
2、我又换了新的三方的数据集，用HeteroLR训练，依然失败；
3、另外，如果我用SBT拿同样的数据进行三方联合是可以成功训练模型的。能贴一下conf和dsl看看吗> 能贴一下conf和dsl看看吗

conf：
{
    ""dsl_version"": 2,
    ""initiator"": {
        ""role"": ""guest"",
        ""party_id"": 211
    },
    ""role"": {
        ""arbiter"": [
            212
        ],
        ""host"": [
            10000,
            212
        ],
        ""guest"": [
            211
        ]
    },
    ""job_parameters"": {
        ""common"": {
            ""job_type"": ""train"",
            ""computing_engine"": ""EGGROLL"",
            ""engines_address"": {},
            ""federated_mode"": ""MULTIPLE"",
            ""task_parallelism"": 1,
            ""computing_partitions"": 4,
            ""federated_status_collect_type"": ""PUSH"",
            ""model_id"": ""arbiter-212#guest-211#host-212_10000#model"",
            ""model_version"": ""202204261529198849290"",
            ""auto_retries"": 0,
            ""auto_retry_delay"": 1,
            ""eggroll_run"": {},
            ""spark_run"": {},
            ""rabbitmq_run"": {},
            ""pulsar_run"": {},
            ""adaptation_parameters"": {
                ""task_nodes"": 1,
                ""task_cores_per_node"": 4,
                ""task_memory_per_node"": 0,
                ""request_task_cores"": 4,
                ""if_initiator_baseline"": true
            }
        }
    },
    ""component_parameters"": {
        ""role"": {
            ""host"": {
                ""0|1"": {
                    ""data_transform_0"": {
                        ""with_label"": false
                    }
                },
                ""1"": {
                    ""reader_0"": {
                        ""table"": {
                            ""name"": ""212_edu_data"",
                            ""namespace"": ""partner""
                        }
                    }
                },
                ""0"": {
                    ""reader_0"": {
                        ""table"": {
                            ""name"": ""10000_civil_data"",
                            ""namespace"": ""partner""
                        }
                    }
                }
            },
            ""guest"": {
                ""0"": {
                    ""data_transform_0"": {
                        ""with_label"": true,
                        ""label_name"": ""default.payment.next.month""
                    },
                    ""reader_0"": {
                        ""table"": {
                            ""name"": ""211_credit_train"",
                            ""namespace"": ""partner""
                        }
                    }
                }
            }
        },
        ""common"": {
            ""hetero_lr_0"": {
                ""batch_size"": 5000,
                ""learning_rate"": 0.001,
                ""max_iter"": 3,
                ""encrypted_mode_calculator_param"": {
                    ""mode"": ""fast""
                }
            },
            ""evaluation_0"": {
                ""eval_type"": ""binary""
            }
        }
    }
}


dsl：
{
    ""components"": {
        ""reader_0"": {
            ""module"": ""Reader"",
            ""output"": {
                ""data"": [
                    ""data""
                ]
            },
            ""provider"": ""fate_flow@1.7.0""
        },
        ""data_transform_0"": {
            ""module"": ""DataTransform"",
            ""input"": {
                ""data"": {
                    ""data"": [
                        ""reader_0.data""
                    ]
                }
            },
            ""output"": {
                ""data"": [
                    ""data""
                ],
                ""model"": [
                    ""model""
                ]
            },
            ""provider"": ""fate@1.7.0""
        },
        ""intersection_0"": {
            ""module"": ""Intersection"",
            ""input"": {
                ""data"": {
                    ""data"": [
                        ""data_transform_0.data""
                    ]
                }
            },
            ""output"": {
                ""data"": [
                    ""data""
                ],
                ""cache"": [
                    ""cache""
                ]
            },
            ""provider"": ""fate@1.7.0""
        },
        ""hetero_lr_0"": {
            ""module"": ""HeteroLR"",
            ""input"": {
                ""data"": {
                    ""data"": [
                        ""intersection_0.data""
                    ]
                }
            },
            ""output"": {
                ""data"": [
                    ""data""
                ],
                ""model"": [
                    ""model""
                ]
            },
            ""provider"": ""fate@1.7.0""
        },
        ""evaluation_0"": {
            ""module"": ""Evaluation"",
            ""input"": {
                ""data"": {
                    ""data"": [
                        ""hetero_lr_0.data""
                    ]
                }
            },
            ""output"": {
                ""data"": [
                    ""data""
                ]
            },
            ""provider"": ""fate@1.7.0""
        }
    }
}

流程图为：
![image](https://user-images.githubusercontent.com/15016772/165889911-4e854732-aaa1-4a5f-bb5c-d88ad62be66d.png)

日志到截图位置就没了：
![image](https://user-images.githubusercontent.com/15016772/165889964-68c05db2-3c3a-480b-a25f-6190d4c055d4.png)
Hetero-SSHE-LR does not support multi-host，it will raise error in later version like fate-v1.8
https://github.com/FederatedAI/FATE/blob/v1.8.0/python/federatedml/linear_model/bilateral_linear_model/hetero_sshe_linear_model.py#L217
![image](https://user-images.githubusercontent.com/13565267/165896395-492d8df8-d370-4cea-bedc-91539e05b417.png)",5,2022-04-26 03:18:04,2022-04-29 06:46:18,2022-04-29 06:46:18
https://github.com/FederatedAI/FATE/issues/3927,[],Provide federatedai/client image dockerfile,"Provide federatedai/client image dockerfile**Is your feature request related to a problem? Please describe.**
After adding a new algorithm in fat, you need to update the corresponding federatedai / client image to interact with the outside world, but the image generation method has not been found in the project.
在FATE里面添加新算法后，需要更新相应的federatedai/client镜像才能与外界进行交互，但在项目工程中暂未发现该镜像生成方式。

**Describe the solution you'd like**
Add the generation method of the image in the fat / build / docker build directory and provide the corresponding dockefile.
在FATE/build/docker-build目录里面添加该镜像的生成方式，并提供相应的DockeFile.
client dockerfile在kubefate工程里面",1,2022-04-21 08:27:19,2022-04-22 07:48:37,2022-04-22 07:48:36
https://github.com/FederatedAI/FATE/issues/3923,[],自定义算法组件在kube-fate下失败,"自定义算法组件在kube-fate下失败**Describe the bug**
自定义了一个算法组件，单机版部署没问题，使用kube-fate(1.7.0 via docker-compose)， 训练没问题，损失也计算出来了，

```
  File ""./fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor
479
    return cloudpickle.loads(func_bin)
480
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'
```

**我只是把HomoLR复制了一份，命名为HomoLR2就出现了问题。有意思的是，HeteroLR就没问题。** 具体看下面的reproduce

**To Reproduce**
Steps to reproduce the behavior:
1. 复制/data/projects/fate/confs-9999/shared_dir/examples/dsl/v2/homo_logistic_regression，为/data/projects/fate/confs-9999/shared_dir/examples/dsl/v2/**homo_logistic_regression2**
2. 复制并修改component，如下：
```
from .components import ComponentMeta

homo_lr_cpn_meta = ComponentMeta(""HomoLR2"")


@homo_lr_cpn_meta.bind_param
def homo_lr_param():
    from federatedml.param.logistic_regression_param import HomoLogisticParam

    return HomoLogisticParam


@homo_lr_cpn_meta.bind_runner.on_guest
def homo_lr_runner_guest():
    from federatedml.linear_model.logistic_regression.homo_logistic_regression2.homo_lr_guest import (
        HomoLRGuest,
    )

    return HomoLRGuest


@homo_lr_cpn_meta.bind_runner.on_host
def homo_lr_runner_host():
    from federatedml.linear_model.logistic_regression.homo_logistic_regression2.homo_lr_host import (
        HomoLRHost,
    )

    return HomoLRHost


@homo_lr_cpn_meta.bind_runner.on_arbiter
def homo_lr_runner_arbiter():
    from federatedml.linear_model.logistic_regression.homo_logistic_regression2.homo_lr_arbiter import (
        HomoLRArbiter,
    )

    return HomoLRArbiter
```



4. 使用example进行测试，**注意把dsl的HomoLR改为HomoLR2**
5. See error
训练没有问题，
![image](https://user-images.githubusercontent.com/18736196/164356376-ab726d54-23b8-4ad4-8b99-45ed000bd948.png)

报错


```
[ERROR] [2022-04-21 01:47:28,922] [202204210144066011370] [13737:140604291467072] - [task_executor._run_] [line:243]: ('Failed to call command: CommandURI(_uri=v1/egg-pair/runTask) to endpoint: nodemanager:45019, caused by: ', <_Rendezvous of RPC that terminated with:
        status = StatusCode.UNKNOWN
        details = ""Exception calling application: 

==== detail start, at 20220421.014728.913 ====
Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor
    return cloudpickle.loads(func_bin)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class
    return super().find_class(module, name)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
    return func(*args, **kw)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task
    f = create_functor(functors[0]._body)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor
    return eggroll_pickle_loads(func_bin)
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads
    return up.load()
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class
    return getattr(importlib.import_module(module), name)
  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
    return func(*args, **kw)
  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_service.py"", line 33, in call
    kwargs=getattr(command_request, '_kwargs'))
  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 94, in dispatch
    raise e
  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 91, in dispatch
    call_result = _method(_instance, *deserialized_args)
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 194, in wrapper
    raise RuntimeError(msg)
RuntimeError: 

==== detail start, at 20220421.014728.909 ====
Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor
    return cloudpickle.loads(func_bin)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class
    return super().find_class(module, name)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
    return func(*args, **kw)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task
    f = create_functor(functors[0]._body)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor
    return eggroll_pickle_loads(func_bin)
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads
    return up.load()
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class
    return getattr(importlib.import_module(module), name)
  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

==== detail end ====



==== detail end ====

""
        debug_error_string = ""{""created"":""@1650505648.914970007"",""description"":""Error received from peer ipv4:192.167.0.2:45019"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1055,""grpc_message"":""Exception calling application: \n\n==== detail start, at 20220421.014728.913 ====\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor\n    return cloudpickle.loads(func_bin)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class\n    return super().find_class(module, name)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper\n    return func(*args, **kw)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task\n    f = create_functor(functors[0]._body)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor\n    return eggroll_pickle_loads(func_bin)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads\n    return up.load()\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class\n    return getattr(importlib.import_module(module), name)\n  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked\n  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper\n    return func(*args, **kw)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_service.py"", line 33, in call\n    kwargs=getattr(command_request, '_kwargs'))\n  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 94, in dispatch\n    raise e\n  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 91, in dispatch\n    call_result = _method(_instance, *deserialized_args)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 194, in wrapper\n    raise RuntimeError(msg)\nRuntimeError: \n\n==== detail start, at 20220421.014728.909 ====\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor\n    return cloudpickle.loads(func_bin)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class\n    return super().find_class(module, name)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper\n    return func(*args, **kw)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task\n    f = create_functor(functors[0]._body)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor\n    return eggroll_pickle_loads(func_bin)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads\n    return up.load()\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class\n    return getattr(importlib.import_module(module), name)\n  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked\n  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\n==== detail end ====\n\n\n\n==== detail end ====\n\n"",""grpc_status"":2}""
>)
Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/client.py"", line 84, in sync_send
    response = _command_stub.call(request.to_proto())
  File ""/opt/app-root/lib/python3.6/site-packages/grpc/_channel.py"", line 604, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/opt/app-root/lib/python3.6/site-packages/grpc/_channel.py"", line 506, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:
        status = StatusCode.UNKNOWN
        details = ""Exception calling application: 

==== detail start, at 20220421.014728.913 ====
Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor
    return cloudpickle.loads(func_bin)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class
    return super().find_class(module, name)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
    return func(*args, **kw)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task
    f = create_functor(functors[0]._body)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor
    return eggroll_pickle_loads(func_bin)
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads
    return up.load()
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class
    return getattr(importlib.import_module(module), name)
  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
    return func(*args, **kw)
  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_service.py"", line 33, in call
    kwargs=getattr(command_request, '_kwargs'))
  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 94, in dispatch
    raise e
  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 91, in dispatch
    call_result = _method(_instance, *deserialized_args)
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 194, in wrapper
    raise RuntimeError(msg)
RuntimeError: 

==== detail start, at 20220421.014728.909 ====
Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor
    return cloudpickle.loads(func_bin)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class
    return super().find_class(module, name)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
    return func(*args, **kw)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task
    f = create_functor(functors[0]._body)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor
    return eggroll_pickle_loads(func_bin)
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads
    return up.load()
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class
    return getattr(importlib.import_module(module), name)
  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

==== detail end ====



==== detail end ====

""
        debug_error_string = ""{""created"":""@1650505648.914970007"",""description"":""Error received from peer ipv4:192.167.0.2:45019"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1055,""grpc_message"":""Exception calling application: \n\n==== detail start, at 20220421.014728.913 ====\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor\n    return cloudpickle.loads(func_bin)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class\n    return super().find_class(module, name)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper\n    return func(*args, **kw)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task\n    f = create_functor(functors[0]._body)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor\n    return eggroll_pickle_loads(func_bin)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads\n    return up.load()\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class\n    return getattr(importlib.import_module(module), name)\n  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked\n  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper\n    return func(*args, **kw)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_service.py"", line 33, in call\n    kwargs=getattr(command_request, '_kwargs'))\n  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 94, in dispatch\n    raise e\n  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 91, in dispatch\n    call_result = _method(_instance, *deserialized_args)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 194, in wrapper\n    raise RuntimeError(msg)\nRuntimeError: \n\n==== detail start, at 20220421.014728.909 ====\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor\n    return cloudpickle.loads(func_bin)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class\n    return super().find_class(module, name)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper\n    return func(*args, **kw)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task\n    f = create_functor(functors[0]._body)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor\n    return eggroll_pickle_loads(func_bin)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads\n    return up.load()\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class\n    return getattr(importlib.import_module(module), name)\n  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked\n  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\n==== detail end ====\n\n\n\n==== detail end ====\n\n"",""grpc_status"":2}""
>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/fateflow/python/fate_flow/worker/task_executor.py"", line 195, in _run_
    cpn_output = run_object.run(cpn_input)
  File ""/data/projects/fate/fate/python/federatedml/model_base.py"", line 209, in run
    method(cpn_input)
  File ""/data/projects/fate/fate/python/federatedml/model_base.py"", line 247, in _run
    this_data_output = func(*params)
  File ""/data/projects/fate/fate/python/federatedml/util/io_check.py"", line 31, in _func
    result = func(*args, **kwargs)
  File ""/data/projects/fate/fate/python/federatedml/linear_model/logistic_regression/homo_logistic_regression2/homo_lr_guest.py"", line 136, in predict
    pred_prob = data_instances.mapValues(lambda v: activation.sigmoid(vec_dot(v.features, self.model_weights.coef_)
  File ""/data/projects/fate/fate/python/fate_arch/common/profile.py"", line 281, in _fn
    rtn = func(*args, **kwargs)
  File ""/data/projects/fate/fate/python/fate_arch/computing/eggroll/_table.py"", line 84, in mapValues
    return Table(self._rp.map_values(func))
  File ""/data/projects/fate/eggroll/python/eggroll/core/aspects.py"", line 30, in wrapper
    result = func(*args, **kwargs)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/roll_pair.py"", line 781, in map_values
    task_results = self._run_job(job=job)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/roll_pair.py"", line 472, in _run_job
    results.append(future.result())
  File ""/opt/rh/rh-python36/root/usr/lib64/python3.6/concurrent/futures/_base.py"", line 432, in result
    return self.__get_result()
  File ""/opt/rh/rh-python36/root/usr/lib64/python3.6/concurrent/futures/_base.py"", line 384, in __get_result
    raise self._exception
  File ""/data/projects/fate/eggroll/python/eggroll/core/datastructure/threadpool.py"", line 51, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/data/projects/fate/eggroll/python/eggroll/core/client.py"", line 97, in sync_send
    raise CommandCallError(command_uri, endpoint, e)
eggroll.core.client.CommandCallError: ('Failed to call command: CommandURI(_uri=v1/egg-pair/runTask) to endpoint: nodemanager:45019, caused by: ', <_Rendezvous of RPC that terminated with:
        status = StatusCode.UNKNOWN
        details = ""Exception calling application: 

==== detail start, at 20220421.014728.913 ====
Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor
    return cloudpickle.loads(func_bin)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class
    return super().find_class(module, name)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
    return func(*args, **kw)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task
    f = create_functor(functors[0]._body)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor
    return eggroll_pickle_loads(func_bin)
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads
    return up.load()
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class
    return getattr(importlib.import_module(module), name)
  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
    return func(*args, **kw)
  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_service.py"", line 33, in call
    kwargs=getattr(command_request, '_kwargs'))
  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 94, in dispatch
    raise e
  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 91, in dispatch
    call_result = _method(_instance, *deserialized_args)
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 194, in wrapper
    raise RuntimeError(msg)
RuntimeError: 

==== detail start, at 20220421.014728.909 ====
Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor
    return cloudpickle.loads(func_bin)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class
    return super().find_class(module, name)
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper
    return func(*args, **kw)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task
    f = create_functor(functors[0]._body)
  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor
    return eggroll_pickle_loads(func_bin)
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads
    return up.load()
  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class
    return getattr(importlib.import_module(module), name)
  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'

==== detail end ====



==== detail end ====

""
        debug_error_string = ""{""created"":""@1650505648.914970007"",""description"":""Error received from peer ipv4:192.167.0.2:45019"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1055,""grpc_message"":""Exception calling application: \n\n==== detail start, at 20220421.014728.913 ====\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor\n    return cloudpickle.loads(func_bin)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class\n    return super().find_class(module, name)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper\n    return func(*args, **kw)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task\n    f = create_functor(functors[0]._body)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor\n    return eggroll_pickle_loads(func_bin)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads\n    return up.load()\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class\n    return getattr(importlib.import_module(module), name)\n  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked\n  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper\n    return func(*args, **kw)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_service.py"", line 33, in call\n    kwargs=getattr(command_request, '_kwargs'))\n  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 94, in dispatch\n    raise e\n  File ""/data/projects/fate/eggroll/python/eggroll/core/command/command_router.py"", line 91, in dispatch\n    call_result = _method(_instance, *deserialized_args)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 194, in wrapper\n    raise RuntimeError(msg)\nRuntimeError: \n\n==== detail start, at 20220421.014728.909 ====\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 30, in create_functor\n    return cloudpickle.loads(func_bin)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 29, in find_class\n    return super().find_class(module, name)\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File ""/data/projects/fate/eggroll/python/eggroll/core/utils.py"", line 187, in wrapper\n    return func(*args, **kw)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/egg_pair.py"", line 258, in run_task\n    f = create_functor(functors[0]._body)\n  File ""/data/projects/fate/eggroll/python/eggroll/roll_pair/__init__.py"", line 32, in create_functor\n    return eggroll_pickle_loads(func_bin)\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 37, in eggroll_pickle_loads\n    return up.load()\n  File ""/data/projects/fate/eggroll/python/eggroll/core/serdes/eggroll_serdes.py"", line 31, in find_class\n    return getattr(importlib.import_module(module), name)\n  File ""/opt/app-root/lib64/python3.6/importlib/__init__.py"", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked\n  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed\n  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import\n  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load\n  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'federatedml.linear_model.logistic_regression.homo_logistic_regression2'\n\n==== detail end ====\n\n\n\n==== detail end ====\n\n"",""grpc_status"":2}""
>)
```








**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
kube fate via docker-compose 1.7.0


**Additional context**
同样是复制纵向LR，就没问题
您好，请问您解决这个问题了吗？我最近也遇到了类似的问题，想请教一下。",1,2022-04-21 02:03:01,2022-05-18 11:42:40,2022-04-21 03:18:00
https://github.com/FederatedAI/FATE/issues/3895,[],spdz加法减法计算错误,"spdz加法减法计算错误(1)错误描述
基于Fate1.6版本开发的spdz加法和减法程序，通过相关的日志打印出相关的秘钥片段是错误的。

(2)算法添加
(2.1)将文件添加到federml目录中，其他的代码片段如下所示：
from fate_arch import session
from federatedml.model_base import ModelBase
from federatedml.param.arithmetic_param import ArithmeticParam
from federatedml.secureprotol.spdz import SPDZ
from federatedml.secureprotol.spdz.tensor.fixedpoint_table import (
    FixedPointTensor,
)
import numpy as np
from federatedml.util import LOGGER
class ArithmeticSubGuest(ModelBase):
    def __init__(self):
        super().__init__()
        self.model_param = ArithmeticParam()

        self.parties = []
        self.local_party = None
        self.other_party = None
        self._set_parties()

        self._summary = {}

    def _set_parties(self):
        parties = []
        # guest
        guest_parties = session.get_latest_opened().parties.roles_to_parties([""guest""])
        # host
        host_parties = session.get_latest_opened().parties.roles_to_parties([""host""])
        # one guest and one host
        if len(guest_parties) != 1 or len(host_parties) != 1:
            raise ValueError(
                f""one guest and one host required, ""
                f""while {len(guest_parties)} guest and {len(host_parties)} host provided""
            )
        parties.extend(guest_parties)
        parties.extend(host_parties)

        local_party = session.get_latest_opened().parties.local_party
        other_party = parties[0] if parties[0] != local_party else parties[1]

        self.local_party = local_party
        self.other_party = other_party
        LOGGER.info(f""local_party is {local_party} , other_party is {other_party} ,parties is {parties}"")

    def fit(self, data_instances, validate_data=None):
        # Print log
        LOGGER.info(""Enter ArithmeticSub guest"")
        # Get user data
        data = data_instances.first()
        if data_instances.count() != 1:
            raise ValueError(""ArithmeticSub-Guest data format error"")
        LOGGER.info('ArithmeticSub-guest is {}'.format(data[0]))

        # use SPDZ
        data = session.computing_session.parallelize([np.array([np.int64(data[0])])], include_key=False, partition=2)
        with SPDZ(
                ""arithmetic"",
                local_party=self.local_party,
                use_mix_rand=False,
        ) as spdz:
            source = [data, self.other_party]
            LOGGER.info('other-party is {}'.format(self.other_party))
            x, y = (
                FixedPointTensor.from_source(""x"", source[0]),  # value
                FixedPointTensor.from_source(""y"", source[1]),  # host
            )

            result = (x - y).get()
            LOGGER.info(f'x is {x.get().take()}')
            LOGGER.info(f'y is {y.get().take()}')
            LOGGER.info(f'result is {result.collect()},{result.take(3)}')
            LOGGER.info('End ArithmeticSub guest')

            return result


class ArithmeticSubHost(ModelBase):
    def __init__(self):
        super().__init__()
        self.model_param = ArithmeticParam()

        self.parties = []
        self.local_party = None
        self.other_party = None
        self._set_parties()

        self._summary = {}

    def _set_parties(self):
        parties = []
        # guest
        guest_parties = session.get_latest_opened().parties.roles_to_parties([""guest""])
        # host
        host_parties = session.get_latest_opened().parties.roles_to_parties([""host""])
        # one guest and one host
        if len(guest_parties) != 1 or len(host_parties) != 1:
            raise ValueError(
                f""one guest and one host required, ""
                f""while {len(guest_parties)} guest and {len(host_parties)} host provided""
            )
        parties.extend(guest_parties)
        parties.extend(host_parties)

        local_party = session.get_latest_opened().parties.local_party
        other_party = parties[0] if parties[0] != local_party else parties[1]

        self.local_party = local_party
        self.other_party = other_party
        LOGGER.info(f""local_party is {local_party} , other_party is {other_party} ,parties is {parties}"")

    def fit(self, data_instances, validate_data=None):
        # Print log
        LOGGER.info(""Enter ArithmeticSub host"")
        # Get user data
        data = data_instances.first()
        if data_instances.count() != 1:
            raise ValueError(""ArithmeticSub-Guest data format error"")
        LOGGER.info('ArithmeticSub-guest is {}'.format(data[0]))

        # use SPDZ
        data = session.computing_session.parallelize([np.array([np.int64(data[0])])], include_key=False, partition=2)
        with SPDZ(
                ""arithmetic"",
                local_party=self.local_party,
                use_mix_rand=False,
        ) as spdz:
            source = [data, self.other_party]
            LOGGER.info('other-party is {}'.format(self.other_party))
            y, x = (
                FixedPointTensor.from_source(""y"", source[0]),  # value
                FixedPointTensor.from_source(""x"", source[1]),  # guest
            )

            result = (x - y).get()
            LOGGER.info(f'x is {x.get().take()}')
            LOGGER.info(f'y is {y.get().take()}')
            LOGGER.info(f'result is {result.collect()},{result.take(3)}')
            LOGGER.info('End ArithmeticSub host')

(2.2)使用自己的pipeline进行任务调试。相关的日志截图如下：
![4648150b38740765b9f25925273f57a](https://user-images.githubusercontent.com/40556406/162929245-3f297802-549f-41db-bba7-106c38db59b9.png)

(2.3)截图说明
我文件中的x值为11，y值的12，而日志中打印结果分别扩大了10000倍。10000恰好为该段的part-id编号。
通过计算x-y，其结果应为-1，而日志中为一个溢出的极大值。

(3)环境介绍
所有的代码均运行在Centos7服务器，并在win10上进行远端调试。


解决方法：将版本升级到1.7.2可以解决该bug。
测试方法：guest值为6，host值为2。日志中将所有的信息打印出来，并且得到正确的结果。
(1)guest的日志为：![微信图片_20220414092807](https://user-images.githubusercontent.com/40556406/163296400-e63866a0-bcb0-4195-8366-0d687ca5225d.png)
![微信图片_20220414092807](https://user-images.githubusercontent.com/40556406/163296400-e63866a0-bcb0-4195-8366-0d687ca5225d.png)



(2)结果为：
![微信图片_20220414092819](https://user-images.githubusercontent.com/40556406/163296438-a81d15b3-82f7-415b-83c9-821fec538bb1.png)

问题得到解决",2,2022-04-12 09:36:41,2022-04-14 01:33:44,2022-04-14 01:33:44
https://github.com/FederatedAI/FATE/issues/3882,[],是否能提供获取所有  t_storage_table_meta 数据接口 和 all partyid (route_table.json) 接口,"是否能提供获取所有  t_storage_table_meta 数据接口 和 all partyid (route_table.json) 接口**Is your feature request related to a problem? Please describe.**
通过服务能够获取所有输入输出表信息，以及party 信息，方便页面编排选择party
/table/list可以获取某个job的输入输出表信息，可参考接口文档：https://federatedai.github.io/FATE-Flow/1.8.0/swagger/感谢，现在按照获取所有 job 然后 调用 /table/list",2,2022-04-08 16:23:05,2022-04-29 02:27:49,2022-04-29 02:27:49
https://github.com/FederatedAI/FATE/issues/3832,['bug'],sir-reader_0 table use,"sir-reader_0 table usehttps://github.com/FederatedAI/FATE/blob/master/examples/dsl/v2/secure_information_retrieval/test_secure_information_retrieval_conf.json

""guest"": {
                ""0"": {
                    ""reader_0"": {
                        ""table"": {
                            ""name"": ""breast_hetero_host"",
                            ""namespace"": ""experiment""
                        }
                    },
                    ""data_transform_0"": {
                        ""with_label"": false
                    }
                }
            },

请问下，这里guest的reader_0用的数据为什么是breast_hetero_host呢？> https://github.com/FederatedAI/FATE/blob/master/examples/dsl/v2/secure_information_retrieval/test_secure_information_retrieval_conf.json
> 
> ""guest"": { ""0"": { ""reader_0"": { ""table"": { ""name"": ""breast_hetero_host"", ""namespace"": ""experiment"" } }, ""data_transform_0"": { ""with_label"": false } } },
> 
> 请问下，这里guest的reader_0用的数据为什么是breast_hetero_host呢？

I think it may be a typo in the conf file. I have already mentioned this issue in #3743. Swapping the table name between the host and the guest can solve this problem.https://github.com/FederatedAI/FATE/pull/3744
ok, thank you.Hi,  this config file is to demonstrate server(guest) securely retrieves value from client(host). Use of datasets do not have to be restricted here as for other modules. Here we keep the choice of data sets consistent with the [earliest version](https://github.com/FederatedAI/FATE/blob/v1.5.0/examples/dsl/v1/secure_information_retrieval/test_secure_information_retrieval_conf.json).",4,2022-03-29 02:17:58,2022-03-31 11:43:37,2022-03-29 06:58:36
https://github.com/FederatedAI/FATE/issues/3825,[],The notebook scripts cannot run successfully when I deploy FATE by KubeFATE,"The notebook scripts cannot run successfully when I deploy FATE by KubeFATE**Describe the bug**
A clear and concise description of what the bug is.
pipeline_tutorial_upload.ipynb will not work due to several issues when I'm using the FATE cluster I deployed by KubeFATE

**To Reproduce**
Steps to reproduce the behavior:
Install FATE by KubeFATE, open the notebook UI, run that script: pipeline_tutorial_upload.ipynb

**Expected behavior**
Succeed

**Screenshots**
![Screen Shot 2022-03-24 at 4 33 21 PM](https://user-images.githubusercontent.com/15973672/159875510-ac26f916-5730-453e-9f10-44c80be39c5b.png)
![Screen Shot 2022-03-24 at 4 35 48 PM](https://user-images.githubusercontent.com/15973672/159875806-87e5a02e-a947-4331-831b-07e381f06a14.png)
![Screen Shot 2022-03-24 at 4 36 12 PM](https://user-images.githubusercontent.com/15973672/159875689-7439af65-635a-4148-9890-34ca4e062c8c.png)



**Desktop (please complete the following information):**
 - OS: MacOS
 - Browser chrome
 - Version some version, not important

**Smartphone (please complete the following information):**
n/a

**Additional context**
I, as a KubeFATE maintainer, will take care of the fix, this issue is used to track my pr in the future.
Well after syncing with team this will be handled on Kubefate side, close the issue.",1,2022-03-24 08:38:05,2022-03-24 12:07:27,2022-03-24 12:07:27
https://github.com/FederatedAI/FATE/issues/3824,[],（1.7）版本手写字符识别的案例报错,"（1.7）版本手写字符识别的案例报错[ERROR] [2022-03-21 08:01:43,386] [202203210801364997640] [12022:140171217692480] - [task_executor._run_] [line:243]: [Errno loading yaml file config from /examples/data/mnist_train/config.yaml failed:] [Errno 2] No such file or directory: '/examples/data/mnist_train/config.yaml'（在guest端的报错）
[ERROR] [2022-03-21 08:01:43,452] [202203210801364997640] [12019:140426159630144] - [task_executor._run_] [line:243]: [Errno 2] No such file or directory: 'examples/data/mnist_train/images'（host端报错）

我不知道那一步有问题，向您请教，谢谢！应该是没有下载数据数据集现在下载好了，现在报错是缺少yaml，识别不到，我访问该目录是有的，请问什么原因呢







------------------&nbsp;原始邮件&nbsp;------------------
发件人: ***@***.***&gt;; 
发送时间: 2022年3月28日(星期一) 下午3:47
收件人: ***@***.***&gt;; 
抄送: ""　　Day  ***@***.***&gt;; ***@***.***&gt;; 
主题: Re: [FederatedAI/FATE] （1.7）版本手写字符识别的案例报错 (Issue #3824)





 
应该是没有下载数据
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you authored the thread.Message ID: ***@***.***&gt;",2,2022-03-24 06:45:39,2022-03-28 10:47:37,2022-03-28 10:47:37
https://github.com/FederatedAI/FATE/issues/3816,['bug'],Bugs in keras-based homo_nn: Does not aggregate(send) all trainable params(`_trainable_weights`) ?,"Bugs in keras-based homo_nn: Does not aggregate(send) all trainable params(`_trainable_weights`) ?**Describe the bug**
In the keras-backend nn model, it does not aggregate(send) all the trainable params. This will make Homo-Federated-Learning less effective. **The pytorch-backend nn does not have this issue.**

For instance, in the given [example](https://github.com/FederatedAI/FATE/blob/master/examples/dsl/v2/homo_nn/keras_homo_dnn_single_layer.json), the number of trainable variables is 31(30 kernel and 1 bias). 
![image](https://user-images.githubusercontent.com/18736196/159400002-7855f1cb-7648-4ecb-80d9-361c66a876ac.png)

**However, only 1 variable(dense/bias) is aggregated.**
![image](https://user-images.githubusercontent.com/18736196/159400797-94c37b89-f7bf-4b4d-84fc-e30265bfb0c1.png)



**To Reproduce**
Steps to reproduce the behavior:
1. FATE standalone 1.7.0
2. Print the aggregated weights. Adding LOGGER code in the following files:
  https://github.com/FederatedAI/FATE/blob/018d051f06298cd01aec957d569ff5760ff0070e/python/federatedml/nn/homo_nn/_version_0.py#L184-L185
![image](https://user-images.githubusercontent.com/18736196/159397837-0ed4cc76-5abe-4283-91de-b2abc8629b78.png)


3. Run the[ keras example](examples/dsl/v2/homo_nn/keras_homo_dnn_single_layer.json) and [pytorch example](https://github.com/FederatedAI/FATE/blob/master/examples/dsl/v2/homo_nn/pytorch_homo_dnn_single_layer.json) respectively.
**Please note, the pytorch example has a typo**: the initiator's party id is wrong. Please see #3814 and #3815 
5. See the corresponding log.
The keras-backend only aggregates one variable. The pytorch-backend aggregates all variables.
![image](https://user-images.githubusercontent.com/18736196/159446423-90471e5e-ec77-487f-9ec3-1fc939ed3a53.png)
![image](https://user-images.githubusercontent.com/18736196/159446535-f5583339-3118-4fed-b166-882cbcaba579.png)


**Expected behavior**
The Keras-backend homo-nn should behave the same as pytorch backend.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - FATE standalone 1.7.0

**Additional context**
I think this bug may be cased by the following code:
https://github.com/FederatedAI/FATE/blob/018d051f06298cd01aec957d569ff5760ff0070e/python/federatedml/nn/backend/tf_keras/nn_model.py#L124-L126

I don't know why needs to trim the device name?Thanks， we have fix this issue lone time ago in [this commit](https://github.com/FederatedAI/FATE/commit/cff4ef5e10be52c079efcac7a0045be8ec830137), and problemly reintroduced in [this commit](https://github.com/FederatedAI/FATE/commit/6e01fdaab6e146cb359a2e6a1899560848e44a26#r69688419)

Would you like to make a pull-request to fix this again? @gxcuit 
> Thanks， we have fix this issue lone time ago in [this commit](https://github.com/FederatedAI/FATE/commit/cff4ef5e10be52c079efcac7a0045be8ec830137), and problemly reintroduced in [this commit](https://github.com/FederatedAI/FATE/commit/6e01fdaab6e146cb359a2e6a1899560848e44a26#r69688419)
> 
> Would you like to make a pull-request to fix this again? @gxcuit

I have re-submitted the PR and removed the comments. 

Please kindly refer to #3821",2,2022-03-22 09:15:07,2022-03-29 06:31:41,2022-03-29 06:31:41
https://github.com/FederatedAI/FATE/issues/3805,[],fate-1.7.2版本，guest方和host方的属性列名称不相同时，执行特定列的分箱操作失败，特征择选操作被卡住，且持续运行，如选择X0、X1列,"fate-1.7.2版本，guest方和host方的属性列名称不相同时，执行特定列的分箱操作失败，特征择选操作被卡住，且持续运行，如选择X0、X1列在fate-1.7.1和fate-1.7.2版本中：
1、guest方和host方的属性列名称不相同时，执行特定列的分箱操作失败，特征择选操作被卡住，且持续运行，如选择X0、X1列；
2、guest方和host方的属性列名称相同时，分箱操作选择特定列，如X0、X1列，则分箱操作和特征择选操作能顺利执行；如果每一方指定特征列进行特征选择，算法参数不要在common设置该问题已在fate-v1.8.0进行修复，issue将会关闭",2,2022-03-14 07:29:51,2022-05-18 06:07:57,2022-05-18 06:07:57
https://github.com/FederatedAI/FATE/issues/3746,[],"纵向逻辑回归，conf.json文件中“component_parameters”中，若不使用""common""来配置“hetero_lr_0”，则job无法停止","纵向逻辑回归，conf.json文件中“component_parameters”中，若不使用""common""来配置“hetero_lr_0”，则job无法停止**Describe the bug**
做纵向逻辑回归，conf.json文件中“component_parameters”中，若不使用""common""来配置“hetero_lr_0”，则job无法停止

**To Reproduce**
Steps to reproduce the behavior:
1.上传guest和host方数据
2.编写conf.json文件
如下：
{
    ""dsl_version"": 2,
    ""initiator"": {
        ""role"": ""guest"",
        ""party_id"": 9999
    },
    ""role"": {
        ""arbiter"": [
            10000
        ],
        ""host"": [
            10000
        ],
        ""guest"": [
            9999
        ]
    },
    ""component_parameters"": {
        ""role"": {
            ""guest"": {
                ""0"": {
                    ""reader_0"": {
                        ""table"": {
                            ""name"": ""breast_hetero_guest"",
                            ""namespace"": ""experiment""
                        }
                    },
                    ""data_transform_0"": {
                        ""with_label"": true,
                        ""label_type"": ""int""
                    },
                    ""data_transform_1"": {
                        ""with_label"": true,
                        ""label_type"": ""int""
                    },
                    ""reader_1"": {
                        ""table"": {
                            ""name"": ""breast_hetero_guest"",
                            ""namespace"": ""experiment""
                        }
                    },
                    ""hetero_lr_0"": {
                        ""penalty"": ""L2"",
                        ""tol"": 0.0001,
                        ""alpha"": 0.01,
                        ""optimizer"": ""rmsprop"",
                        ""batch_size"": -1,
                        ""learning_rate"": 0.15,
                        ""init_param"": {
                            ""init_method"": ""zeros""
                        },
                        ""max_iter"": 30,
                        ""early_stop"": ""diff"",
                        ""cv_param"": {
                            ""n_splits"": 5,
                            ""shuffle"": false,
                            ""random_seed"": 103,
                            ""need_cv"": false
                        },
                        ""validation_freqs"": 1,
                        ""early_stopping_rounds"": 3,
                        ""metrics"": [],
                        ""use_first_metric_only"": false,
                        ""sqn_param"": {
                            ""update_interval_L"": 3,
                            ""memory_M"": 5,
                            ""sample_size"": 5000,
                            ""random_seed"": null
                        }
                    },
                    ""evaluation_0"": {
                        ""eval_type"": ""binary""
                    }
                }
            },
            ""host"": {
                ""0"": {
                    ""reader_0"": {
                        ""table"": {
                            ""name"": ""breast_hetero_guest"",
                            ""namespace"": ""experiment""
                        }
                    },
                    ""data_transform_0"": {
                        ""with_label"": false
                    },
                    ""reader_1"": {
                        ""table"": {
                            ""name"": ""breast_hetero_guest"",
                            ""namespace"": ""experiment""
                        }
                    },
                    ""hetero_lr_0"": {
                        ""penalty"": ""L2"",
                        ""tol"": 0.0001,
                        ""alpha"": 0.01,
                        ""optimizer"": ""rmsprop"",
                        ""batch_size"": -1,
                        ""learning_rate"": 0.15,
                        ""init_param"": {
                            ""init_method"": ""zeros""
                        },
                        ""max_iter"": 30,
                        ""early_stop"": ""diff"",
                        ""cv_param"": {
                            ""n_splits"": 5,
                            ""shuffle"": false,
                            ""random_seed"": 103,
                            ""need_cv"": false
                        },
                        ""validation_freqs"": 1,
                        ""early_stopping_rounds"": 3,
                        ""metrics"": [],
                        ""use_first_metric_only"": false,
                        ""sqn_param"": {
                            ""update_interval_L"": 3,
                            ""memory_M"": 5,
                            ""sample_size"": 5000,
                            ""random_seed"": null
                        }
                    },
                    ""evaluation_0"": {
                        ""eval_type"": ""binary""
                    }
                }
            }
        }
    }
}
3.提交执行

**Expected behavior**
期望job正常执行，得到结果

**Screenshots**
实际执行60多个小时，但是hetero_lr_0模块的日志显示 “pid: 10663, elapsed: 83808 ms”  ，最后我手动停止了这个任务

![image](https://user-images.githubusercontent.com/45615979/155922595-6148c4cc-445e-47a4-a423-3ca89615ef4c.png)

![image](https://user-images.githubusercontent.com/45615979/155922464-6cdf5412-de85-469a-8b2a-08e7628bc364.png)



**Desktop (please complete the following information):**
 - OS: linux
 - Browser chrome
 - Version  fate1.7.0  fate1.7.1


我将hetero_lr_0的参数放在role的guest和host中，结果就是job无法停止。
若我将hetero_lr_0的参数放在common中，common与role同级，那么job可以正常结束。
![image](https://user-images.githubusercontent.com/45615979/155930847-78957356-e3d1-4c26-8068-66b3e79c7abb.png)
> 我将hetero_lr_0的参数放在role的guest和host中，结果就是job无法停止。 若我将hetero_lr_0的参数放在common中，common与role同级，那么job可以正常结束。 ![image](https://user-images.githubusercontent.com/45615979/155930847-78957356-e3d1-4c26-8068-66b3e79c7abb.png)

我感觉问题可能出在，除了host和guest，你还需指定arbiter 的参数搞定了，多谢",3,2022-02-28 05:44:37,2022-02-28 07:12:56,2022-02-28 07:12:56
https://github.com/FederatedAI/FATE/issues/3704,[],有没有微信交流群？,"有没有微信交流群？搜了下没找到对应的交流群，自己建了个，大家交流下 :)

![2487a11ce8ce581f18a6f2426a429b2](https://user-images.githubusercontent.com/57430215/154004620-ed637fe6-b13c-4f3f-ae8c-07887ab5874f.jpg)
https://github.com/FederatedAI/FATE-Community#communicating
Here are some of the channels we use to communicate and contribute.

Mailing list: Join the FATE [mailing list](https://groups.io/g/Fate-FedAI), connect with the community and learn about the latest news and information from FATE team, as well as any discussion and feedback around FATE project or community.

Issue tracker: Use the [GitHub issue tracker](https://github.com/FederatedAI/FATE/issues) to file bugs and features request. If you need help, bring your questions via mailing list.

WeChat assistant: FATE's official technology WeChat groups, which provide technical support for any developers and users of FATE. You can communicate and learn with technical enthusiasts from various industries, and get the latest federated learning information. Join us via WeChat ID: FATEZS001

FATE是有官方社区微信交流群的，建议联系""FATEZS001""，FATE开源社区小助手去了解。

> https://github.com/FederatedAI/FATE-Community#communicating Here are some of the channels we use to communicate and contribute.
> 
> Mailing list: Join the FATE [mailing list](https://groups.io/g/Fate-FedAI), connect with the community and learn about the latest news and information from FATE team, as well as any discussion and feedback around FATE project or community.
> 
> Issue tracker: Use the [GitHub issue tracker](https://github.com/FederatedAI/FATE/issues) to file bugs and features request. If you need help, bring your questions via mailing list.
> 
> WeChat assistant: FATE's official technology WeChat groups, which provide technical support for any developers and users of FATE. You can communicate and learn with technical enthusiasts from various industries, and get the latest federated learning information. Join us via WeChat ID: FATEZS001
> 
> FATE是有官方社区微信交流群的，建议联系""FATEZS001""，FATE开源社区小助手去了解。

No response from wechat assistor :(可以加我一下吗，我的微信号是yjx11245013",3,2022-02-15 06:25:31,2022-03-02 15:24:24,2022-03-02 15:24:24
https://github.com/FederatedAI/FATE/issues/3700,[],tensorflow 找不到,"tensorflow 找不到报错信息
ModuleNotFoundError: No module named 'tensorflow'

fate 版本 1.7.0 部署使用的docker 部署的kubefate,使用notebook 运行
使用breast_homo_guest 做横向联邦
运行这个ipython notebook
https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/pipeline_tutorial_homo_nn.ipynb

运行到此处报错
![image](https://user-images.githubusercontent.com/19608744/153751799-50653b52-cb64-4168-bf5e-725b4556f83a.png)

![8c0931a8bd7be441df54927e41c677b](https://user-images.githubusercontent.com/19608744/153751835-ee56aa00-b982-4594-b169-c2b6dd50d96b.png)

没有安装tf是需要本地安装吗？还是在哪个容器安装？",2,2022-02-13 11:54:15,2022-06-21 09:54:47,2022-03-02 15:24:48
https://github.com/FederatedAI/FATE/issues/3691,[],请问纵向逻辑回归残差保护解决了吗？,"请问纵向逻辑回归残差保护解决了吗？据了解，纵向LR可能会存在标签泄漏问题，请问这个是否可以解决？
![image](https://user-images.githubusercontent.com/13927738/152639722-bfa7b4d5-4607-41d4-967b-57746f535b67.png)
目前来看，DP-HE-Hybird 的方案能很快解决该问题
Hetero-LR(with Arbiter role) implemented in FATE is not same as your description. But the problem above(when rank(xb) =|B|) also existed in Hetero-LR(with Arbiter role). It will be fixed in the next version. Please stay tuned. 
Until a new version is released, you can use hetero-sshe-lr algorithm instead.> 据了解，纵向LR可能会存在标签泄漏问题，请问这个是否可以解决？ ![image](https://user-images.githubusercontent.com/13927738/152639722-bfa7b4d5-4607-41d4-967b-57746f535b67.png) 目前来看，DP-HE-Hybird 的方案能很快解决该问题

您好，有这个图片的出处吗? Thanks!Paper here: https://arxiv.org/pdf/2102.08504.pdf",3,2022-02-05 11:15:43,2022-03-01 05:48:32,2022-03-01 05:48:32
https://github.com/FederatedAI/FATE/issues/3687,['bug'],FixedPointNumber bug,"FixedPointNumber bugFind a bug about FixedPointNumber by following code:
```
from federatedml.secureprotol.fixedpoint import FixedPointNumber
a = FixedPointNumber(encoding=63172158976443095, exponent=10, n=2305843009213693952)
a.decode()
Out[59]: 57454.743888632125
b = FixedPointNumber(encoding=318909341158290850, exponent=13, n=2305843009213693952)
b.decode()
Out[61]: 70.81209866439472
c=a+b
c.decode()
Out[63]: 181.55598729651896
```

The result for c.decode is obviously wrong. Another example:
```
n=2<<60
b_enc = FixedPointNumber.encode(5.4388888888888888888888, n=n, max_int= n//2)
b_enc = FixedPointNumber.encode(0.0000043888888888, n=n, max_int= n//2)
c_enc=a_enc+b_enc
c_enc.decode()
Out[56]: -0.0018893811111099956
```
Also, cann't get a correct result of fate1.7's HeteroPearson while fate1.6 can.  I was trying to locate the bug, then find this bug. 

 HeteroPearson problem is fixed in 1.7.1.  Please read the readme file for 1.7.1
 > HeteroPearson problem is fixed in 1.7.1. Please read the readme file for 1.7.1

Sorry , but I did ran the test with 1.7.1, so please check it again and run the above code from your side and confirm this bug. There are two problems here.
1. The problem of HeteroPearson was triggered by a bug in spdz. The bug was fixed in 1.7.1. You can check HeteroPearson result. IF any problem , please tell us.

2.  The case of FixedPointNumber above code is because of the precision overflown=2<<100
a_enc = FixedPointNumber.encode(5.4388888888888888888888, n=n, max_int= n//2)
b_enc = FixedPointNumber.encode(0.0000043888888888, n=n, max_int= n//2)
c_enc=a_enc+b_enc
c_enc.decode()n=2<<60
a_enc = FixedPointNumber.encode(5.43, n=n, max_int= n//2)
b_enc = FixedPointNumber.encode(0.01, n=n, max_int= n//2)
c_enc=a_enc+b_enc
c_enc.decode()> There are two problems here.
> 
> 1. The problem of HeteroPearson was triggered by a bug in spdz. The bug was fixed in 1.7.1. You can check HeteroPearson result. IF any problem , please tell us.
> 2. The case of FixedPointNumber above code is because of the precision overflow

Yes, I do have a problem with SPDZ. I ran some tests of hetero_pearson with breast_hetero_guest and breast_hetero_host.  If I calculate the cross corr matrix in plaintext, the first row of this matrix is:

```
[ 0.96953704  0.35257222  0.96947443  0.96274416  0.21311972  0.53531431
6541
  0.688235    0.83031597  0.18572735 -0.253691    0.71506376 -0.11169009
6542
  0.69719919  0.75737165 -0.23069028  0.20460675  0.18690313  0.35812593
6543
 -0.12812052 -0.03748756]
```
However, if I run hetero_pearson to calculate it. The first row of this matrix(before taking -1 or 1 as corr thresholds), I get a wrong result like this:
```
[-2.3295853722676476 33.08548396752294 0.6036733275926304
771
  -3.6122762742297967 -1796.6690452298524 55.3797565186993
772
  -0.04546776300293382 -551.7097829655339 -34.08303029413725
773
  3.204695583572869 109.69059633200901 -4.879499589378843
774
  -111.94774360902494 -47.04981841213806 -94.10996973508878
775
  24.815187583328814 -101.18459851591842 96.22118737589734
776
  -0.10476026461448759 47.67639461626979]
```
Also, the result varies every time I run hetero_pearson.

Please verify this from your side. 



  thanks for your feedback. The default value of q_field in spdz is small, so the problem is triggered.
 you can change ： self.q_field = self.public_key.n in spdz file
In the next version, we will fix this bug.",7,2022-01-26 10:29:16,2022-02-14 02:49:55,2022-02-14 02:49:55
https://github.com/FederatedAI/FATE/issues/3680,[],No module named 'tensorflow',"No module named 'tensorflow'我想用纵向神经网络的时候报错No module named 'tensorflow' 
fate1.6 尝试装tensorflow也装不上 
请问有什么解决办法吗tensorflow 安装问题建议去tensorflow官网看看",1,2022-01-25 02:04:20,2022-03-02 15:25:51,2022-03-02 15:25:51
https://github.com/FederatedAI/FATE/issues/3596,"['bug', 'federatedml']",Is Guest `compute_loss` in `hetero lr ` calculated right?,"Is Guest `compute_loss` in `hetero lr ` calculated right?**Is your feature request related to a problem? Please describe.**

I noticed that in fate 1.7.0, the `compute_loss` of Guest in `hetero-lr` has updated. However, I think the code/logic can be optimized.  
The format of  loss `loss = (1/N)*∑(log2 - 1/2*ywx + 1/8*(wx)^2), where y is label, w is model weight and x is features` **is  under the assumption that the label is either -1 or 1.** (Please kindly refer ref[1]). Therefore, when computing `ywx`, it is not necessary to judge `y==1 or not`.

https://github.com/FederatedAI/FATE/blob/6a9c2dd6ff95fcab6f336a2e188a0c58f3777d39/python/federatedml/optim/gradient/hetero_lr_gradient_and_loss.py#L77-L91

**Describe the solution you'd like**
Optimize the logic of calculating loss.

**Describe alternatives you've considered**
I have already submitted a PR. Please kindly refer to #3599 .

NOTE: This update does not change the result. The flowing test are based on the  examples [hetero_lr_normal_conf.json](https://github.com/FederatedAI/FATE/blob/6a9c2dd6ff95fcab6f336a2e188a0c58f3777d39/examples/dsl/v2/hetero_logistic_regression/hetero_lr_normal_conf.json) and  [hetero_lr_normal_dsl.json](https://github.com/FederatedAI/FATE/blob/6a9c2dd6ff95fcab6f336a2e188a0c58f3777d39/examples/dsl/v2/hetero_logistic_regression/hetero_lr_normal_dsljson).

Before optimization, the loss is :
```
[INFO] [2022-01-05 10:07:18,599] [202201051005288375370] [2428:139915861284672] - [base_linear_model_arbiter.fit] [line:129]: iter: 0,  loss:0.6931471805599453, is_converged: False
[INFO] [2022-01-05 10:07:28,858] [202201051005288375370] [2428:139915861284672] - [base_linear_model_arbiter.fit] [line:129]: iter: 1,  loss:50.081458179851246, is_converged: False
[INFO] [2022-01-05 10:07:38,926] [202201051005288375370] [2428:139915861284672] - [base_linear_model_arbiter.fit] [line:129]: iter: 2,  loss:3.4059021475555524, is_converged: False
[INFO] [2022-01-05 10:07:49,056] [202201051005288375370] [2428:139915861284672] - [base_linear_model_arbiter.fit] [line:129]: iter: 3,  loss:0.8378388622788911, is_converged: False
[INFO] [2022-01-05 10:07:59,385] [202201051005288375370] [2428:139915861284672] - [base_linear_model_arbiter.fit] [line:129]: iter: 4,  loss:0.4590097444373968, is_converged: False
[INFO] [2022-01-05 10:08:09,617] [202201051005288375370] [2428:139915861284672] - [base_linear_model_arbiter.fit] [line:129]: iter: 5,  loss:0.38025476990824936, is_converged: False
[INFO] [2022-01-05 10:08:19,102] [202201051005288375370] [2428:139915861284672] - [base_linear_model_arbiter.fit] [line:129]: iter: 6,  loss:0.36010687727146123, is_converged: False
[INFO] [2022-01-05 10:08:29,389] [202201051005288375370] [2428:139915861284672] - [base_linear_model_arbiter.fit] [line:129]: iter: 7,  loss:0.3531603116504674, is_converged: False
[INFO] [2022-01-05 10:08:40,491] [202201051005288375370] [2428:139915861284672] - [base_linear_model_arbiter.fit] [line:129]: iter: 8,  loss:0.34931024599470156, is_converged: False
[INFO] [2022-01-05 10:08:51,619] [202201051005288375370] [2428:139915861284672] - [base_linear_model_arbiter.fit] [line:129]: iter: 9,  loss:0.34617299941223556, is_converged: False
```

After optimization, the loss is : 
```
[INFO] [2022-01-05 11:04:01,939] [202201051103012517110] [27002:139957631616832] - [base_linear_model_arbiter.fit] [line:129]: iter: 0,  loss:0.6931471805599453, is_converged: False
[INFO] [2022-01-05 11:04:06,874] [202201051103012517110] [27002:139957631616832] - [base_linear_model_arbiter.fit] [line:129]: iter: 1,  loss:50.081458179851246, is_converged: False
[INFO] [2022-01-05 11:04:11,948] [202201051103012517110] [27002:139957631616832] - [base_linear_model_arbiter.fit] [line:129]: iter: 2,  loss:3.4059021475555524, is_converged: False
[INFO] [2022-01-05 11:04:16,346] [202201051103012517110] [27002:139957631616832] - [base_linear_model_arbiter.fit] [line:129]: iter: 3,  loss:0.8378388622788911, is_converged: False
[INFO] [2022-01-05 11:04:22,247] [202201051103012517110] [27002:139957631616832] - [base_linear_model_arbiter.fit] [line:129]: iter: 4,  loss:0.45900974443739684, is_converged: False
[INFO] [2022-01-05 11:04:27,171] [202201051103012517110] [27002:139957631616832] - [base_linear_model_arbiter.fit] [line:129]: iter: 5,  loss:0.38025476990824936, is_converged: False
[INFO] [2022-01-05 11:04:33,313] [202201051103012517110] [27002:139957631616832] - [base_linear_model_arbiter.fit] [line:129]: iter: 6,  loss:0.36010687727146123, is_converged: False
[INFO] [2022-01-05 11:04:38,238] [202201051103012517110] [27002:139957631616832] - [base_linear_model_arbiter.fit] [line:129]: iter: 7,  loss:0.35316031165046735, is_converged: False
[INFO] [2022-01-05 11:04:43,996] [202201051103012517110] [27002:139957631616832] - [base_linear_model_arbiter.fit] [line:129]: iter: 8,  loss:0.34931024599470156, is_converged: False
[INFO] [2022-01-05 11:04:48,494] [202201051103012517110] [27002:139957631616832] - [base_linear_model_arbiter.fit] [line:129]: iter: 9,  loss:0.34617299941223556, is_converged: False
```
The above-mentioned log can be found in the `arbiter/DEBUG.log`



**Additional context**
REF[1] The paper 
https://github.com/FederatedAI/FATE/blob/6a9c2dd6ff95fcab6f336a2e188a0c58f3777d39/doc/federatedml_component/logistic_regression.md?plain=1#L46-L47
![image](https://user-images.githubusercontent.com/18736196/148156442-a96c5748-bfc4-4ea4-89f7-f25279743cbb.png)
paillier add operator is faster than scalar multiply. So , add this optim in 1.7.0

OK, I see. However, it brings the cost of two  `applyPartitions` operations. Does this make scene? How should we balance the computation cost between  `applyPartitions`  and paillier add operator?

> paillier add operator is faster than scalar multiply. So , add this optim in 1.7.0

@gxcuit Maybe you can refer to https://github.com/FederatedAI/FATE/blob/v1.6.1/python/federatedml/optim/gradient/hetero_lr_gradient_and_loss.py#L78, 
in older version, fate's developer use the same strategy as your pr. 
But in fate-v1.7.0, the logic changes due to two reason:
    (1) paillier add operator is faster than scalar multiply
    (2) the code in your pr, will reproduce two encrypted table, which is more bigger.
> @gxcuit Maybe you can refer to https://github.com/FederatedAI/FATE/blob/v1.6.1/python/federatedml/optim/gradient/hetero_lr_gradient_and_loss.py#L78, in older version, fate's developer use the same strategy as your pr. But in fate-v1.7.0, the logic changes due to two reason: (1) paillier add operator is faster than scalar multiply (2) the code in your pr, will reproduce two encrypted table, which is more bigger.

@mgqa34 
Thanks for your reply. Now I know that fate-v1.7.0 is much more efficient, and my previous PR does not make scene.

**I still have one question.**
In https://github.com/FederatedAI/FATE/blob/32540492623568ecd1afcb367360133616e02fa3/python/federatedml/optim/gradient/hetero_lr_gradient_and_loss.py#L115-L117

**`host_forward got from host is 0.25 * wx_host`, instead of `wx_host`**, please kindly refer to https://github.com/FederatedAI/FATE/blob/32540492623568ecd1afcb367360133616e02fa3/python/federatedml/optim/gradient/hetero_lr_gradient_and_loss.py#L156-L163

So, `wxg_wxh = 0.25 * wx_host * wx_guest`

Therefore, is  `2 * wxg_wxh` **calculated right? I think it should** be `8* wxg_wxh`, just as I PR in #3599 

Looking for your reply. @mgqa34  @dylan-fan 




Hi,
Thank you for pointing out the error and submitting PR. You are right on the loss calculation. We will check and merge the pr soon.",5,2022-01-05 03:31:24,2022-02-08 05:56:14,2022-02-08 05:56:14
https://github.com/FederatedAI/FATE/issues/3595,"['bug', 'prs-welcome']",FATE1.7.0下载组件输出tracking时报错,"FATE1.7.0下载组件输出tracking时报错FATE环境1.7, 部署方式为Spark, 存储使用的localfs

请求接口: http://192.168.1.225:9380/v1/tracking/component/output/data/download

入参:
{
    ""job_id"": ""202201040605113096980"",
    ""role"": ""guest"",
    ""party_id"": 9999,
    ""component_name"": ""heteroLR_binaryevaluation_0"",
    ""limit"":-1
}

响应: 
{
    ""retcode"": 100,
    ""retmsg"": ""non-hexadecimal number found in fromhex() arg at position 1""
}

程序报错
Traceback (most recent call last):
  File ""/data/projects/fate/fateflow/python/fate_flow/apps/tracking_app.py"", line 260, in component_output_data_all
    for k, v in output_table.collect():
  File ""/data/projects/fate/fate/python/fate_arch/storage/localfs/_table.py"", line 83, in _collect
    yield hdfs_utils.deserialize(line.rstrip())
  File ""/data/projects/fate/fate/python/fate_arch/common/hdfs_utils.py"", line 29, in deserialize
    return fields[0], pickle.loads(bytes.fromhex(tmp_value))
EOFError: Ran out of input

原因是hdfs_utils中, 读取到的内容,看起来是已经结束了的, 但是还在读所以报错, 打印出了读取到的内容如下
`[hdfs_utils.deserialize] [line:28]: fields: crc^@^@^@^B^@-|GH>^_^Eh]oESCl\'4^Nc^SFc;^PY^N^V) i^_9bOQwϹ25^A]wUt^?o+^G
<U+F520>[f(^?M^]#Q^Q-Ge_U/&S &j^KF^Q(>^U͔E)^T^N^]٧iEÑ^Wig;-^PMn^^P^]39m` tmp_value:`

本来是想自己修改掉, 但是不太清楚咱们这边是否有后续逻辑跟进!
找到问题了, 是因为读取了隐藏的.crc文件, 过滤掉就可以了, 在_table.py的第143行加上过滤就正常了 
`if file_info.base_name == ""_SUCCESS"" or file_info.base_name.endswith("".crc""):`欢迎给我们提pr，成为FATE contributor> 找到问题了, 是因为读取了隐藏的.crc文件, 过滤掉就可以了, 在_table.py的第143行加上过滤就正常了
> `if file_info.base_name == ""_SUCCESS"" or file_info.base_name.endswith("".crc""):`
谢谢报告，确实没有处理干净

翻了下hdfs的做法，是把所有`.`开头跟`_`开头的文件都过滤掉
<img width=""648"" alt=""image"" src=""https://user-images.githubusercontent.com/18696809/148174807-0794bf47-2db3-45ab-bd6c-7500f5d13dc0.png"">
我觉得我们合理的做法是遵循这个约定，改成
```
if file_info.base_name.startswith(""."") or file_info.base_name.startswith(""_""):
```
如果你愿意，可以提个pr给我们！
或者我们将在下个版本 `1.7.1`修复这个问题
fix in #3616, thanks report.",4,2022-01-05 03:28:15,2022-01-07 08:15:30,2022-01-07 08:15:30
https://github.com/FederatedAI/FATE/issues/3593,[],serving param is not as expected after deploy pipeline-hetero-sbt model to fate serving,"serving param is not as expected after deploy pipeline-hetero-sbt model to fate servingHi

我按照[教程1](https://github.com/FederatedAI/FATE/blob/6a9c2dd6ff95fcab6f336a2e188a0c58f3777d39/examples/pipeline/hetero_sbt/pipeline-hetero-sbt-binary-with-predict.py) 和 [教程2](https://blog.51cto.com/AIXhao/3312938) 完成了一个纵向联邦学习的完整流程，但是服务参数的特征数少于预期。

我的步骤如 [README](https://github.com/kaiwang0112006/fate_demo/tree/main/vfl001)：

1. 按照demo运行python脚本进行了数据上传和模型训练，并通过 pipeline.dump(""pipeline_saved.pkl"") 保存了模型文件在guest 9998上。
  
        # to docker://10.240.16.39 模拟公司A
        python pipeline_upload.py --name=breast_hetero_host_1 --namespace=breast_hetero_host_1_namespace --file=/data/projects/fate/rootwork/fate_demo/vfl001/breast_hetero_host_1.csv --partition=4 --id_delimiter=',' --guest=9998
        # to docker://10.240.16.40 模拟公司B
        python pipeline_upload.py --name=breast_hetero_host_2 --namespace=breast_hetero_host_2_namespace --file=/data/projects/fate/rootwork/fate_demo/vfl001/breast_hetero_host_2.csv --partition=4 --id_delimiter=',' --guest=9999
        # to docker://10.240.16.39 模拟公司A
        python trainhard.py

2. 编写配置文件如 [README](https://github.com/kaiwang0112006/fate_demo/blob/main/vfl001/README.md) 所示

3. deploy, load和bind模型

        # to docker://10.240.16.39 模拟公司B
        docker exec -it confs-9999_client_1 bash
        flow model deploy --model-id guest-9998#host-9999#model --model-version 202201041038204540850
        
        # to docker://10.240.16.40 模拟公司B
        docker exec -it confs-9999_client_1 bash
        flow model load -c /data/projects/fate/rootwork/fate_demo/vfl001/servering/publish_load_model.json
        
        # to docker://10.240.16.39 模拟公司A
        docker exec -it confs-9998_client_1 bash
        flow model load -c /data/projects/fate/rootwork/fate_demo/vfl001/servering/publish_load_model.json
        flow model bind -c /data/projects/fate/rootwork/fate_demo/vfl001/servering/bind_model_service.json

这一步只有公司A可以bind，在公司B做bind会报错，

    {
    ""retcode"": 104,
    ""retmsg"": ""model 202201041053297198700_guest#9998#guest-9998#host-9999#model is not exist ""
    }

最终在fate-serving可以看到模型服务验证页面

 ![avatar](https://raw.githubusercontent.com/kaiwang0112006/fate_demo/main/vfl001/plot/serving.JPG)

但是我预期纵向联邦学习的模型应该包含两方的特征输入，就像fateboard里看到的:

 ![avatar](https://raw.githubusercontent.com/kaiwang0112006/fate_demo/main/vfl001/plot/model.JPG)

所以我认为模型是对的，但是在部署的过程中有哪些地方有错误吗？I realized that a self-designed adapter is needed (https://fate-serving.readthedocs.io/en/develop/service/adapter/). I'll close this issuse",1,2022-01-04 11:26:32,2022-01-05 06:02:55,2022-01-05 06:02:55
https://github.com/FederatedAI/FATE/issues/3584,[],how to delete uploaded data?,"how to delete uploaded data?I upload data following the instruction of [upload script](https://github.com/FederatedAI/FATE/blob/master/examples/pipeline/demo/pipeline-upload.py)
. After reading the source code of https://github.com/FederatedAI/FATE/blob/6a9c2dd6ff95fcab6f336a2e188a0c58f3777d39/python/fate_client/pipeline/backend/pipeline.py, I cannot find a function to delete the uploaded data. I'm wondering is there a fast way to do that?> I upload data following the instruction of [upload script](https://github.com/FederatedAI/FATE/blob/master/examples/pipeline/demo/pipeline-upload.py) . After reading the source code of https://github.com/FederatedAI/FATE/blob/6a9c2dd6ff95fcab6f336a2e188a0c58f3777d39/python/fate_client/pipeline/backend/pipeline.py, I cannot find a function to delete the uploaded data. I'm wondering is there a fast way to do that?

Hi, You can delete table data with table delete
`flow table delete --name(table name) --namespace`

```
parameter name	required	type	description
name	yes	string	fate table name
namespace	yes	string	fate table namespace
```
ok. thx for the quick support. I'll close this issue> ok. thx for the quick support. I'll close this issue

Note: If you use FATE 1.7+, please change parameter name from  `name` to `table-name`.
You can find the command usage by `flow table info -h`",3,2021-12-30 07:36:11,2021-12-31 09:23:41,2021-12-31 08:48:03
https://github.com/FederatedAI/FATE/issues/3522,"['bug', 'document']",dev-doc: `Fate-flow` need to be restarted after developing/modifying parameters of the model,"dev-doc: `Fate-flow` need to be restarted after developing/modifying parameters of the modelAfter developing/modifying parameters of the model, the `fate-flow` need to be restarted. Otherwise, it will cause `redundant parameter` error as follows:
```
{
    ""retcode"": 100,
    ""retmsg"": ""Component homo_lr_0, module HomoLRMP, has redundant parameter test""
}
```

Currently, the develop doc does not mention this point.The debug mode of fate flow V1.7 can solve this problem, you can start the server through：
""python fate_flow_server.py --debug""> The debug mode of fate flow V1.7 can solve this problem, you can start the server through： ""python fate_flow_server.py --debug""

Ok, I will try it later. Does the develop doc need to mention this point in order to make it more friendly to developers?In. FATE-v1.8, develop doc is modified according to this issue, thanks for your suggestion",3,2021-12-08 06:45:49,2022-04-25 05:58:48,2022-04-25 05:58:48
https://github.com/FederatedAI/FATE/issues/3519,[],Toy test error: Cannot allocate memory ,"Toy test error: Cannot allocate memory 在安装1.7.0镜像后进行toy测试。出现报错
lmdb.MemoryError: /data/projects/fate/data/202112070925038649660_secure_add_example_0_0_guest_10000/26262630-5740-11ec-afef-0242ac110002/2: Cannot allocate memory

另外在进行任何测试时  打开浏览器输入127.0.0.1:8080或本地ip：8080均无法打开fateboard

设备内存32G 


toy test job 202112070925038649660 is running
toy test job 202112070925038649660 is failed
[ERROR] [2021-12-07 09:30:16,653] [202112070925038649660] [2882:274907579520] - [task_executor._run_] [line:243]: /data/projects/fate/data/202112070925038649660_secure_add_example_0_0_guest_10000/26262630-5740-11ec-afef-0242ac110002/2: Cannot allocate memory
Traceback (most recent call last):
File ""/data/projects/fate/fateflow/python/fate_flow/worker/task_executor.py"", line 195, in _run_
cpn_output = run_object.run(cpn_input)
File ""/data/projects/fate/fate/python/federatedml/toy_example/secure_add_guest.py"", line 107, in run
self.sync_share_to_host()
File ""/data/projects/fate/fate/python/federatedml/toy_example/secure_add_guest.py"", line 85, in sync_share_to_host
idx=0)
File ""/data/projects/fate/fate/python/fate_arch/federation/transfer_variable.py"", line 279, in remote
return self.remote_parties(obj=obj, parties=parties, suffix=suffix)
File ""/data/projects/fate/fate/python/fate_arch/federation/transfer_variable.py"", line 193, in remote_parties
v=obj, name=name, tag=tag, parties=parties, gc=self._remote_gc
File ""/data/projects/fate/fate/python/fate_arch/federation/standalone/_federation.py"", line 39, in remote
return self._federation.remote(v=v, name=name, tag=tag, parties=parties)
File ""/data/projects/fate/fate/python/fate_arch/_standalone.py"", line 565, in remote
name=saved_name, namespace=v.namespace, need_cleanup=False
File ""/data/projects/fate/fate/python/fate_arch/_standalone.py"", line 281, in save_as
dup.put_all(self.collect())
File ""/data/projects/fate/fate/python/fate_arch/_standalone.py"", line 301, in put_all
for k, v in kv_list:
File ""/data/projects/fate/fate/python/fate_arch/_standalone.py"", line 104, in collect
env = s.enter_context(self._get_env_for_partition(p))
File ""/data/projects/fate/fate/python/fate_arch/_standalone.py"", line 285, in _get_env_for_partition
return _get_env(self._namespace, self._name, str(p), write=write)
File ""/data/projects/fate/fate/python/fate_arch/_standalone.py"", line 800, in _get_env
return _open_env(_path, write=write)
File ""/data/projects/fate/fate/python/fate_arch/_standalone.py"", line 824, in _open_env
raise e
File ""/data/projects/fate/fate/python/fate_arch/_standalone.py"", line 816, in _open_env
map_size=10_737_418_240,
lmdb.MemoryError: /data/projects/fate/data/202112070925038649660_secure_add_example_0_0_guest_10000/26262630-5740-11ec-afef-0242ac110002/2: Cannot allocate memory

![1638871139(1)](https://user-images.githubusercontent.com/24326352/145007997-3992dcbd-bf22-4d89-a536-a4b5d9454cd9.png)",1,2021-12-07 09:40:52,2021-12-09 00:52:43,2021-12-09 00:52:43
https://github.com/FederatedAI/FATE/issues/3518,[],Fate On Spark ，上传大数据量文件到HDFS，会发生OOM,"Fate On Spark ，上传大数据量文件到HDFS，会发生OOM**Describe the bug**
将本地一个大小300G的文件，通过fate_client上传到HDFS上，查看HDFS Browse Directory，在HDFS上文件差不多会有2倍的膨胀（相对于本地原始文件），当最终感觉文件上传完毕之后(查看H DFS上文件大小不再变化)，fateboard任务还在继续运行，查看fate_flow所在服务器会看到使用内存在一直增大（这个使用内存一直增大在文件上传前期是不存在的），直至发生OOM

使用的服务器是centos 7.3 \32core\128G\磁盘是1000G

我怀疑是由于文件上传最终对上传的文件进行count导致，拉取所有文件到内存所致hdfs大文件出现OOM问题已经在1.8.0版本修复",1,2021-12-07 07:56:41,2022-04-28 13:00:12,2022-04-28 13:00:12
https://github.com/FederatedAI/FATE/issues/3516,['bug'],Bug and Ferture Request for `flow_client`: Relative path issue in `upload-conf.json` when uploading data.,"Bug and Ferture Request for `flow_client`: Relative path issue in `upload-conf.json` when uploading data.**Describe the bug**
一个Bug，一个Feature request。

** 1 Bug：**
在使用`flow data upload`上传文件的时候，配置文件`""file"":`选项，如果使用相对路径`../`访问上级目录会有问题，而`./`则正常。这是下述代码导致的：
详情见 `cli_utils.py` --》`def check_abs_path(path)`方法: 只是简单的进行替换:

```
def check_abs_path(path):
    if not os.path.isabs(path):
        return os.path.join(os.path.abspath(os.curdir), path).replace(""./"", """")
    return path
```

** 2 Ferture Request**
在处理`upload-conf.json`涉及相对路径的时候，有些confusing。目前实现的是：相对路径是相对于flow-client运行的路径。而我感觉，既然相对路径是在`upload-conf.json`中定义的，那么相对路径应该是相对于`upload-conf.json`这个文件所在的目录，而不是`flow-client` 工作目录。 

目前如果`upload.json`中`file`使用相对路径，当`flow-client` 工作在不同目录（其他配置都不变），会有bug。



**Additional context**
不知道我的表述是否清楚？是否reasonable？If so, may I submit a PR?
https://github.com/FederatedAI/FATE/commit/8a6ec59c509508764e8c711984d1f072d112d8ab#diff-aa637d9a5cb67f9f347398e9c443342517a6b317b498f757968f409a4ca670a7R135

不太确定为什么加 `replace(""./"", """")`，看起来像个 bug

这里如果是相对路径的话，应该是相对于当前工作目录的路径，这个逻辑还是先不要动了",1,2021-12-06 15:29:10,2021-12-09 12:55:21,2021-12-09 12:55:12
https://github.com/FederatedAI/FATE/issues/3515,[],python容器一直重启，报错：Cannot assign requested address,"python容器一直重启，报错：Cannot assign requested address安装版本：kubefate1.5.0    serving: 2.0.0  集群部署
问题详情： python容器一直重启，logs报错如下：
{""created"":""@1638784764.237329786"",""description"":""No address added out of total 1 resolved"",""file"":""src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":394,""referenced_errors"":[{""created"":""@1638784764.237323739"",""description"":""Unable to configure socket"",""fd"":13,""file"":""src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":217,""referenced_errors"":[{""created"":""@1638784764.237316178"",""description"":""Cannot assign requested address"",""errno"":99,""file"":""src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":190,""os_error"":""Cannot assign requested address"",""syscall"":""bind""}]}]}
尝试解决：（看到有朋友说需要修改IP，我不太懂这个IP指的是哪里的IP），因此尝试修改docker-compose.yml文件中 ipv4_address: 192.167.0.100（具体位置如下），并将其修改为本机ip和其他任意地址后，并重新使用docker-compose up 重新部署后也没有解决这个问题。

 ` python:
    image: ""federatedai/python:${TAG}""
    ports:
      - ""9360:9360""
      - ""9380:9380""
    restart: always
    volumes:
      - shared_dir_federatedml:/data/projects/fate/python/federat
      - shared_dir_examples:/data/projects/fate/examples
      - download_dir:/data/projects/fate/python/download_dir
      - ./confs/fate_flow/conf:/data/projects/fate/conf
      - ./confs/eggroll/conf:/data/projects/fate/eggroll/conf
      - fate_flow_logs:/data/projects/fate/logs
    depends_on:
      - mysql
      - rollsite
      - clustermanager
      - nodemanager
    networks:
      fate-network:
        ipv4_address: 192.167.0.100
`


请问有朋友知道这个问题应该怎么解决吗，感谢感谢~我上次看到这个问题，是我setup.conf配置文件中没有改各个hostip为真实ip造成的。",1,2021-12-06 11:50:35,2022-02-24 09:05:45,2022-02-24 09:05:45
https://github.com/FederatedAI/FATE/issues/3507,[],Is there any way to define different model parameters for host/guest/arbiter sepreately？,"Is there any way to define different model parameters for host/guest/arbiter sepreately？**Is your feature request related to a problem? Please describe.**
当我自定义model的时候，有没有方法可以为host、guest、arbiter 分别指定 parameter class？ 目前他们公用一个parameter class，如果我想让host、guest、arbiter处理不同的参数，目前的办法好像只有设置默认值？

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
You are right, fate in current version (<=v1.7) does not support defining  different parameter object for different role, to achieve this, all parameters should be included in the same component parameter object, then users can specified the role parameters in component::role:: filed in  job runtime config. 
If many developers or users want this feature, we will take it into consideration in fate v2.0+.",1,2021-12-03 15:03:18,2021-12-13 07:33:25,2021-12-13 07:33:25
https://github.com/FederatedAI/FATE/issues/3503,['bug'],Intersection component RSA computing does not run properly in parallel,"Intersection component RSA computing does not run properly in parallel**Describe the bug**
In FATE1.7.0 standalone,
I use parameters `intersect_0 = Intersection(name=""intersection_0"", rsa_params={
                                      ""hash_method"": ""sha256"",
                                      ""final_hash_method"": ""sha256"",
                                      ""split_calculation"": False,
                                      ""random_bit"": 128,
                                      ""key_length"": 2048})` 
to run `example/pipeline/demo/pipeline-quick-demo.py` which train_data is replaced with default_credit_hetero_guest/host.

However, rsa computings in `fate/python/federatedml/statistic/intersect/rsa_intersect/rsa_intersect_host.py` are not parallel.
Includes:
Line 158 `prvkey_ids_process_pair = self.cal_prvkey_ids_process_pair`
Line 178 `host_sign_guest_ids = guest_pubkey_ids.map(lambda k, v: (k, self.sign_id(...`
They both use CTable.map() and submit tasks to process pool. However, `LOGGER.info(os.getpid())` shows LOGs are likes:
pid = 60489
pid = 60489
pid = 60489
... rather than showing a cluttered id in true parallel.
I found this because the time of these RSA operations did not change after I changed the task_cores parameter from 4 to 16.
Is this a bug?


**To Reproduce**
Steps to reproduce the behavior:
1. FATE 1.7.0 standalone - only change conf about task_cores = 4(16), cores_per_node: 128
2. add `LOGGER.info(""intersection tid = {}"".format(os.getpid()))` in `fate/python/federatedml/statistic/intersect/rsa_intersect/rsa_intersect_base.py` Line 149 
3. run `example/pipeline/demo/pipeline-quick-demo.py` with train_data = default_credit_hetero_guest/host


**ScreenShot**
In secure boost component, paillier encrypt, CTable.mapvalue(encrypt) is in parallel
![pic1](https://user-images.githubusercontent.com/48517447/144534814-3b1b5501-e76d-4cdf-989f-a9f975927696.png)

Howver in RSA-intersection component, it shows:
![pic2](https://user-images.githubusercontent.com/48517447/144534871-37c22b5e-8f0c-4d99-83c6-08570ffbcf40.png)
>However, rsa computings in fate/python/federatedml/statistic/intersect/rsa_intersect/rsa_intersect_host.py are not parallel.
No, standalone mode is implemented parallel and I could not reproduce your `same pid` issue. What I did found is that `RSA` create so may partition in that stage and tid seemed all same like this:
```
root /fate (master) $ cat fateflow/logs/202112281127357348520/host/10000/intersection_0/INFO.log  | grep tid
[INFO] [2021-12-28 11:27:53,531] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,532] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,532] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,532] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,533] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,533] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,533] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,533] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,533] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,534] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,534] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,534] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,534] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,534] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,535] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,535] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,535] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,535] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,535] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,536] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,536] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,536] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,536] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,536] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,537] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,537] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,537] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,537] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,537] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,538] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,538] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,538] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,538] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,538] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,539] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,539] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,539] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,539] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,539] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,540] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,540] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,540] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,540] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,540] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,541] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,541] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,541] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,541] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,541] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,542] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,542] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,542] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,542] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,542] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,543] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,543] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,543] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,543] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,543] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,543] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,544] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,544] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,544] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,544] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,544] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,545] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,545] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,545] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,545] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,545] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,546] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,546] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,546] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,546] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,546] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,547] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,547] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,547] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,547] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,547] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,548] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,548] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,548] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,548] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,548] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,549] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,549] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,549] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,549] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,549] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,550] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,550] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,550] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,550] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,550] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,551] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,551] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,551] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,551] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,551] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,552] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,552] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,552] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,552] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,552] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,553] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,553] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,553] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,553] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,553] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,554] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,554] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,554] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,554] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,554] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,555] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,555] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,555] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,555] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,555] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,556] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,556] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,556] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,556] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,556] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,557] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,557] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,557] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,557] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,557] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,558] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,558] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,558] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,558] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,558] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,559] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,559] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,559] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,559] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,559] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,560] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,560] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,560] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,560] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,560] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,561] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,561] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,561] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,561] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,561] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,562] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,562] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,562] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,562] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,562] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,563] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,563] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,563] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,563] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,563] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,564] [202112281127357348520] [19031:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19031
[INFO] [2021-12-28 11:27:53,585] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,585] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,585] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,586] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,586] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,586] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,586] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,586] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,587] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,587] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,587] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,587] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,587] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,588] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,588] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,588] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,588] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,588] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,589] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,589] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,589] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,589] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,589] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,590] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,590] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,590] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,590] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,590] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,591] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,591] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,591] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,591] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,591] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,592] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,592] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,592] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,592] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,592] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,593] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,593] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,593] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,593] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,593] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,594] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,594] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,594] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,594] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,594] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,595] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,595] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,595] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,595] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,595] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,596] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,596] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,596] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,596] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,596] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,597] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,597] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,597] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,597] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,597] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,598] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,598] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,598] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,598] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,598] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,598] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,599] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,599] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,599] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,599] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,599] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,600] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,600] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,600] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,600] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,600] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,601] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,601] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,601] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,601] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,601] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,602] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,602] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,602] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,602] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,602] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,603] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,603] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,603] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,603] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,603] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,604] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,604] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,604] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,604] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,604] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,605] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,605] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,605] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,605] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,605] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,606] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,606] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,606] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,606] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,606] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,607] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,607] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,607] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,607] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,607] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,608] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,608] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,608] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,608] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,608] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,609] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,609] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,609] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,609] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,609] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,610] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,610] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,610] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,610] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,610] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,611] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,611] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,611] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,611] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,611] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,612] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,612] [202112281127357348520] [19034:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19034
[INFO] [2021-12-28 11:27:53,634] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,635] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,636] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,636] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,637] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,637] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,637] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,638] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,638] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,639] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,639] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,639] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,640] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,640] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,641] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,641] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,641] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,642] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,642] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,643] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,643] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,643] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,644] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,644] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,645] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,645] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,645] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,646] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,646] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,646] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,647] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,647] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,648] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,648] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,649] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,649] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,649] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,650] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,650] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,650] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,651] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,651] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,652] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,652] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,653] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,653] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,653] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,654] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,654] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,654] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,655] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,655] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,656] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,656] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,656] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,657] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,657] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,658] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,658] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,658] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,659] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,659] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,660] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,660] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,660] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,661] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,661] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,662] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,662] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,662] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,663] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,663] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,664] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,664] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,664] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,665] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,665] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,666] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,666] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,667] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,667] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,667] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,668] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,668] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,669] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,669] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,669] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,670] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,670] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,671] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,671] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,671] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,672] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,672] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,673] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,673] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,673] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,674] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,674] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,675] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,675] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,675] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,676] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,676] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,677] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,677] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,677] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,678] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,678] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,678] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,679] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,679] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,680] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,680] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,680] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,681] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,681] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,682] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,682] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,682] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,683] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,683] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,684] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,684] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,684] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,685] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,685] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,686] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,686] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,686] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,687] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,687] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,687] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,688] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,688] [202112281127357348520] [19032:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19032
[INFO] [2021-12-28 11:27:53,711] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,712] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,713] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,713] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,713] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,714] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,714] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,715] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,715] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,716] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,716] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,716] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,717] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,717] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,718] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,718] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,718] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,719] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,719] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,719] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,720] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,720] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,721] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,721] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,722] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,722] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,722] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,723] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,723] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,723] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,724] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,724] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,725] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,725] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,726] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,726] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,726] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,727] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,727] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,728] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,728] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,728] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,729] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,729] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,729] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,730] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,730] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,731] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,731] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,731] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,732] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,732] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,733] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,733] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,733] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,734] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,734] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,735] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,735] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,736] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,736] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,736] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,737] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,737] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,738] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,738] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,738] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,739] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,739] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,739] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,740] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,740] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,741] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,741] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,742] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,742] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,742] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,743] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,743] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,744] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,744] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,744] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,745] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,745] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,746] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,746] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,746] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,747] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,747] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,748] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,748] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,748] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,749] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,749] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,750] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,750] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,750] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,751] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,751] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,752] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,752] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,753] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,753] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,753] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,754] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,754] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,754] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,755] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,755] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,756] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,756] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,757] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,757] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,757] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,758] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,758] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,759] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,759] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,759] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,760] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,760] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,761] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,761] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,761] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,762] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,762] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,762] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,763] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,763] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,763] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,764] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,764] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,764] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,765] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,765] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,765] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
[INFO] [2021-12-28 11:27:53,766] [202112281127357348520] [19033:139693507371392] - [rsa_intersect_base.prvkey_id_process] [line:148]: intersection tid = 19033
```
>I found this because the time of these RSA operations did not change after I changed the task_cores parameter from 4 to 16.
Is this a bug?

Yes, it's a bug in a sense. We use ProcessExecutorPool with num_worker=None https://github.com/FederatedAI/FATE/blob/6a9c2dd6ff95fcab6f336a2e188a0c58f3777d39/python/fate_arch/_standalone.py#L342
and according to [this](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor), num_worker is set to `5 * processor`. I'll fix this issue in next release version to makes `task_cores` parameter take effect in standalone mode.

By the way, since `standalone mode` is implemented as a `light development test environment`, it's not suite for `benchmark test`.",1,2021-12-02 12:03:36,2022-04-15 09:46:36,2022-04-15 09:46:36
https://github.com/FederatedAI/FATE/issues/3498,[],What is the function in `self.model_param` when developing  a new algorithm component?,"What is the function in `self.model_param` when developing  a new algorithm component?**Describe the bug**
Hi, 我在[根据教程](https://fate.readthedocs.io/en/latest/develop/develop_guide)自定义新的component的时候，遇到了一个问题。如果没有在`__init__` 方法中给`self.model_param`赋值的话，则无法通过`params` 获得参数。教程中并没有给出这个步骤，是代码的问题还是教程文档的问题。

下面以我想重新开发`homo_lr` 为例。
**To Reproduce**
Steps to reproduce the behavior:
1. 复制一份`federatedml/param/logistic_regression_param.py`， 并将其中的`class HomoLogisticParam` 重命名为`class PoisonParam`。其余都不变。
2. 修改conf文件
3. 如果仅做这些修改，打印的log显示，params的类型没有改变，并且参数值未传递
`[DEBUG] [2021-12-02 10:12:42,083] [5866:140442401756992] - homo_lr_host_mp.py[line:65]: params is <federatedml.param.logistic_regression_param.HomoLogisticParam object at 0x7fbb34c8b358>
`
但component已经收到数据，并且类型为PoisonParam
`[DEBUG] [2021-12-02 10:12:41,319] [5866:140442401756992] - component_properties.py[line:73]: {'PoisonParam': {'penalty': 'L2', 'tol': 1e-05, 'alpha': 0.01, 'optimizer': 'sgd', 'batch_size': -1, 'learning_rate': 0.15, 'init_param': {'init_method': 'zeros', 'init_const': 1, 'fit_intercept': True, 'random_seed': None}, 'max_iter': 18, 'early_stop': 'diff', 'encrypt_param': {'method': None, 'key_length': 1024}, 'predict_param': {'threshold': 0.5}, 'cv_param': {'n_splits': 5, 'mode': 'hetero', 'role': 'guest', 'shuffle': True, 'random_seed': 1, 'need_cv': False, 'output_fold_history': True, 'history_value_type': 'score'}, 'decay': 1, 'decay_sqrt': True, 'multi_class': 'ovr', 'validation_freqs': 1, 'stepwise_param': {'score_name': 'AIC', 'mode': 'hetero', 'role': 'guest', 'direction': 'both', 'max_step': 10, 'nvmin': 2, 'nvmax': None, 'need_stepwise': False}, 'early_stopping_rounds': None, 'metrics': ['auc', 'ks'], 'use_first_metric_only': False, 'floating_point_precision': 23, 're_encrypt_batches': 2, 'aggregate_iters': 1, 'use_proximal': False, 'mu': 0.1, 'poison': True}, 'dsl_version': 2, 'initiator': {'role': 'guest', 'party_id': 10000}, 'role': {'guest': [10000], 'host': [9999], 'arbiter': [9999]}, 'job_parameters': {'job_type': 'train', 'work_mode': 0, 'backend': 0, 'computing_engine': 'STANDALONE', 'federation_engine': 'STANDALONE', 'storage_engine': 'STANDALONE', 'engines_address': {'computing': {'nodes': 1, 'cores_per_node': 20}, 'federation': {'nodes': 1, 'cores_per_node': 20}, 'storage': {'nodes': 1, 'cores_per_node': 20}}, 'federated_mode': 'SINGLE', 'task_parallelism': 1, 'computing_partitions': 4, 'federated_status_collect_type': 'PULL', 'model_id': 'arbiter-9999#guest-10000#host-9999#model', 'model_version': '202112021012005851198', 'eggroll_run': {'eggroll.session.processors.per.node': 4}, 'spark_run': {}, 'rabbitmq_run': {}, 'pulsar_run': {}, 'adaptation_parameters': {'task_nodes': 1, 'task_cores_per_node': 4, 'task_memory_per_node': 0, 'request_task_cores': 4, 'if_initiator_baseline': False}}, 'component_parameters': {'common': {'dataio_0': {'with_label': True, 'output_format': 'dense'}, 'dataio_1': {'with_label': True, 'output_format': 'dense'}, 'homo_lr_0': {'penalty': 'L2', 'tol': 1e-05, 'alpha': 0.01, 'optimizer': 'sgd', 'batch_size': -1, 'learning_rate': 0.15, 'init_param': {'init_method': 'zeros'}, 'max_iter': 18, 'aggregate_iters': 1, 'early_stop': 'diff', 'encrypt_param': {'method': None}, 'poison': True, 'validation_freqs': 1}, 'evaluation_0': {'eval_type': 'binary'}}, 'role': {'guest': {'0': {'reader_0': {'table': {'name': 'breast_homo_guest', 'namespace': 'experiment'}}, 'reader_1': {'table': {'name': 'homo_breast_test', 'namespace': 'experiment'}}}}, 'host': {'0': {'reader_0': {'table': {'name': 'breast_homo_host', 'namespace': 'experiment'}}, 'reader_1': {'table': {'name': 'homo_breast_test', 'namespace': 'experiment'}}, 'evaluation_0': {'need_run': True}}}}}, 'conf_path': 'homo_lr_mp_conf.json', 'dsl_path': 'homo_lr_mp_dsl.json', 'local': {'role': 'host', 'party_id': 9999}, 'CodePath': 'federatedml/linear_model/logistic_regression/homo_logsitic_regression/homo_lr_host_mp.py/HomoLRHost', 'module': 'HomoLRMP', 'output_data_name': ['data']}`

注：尽管component 收到runtime conf的参数，但实际训练的时候仍然使用默认值。

如果将`homo_lr_base.py` 的`__init__`函数中的`self.model_param = HomoLogisticParam()`进行修改，则可解决这个问题。

**Expected behavior**
是否应该在文档中加上修改self.model_param的这个步骤？
**Additional context**
Add any other context about the problem here.
Yes, you're right,  thanks for pointing out. Can you make a pull request to fix this, or if you have no time, we will fix it later. > Yes, you're right, thanks for pointing out. Can you make a pull request to fix this, or if you have no time, we will fix it later.

Hi, Thanks for your reply.
I will make a pull request.",2,2021-12-02 02:39:21,2021-12-06 04:38:12,2021-12-06 04:38:12
https://github.com/FederatedAI/FATE/issues/3474,[],When load model，grpc connected fail,"When load model，grpc connected failFATE v1.6.1 docker-compose deploy
I followed the tutorial in the [doc](https://github.com/FederatedAI/KubeFATE/blob/master/docker-deploy/README_zh.md) to do the hetero_lr_job. It succeeded.

I changed `conf/service_conf.yaml`
```servings:
  hosts:
    - 192.168.0.108:8000
    - 192.168.0.107:8000
```

After I changed the `examples/publish_load_model.json` with my model version and model id, I executed 
`python fate_flow_client.py -f load -c examples/publish_load_model.json`
I got the error the following
![fa782168b2be2266cff118540e16bb4](https://user-images.githubusercontent.com/33534276/143513851-035552dd-6117-4d40-ad09-d04081eacedc.png)

I found the log in the guest and the exception code. It seems that the gRPC connection fail for 3 times. 
![9d7e898cc4366395215a3902447c6ed](https://user-images.githubusercontent.com/33534276/143513243-c1136dce-60a8-4048-9a90-6ad2d8c1dfb9.png)
![7736dd5dc6b4c6ce255c342ca12aac0](https://user-images.githubusercontent.com/33534276/143513292-cabf891e-f8cc-450b-bdb5-542e87dfdd5e.png)
How can I fix it?

Change the docker-compose.yml ( file path is `/data/projects/fate/serving-${party_id}` ) of every node，and restart the containers.
The fate-network should be as follow
![f2f21491fbd22af3dae6548d9fc9816](https://user-images.githubusercontent.com/33534276/143682687-728a7b9c-9d7a-4a5a-8764-84d576995c7c.png)

Restart container commands. 
```
cd /data/projects/fate/serving-${party_id}
docker-compose down
docker-compose up -d
```",1,2021-11-26 01:50:46,2021-11-27 13:16:16,2021-11-27 13:16:16
https://github.com/FederatedAI/FATE/issues/3469,[],Cannot successfully cluster-deploy 1.7.0 allinone,"Cannot successfully cluster-deploy 1.7.0 allinone### Env
OS: CentOS Linux release 7.9.2009 (Core)
Pkg：fate_cluster_install_1.7.0_release-c7-u18.tar.gz
Install method: cluster-deploy/doc/fate_on_eggroll/fate-allinone_deployment_guide.zh.md

Note: All preparations have been finished and passed the check script. 

### Replicate
After  installation of a single-party host. I execure:
""flow test toy -gid 9998 -hid 9998""

Then the following errors occured:
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
keyring 23.2.1 requires SecretStorage>=3.2, which is not installed.
fate-test 0.1 requires pandas>=1.1.5, which is not installed.
**keyring 23.2.1 requires importlib-metadata>=3.6, but you have importlib-metadata 1.7.0 which is incompatible.
markdown 3.3.6 requires importlib-metadata>=4.4, but you have importlib-metadata 1.7.0 which is incompatible.**

### Trials
I tried to upgrade importlib-metadata, but the following error occurs:

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/common/python/venv/bin/flow"", line 33, in <module>
    sys.exit(load_entry_point('fate-client==0.1', 'console_scripts', 'flow')())
  File ""/data/projects/fate/common/python/venv/bin/flow"", line 25, in importlib_load_entry_point
    return next(matches).load()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 194, in load
    module = import_module(match.group('module'))
  File ""/data/projects/fate/common/miniconda3/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/fate_client-0.1-py3.6.egg/flow_client/flow.py"", line 20, in <module>
    from ruamel import yaml
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/ruamel.yaml-0.16.13-py3.6.egg/ruamel/__init__.py"", line 1, in <module>
    __import__('pkg_resources').declare_namespace(__name__)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 3238, in <module>
    @_call_aside
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 3222, in _call_aside
    f(*args, **kwargs)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 3251, in _initialize_master_working_set
    working_set = WorkingSet._build_master()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 569, in _build_master
    return cls._build_from_requirements(__requires__)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 582, in _build_from_requirements
    dists = ws.resolve(reqs, Environment())
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 770, in resolve
    raise DistributionNotFound(req, requirers)
**pkg_resources.DistributionNotFound: The 'importlib-metadata<2.0.0,>=1.6.0' distribution was not found and is required by poetry**

Does any successfully deployed the new version? 
Should i use centos 7.2 ?Hello, you need to execute source /data/projects/fate/bin/init_env.sh before you deploy and run toy. The version in the information is higher than the version provided by the deployment package. For example, the keyring is 21.8.0 and the markdown is 3.3. 5. Has there been any manual upgrade or installation? It is recommended to clean up the environment and redeploy it. > 
1. I didn't notice that ""source /data/projects/fate/bin/init_env.sh ""before  you deploy, if you mean deploy is install fate.
2. It's obvious that i executed ""source ..."" before i run this toy example. 
Thanks for your reply, i'll try to redeploy it.execute source /data/projects/fate/bin/init_env.sh before run toy, mainly to solve the problem of why the version of the dependent package is higher than the version in the deployment package. You can re-pull the package, and then also clean up the files in the /data/temp directory. Successfully deployed on CentOS 7.4, thanks a lot!",4,2021-11-24 07:03:22,2021-11-26 02:53:32,2021-11-26 02:53:32
https://github.com/FederatedAI/FATE/issues/3457,[],request for supporting re-run job/component,"request for supporting re-run job/component**Is your feature request related to a problem? Please describe.**
当我修改job的参数的时候，我想重新运行这个job，或者重新运行具体的component。目前的版本（1.6.0），我只能重新提交一个job，这将生成新的job，以及对应job-id。对开发者来说，维护海量job不是很友好。

**Describe the solution you'd like**
可以重新运行job，或job里具体的某一个组件。在fate-board中，不生成新的job-id、

**Describe alternatives you've considered**
目前我的方法是，手动调用task_executor.py，并指定参数

**Additional context**
Add any other context or screenshots about the feature request here.
这个可以自己写代码实现， 1.7里面好像已经支持了，但是不支持修改组件后自动创建组件及其下游组件的新版本，这个功能我实现了。需要修改他们源代码> 这个可以自己写代码实现， 1.7里面好像已经支持了，但是不支持修改组件后自动创建组件及其下游组件的新版本，这个功能我实现了。需要修改他们源代码

1.7 里如何重新运行component？ 1.6 之前的版本可以直接 调用 `task_executor` ， 1.7 之后不行了",2,2021-11-23 07:31:53,2022-04-13 05:58:22,2022-04-13 05:58:22
https://github.com/FederatedAI/FATE/issues/3318,[],2个bug,"2个bug**Describe the bug**
1. homo_feature_binning 中数据未进行转换
2. 手动指定category字段时，bucket分箱统计结果顺序问题
提交PR :https://github.com/FederatedAI/FATE/pull/3317

Copy the reply from #3317 
Q1: we have already fixed in develop-1.7 before sep.26, https://github.com/FederatedAI/FATE/blob/develop-1.7/python/federatedml/feature/homo_feature_binning/homo_binning_cpn.py#L56
Q2: In our design, the order of col_index is not importance, the returning result will always start from smaller column to bigger one.

If there are no more reply, this issue and the corresponding pr will be closed soon",1,2021-11-09 07:17:05,2021-11-12 10:45:00,2021-11-12 10:45:00
https://github.com/FederatedAI/FATE/issues/3308,[],[HomoLR]Why Fate add RandomPads to Paillier Encrypted Model directly without Encrypting RandomPads?,"[HomoLR]Why Fate add RandomPads to Paillier Encrypted Model directly without Encrypting RandomPads?Let us use a binary situation as an example,
RandomPadsCypher will add randA to Model A, add randB to Model b, 
where randA plus randB equals zero. (indeed i know its not safe to use secure aggregator in binary situation, just for example)

if we call pallier function P(x)
as far as i know, P(modelA) + P(randA) = P(modelB)+P(randB)
while P(modelA) + randA != P(modelB)+randB
But in Fate, it seems like randomPads will add to Paillier encrypted model directly,
and i couldnt find the part of code to encrypt those random pads
Did i miss somthing? 
Or there is 'P(0）=0' in pallier algorithm？1. if model $X$ is encrypted using `Pallier`, $X + a$ means $X + encrypt(a)$, which is implemented using python's magic method  `__add__`
2. if model is plain, adding random padding is sufficent to cover the original infomation, which is what `SecureAggregation` do. > 1. if model $X$ is encrypted using `Pallier`, $X + a$ means $X + encrypt(a)$, which is implemented using python's magic method  `__add__`
> 2. if model is plain, adding random padding is sufficent to cover the original infomation, which is what `SecureAggregation` do.

Thanks a lot, I read __add_scalar function before, but didnt notice that there is other more important code in __add_fixpointnumber,
```python
    def __add_fixpointnumber(self, encoded):
        """"""return PaillierEncryptedNumber: z = E(x) + FixedPointNumber(y)
        """"""
        if self.public_key.n != encoded.n:
            raise ValueError(""Attempted to add numbers encoded against different public keys!"")

        # their exponents must match, and align.
        x, y = self.__align_exponent(self, encoded)

        encrypted_scalar = x.public_key.raw_encrypt(y.encoding, 1)
        encryptednumber = self.__raw_add(x.ciphertext(False), encrypted_scalar, x.exponent)

        return encryptednumber
```
`return PaillierEncryptedNumber: z = E(x) + y`
This annotion misslead me.
maybe it should be replaced with `return PaillierEncryptedNumber: z = E(x) + E(scalar)`?",2,2021-11-08 13:00:20,2021-11-12 08:54:30,2021-11-12 08:54:30
https://github.com/FederatedAI/FATE/issues/3284,[],任务失败，fate board不显示报错信息,"任务失败，fate board不显示报错信息![image](https://user-images.githubusercontent.com/47730754/140473049-3268b3f5-780d-40f4-958a-f331e3484018.png)
![image](https://user-images.githubusercontent.com/47730754/140473087-4becabb6-fa40-4f48-89b6-53ffa65ee469.png)

![image](https://user-images.githubusercontent.com/47730754/140473171-35e5207b-db78-48bb-872d-80d780fd3ec5.png)

跑的是官方例子pipeline-hetero-sbt-binary-with-missing-value.py做的修改，跑自己的数据，就几十条。但不知道为啥，任务失败，但是fate board不显示报错信息，这样子没法排查May be the error is in host,  give more information  of figure 2. Click the tag ""Schedule Log"". you are right,the error  is in guest,not in host.",2,2021-11-05 07:23:24,2022-04-28 13:01:32,2022-04-28 13:01:32
https://github.com/FederatedAI/FATE/issues/3243,[],HeteroLR 同步式和异步式相关问题,"HeteroLR 同步式和异步式相关问题看代码在有两个及以上host时使用_centralized_compute_gradient(hetero_linear_model_gradient.py)，其中guest方在计算fore_gradient时，使用自身的half_d以及host方发送来的host_forward，得到的fore_gradient结果发送给各host方，其中的half_d在代码中是未经加密的，这样的方式不会造成信息的泄露吗？在使用异步式计算时half_d时经过加密再发送的，在同步和异步的选择上是基于什么考虑的？host_forward是加密的，因此得到fore_gradient以后，fore_gradient是加密的。而在异步状态下，需要把half_d直接发送出去，所以需要额外加密一下。> host_forward是加密的，因此得到fore_gradient以后，fore_gradient是加密的。而在异步状态下，需要把half_d直接发送出去，所以需要额外加密一下。

是不是可以理解为未加密的half_d和加密过后的host_forward在guest方聚合成d后 这个d也算是加密的？这样的同步式操作是不是只能针对两个及以上的host方才能使用？> > host_forward是加密的，因此得到fore_gradient以后，fore_gradient是加密的。而在异步状态下，需要把half_d直接发送出去，所以需要额外加密一下。
> 
> 是不是可以理解为未加密的half_d和加密过后的host_forward在guest方聚合成d后 这个d也算是加密的？这样的同步式操作是不是只能针对两个及以上的host方才能使用？

这句话前半句是对的，d也是加密的。请注意，密钥的私钥部分只掌握在arbiter手中，因此，哪怕只有一个host，host也无法通过d，解出half_d> > > host_forward是加密的，因此得到fore_gradient以后，fore_gradient是加密的。而在异步状态下，需要把half_d直接发送出去，所以需要额外加密一下。
> > 
> > 
> > 是不是可以理解为未加密的half_d和加密过后的host_forward在guest方聚合成d后 这个d也算是加密的？这样的同步式操作是不是只能针对两个及以上的host方才能使用？
> 
> 这句话前半句是对的，d也是加密的。请注意，密钥的私钥部分只掌握在arbiter手中，因此，哪怕只有一个host，host也无法通过d，解出half_d

假设只有一个host方的场景使用同步式：host方将host_forward加密后发送给guest方，guest方将自身的half_d和接收到的加密数据[host_forward]进行join操作，得到[fore_gradient]后发送给host方，这里的[fore_gradient]虽然是密文，但是其中包含的数据是host方本身就有的一部分数据[host_forward]和未加密的half_d，是不是可以理解为host方不需要解密，只需要进行拆分就可以得到half_d？> > > > host_forward是加密的，因此得到fore_gradient以后，fore_gradient是加密的。而在异步状态下，需要把half_d直接发送出去，所以需要额外加密一下。
> > > 
> > > 
> > > 是不是可以理解为未加密的half_d和加密过后的host_forward在guest方聚合成d后 这个d也算是加密的？这样的同步式操作是不是只能针对两个及以上的host方才能使用？
> > 
> > 
> > 这句话前半句是对的，d也是加密的。请注意，密钥的私钥部分只掌握在arbiter手中，因此，哪怕只有一个host，host也无法通过d，解出half_d
> 
> 假设只有一个host方的场景使用同步式：host方将host_forward加密后发送给guest方，guest方将自身的half_d和接收到的加密数据[host_forward]进行join操作，得到[fore_gradient]后发送给host方，这里的[fore_gradient]虽然是密文，但是其中包含的数据是host方本身就有的一部分数据[host_forward]和未加密的half_d，是不是可以理解为host方不需要解密，只需要进行拆分就可以得到half_d？

这不是很容易的，fore_gradient是half_d和host_forward相加得到，Paillier在相加时，是先对half_d加密，再进行相加操作。因此想直接解出来是不容易的。不过，这个疑问是合理的，这里加法没有加混淆，去试的话是有机会试出来，但是如果加混淆，计算成本会非常高，所以做了个取舍了。> > > > > host_forward是加密的，因此得到fore_gradient以后，fore_gradient是加密的。而在异步状态下，需要把half_d直接发送出去，所以需要额外加密一下。
> > > > 
> > > > 
> > > > 是不是可以理解为未加密的half_d和加密过后的host_forward在guest方聚合成d后 这个d也算是加密的？这样的同步式操作是不是只能针对两个及以上的host方才能使用？
> > > 
> > > 
> > > 这句话前半句是对的，d也是加密的。请注意，密钥的私钥部分只掌握在arbiter手中，因此，哪怕只有一个host，host也无法通过d，解出half_d
> > 
> > 
> > 假设只有一个host方的场景使用同步式：host方将host_forward加密后发送给guest方，guest方将自身的half_d和接收到的加密数据[host_forward]进行join操作，得到[fore_gradient]后发送给host方，这里的[fore_gradient]虽然是密文，但是其中包含的数据是host方本身就有的一部分数据[host_forward]和未加密的half_d，是不是可以理解为host方不需要解密，只需要进行拆分就可以得到half_d？
> 
> 这不是很容易的，fore_gradient是half_d和host_forward相加得到，Paillier在相加时，是先对half_d加密，再进行相加操作。因此想直接解出来是不容易的。不过，这个疑问是合理的，这里加法没有加混淆，去试的话是有机会试出来，但是如果加混淆，计算成本会非常高，所以做了个取舍了。

感谢您的解答！",6,2021-10-27 10:51:26,2021-11-15 09:48:38,2021-11-15 09:48:38
https://github.com/FederatedAI/FATE/issues/3238,[],"[ERROR] 1.0.1-final _DTable object has no attribute ""get_meta""","[ERROR] 1.0.1-final _DTable object has no attribute ""get_meta""when I run the quick_run.py
I get a fail logs in fate_flow_schedule.log
like below
```
""2021-10-27 08:52:17,190 - task_scheduler.py[line:228] - INFO: job 20211027085214806711127 component dataio_0 run on guest 10000 status is notRunning""
""2021-10-27 08:52:17,191 - task_scheduler.py[line:228] - INFO: job 20211027085214806711127 component dataio_0 run on host 10000 status is notRunning""
""2021-10-27 08:52:17,954 - job_controller.py[line:123] - INFO: job 20211027085214806711127 component dataio_0 host 10000 status running""
""2021-10-27 08:52:17,979 - job_controller.py[line:123] - INFO: job 20211027085214806711127 component dataio_0 guest 10000 status running""
""2021-10-27 08:52:18,063 - job_controller.py[line:123] - INFO: job 20211027085214806711127 component dataio_0 host 10000 status failed""
""2021-10-27 08:52:18,113 - job_controller.py[line:123] - INFO: job 20211027085214806711127 component dataio_0 guest 10000 status failed""
""2021-10-27 08:52:18,193 - task_scheduler.py[line:228] - INFO: job 20211027085214806711127 component dataio_0 run on guest 10000 status is failed""
""2021-10-27 08:52:18,194 - task_scheduler.py[line:228] - INFO: job 20211027085214806711127 component dataio_0 run on host 10000 status is failed""
""2021-10-27 08:52:18,194 - task_scheduler.py[line:154] - INFO: job 20211027085214806711127 component dataio_0 run failed""
""2021-10-27 08:52:19,885 - job_controller.py[line:196] - INFO: job 20211027085214806711127 on guest 10000 start to clean""
""2021-10-27 08:52:19,889 - job_controller.py[line:202] - INFO: job 20211027085214806711127 component dataio_0 on guest 10000 clean done""
""2021-10-27 08:52:19,889 - job_controller.py[line:207] - INFO: job 20211027085214806711127 on guest 10000 clean done""
""2021-10-27 08:52:20,911 - job_controller.py[line:196] - INFO: job 20211027085214806711127 on host 10000 start to clean""
""2021-10-27 08:52:20,915 - job_controller.py[line:202] - INFO: job 20211027085214806711127 component dataio_0 on host 10000 clean done""
""2021-10-27 08:52:20,916 - job_controller.py[line:207] - INFO: job 20211027085214806711127 on host 10000 clean done""
""2021-10-27 08:52:21,985 - job_controller.py[line:196] - INFO: job 20211027085214806711127 on arbiter 10000 start to clean""
""2021-10-27 08:52:21,986 - job_controller.py[line:207] - INFO: job 20211027085214806711127 on arbiter 10000 clean done""
""2021-10-27 08:52:21,988 - task_scheduler.py[line:111] - INFO: job 20211027085214806711127 finished, status is failed""

```


In ERROR.log (/dataio_0 for guest)
```

- [ ] ""2021-10-27 08:52:17,992 - task_executor.py[line:120] - ERROR: '_DTable' object has no attribute 'get_meta'""
- [ ] Traceback (most recent call last):
- [ ]   File ""/data/projects/fate/python/fate_flow/driver/task_executor.py"", line 109, in run_task
- [ ]     run_object.run(parameters, task_run_args)
- [ ]   File ""/data/projects/fate/python/federatedml/model_base.py"", line 157, in run
- [ ]     self._run_data(args[""data""], stage)
- [ ]   File ""/data/projects/fate/python/federatedml/model_base.py"", line 131, in _run_data
- [ ]     self.data_output = self.fit(data)
- [ ]   File ""/data/projects/fate/python/federatedml/util/data_io.py"", line 736, in fit
- [ ]     return self.reader.read_data(data_inst, ""fit"")
- [ ]   File ""/data/projects/fate/python/federatedml/util/data_io.py"", line 119, in read_data
- [ ]     self.generate_header(input_data, mode=mode)
- [ ]   File ""/data/projects/fate/python/federatedml/util/data_io.py"", line 83, in generate_header
- [ ]     header = input_data.get_meta(""header"")
- [ ] AttributeError: '_DTable' object has no attribute 'get_meta'
- [ ] 

```


IN ERROR.log (/dataio_0 for host)

```
""2021-10-27 09:32:17,221 - task_executor.py[line:120] - ERROR: Count of data_instance is 0""
Traceback (most recent call last):
  File ""/data/projects/fate/python/fate_flow/driver/task_executor.py"", line 109, in run_task
    run_object.run(parameters, task_run_args)
  File ""/data/projects/fate/python/federatedml/model_base.py"", line 157, in run
    self._run_data(args[""data""], stage)
  File ""/data/projects/fate/python/federatedml/model_base.py"", line 131, in _run_data
    self.data_output = self.fit(data)
  File ""/data/projects/fate/python/federatedml/util/data_io.py"", line 736, in fit
    return self.reader.read_data(data_inst, ""fit"")
  File ""/data/projects/fate/python/federatedml/util/data_io.py"", line 115, in read_data
    abnormal_detection.empty_table_detection(input_data)
  File ""/data/projects/fate/python/federatedml/util/abnormal_detection.py"", line 25, in empty_table_detection
    raise ValueError(""Count of data_instance is 0"")
ValueError: Count of data_instance is 0
```

why this error is occurred and How can I solve this problem?

I should use the version v1.1 of FATE",1,2021-10-27 00:22:28,2021-12-06 07:04:24,2021-10-27 03:47:23
https://github.com/FederatedAI/FATE/issues/3143,[],Does Fate Flow support login? / Fate Flow支持登录吗？,"Does Fate Flow support login? / Fate Flow支持登录吗？I get an error message saying I have to login first when I try some examples provided (e.g. `flow job submit -c homo_lr_train_eval_conf.json -d homo_lr_train_eval_dsl.json`):
```
{
    ""code"": 10015,
    ""msg"": ""Please login first!""
}
```

I try to search for related issue/wiki but unfortunately I didn't find anything useful. According to the commit (FederatedAI/FATE-Board@e6b3456280831bb717c7d61964a271385882a998), it seems to be introduced in 1.5.x which is half year ago already. so I'm wondering if the fate flow doesn't support login (then it breaks everything) indeed, or there is something wrong with my configuration?

--- 

我在尝试example中的例子时（例如`flow job submit -c homo_lr_train_eval_conf.json -d homo_lr_train_eval_dsl.json`）报错提示未登录：
```
{
    ""code"": 10015,
    ""msg"": ""Please login first!""
}
```
翻了下没有找到相关的issue/wiki说明这个，似乎是1.5.x引入的校验 (FederatedAI/FATE-Board@e6b3456280831bb717c7d61964a271385882a998)，目前flow是还没有实现相关的支持吗？

please check your machine information. 10015 is not code of flowI found what went wrong. The English document is not clear enough. The fate client should be installed in the container and linked with fate flow server (default port 9380). Port 8080 is running fate board which has nothing to do with fate flow.

I think #3123 is caused by same issue.

For people who are also looking for a solution, besides running flow in the container, an alternative solution is to expose container's 9380 to outside (e.g. `-p 9380:9380`) so you can run flow in the host.> I found what went wrong. The English document is not clear enough. The fate client should be installed in the container and linked with fate flow server (default port 9380). Port 8080 is running fate board which has nothing to do with fate flow.
> 
> I think #3123 is caused by same issue.
> 
> For people who are also looking for a solution, besides running flow in the container, an alternative solution is to expose container's 9380 to outside (e.g. `-p 9380:9380`) so you can run flow in the host.

一开始我也是这么想的，但是我打开了9380端口之后，依旧无法使用，最新版本的fate把原来的客户端与flow合并了，并没有9380端口在容器内部启动",3,2021-10-06 09:37:26,2021-11-19 09:24:31,2021-10-12 19:24:09
https://github.com/FederatedAI/FATE/issues/3133,[],No module named 'fate_arch',"No module named 'fate_arch'![image](https://user-images.githubusercontent.com/91310466/134634007-fda069ca-06db-4389-af7b-79a2cf8664a4.png)
主机单机部署fate_1.6.0：
 使用 python ${fate}/fate_flow/fate_flow_client.py -f submit_job -d xxx_dsl.json -c xxx_conf.json命令提交任务时，报错No module named 'fate_arch'
![image](https://user-images.githubusercontent.com/91310466/134634528-fe65612b-4304-4629-bd7a-b2c5350d2274.png)
PYTHONPATH为fate_flow所在目录的上级目录
fate_arch目录存在且并未进行任何修改
![image](https://user-images.githubusercontent.com/91310466/134634719-0cf538c8-b550-4e90-a994-952acc4c3b28.png)
请问是哪里可能出现了问题？怎么修改source bin/init_env.sh first may help.",1,2021-09-24 07:23:12,2021-10-06 08:10:14,2021-10-06 08:10:14
https://github.com/FederatedAI/FATE/issues/3132,[],运行出现奇怪错误，急需帮助,"运行出现奇怪错误，急需帮助部署两个k8s集群做联邦（CentOS），运行测试算例，出现一个很奇怪的错误。在work_mode=1情况下，虽然运行官方的例子整个pipeline没有问题，但一旦对table或namespace重命名后(比如把namespace的experiment改名为experiment_b)，pipeline的reader就无法通过。

jupyter-notebook里上传数据
!flow data upload -c upload_guest.json --drop
!flow data upload -c upload_host.json --drop
运行计算
!flow job submit -c hetero_lr_normal_conf.json -d hetero_lr_normal_dsl.json
因为issues里面不能上传json文件，所以我加了.txt后缀，如果下载运行的话，可以把文件名的.txt后缀删除
[hetero_lr_normal_conf.json.txt](https://github.com/FederatedAI/FATE/files/7221995/hetero_lr_normal_conf.json.txt)
[hetero_lr_normal_dsl.json.txt](https://github.com/FederatedAI/FATE/files/7221997/hetero_lr_normal_dsl.json.txt)
[upload_guest.json.txt](https://github.com/FederatedAI/FATE/files/7221998/upload_guest.json.txt)
[upload_host.json.txt](https://github.com/FederatedAI/FATE/files/7221999/upload_host.json.txt)

fateboard里面数据确实读了，证明上传数据成功，但reader就是无法通过，且没有任何出错提示。
![image](https://user-images.githubusercontent.com/28068712/134611625-0b078e6a-3c97-4c91-987a-2a810ad0dd2b.png)
![image](https://user-images.githubusercontent.com/28068712/134611696-6f807700-352a-41c0-97b1-eb925864ce8d.png)
![image](https://user-images.githubusercontent.com/28068712/134611712-e3af8f8c-aa31-4a8b-b9dd-1d329a458af7.png)

奇怪的问题就是：在保证各个json文件的table name和namespace一致的情况下，虽然采用原始默认的table name和namespace运行pipeline全绿，为什么稍微改动表名或namespace，就无法运行通过？难道除了这四个json文件，还有哪里需要同步改动？
奇怪的问题就是：在保证各个json文件的table name和namespace一致的情况下，虽然采用原始默认的table name和namespace运行pipeline全绿，为什么稍微改动表名或namespace，就无法运行通过？难道除了这四个json文件，还有哪里需要同步改动？已经解决，与重命名无关，根本原因是work_mode=1（多集群）时候，每方需要在自己的notebook里提交自己的数据。如果要重命名，确保各方数据的新名称与json文件的设置保持一致即可。",2,2021-09-24 03:08:30,2021-09-24 09:47:18,2021-09-24 09:47:18
https://github.com/FederatedAI/FATE/issues/3131,[],how to disable paillier encrypt during training,"how to disable paillier encrypt during training

""encrypt_param"": {""method"": ""Paillier"", ""key_length"": 2048},

try to disable encrypt by setting: ""encrypt_param"": {""method"": """", ""key_length"": 2048},
report error:
Component hetero_lr_0, module HeteroLR, does not pass component check, error msg is encrypt_param's method  not supported


so is it possible to disable encrypt?

thank youNo, it is not supported in hetero-lr",1,2021-09-23 01:33:39,2021-10-06 08:10:29,2021-10-06 08:10:29
https://github.com/FederatedAI/FATE/issues/3087,[],Install FATE using Docker Failed,"Install FATE using Docker FailedFATE Stand-alone Deployment.
Install FATE using Docker Failed.
#Get code
wget https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/docker_standalone_fate_1.6.1.tar.gz
tar -xzvf docker_standalone_fate_1.6.1.tar.gz

#Execute the command
cd docker_standalone_fate_$1.6.1
bash install_standalone_docker.sh.

Then I found fate container can not be started. The logs say: Unable to access jarfile fateboard.jar
换成1.5.1的镜像就可以了,注意要把之前的镜像删掉> 换成1.5.1的镜像就可以了,注意要把之前的镜像删掉

后来按照install FATE in host直接安装了1.6.1，倒也方便Thanks for pointing out the problem. We have fixed this bug and refresh the release deploy packages. You can try it again.",3,2021-09-08 17:04:59,2021-10-06 08:15:23,2021-10-06 08:15:23
https://github.com/FederatedAI/FATE/issues/3071,[],fate hetero nn 多方部署 莫名报错,"fate hetero nn 多方部署 莫名报错**Describe the bug**
当通过 docker-compose 或者 k8s 训练 hetero_nn 的时候都会出现各自的错误

**To Reproduce**
### 在 k8s 上：
1. 首先，上传 `vehicle_scale_hetero_guest.csv` 和 `vehicle_scale_hetero_host.csv` 数据
2. 然后，直接使用 example 中的 `examples/dsl/v2/hetero_nn/test_hetero_nn_binary_with_early_stop_conf.json` 和 `examples/dsl/v2/hetero_nn/test_hetero_nn_binary_with_early_stop_dsl.json`，将 conf 中的数据来源改成和第一步提交的数据一致之后，来提交模型训练任务。
3. 在神经网络模块训练完成之后，出现如下报错，虽然它写了要加 tag，但我不知道这个 tag 加到哪里、以及 tag 究竟意味着什么。
![image](https://user-images.githubusercontent.com/62225631/131937294-63585edb-4c90-4eb2-80f4-9b9de72b5fc6.png)

### 在 docker-compose 上：
1. 前两步相同，
2. 在神经网络模块训练完成之后，出现如下情况：dsl 图像显示任务出错，但是查看 dashboard 却没有任何的报错信息
![image](https://user-images.githubusercontent.com/62225631/131956383-c1534466-ea4e-4429-8167-5c387b7b8133.png)
![image](https://user-images.githubusercontent.com/62225631/131956469-ff37e048-2430-42cb-966a-99325261c7d1.png)

刚刚查看了 k8s 部署时候的 Schedule Log，发现如下部分：
```
611 [INFO] [2021-09-03 07:12:51,459] [1:139795833526016] - job_saver.py[line:81]: try to update job 2021090302415287588011 task 2021090302415287588011_reader_evaluate 0

612 [WARNING] [2021-09-03 07:12:51,480] [1:139795833526016] - job_saver.py[line:86]: job 2021090302415287588011 task 2021090302415287588011_reader_evaluate 0 update does not take effect

613 [INFO] [2021-09-03 07:12:51,543] [1:139795833526016] - job_saver.py[line:45]: try to update job 2021090302415287588011 status to failed

614 [INFO] [2021-09-03 07:12:51,588] [1:139795833526016] - job_saver.py[line:56]: update job 2021090302415287588011 status does not take effect

615 [INFO] [2021-09-03 07:12:52,106] [1:139798191613696] - federated_scheduler.py[line:92]: stop job 2021090302415287588011 success

616 [INFO] [2021-09-03 07:12:52,106] [1:139798191613696] - dag_scheduler.py[line:448]: stop job 2021090302415287588011 with failed successfully

617 [INFO] [2021-09-03 07:12:52,106] [1:139798191613696] - federated_scheduler.py[line:107]: try to clean job 2021090302415287588011

618 [INFO] [2021-09-03 07:12:53,254] [1:139798666213120] - job_controller.py[line:379]: Job 2021090302415287588011 on guest 2 start to clean

619 [INFO] [2021-09-03 07:12:53,254] [1:139798666213120] - job_controller.py[line:382]: job 2021090302415287588011 on guest 2 clean done

620 [INFO] [2021-09-03 07:12:53,836] [1:139798191613696] - federated_scheduler.py[line:110]: clean job 2021090302415287588011 success

621 [INFO] [2021-09-03 07:12:53,836] [1:139798191613696] - dag_scheduler.py[line:505]: Job 2021090302415287588011 finished with failed, done

622 [INFO] [2021-09-03 07:12:53,836] [1:139798191613696] - dag_scheduler.py[line:325]: finish scheduling job 2021090302415287588011

623 [INFO] [2021-09-03 07:12:58,620] [1:139798191613696] - dag_scheduler.py[line:193]: the number of updates has been exceeded

624 [INFO] [2021-09-03 07:13:02,413] [1:139798191613696] - dag_scheduler.py[line:193]: the number of updates has been exceeded

625 [INFO] [2021-09-03 07:13:06,247] [1:139798191613696] - dag_scheduler.py[line:193]: the number of updates has been exceeded

626 [INFO] [2021-09-03 07:13:10,001] [1:139798191613696] - dag_scheduler.py[line:193]: the number of updates has been exceeded
```Use ""pip list | grep tensorflow"" to show the version of tf, in tensorflow higher version > 2.4.1，it may cause some confusing error请问在docker-compose部署 训练时报错the number of updates has been exceeded  你后来解决了吗",3,2021-09-03 05:51:57,2021-12-21 02:31:21,2021-10-06 08:15:48
https://github.com/FederatedAI/FATE/issues/3004,[],job/log bug,"job/log bug**在job/log接口中参数获取错误**
代码路径：/FederatedAI/FATE/blob/master/python/fate_flow/apps/job_app.py  142行
@manager.route('/log', methods=['get'])
def job_log():
    job_id = request.json.get('job_id', '')
    job_log_dir = job_utils.get_job_log_directory(job_id=job_id)    是否应该改为  request.args

**在data/upload接口中参数获取错误**
/FATE/blob/master/python/fate_flow/apps/data_access_app.py    51行 
job_config = request.args.to_dict()
        if ""namespace"" in job_config and ""table_name"" in job_config:
            pass
        else:
            # higher than version 1.5.1, support eggroll run parameters
            job_config = json_loads(list(job_config.keys())[0])
        job_config['file'] = filename
post请求的参数获取是否应该改为 request.form 或者request.valuesflow 所有 api 接口都是接收 json
`data_access_app` 里的 `request.args` 应该为了是向后兼容 @jarviszeng-zjc 看下get请求&nbsp;也用request.json接受吗，感觉有点怪


------------------&nbsp;原始邮件&nbsp;------------------
发件人:                                                                                                                        ""FederatedAI/FATE""                                                                                    ***@***.***&gt;;
发送时间:&nbsp;2021年8月26日(星期四) 上午10:59
***@***.***&gt;;
***@***.******@***.***&gt;;
主题:&nbsp;Re: [FederatedAI/FATE] job/log bug (#3004)





 
flow 所有 api 接口都是接收 json
 data_access_app 里的 request.args 应该为了是向后兼容 @jarviszeng-zjc 看下
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.
Triage notifications on the go with GitHub Mobile for iOS or Android.data_access_app 里的 request.args 使用python clinet 上传数据集是正常的  接口访问会出错。需要上传file的话  就不能使用json了吧",3,2021-08-26 02:17:40,2021-10-06 08:16:02,2021-10-06 08:16:02
https://github.com/FederatedAI/FATE/issues/2977,[],"""predict_score"" for logistic regression test on Fate Board wrong in magnitude","""predict_score"" for logistic regression test on Fate Board wrong in magnitude**Describe the bug**
In fateboard, the ""predict_score"" for logistic regression test will sometimes be larger than 1. For example, look at the following screenshot:
![Screen Shot 2021-08-17 at 4 31 29 PM](https://user-images.githubusercontent.com/62225631/129692241-01f66ac4-7466-44af-a86d-fca5cde86178.png)
and after a deeper insight, I find that it's actually missing a magnitude specification. For example, the value 2.719735 is actually 2.719735...e-12. But somehow the magnitude ""e-12"" is missing, which really confuses users.
![Screen Shot 2021-08-17 at 4 34 03 PM](https://user-images.githubusercontent.com/62225631/129692652-d23ae2a5-4f4d-4cc8-8147-07995c6722c4.png)
We will fix this issue in FATE-Board V1.6.1，thanks for your carefulness.",1,2021-08-17 08:35:39,2021-08-18 08:55:19,2021-08-18 08:55:19
https://github.com/FederatedAI/FATE/issues/2976,[],fate on spark有没有计划以kafka替代rabbitmq呢,"fate on spark有没有计划以kafka替代rabbitmq呢**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
暂无计划",1,2021-08-16 10:30:38,2021-10-06 08:16:21,2021-10-06 08:16:21
https://github.com/FederatedAI/FATE/issues/2955,[],Poor Implementation of SPDZ protocol,"Poor Implementation of SPDZ protocol**Describe the bug**
The implementation of SPDZ protocol is incomplete, crucial features of the original protocol are truncated.
1. Not secure against active malicious adversary.
Without using information theoretic MACs or zero-knowledge proofs as the original protocol, active malicious adversary can cheat on their inputs and render incorrect results.  

2. Only support two parties.
The implementation uses Paillier encryption to produce beaver triplets which makes it limit to two parties while the original SPDZ has no party number limits. Should use somewhat FHE or leveled FHE like BGV, BFV to produce beaver triplets instead. Thanks report. 

>     1. Not secure against active malicious adversary.
>        Without using information theoretic MACs or zero-knowledge proofs as the original protocol, active malicious adversary can cheat on their inputs and render incorrect results.

The `secure against active malicious adversary` is not the goal we try to achieve in recent version, but pull requests are always welcome.

>     2. Only support two parties.
>        The implementation uses Paillier encryption to produce beaver triplets which makes it limit to two parties while the original SPDZ has no party number limits. Should use somewhat FHE or leveled FHE like BGV, BFV to produce beaver triplets instead.

Yes, we support two parties now becasue we meat some fixpoint-encoder issue, not becuase we use the `Paillier encryption` 
protocol. And we don't see the point of goodness to use somewhat FHE to genearete beaver triplets. Could you please provide any more detailed explanation？
> Thanks report.
> 
> > ```
> > 1. Not secure against active malicious adversary.
> >    Without using information theoretic MACs or zero-knowledge proofs as the original protocol, active malicious adversary can cheat on their inputs and render incorrect results.
> > ```
> 
> The `secure against active malicious adversary` is not the goal we try to achieve in recent version, but pull requests are always welcome.
> 
> > ```
> > 2. Only support two parties.
> >    The implementation uses Paillier encryption to produce beaver triplets which makes it limit to two parties while the original SPDZ has no party number limits. Should use somewhat FHE or leveled FHE like BGV, BFV to produce beaver triplets instead.
> > ```
> 
> Yes, we support two parties now becasue we meat some fixpoint-encoder issue, not becuase we use the `Paillier encryption`

**_> My mistake, if Paillier supports multiplication between ciphertext and constants, and you followed the SPDZ2's triplets generation protocol, fate's beaver_triplets doesn't have party number limitation. Also, can you give me more details of this fixpoint-encoder issue? I am working on the two party limit issue. And can we use Microsoft Seal's CKKS instead of Paillier to fix this issue?_**

> protocol. And we don't see the point of goodness to use somewhat FHE to genearete beaver triplets. Could you please provide any more detailed explanation？

**_> SPDZ2's` paper do mention some benefits of using Leveled FHE BGV over Paillier at the last paragraphy of its Introduction section._**

**_> One more thing to mention, SPDZ's unit test shows errors : module ""fate_arch.session"" has no attirbute ""init""._**@weiwee 
**Good News**: The party number limit of SPDZ can be removed by using Microsoft Seal's CKKS as well as all the redundant fixpoint encoding and decoding code.   I tested the modified SPDZ in hetero_pearson with three parties:1 Guest, 2 Hosts, and the data I used is breast_hetero_guest and breast_hetero_host. 
Here is the first corr row:

>  [0.9739451840944395 0.42625413530776457 0.9755094378288185
>  0.9697823086090491 0.05205141649569391 0.561382889899243
>  0.7069897199142066 0.7481190687094059 -0.19798667287048125
>  -0.5964790654824891 0.4159688144789205 -0.08748228172470007
>  0.38501085106115696 0.758014080265595 -0.44258157229279843
>  -0.014301103237581181 0.13761181368069303 0.43303178978547496
>  -0.5738301629347646 -0.388682601417217]

And the first corr row using original  SPDZ:

> [0.97387 0.42623 0.97545 0.96972 0.052039999999999996 0.56136
>  0.7069300000000001 0.74806 -0.19799 -0.59645 0.41595000000000004 -0.08748
>  0.38499 0.75797 -0.44254 -0.014299999999999998 0.13759 0.433
>  -0.5738000000000001 -0.38864]

We can see the results are pretty close.  

**Bad News** :   The ciphertext is too large(nearly 1.9GB) for the tested table(Using CKKS.tensor for encryption). It caused memory error when I tried to remote. To pass the test, I cut breast_hetero_guest to 10 rows.  

Nary a douobt. This large ciphertext size issue will limit its applications. Have to work on that latter.    Close this issue first.",3,2021-08-09 03:56:36,2021-08-25 10:11:15,2021-08-25 10:11:15
https://github.com/FederatedAI/FATE/issues/2942,[],FATEBoard need user and password ，FATEBoard 登录需要用户名密码？,"FATEBoard need user and password ，FATEBoard 登录需要用户名密码？fate 1.5.2
![image](https://user-images.githubusercontent.com/67529426/127945437-a1d4a1de-cdf2-4adc-be6c-4c08c695b7a1.png)
admin/adminin fate v1.5.2, you will use fateboard v1.5.3![image](https://user-images.githubusercontent.com/67529426/127946177-311e89f3-5670-4c4f-a56a-a25b7b185cec.png)
 Incorrect username or passwordhow to use use fateboard v1.5.3> how to use use fateboard v1.5.3

Make sure that the conf/application.properties is updated. 
It should have config like: 
    server.board.login.username=admin
    server.board.login.password=admin

@wangyuqian1007![image](https://user-images.githubusercontent.com/25281865/174215208-84db705b-9745-4fa0-b9fc-3cb5e2b8028e.png)
![image](https://user-images.githubusercontent.com/25281865/174215240-598380af-44ef-4cf7-941c-096a86672199.png)
The application.propertis is configed admin/admin, but FATEBoard still cannot login. What is wrong?",6,2021-08-03 01:56:48,2022-06-17 02:52:51,2021-08-09 02:18:15
https://github.com/FederatedAI/FATE/issues/2909,[],pipeline: command not found,"pipeline: command not found**Describe the bug**
A clear and concise description of what the bug is.

按照git文档成功安装fate-client后无法使用pipeline

**To Reproduce**
Steps to reproduce the behavior:
通过docker完成单机部署并通过单元测试和toy测试验证，通过whl安装fate-client
```
Singularity> pip show fate-client
Name: fate-client
Version: 0.3.0.post1
Summary: Clients for FATE, including flow_client and pipeline
Home-page: https://fate.fedai.org/
Author: FederatedAI
Author-email: contact@FedAI.org
License: Apache-2.0
Location: /root/.local/lib/python3.6/site-packages
Requires: flask, setuptools, tensorflow, click, ruamel.yaml, requests-toolbelt, loguru, requests
Required-by: fate-test
Singularity> pipeline init --help
bash: pipeline: command not found
```

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
edit:
由于没有平台root权限，利用singularity3.7的fakeroot在Linux平台上进入环境。

使用裸机尝试可以安装成功。

验证是安装路径在/root/.local/bin下，使用`export PATH=$PATH:/root/.local/bin`解决",1,2021-07-23 03:35:07,2021-07-23 07:43:35,2021-07-23 07:43:35
https://github.com/FederatedAI/FATE/issues/2893,[],发布模型时一个成功一个失败,"发布模型时一个成功一个失败刚训练好的模型，然后模型在各方均部署成功。但发布模型的时候一个成功一个失败，请问时什么原因？
请求：
{
    ""initiator"": {
        ""role"": ""guest"",
        ""party_id"": ""9999""
    },
    ""job_parameters"": {
        ""work_mode"": 1,
        ""model_id"": ""arbiter-10000#guest-9999#host-8888#model"",
        ""model_version"": ""2021071909181355014422""
    },
    ""role"": {
        ""arbiter"": [
            ""10000""
        ],
        ""host"": [
            ""8888""
        ],
        ""guest"": [
            ""9999""
        ]
    }
}
响应：
{
    ""data"": {
        ""detail"": {
            ""guest"": {
                ""9999"": {
                    ""retcode"": 123,
                    ""retmsg"": ""model initialization error, please check if the model exists and the configuration of the FATEFLOW load model process is correct. ""
                }
            },
            ""host"": {
                ""8888"": {
                    ""retcode"": 0,
                    ""retmsg"": ""success""
                }
            }
        },
        ""guest"": {
            ""9999"": 123
        },
        ""host"": {
            ""8888"": 0
        }
    },
    ""jobId"": ""2021071909183592077723"",
    ""retcode"": 101,
    ""retmsg"": ""failed""
}FATE version? post your `conf/service_conf.yaml` plz. which backend are you using? plz post its config too.The problem has been resolved。
Thanks for your helpHow did you solve it?  I have met the same problem.",3,2021-07-19 09:47:31,2021-09-28 02:01:02,2021-08-26 03:00:03
https://github.com/FederatedAI/FATE/issues/2845,[],Homogeneous Logistic Regression using Gradient Aggregation not implemented?,"Homogeneous Logistic Regression using Gradient Aggregation not implemented?Hi,
I was going through the code of homo-lr, and I wonder if the method described in the README is not implemented in the code.

**Mismatch between README and code?**
Described in https://github.com/FederatedAI/FATE/blob/7f81d654bf9ddbb1207bbd3c02e130f5ea54b1dc/python/federatedml/linear_model/logistic_regression/README.rst 
<img width=""1227"" alt=""Screen Shot 2021-06-27 at 2 14 24 PM"" src=""https://user-images.githubusercontent.com/59325005/123535036-03729e00-d754-11eb-8a41-8505564df62a.png"">
the homogeneous LR sends and aggregates **gradients** in each iteration. However, in the code, it is the **model weights** that are sent and aggregated:
https://github.com/FederatedAI/FATE/blob/7f81d654bf9ddbb1207bbd3c02e130f5ea54b1dc/python/federatedml/linear_model/logistic_regression/homo_logsitic_regression/homo_lr_host.py#L94-L95

and the gradient is computed and applied for weight update **within** each host
https://github.com/FederatedAI/FATE/blob/7f81d654bf9ddbb1207bbd3c02e130f5ea54b1dc/python/federatedml/linear_model/logistic_regression/homo_logsitic_regression/homo_lr_host.py#L117-L131

There seems to be a mismatch between the README and the code.  

**Describe the solution you'd like**
Is homo-lr using gradient aggregation not implemented or is the code elsewhere? Could you refer to the code if it is elsewhere?
If not implemented, could you explain if any difficulties were encountered that blocked the implementation? (e.g. encryption, numerical precision, etc.)?

**Describe alternatives you've considered**
I did an implementation to use the gradient aggregation described in the README.rst above (for simplicity I did not use encryption)
https://github.com/xingzhis/pretendFLR
It works well if precise gradients (of the cross-entropy loss) are used, but if Taylor expansion gradient is used (so that additively homomorphic encryption can be used), the numerical errors accumulate, resulting in a failure to learn the model correctly.

Thank you very much!
@xingzhis Sorry for misunderstanding you. We do aggregate the model_weights instead of gradient. The Readme will be fix in next version. 

@tanmc123 Thank you very much!",2,2021-06-27 06:51:10,2021-07-06 01:18:13,2021-07-06 01:18:13
https://github.com/FederatedAI/FATE/issues/2841,[],A typo of readme,"A typo of readme**Describe the bug**
A typo of readmeClosed after merge code.",1,2021-06-20 14:48:15,2021-11-24 06:30:39,2021-11-24 06:30:39
https://github.com/FederatedAI/FATE/issues/2834,[],Homogeneous Neural Network,"Homogeneous Neural Network**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to https://github.com/FederatedAI/FATE/blob/master/examples/pipeline/homo_nn/pipeline_homo_nn_multi_layer.py
2. Update the neural network to be the following

def main(config=""../../config.yaml"", namespace=""""):
    homo_nn_0 = HomoNN(name=""homo_nn_0"",
                       max_iter=epochs,
                       batch_size=batch_size)
    homo_nn_0.add(Dense(units=nodes_per_layer[0], input_shape=(12,), activation=""relu""))
    homo_nn_0.add(Dropout(dropout))
    homo_nn_0.add(Dense(units=nodes_per_layer[1], activation=""relu""))
    homo_nn_0.add(Dropout(dropout))
    homo_nn_0.add(Dense(units=nodes_per_layer[2], activation=""relu""))
    homo_nn_0.add(Dropout(dropout))
    homo_nn_0.add(Dense(units=1, activation=""relu""))
    homo_nn_0.compile(optimizer=optimizers.Adam(learning_rate=0.05),
                      metrics=[""MeanSquaredError""],
                      loss=""mean_squared_error"")

**Expected behavior**
I expect the NN to output a single prediction of the target variable which is a continuous variable

**Errors**
ValueError: A target array with shape (16, 1) was passed for an output of shape (None, 151) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output.

**Question**
How can I perform a regression prediction using neural network, instead of a categorical prediction shown in all 3 homo_nn examples?

Note that the error goes away if i change loss=""mean_absolute_error"" instead.Homo Neural Network does not support regression task now, only supports classification task.thanks for your reply! would be great if that can be made clearer in the documentation.",2,2021-06-16 13:52:27,2021-06-22 12:50:51,2021-06-22 12:50:51
https://github.com/FederatedAI/FATE/issues/2820,[],No module named 'pipeline,"No module named 'pipeline**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
/home/mingzhu/anaconda3/envs/py38/bin/python3.8 /home/mingzhu/ocr/Text-Opinion-Mining/FATE/examples/benchmark_quality/hetero_lr/pipeline-lr-binary.py
Traceback (most recent call last):
  File ""/home/mingzhu/ocr/Text-Opinion-Mining/FATE/examples/benchmark_quality/hetero_lr/pipeline-lr-binary.py"", line 19, in <module>
    from pipeline.backend.pipeline import PipeLine
ModuleNotFoundError: No module named 'pipeline'
`pip install fate_client`",1,2021-06-07 10:40:40,2021-07-09 03:13:25,2021-07-09 03:13:25
https://github.com/FederatedAI/FATE/issues/2817,[],BUG：Please check that you are connecting to the correct HDFS RPC port,"BUG：Please check that you are connecting to the correct HDFS RPC port**Describe the bug**
KubeFATE部署完之后，准备数据上传，结果出现HDFS的报错异常，刚部署上就出现异常，请问有么有大佬遇到此问题

**To Reproduce**
Steps to reproduce the behavior:
1. 准备mnist数据
2. 仿照upload文件写文件命名
3.执行数据上传命令
4. See error

**Expected behavior**





**Screenshots**
![image](https://user-images.githubusercontent.com/40295475/120348131-780e1480-c32f-11eb-91c7-b7319844413f.png)

![image](https://user-images.githubusercontent.com/40295475/120348195-8a884e00-c32f-11eb-924c-a297529db6bc.png)

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
数据处理有一些问题",1,2021-06-01 15:17:39,2021-06-02 01:15:28,2021-06-02 01:15:07
https://github.com/FederatedAI/FATE/issues/2773,['bug'],sm3 encode error,"sm3 encode errorsm3 在base64下编码错误,多加了encode语句

![image](https://user-images.githubusercontent.com/56598312/117402353-cc90c080-af38-11eb-8824-86a47ae34fc2.png)
![image](https://user-images.githubusercontent.com/56598312/117402381-dadedc80-af38-11eb-80c7-2bf844478912.png)
感谢提起issue。
截图中错误已计划在1.7版本修复，使用当前1.6版本可以先按照上述截图修改对应文件：
https://github.com/FederatedAI/FATE/blob/master/python/federatedml/secureprotol/hash/hash_factory.py#L67",1,2021-05-07 05:34:58,2021-12-22 04:46:08,2021-12-22 04:46:08
https://github.com/FederatedAI/FATE/issues/2758,['bug'],Homo_onehot存在特征不能对齐的bug！,"Homo_onehot存在特征不能对齐的bug！**1、问题描述**
在真实场景下使用横向onehot对数据做处理，之后再用横向LR模型，得到的结果与预期相差非常大（远远不如只使用本地数据），完全不可用。
**2、实验过程**
经过多次实验，发现横向LR得到的结果均很差，Debug横向LR的计算逻辑，没有发现问题，十分无助。后来，无意中检查了onehot之后的特征列，如下图所示：
![image](https://user-images.githubusercontent.com/34981842/115832226-3c854e00-a445-11eb-92b5-5f3cf59dfe17.png)
上图是guest方使用homo_onehot之后特征顺序的结果
![image](https://user-images.githubusercontent.com/34981842/115832412-76eeeb00-a445-11eb-9ae4-f395d4de6642.png)
上图是host方使用homo_onehot之后特征顺序的结果
可以看到，使用homo_onehot得到的特征顺序不一致！
**不免有些疑惑，计算的时候是根据字段名还是字段顺序计算的？**
![image](https://user-images.githubusercontent.com/34981842/115832694-c46b5800-a445-11eb-8643-133dafe1ba3f.png)
上图是guest方第一次本地计算的参数结果
![image](https://user-images.githubusercontent.com/34981842/115832784-e1079000-a445-11eb-9426-73f5d9c5bc88.png)
 上图是host方第一次本地计算的参数结果
由于在实验条件下两边上传了一样的数据，可以看到对应特征名下的参数值是相同的，接下来看一下聚合的结果：
![image](https://user-images.githubusercontent.com/34981842/115833183-53787000-a446-11eb-93d4-3c6a9f848dc5.png)
**3、结论**
不免有些惊讶，竟然是根据特征顺序进行计算的，但是homo_onehot得到的特征顺序又不一致，导致参数更新时梯度信息混乱，得到十分意外的结果。
感谢FATE平台，感谢FATE开源社区，希望FATE能够越来越好！！确实有这个问题，预计将在1.5.2修复，感谢提出",1,2021-04-23 07:15:31,2021-06-09 03:28:45,2021-06-09 03:28:45
https://github.com/FederatedAI/FATE/issues/2752,[],请问下，现在serving支持dsl_v2了吗？（因模型部署上线后预测无结果而有的猜疑....）,"请问下，现在serving支持dsl_v2了吗？（因模型部署上线后预测无结果而有的猜疑....）（dockers部署的）用最新版的fate跑pipeline训练出的模型在部署上线时，过程都是success，但是到推理测试就是啥结果也没，返回结果像下面这样：

- {""retcode"":0,""retmsg"":"""",""data"":{""modelId"":""guest#9999#guest-9999#host-10000#model"",""modelVersion"":""2021041903133901753497"",""timestamp"":1618802149453},""flag"":0}

然后瞅了下serving-server容器，发现在运行加载模型命令（**_flow model load -c publish_load_model.json_**）的时候，这个容器logs里会报这个错：

- DSLParser init catch error:{}
 org.json.JSONException: JSONObject[""input""] not found.

就怀疑是解析dsl时出错了，所以想问下现在的serving是不是还不支持dsl_v2？如果是的话，难道还要用dsl_v1来上线？或是其他什么方式？大佬们求解orz
@nemirorox @weiwee @jarviszeng-zjc @tanmc123 @ioracion 2.0.4的fate serving是支持DSL 2.0的@jiahaoc1993 感谢回复！
希望能再帮忙解疑下：就是在加载模型时，serving-server容器后台会报这个错：
![log](https://user-images.githubusercontent.com/17641924/115361589-4dd71c00-a1f3-11eb-842d-2583db337716.png)
我部属的流程是这样的（按照pipeline预测例子弄的，是不是这样不对？）：
![微信截图_20210420161000](https://user-images.githubusercontent.com/17641924/115361710-69422700-a1f3-11eb-8699-856d5f45b145.png)

@ioracion 容器版本的serving是2.0.0的，所以报错正常@jiahaoc1993 非常感谢！问题解决了",4,2021-04-19 09:47:17,2021-04-21 07:24:10,2021-04-21 07:24:10
https://github.com/FederatedAI/FATE/issues/2751,[],安装fate-client之后无法使用flow命令,"安装fate-client之后无法使用flow命令**环境**：使用docker-compose 配置的fate1.5版本
使用清华源安装fate-client
**错误**：
![image](https://user-images.githubusercontent.com/39814669/115117798-ecd0fd80-9fd2-11eb-8b4a-b9d4ff8308f9.png)

**RuntimeError**: Click will abort further execution because Python 3 was configured to use ASCII as encoding for the environment. Consult https://click.palletsprojects.com/python3/ for mitigation steps.

Additional information: on this system no suitable UTF-8 locales were discovered. This most likely requires resolving by reconfiguring the locale system.
尝试的解决办法：
export LC_ALL=en_US.UTF-8
export LANG=en_US.UTF-8
![image](https://user-images.githubusercontent.com/39814669/115117938-8b5d5e80-9fd3-11eb-8510-517e269dc11e.png)
**仍然不行**
![image](https://user-images.githubusercontent.com/39814669/115117975-afb93b00-9fd3-11eb-8533-9710167bdf1b.png)

请问如何解决这个问题，或者说还有无其他办法让我可以使用flow命令？已解决，系统缺少utf-8的字符编码语言包，下载安装设置环境变量即可。",1,2021-04-17 15:23:03,2021-04-25 06:04:56,2021-04-25 06:04:56
https://github.com/FederatedAI/FATE/issues/2748,[],车联网与联邦学习结合的话用standalone还是cluster,"车联网与联邦学习结合的话用standalone还是clusterPlease briefly describe the scenario of your demands in Internet of Vehicles, how to combine your demands with federated learning. Generally, computers on vehicles have limited capability which can hardly cover training assignments locally. If you send some data collected from sensors to computing server, you must take latency among all vehicles and servers. Also, cost is always a big deal.感谢您的回答，具体场景确实如您所说。我们的想法是在这样的场景下进行，车辆之间要想互相通信，需要借助每个路口的路边单元，可以把路边单元看做一个基站，通过基站决定走哪条信道，从而使整个网络中的信道负载达到最优，最主要的部分是路边单元收集车辆通信请求数据后，传到联邦学习的框架里，训练完成后返回方案。但是现在我们不知道该如何将路边单元的数据传到联邦学习框架里，以及是否有已有的合适的算法，感谢您的指导，非常期待您对以上问题的解答，万分感谢



> Please briefly describe the scenario of your demands in Internet of Vehicles, how to combine your demands with federated learning. Generally, computers on vehicles have limited capability which can hardly cover training assignments locally. If you send some data collected from sensors to computing server, you must take latency among all vehicles and servers. Also, cost is always a big deal.

",2,2021-04-15 12:03:27,2021-05-26 13:40:04,2021-05-26 13:40:04
https://github.com/FederatedAI/FATE/issues/2746,[],Where is the proto_generate.sh?,"Where is the proto_generate.sh?**Describe the bug**
Where is the [proto_generate.sh](https://github.com/FederatedAI/FATE/blob/master/doc/python/federatedml/protobuf/proto_generate.sh)?
https://github.com/FederatedAI/FATE/blob/178f04d1a58181359d6550b4673d4b4dc72a778f/doc/develop_guide.rst#L266

By the way, how to recompile the whole FATE after generating files from `.proto` files.It is located here: https://github.com/FederatedAI/FATE/blob/master/python/fate_arch/protobuf/generate_py.sh

We will fix the doc later. Thank you for pointing out.",1,2021-04-14 10:38:29,2021-07-09 03:15:40,2021-07-09 03:15:40
https://github.com/FederatedAI/FATE/issues/2744,[],NameError: name 'mpa' is not defined,"NameError: name 'mpa' is not defined**Describe the bug**
When I tried some homo secureboost pipeline examples, like [pipeline-homo-sbt-binary-with-predict.py](https://github.com/FederatedAI/FATE/blob/master/examples/pipeline/homo_sbt/pipeline-homo-sbt-binary-with-predict.py), the FATE board showed that `NameError: name 'mpa' is not defined`.
In CLI, it told me to check out the FATE board:
```
root@66242165a59e:/fate/examples/pipeline/homo_sbt# pipeline_script=pipeline-homo-sbt-binary-with-predict.py
root@66242165a59e:/fate/examples/pipeline/homo_sbt# python ${pipeline_script} 
2021-04-13 12:01:41.023722: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-13 12:01:41.023835: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-04-13 12:01:47.061 | INFO     | pipeline.utils.invoker.job_submitter:monitor_job_status:121 - Job id is 2021041312014588872320
                   Job is still waiting, time elapse: 0:00:02
2021-04-13 12:01:50Running component reader_1, time elapse: 0:00:08
2021-04-13 12:01:57Running component reader_0, time elapse: 0:00:16
2021-04-13 12:02:04Running component dataio_0, time elapse: 0:00:23
2021-04-13 12:02:11Running component dataio_1, time elapse: 0:00:30
2021-04-13 12:02:18Running component homo_secureboost_0, time elapse: 0:02:57
2021-04-13 12:04:46Running component evaluation_0, time elapse: 0:03:17
2021-04-13 12:05:07.239 | INFO     | pipeline.utils.invoker.job_submitter:monitor_job_status:129 - Job is success!!! Job id is 2021041312014588872320
2021-04-13 12:05:07.241 | INFO     | pipeline.utils.invoker.job_submitter:monitor_job_status:130 - Total time: 0:03:20
2021-04-13 12:05:14.805 | INFO     | pipeline.utils.invoker.job_submitter:monitor_job_status:121 - Job id is 2021041312051216963123
                   Job is still waiting, time elapse: 0:00:03
2021-04-13 12:05:19Running component reader_1, time elapse: 0:00:10
2021-04-13 12:05:25Running component dataio_0, time elapse: 0:00:16
2021-04-13 12:05:32Running component homo_secureboost_0, time elapse: 0:00:31
2021-04-13 12:05:49.966 | ERROR    | __main__:main:103 - An error has been caught in function 'main', process 'MainProcess' (71126), thread 'MainThread' (140338903152448):
Traceback (most recent call last):

  File ""pipeline-homo-sbt-binary-with-predict.py"", line 114, in <module>
    main()
    └ <function main at 0x7fa33253be18>

> File ""pipeline-homo-sbt-binary-with-predict.py"", line 103, in main
    predict_pipeline.predict(job_parameters)
    │                │       └ <pipeline.runtime.entity.JobParameters object at 0x7fa2c8beacf8>
    │                └ <function PipeLine.predict at 0x7fa2c8bca0d0>
    └ <pipeline.backend.pipeline.PipeLine object at 0x7fa2c8beacc0>

  File ""/usr/local/lib/python3.6/site-packages/pipeline/backend/pipeline.py"", line 501, in predict
    self._initiator.party_id)
    │    │          └ 10000
    │    └ namespace(party_id=10000, role='guest')
    └ <pipeline.backend.pipeline.PipeLine object at 0x7fa2c8beacc0>
  File ""/usr/local/lib/python3.6/site-packages/pipeline/utils/invoker/job_submitter.py"", line 136, in monitor_job_status
    raise ValueError(f""Job is failed, please check out job {job_id} by fate board or fate_flow cli"")

ValueError: Job is failed, please check out job 2021041312051216963123 by fate board or fate_flow cli
Traceback (most recent call last):
  File ""pipeline-homo-sbt-binary-with-predict.py"", line 114, in <module>
    main()
  File ""pipeline-homo-sbt-binary-with-predict.py"", line 103, in main
    predict_pipeline.predict(job_parameters)
  File ""/usr/local/lib/python3.6/site-packages/loguru/_logger.py"", line 1220, in catch_wrapper
    return function(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/pipeline/backend/pipeline.py"", line 501, in predict
    self._initiator.party_id)
  File ""/usr/local/lib/python3.6/site-packages/pipeline/utils/invoker/job_submitter.py"", line 136, in monitor_job_status
    raise ValueError(f""Job is failed, please check out job {job_id} by fate board or fate_flow cli"")
ValueError: Job is failed, please check out job 2021041312051216963123 by fate board or fate_flow cli
root@66242165a59e:/fate/examples/pipeline/homo_sbt# pipeline_script=pipeline-homo-sbt-binary-with-predict.py
```
And what FATE board's feedback was:
```
1
[ERROR] [2021-04-13 12:05:44,701] [76032:139781429630784] - task_executor.py[line:191]: name 'mpa' is not defined
2
Traceback (most recent call last):
3
  File ""./fate/python/fate_flow/operation/task_executor.py"", line 168, in run_task
4
    run_object.run(component_parameters_on_party, task_run_args)
5
  File ""./fate/python/federatedml/model_base.py"", line 101, in run
6
    this_data_output = func(*params)
7
  File ""./fate/python/federatedml/ensemble/boosting/boosting_core/boosting.py"", line 530, in load_model
8
    self.set_model_param(model_param)
9
  File ""./fate/python/federatedml/ensemble/boosting/homo/homo_secureboost_client.py"", line 159, in set_model_param
10
    self.classes_ = list(mpa(int, model_param.classes_))
11
NameError: name 'mpa' is not defined
```

**To Reproduce**
Steps to reproduce the behavior:
1. [Stand-alone Deployment](https://github.com/FederatedAI/FATE/tree/master/standalone-deploy)
2. Follow the [Pipeline Examples](https://github.com/FederatedAI/FATE/blob/master/examples/pipeline/README.rst) steps 1~4
3. Modify some lines in
https://github.com/FederatedAI/FATE/blob/178f04d1a58181359d6550b4673d4b4dc72a778f/examples/pipeline/demo/pipeline-upload.py#L46-L60
Change `breast_hetero_guest` and `breast_hetero_host` to `breast_homo_guest` and `breast_homo_host` in these lines respectively.
4. Follow the [Homo SecureBoost Pipeline Example Usage Guide.](https://github.com/FederatedAI/FATE/blob/master/examples/pipeline/homo_sbt/README.md)
```
root@66242165a59e:/fate/examples/pipeline/homo_sbt# pipeline_script=pipeline-homo-sbt-binary-with-predict.py
root@66242165a59e:/fate/examples/pipeline/homo_sbt# python ${pipeline_script}
```
5. See errors.

**Expected behavior**
A good result with job id in FATE board.

**Screenshots**
![image](https://user-images.githubusercontent.com/45092129/114555080-f0dee180-9c99-11eb-91c4-d2bad54d03fc.png)
Job 20 is training, successfully finished, and job 23 is predicting, failed.
Job 20:
![image](https://user-images.githubusercontent.com/45092129/114555508-59c65980-9c9a-11eb-9a2f-ba09f7121df9.png)
Job 23:
![image](https://user-images.githubusercontent.com/45092129/114555828-a9a52080-9c9a-11eb-9238-731506f739d2.png)

**Desktop (please complete the following information):**
 - OS: Ubuntu Server 18.04, on VMWare
 - Network of VM: bridged network
 - FATE Stand-alone Version 1.6.0

**Additional context**
If need any other context about the problem, I'm willing to provide with.
This a typo in homo-sbt. Rename 'mpa' to 'map' in homo_secureboost_client.py line 159.
Thanks for your prompt report, we've fixed this problem in the master branch.Code already merge into master,  if user uses release package of fate-v1.6.0，should be updated into master's code.
code path: python/federatedml/ensemble/boosting/homo/homo_secureboost_client.py",2,2021-04-13 13:02:48,2021-04-25 06:11:04,2021-04-25 06:10:13
https://github.com/FederatedAI/FATE/issues/2742,[],请问一下FATE对具体的Linux版本有要求吗？比如SUSE、centos，或者 ubantu这些linux都支持吗？,请问一下FATE对具体的Linux版本有要求吗？比如SUSE、centos，或者 ubantu这些linux都支持吗？You can use docker to experience FATE and to avoid problems in some distributions of Linux.,1,2021-04-13 01:14:41,2021-05-26 13:40:21,2021-05-26 13:40:21
https://github.com/FederatedAI/FATE/issues/2739,[],fate集群，使用mnist训练数据模型，按照彭路工程师发布的博客操作，为什么任务出错了？,"fate集群，使用mnist训练数据模型，按照彭路工程师发布的博客操作，为什么任务出错了？我按照这个彭路工程师的操作，https://my.oschina.net/u/4238514/blog/3279779
使用fate集群，mnist数据集来训练模型，
为什么任务训练出错了。。
看不懂报错信息55555
不知道怎么解决，求大神指教，，


任务提交成功，但是执行失败，报错信息如下：
![image](https://user-images.githubusercontent.com/50815935/114028361-f9ef3d80-98aa-11eb-82b6-00675187d554.png)


修改损失函数为categorical_crossentpoty时，报错如下：
![image](https://user-images.githubusercontent.com/50815935/114028241-d88e5180-98aa-11eb-98d3-a713e943f377.png)

[ERROR] [2021-04-08 12:07:35,634] [2600:140528646215488] - task_executor.py[line:191]: You are passing a target array of shape (30000, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:

```

from keras.utils import to_categorical

y_binary = to_categorical(y_int)

y_binary = to_categorical(y_int)

```

Alternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.

Traceback (most recent call last):

  File ""./fate/python/fate_flow/operation/task_executor.py"", line 168, in run_task

    run_object.run(component_parameters_on_party, task_run_args)

  File ""./fate/python/federatedml/model_base.py"", line 101, in run

    this_data_output = func(*params)

  File ""./fate/python/federatedml/nn/homo_nn/enter_point.py"", line 107, in fit

    _version_0.client_fit(self=self, data_inst=data)

  File ""./fate/python/federatedml/nn/homo_nn/_version_0.py"", line 161, in client_fit

    self.nn_model.train(data, aggregate_every_n_epoch=self.aggregate_every_n_epoch)

  File ""./fate/python/federatedml/nn/backend/tf_keras/nn_model.py"", line 250, in train

    self._model.fit(x=data, epochs=epochs, verbose=1, shuffle=True, **left_kwargs)

  File ""/opt/app-root/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 808, in fit

    use_multiprocessing=use_multiprocessing)

  File ""/opt/app-root/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator_v1.py"", line 593, in fit

    steps_name='steps_per_epoch')

  File ""/opt/app-root/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator_v1.py"", line 259, in model_iteration

    batch_outs = batch_function(*batch_data)

  File ""/opt/app-root/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 1063, in train_on_batch

    extract_tensors_from_dataset=True)

  File ""/opt/app-root/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 2336, in _standardize_user_data




构建出的模型json：
{""class_name"": ""Sequential"", ""config"": {""name"": ""sequential"", ""layers"": [{""class_name"": ""InputLayer"", ""config"": {""batch_input_shape"": [null, 784], ""dtype"": ""float32"", ""sparse"": false, ""ragged"": false, ""name"": ""dense_input""}}, {""class_name"": ""Dense"", ""config"": {""name"": ""dense"", ""trainable"": true, ""batch_input_shape"": [null, 784], ""dtype"": ""float32"", ""units"": 512, ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""Dense"", ""config"": {""name"": ""dense_1"", ""trainable"": true, ""dtype"": ""float32"", ""units"": 256, ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""Dense"", ""config"": {""name"": ""dense_2"", ""trainable"": true, ""dtype"": ""float32"", ""units"": 10, ""activation"": ""softmax"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}]}, ""keras_version"": ""2.4.0"", ""backend"": ""tensorflow""}
兄弟问题解决了吗",2,2021-04-08 12:45:01,2021-11-11 05:42:31,2021-04-11 05:05:26
https://github.com/FederatedAI/FATE/issues/2665,[],evaluation评测结果是否又bug？！,"evaluation评测结果是否又bug？！前端显示是否存在bug？
如下图所示，通过混淆矩阵手动计算的精确率和召回率，和前端显示的完全不一致，是前端显示的bug吗？

![微信图片_20210319152059](https://user-images.githubusercontent.com/34981842/111744869-d5fd9500-88c6-11eb-9f32-e523535eb208.jpg)

hi，
上面那个的threshold是50%分位点（Quantile），
下面那个的threshold是0.5（实值），二者是不一样的。Problem solved, issue close",2,2021-03-19 07:25:11,2021-04-05 02:03:26,2021-04-05 02:03:25
https://github.com/FederatedAI/FATE/issues/2646,[],请教Homo_LR训练卡在initialized model！！,"请教Homo_LR训练卡在initialized model！！请教一下为什么使用homo_lr建模时，一直卡在initialized model，可能是什么原因造成的？
log信息如下图所示
![image](https://user-images.githubusercontent.com/34981842/111022797-0b5e3a80-8410-11eb-8bb0-1f2c7b2952e9.png)
![image](https://user-images.githubusercontent.com/34981842/111022805-216bfb00-8410-11eb-8f9a-26970934046b.png)
没有问题了，其中某一方数据异常，导致整体进度卡住。",1,2021-03-13 07:25:50,2021-04-23 02:55:55,2021-04-23 02:55:55
https://github.com/FederatedAI/FATE/issues/2643,[],lmdb.MemoryError: Cannot allocate memory,"lmdb.MemoryError: Cannot allocate memoryI have successfully installed FATE. However, when I try to run sh ./python/federatedml/test/run_test.sh with requiring memory allocation, I always get this kind of error:

Traceback (most recent call last):
  File ""/home/Data/standalone-fate-master-1.5.0/python/federatedml/test/../util/test/data_io_test.py"", line 279, in setUp
    self.table1 = session.parallelize(self.data, include_key=True, partition=16)
  File ""/home/Data/standalone-fate-master-1.5.0/python/fate_arch/session/_session.py"", line 277, in parallelize
    return get_latest_opened().computing.parallelize(data, partition=partition, include_key=include_key, **kwargs)
  File ""/home/Data/standalone-fate-master-1.5.0/python/fate_arch/computing/standalone/_csession.py"", line 54, in parallelize
    table = self._session.parallelize(data=data, partition=partition, include_key=include_key, **kwargs)
  File ""/home/Data/standalone-fate-master-1.5.0/python/fate_arch/_standalone.py"", line 291, in parallelize
    table.put_all(data)
  File ""/home/Data/standalone-fate-master-1.5.0/python/fate_arch/_standalone.py"", line 235, in put_all
    env = s.enter_context(self._get_env_for_partition(p, write=True))
  File ""/home/Data/standalone-fate-master-1.5.0/python/fate_arch/_standalone.py"", line 221, in _get_env_for_partition
    return _get_env(self._namespace, self._name, str(p), write=write)
  File ""/home/Data/standalone-fate-master-1.5.0/python/fate_arch/_standalone.py"", line 616, in _get_env
    return _open_env(_path, write=write)
  File ""/home/Data/standalone-fate-master-1.5.0/python/fate_arch/_standalone.py"", line 633, in _open_env
    raise e
  File ""/home/Data/standalone-fate-master-1.5.0/python/fate_arch/_standalone.py"", line 626, in _open_env
    map_size=10_737_418_240)
lmdb.MemoryError: /home/Data/standalone-fate-master-1.5.0/data/test_dataio_0.630400495002178/ccde6610-82d7-11eb-846e-9522b2cd674b/8: Cannot allocate memory

This also happens for other tests requiring memory allocations. For other tests, it works properly.

How to solve this issue?from error message, it seems lmdb filled to allocate memory, how many ram and disk you used to deploy `FATE`?Even when I run this simple example ""python ./examples/toy_example/run_toy_example.py 10000 10000 0"", it also makes a memory allocation error. When I check the system monitor, it is less than 100 MB for running that example. The computer has 88 GB of RAM.

To run FATE, I use virtual env. Does it make any effect?

When I run sh init.sh init after entering the fate folder, it shows like this:

(fate) [yes@mercury9 standalone-fate-master-1.5.0-new]$ sh init.sh init
""Red Hat Enterprise Linux Workstation""
Not support this system.
Requirement already satisfied: beautifultable==1.0.0 from file:///home/yes/Data/urise/standalone-fate-master-1.5.0-new/system-package/python-package/beautifultable-1.0.0-py2.py3-none-any.whl in ./miniconda3-fate/lib/python3.7/site-packages (1.0.0)
Requirement already satisfied: cos-python-sdk-v5==1.8.0 from file:///home/yes/Data/urise/standalone-fate-master-1.5.0-new/system-package/python-package/cos_python_sdk_v5-1.8.0-py3-none-any.whl in ./miniconda3-fate/lib/python3.7/site-packages (1.8.0)
Requirement already satisfied: requests-toolbelt==0.9.1 from file:///home/yes/Data/urise/standalone-fate-master-1.5.0-new/system-package/python-package/requests_toolbelt-0.9.1-py2.py3-none-any.whl in ./miniconda3-fate/lib/python3.7/site-packages (0.9.1)
Requirement already satisfied: wcwidth in ./miniconda3-fate/lib/python3.7/site-packages (from beautifultable==1.0.0) (0.2.5)
Requirement already satisfied: requests>=2.8 in ./miniconda3-fate/lib/python3.7/site-packages (from cos-python-sdk-v5==1.8.0) (2.22.0)
Requirement already satisfied: six in ./miniconda3-fate/lib/python3.7/site-packages (from cos-python-sdk-v5==1.8.0) (1.12.0)
Requirement already satisfied: dicttoxml in ./miniconda3-fate/lib/python3.7/site-packages (from cos-python-sdk-v5==1.8.0) (1.7.4)
Requirement already satisfied: idna<2.9,>=2.5 in ./miniconda3-fate/lib/python3.7/site-packages (from requests>=2.8->cos-python-sdk-v5==1.8.0) (2.8)
Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./miniconda3-fate/lib/python3.7/site-packages (from requests>=2.8->cos-python-sdk-v5==1.8.0) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./miniconda3-fate/lib/python3.7/site-packages (from requests>=2.8->cos-python-sdk-v5==1.8.0) (1.24.2)
Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3-fate/lib/python3.7/site-packages (from requests>=2.8->cos-python-sdk-v5==1.8.0) (2020.6.20)
PROJECT_BASE: /home/yes/Data/urise/standalone-fate-master-1.5.0-new
PYTHONPATH: /home/yes/Data/urise/standalone-fate-master-1.5.0-new/python
EGGROLL_HOME: 
SPARK_HOME: 
found service conf: /home/yes/Data/urise/standalone-fate-master-1.5.0-new/conf/service_conf.yaml
fate flow http port: 9380, grpc port: 9360

killing: yes 31054  0.7  0.0 1069160 80808 pts/3   Sl   13:03   4:32 python /home/yes/Data/urise/standalone-fate-master-1.5.0/python/fate_flow/fate_flow_server.py
killed by SIGTERM
service start sucessfully. pid: 31826
status:yes 31826 30.4  0.0 845516 74472 pts/1    Sl+  22:58   0:01 python /home/yes/Data/urise/standalone-fate-master-1.5.0-new/python/fate_flow/fate_flow_server.py
python  31826 yes   13u  IPv4 563909571      0t0  TCP localhost.localdomain:boxp (LISTEN)
python  31826 yes   10u  IPv6 563909570      0t0  TCP localhost.localdomain:9360 (LISTEN)
service not running
service start sucessfully. pid: 32318
status:
        yes 32318  213  0.9 6899284 881724 pts/1  Sl+  22:58   0:21 /home/yes/Data/urise/standalone-fate-master-1.5.0-new/jdk/jdk1.8.0_192/bin/java -Dspring.config.location=/home/yes/Data/urise/standalone-fate-master-1.5.0-new/fateboard/conf/application.properties -DFATE_DEPLOY_PREFIX=/home/yes/Data/urise/standalone-fate-master-1.5.0-new/logs/ -Dssh_config_file=/home/yes/Data/urise/standalone-fate-master-1.5.0-new/fateboard/ssh/ -Xmx2048m -Xms2048m -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:gc.log -XX:+HeapDumpOnOutOfMemoryError -jar /home/yes/Data/urise/standalone-fate-master-1.5.0-new/fateboard/fateboard.jar
(fate) [yes@mercury9 standalone-fate-master-1.5.0-new]$",2,2021-03-12 02:59:55,2021-04-23 02:56:09,2021-04-23 02:56:09
https://github.com/FederatedAI/FATE/issues/2641,[],Homo OneHot Encoder是否有bug,"Homo OneHot Encoder是否有bug使用横向onehot--‘’Homo_OneHot_Encoder‘’，测试集单独onehot_1，输入数据集为测试数据集及onehot_0的model，特征可以保持一致。同时也会存在问题，测试集单独onehot_1的得到的特征同训练集onehot_0一致，但onehot_1之后out_put的数据全是0。
```json
""one_hot_0"":{
   ""module"":""HomoOneHotEncoder"",
   ""input"":{
      ""data"":{
         ""data"":[
            ""dataio_0.data""
         ]
      }
   },
   ""output"":{
      ""data"":[
         ""data""
      ],
      ""model"":[
         ""model""
      ]
   }
},
""one_hot_1"":{
   ""module"":""HomoOneHotEncoder"",
   ""input"":{
      ""data"":{
         ""data"":[
            ""dataio_1.data""
         ]
      },
      ""model"":[
         ""one_hot_0.model""
      ]
   },
   ""output"":{
      ""data"":[
         ""data""
      ],
      ""model"":[
         ""model""
      ]
   }
},
```
@xiaoqing928 感谢提出，预计会在1.5.2和1.6同步修复",1,2021-03-11 02:58:27,2021-06-09 03:29:15,2021-06-09 03:29:15
https://github.com/FederatedAI/FATE/issues/2639,[],OnehotEncoder逻辑是否有Bug,"OnehotEncoder逻辑是否有Bugsklearn的onehot分两个步骤：
1、将训练集与测试集的数据合并做fit
2、分别对训练集数据、测试集数据做tranform得到2份结果

但fate的onehot，却是训练集单独onehot_0，输入数据集只有训练集自身的数据，测试集单独onehot_1，输入数据集为测试数据集及onehot_0的model，并没有将2个数据集合并做fit的过程：
""one_hot_0"": {
            ""module"": ""OneHotEncoder"",
            ""input"": {
                ""data"": {
                    ""data"": [
                        ""intersection_0.data""
                    ]
                }
            },
            ""output"": {
                ""data"": [
                    ""data""
                ],
                ""model"": [
                    ""model""
                ]
            }
        },
        ""one_hot_1"": {
            ""module"": ""OneHotEncoder"",
            ""input"": {
                ""data"": {
                    ""data"": [
                        ""intersection_1.data""
                    ]
                },
                ""model"": [
                    ""one_hot_0.model""
                ]
            },
            ""output"": {
                ""data"": [
                    ""data""
                ],
                ""model"": [
                    ""model""
                ]
            }
        },你好，我使用过横向onehot--‘’Homo_OneHot_Encoder‘’，跟你的处理一样，测试集单独onehot_1，输入数据集为测试数据集及onehot_0的model，特征可以保持一致。同时也会存在问题，测试集单独onehot_1的得到的特征同训练集onehot_0一致，但onehot_1之后out_put的数据全是0。很奇怪，希望有人能解答一下。他的逻辑是没有问题的，fate会把训练集onehot之后的特征记录下来，测试集只需和训练集保持一致即可> 他的逻辑是没有问题的，fate会把训练集onehot之后的特征记录下来，测试集只需和训练集保持一致即可

如果测试集里有的特征值在训练集里是没有的呢，比如城市，训练集里有上海、北京、深圳，训练集onehot之后扩展出来的特征列是city_上海，city_北京，city_深圳，那测试集里出现城市是成都的呢？

难道必须要保证测试集里的特征值不要超出训练集里包含的取值吗？这样数据准备很麻烦啊


> > 他的逻辑是没有问题的，fate会把训练集onehot之后的特征记录下来，测试集只需和训练集保持一致即可
> 
> 如果测试集里有的特征值在训练集里是没有的呢，比如城市，训练集里有上海、北京、深圳，训练集onehot之后扩展出来的特征列是city_上海，city_北京，city_深圳，那测试集里出现城市是成都的呢？
> 
> 难道必须要保证测试集里的特征值不要超出训练集里包含的取值吗？这样数据准备很麻烦啊

你说的这个问题的话，即使加进了验证数据，未来遇到真实预测数据的时候，一样有可能遇到从未出现的值，这个无法避免的。现在的逻辑是，如果测试集出现训练集没有的值，则转化的所有值都为0.",4,2021-03-10 15:22:18,2021-07-02 08:14:41,2021-07-02 08:14:41
https://github.com/FederatedAI/FATE/issues/2619,[],"运行dsl v2中的hetero fast secureboost报错""__init__() got an unexpected keyword argument 'sparse_optmization'""","运行dsl v2中的hetero fast secureboost报错""__init__() got an unexpected keyword argument 'sparse_optmization'""想跑下dsl_v2的hetero fast secureboost，运行代码：python fate_flow_client.py -f submit_job -d examples/test_fast_sbt_dsl.json -c examples/test_fast_sbt_mix_conf.json python，报错：
{
    ""retcode"": 100,
    ""retmsg"": ""__init__() got an unexpected keyword argument 'sparse_optmization'""
}

which fate version？> ""**init**() got an unexpected keyword argument 'sparse_optmization'""

hi, how to solve this issue?> which fate version？

1.5@peiji1981 @furuifr  hetero fast secure boost typo `sparse_optmization` problem has been solved in FATE-1.6  
ref to #2331 

# FATE-1.5
![fate_old_1 5](https://user-images.githubusercontent.com/1425792/133391832-f46ff3ae-5b57-4d4e-9ce1-0e90788f4bb3.png)

# FATE-1.6
![fate_new_1 6](https://user-images.githubusercontent.com/1425792/133392080-58132ef1-42d7-4f4a-99e8-62ff270242c1.png)

 thanks report 
it's typo issue and fix in 1.5.1, close issue",5,2021-02-26 17:25:30,2021-09-15 08:04:43,2021-09-15 08:04:43
https://github.com/FederatedAI/FATE/issues/2512,[],fate_flow/fate_flow_server.py can not find module concurrent,"fate_flow/fate_flow_server.py can not find module concurrentWhen use docker-compose deploy, confs10000_python_1 container can not start

2efaa2f15508        federatedai/serving-proxy:2.0.0-release    ""/bin/sh -c 'java -D…""   5 hours ago         Up 5 hours                     0.0.0.0:8059->8059/tcp, 0.0.0.0:8869->8869/tcp, 8879/tcp   serving10000_serving-proxy_1
6e29ba2b0188        federatedai/serving-server:2.0.0-release   ""/bin/sh -c 'java -c…""   5 hours ago         Up 5 hours                     0.0.0.0:8000->8000/tcp                                     serving10000_serving-server_1
b6e112e32611        federatedai/client:1.5.0-release           ""/bin/sh -c 'flow in…""   5 hours ago         Up 5 hours                     0.0.0.0:20000->20000/tcp                                   confs10000_client_1
a9c2bfe8c22b        federatedai/fateboard:1.5.0-release        ""/bin/sh -c 'java -D…""   5 hours ago         Up 5 hours                     0.0.0.0:8080->8080/tcp                                     confs10000_fateboard_1
6a8a992e8572        federatedai/python:1.5.0-release           ""/bin/bash -c 'sleep…""   5 hours ago         Restarting (1) 7 seconds ago                                                              confs10000_python_1
4f07ea6513ef        federatedai/eggroll:1.5.0-release          ""/tini -- bash -c 'j…""   5 hours ago         Up 5 hours                     0.0.0.0:9370->9370/tcp                                     confs10000_rollsite_1
d0b65af2773f        federatedai/eggroll:1.5.0-release          ""/tini -- bash -c 'j…""   5 hours ago         Up 5 hours                     4670/tcp                                                   confs10000_clustermanager_1
67c20ddef329        federatedai/eggroll:1.5.0-release          ""/tini -- bash -c 'j…""   5 hours ago         Up 5 hours                     4671/tcp                                                   confs10000_nodemanager_1

docker log as follow:
Traceback (most recent call last):
  File ""./fate_flow/fate_flow_server.py"", line 20, in <module>
    from concurrent import futures
ImportError: No module named concurrentme tooalready solved",2,2021-01-28 07:50:41,2022-04-20 07:18:49,2022-04-20 07:18:14
https://github.com/FederatedAI/FATE/issues/2452,[],线性回归模型在线测试无预测结果,"线性回归模型在线测试无预测结果host.csv数据
`id,x1
1,1
2,2
3,3
4,4
5,5
6,6
7,7
8,8`
guest.csv数据
`id,y,x0
1,1,0
2,2,1
3,3,0
4,4,1`
job_conf.json
```
{
    ""initiator"": {
        ""role"": ""guest"",
        ""party_id"": 9999
    },
    ""job_parameters"": {
        ""work_mode"": 1
    },
    ""role"": {
        ""guest"": [
            9999
        ],
        ""host"": [
            10000
        ],
        ""arbiter"": [
            10000
        ]
    },
    ""role_parameters"": {
        ""guest"": {
            ""args"": {
                ""data"": {
                    ""train_data"": [
                        {
                            ""name"": ""test"",
                            ""namespace"": ""test""
                        }
                    ]
                }
            },
            ""dataio_0"": {
                ""with_label"": [
                    true
                ],
                ""label_name"": [
                    ""y""
                ],
                ""label_type"": [
                    ""float""
                ],
                ""output_format"": [
                    ""dense""
                ],
                ""missing_fill"": [
                    true
                ],
                ""outlier_replace"": [
                    false
                ]
            },
            ""evaluation_0"": {
                ""eval_type"": [
                    ""regression""
                ],
                ""pos_label"": [
                    1
                ]
            }
        },
        ""host"": {
            ""args"": {
                ""data"": {
                    ""train_data"": [
                        {
                            ""name"": ""test"",
                            ""namespace"": ""test""
                        }
                    ]
                }
            },
            ""dataio_0"": {
                ""with_label"": [
                    false
                ],
                ""output_format"": [
                    ""dense""
                ],
                ""outlier_replace"": [
                    false
                ]
            },
            ""evaluation_0"": {
                ""need_run"": [
                    false
                ]
            }
        }
    },
    ""algorithm_parameters"": {
        ""hetero_linr_0"": {
            ""penalty"": ""L2"",
            ""optimizer"": ""sgd"",
            ""tol"": 0.001,
            ""alpha"": 0.01,
            ""max_iter"": 20,
            ""early_stop"": ""weight_diff"",
            ""batch_size"": -1,
            ""learning_rate"": 0.15,
            ""decay"": 0.0,
            ""decay_sqrt"": false,
            ""init_param"": {
                ""init_method"": ""zeros""
            },
            ""encrypted_mode_calculator_param"": {
                ""mode"": ""fast""
            }
        }
    }
}
```
dsl_conf.json
`{
    ""components"" : {
        ""dataio_0"": {
            ""module"": ""DataIO"",
            ""input"": {
                ""data"": {
                    ""data"": [
                        ""args.train_data""
                    ]
                }
            },
            ""output"": {
                ""data"": [""train""],
                ""model"": [""dataio""]
            }
         },
         ""intersection_0"": {
             ""module"": ""Intersection"",
             ""input"": {
                 ""data"": {
                     ""data"": [
                         ""dataio_0.train""
                     ]
                 }
             },
             ""output"": {
                 ""data"": [""train""]
             }
         },
        ""hetero_linr_0"": {
            ""module"": ""HeteroLinR"",
            ""input"": {
                ""data"": {
                    ""train_data"": [""intersection_0.train""]
                }
            },
            ""output"": {
                ""data"": [""train""],
                ""model"": [""hetero_linr""]
            }
        },
        ""evaluation_0"": {
            ""module"": ""Evaluation"",
            ""input"": {
                ""data"": {
                    ""data"": [""hetero_linr_0.train""]
                }
            }
        }
    }
}`
调用在线推理接口
curl -X POST -H 'Content-Type: application/json' -i 'http://localhost:8059/federation/v1/inference' --data '{
  ""head"": {
    ""serviceId"": ""test""
  },
  ""body"": {
    ""featureData"": {
      ""x0"": 1
    },
    ""sendToRemoteFeatureData"": {
      ""id"": ""5""
    }
  }
}'
预测结果
{
    ""retcode"": 0,
    ""retmsg"": """",
    ""data"": {
        ""modelId"": ""guest#9999#arbiter-10000#guest-9999#host-10000#model"",
        ""modelVersion"": ""2021010412180273130115"",
        ""timestamp"": 1609809836490
    },
    ""flag"": 0
}
无预测结果值，是线性回归算法的bug吗Serving目前不支持线性模型。所有支持的算法组件请参考：https://github.com/FederatedAI/FATE-Serving/tree/master/fate-serving-federatedml/src/main/java/com/webank/ai/fate/serving/federatedml/model 

Serving currently does not support online inference with HeteroLinR model. For supported modules, please refer here:  https://github.com/FederatedAI/FATE-Serving/tree/master/fate-serving-federatedml/src/main/java/com/webank/ai/fate/serving/federatedml/model",1,2021-01-07 02:46:35,2021-01-15 08:42:48,2021-01-15 08:42:48
https://github.com/FederatedAI/FATE/issues/2449,[],"Pipeline Examples 'Connection refused, Please check if the fate flow service is started'","Pipeline Examples 'Connection refused, Please check if the fate flow service is started'Hi there,

I am interested in using FATE to do some research problems. 

I used the Stand-alone Deployment Guide with docker  environment successfully. 

Unfortunately, when I try to execute the command in the demo folder below:
root@e7e3434e58b1:/fate/examples/pipeline/demo# python pipeline-upload.py -base /fate

I got these error informations below:
2021-01-06 08:47:03.424 | ERROR    | __main__:main:63 - An error has been caught in function 'main', process 'MainProcess' (369), thread 'MainThread' (139780264617792):
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/site-packages/pipeline/utils/invoker/job_submitter.py"", line 105, in upload_data
    raise ValueError

ValueError


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""pipeline-upload.py"", line 73, in <module>
    main(args.base)
    │    │    └ '/fate'
    │    └ Namespace(base='/fate')
    └ <function main at 0x7f2120e02e18>

> File ""pipeline-upload.py"", line 63, in main
    pipeline_upload.upload(work_mode=work_mode, backend=backend, drop=1)
    │               │                │                  └ <Backend.EGGROLL: 0>
    │               │                └ <WorkMode.STANDALONE: 0>
    │               └ <function PipeLine.upload at 0x7f20e9652950>
    └ <pipeline.backend.pipeline.PipeLine object at 0x7f20e9643b00>

  File ""/usr/local/lib/python3.6/site-packages/pipeline/backend/pipeline.py"", line 495, in upload
    self._train_job_id, detail_info = self._job_invoker.upload_data(upload_conf, int(drop))
    │    │                            │    │            │           │                └ 1
    │    │                            │    │            │           └ {'file': '/fate/examples/data/breast_hetero_guest.csv', 'table_name': 'breast_hetero_guest', 'namespace': 'experiment', 'head...
    │    │                            │    │            └ <function JobInvoker.upload_data at 0x7f20e96482f0>
    │    │                            │    └ <pipeline.utils.invoker.job_submitter.JobInvoker object at 0x7f20e9643ac8>
    │    │                            └ <pipeline.backend.pipeline.PipeLine object at 0x7f20e9643b00>
    │    └ None
    └ <pipeline.backend.pipeline.PipeLine object at 0x7f20e9643b00>
  File ""/usr/local/lib/python3.6/site-packages/pipeline/utils/invoker/job_submitter.py"", line 113, in upload_data
    raise ValueError(""job submit failed, err msg: {}"".format(result))
                                                             └ {'retcode': 100, 'retmsg': 'Connection refused, Please check if the fate flow service is started'}

ValueError: job submit failed, err msg: {'retcode': 100, 'retmsg': 'Connection refused, Please check if the fate flow service is started'}
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/pipeline/utils/invoker/job_submitter.py"", line 105, in upload_data
    raise ValueError
ValueError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""pipeline-upload.py"", line 73, in <module>
    main(args.base)
  File ""pipeline-upload.py"", line 63, in main
    pipeline_upload.upload(work_mode=work_mode, backend=backend, drop=1)
  File ""/usr/local/lib/python3.6/site-packages/loguru/_logger.py"", line 1220, in catch_wrapper
    return function(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/pipeline/backend/pipeline.py"", line 495, in upload
    self._train_job_id, detail_info = self._job_invoker.upload_data(upload_conf, int(drop))
  File ""/usr/local/lib/python3.6/site-packages/pipeline/utils/invoker/job_submitter.py"", line 113, in upload_data
    raise ValueError(""job submit failed, err msg: {}"".format(result))
ValueError: job submit failed, err msg: {'retcode': 100, 'retmsg': 'Connection refused, Please check if the fate flow service is started'}


Any suggestions are welcome.
Thanks
When initialize pipeline ""pipeline init --ip 127.0.0.1 --port 9380"",
change 127.0.0.1 to the contianer ip,  such as ""192.167.1.100""
It works for me.Find your contianer ip as follow
![image](https://user-images.githubusercontent.com/14838533/103766154-4d0de600-5059-11eb-82ba-1059aa453419.png)
> Find your contianer ip as follow
> ![image](https://user-images.githubusercontent.com/14838533/103766154-4d0de600-5059-11eb-82ba-1059aa453419.png)

Yes, it works! Thanks soooo much.",3,2021-01-06 08:53:33,2021-01-06 12:05:02,2021-01-06 12:05:02
https://github.com/FederatedAI/FATE/issues/2440,[],为什么按照案例跑不起来,"为什么按照案例跑不起来做了个简单的LR 两份数据上传成功
# python /home/fate/standalone-fate-master-1.5.0/python/fate_flow/fate_flow_client.py -f table_info -n experiment -t breast_hetero_host
{
    ""data"": {
        ""count"": 570,
        ""namespace"": ""experiment"",
        ""partition"": 1,
        ""schema"": {
            ""header"": ""x0,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19"",
            ""sid"": ""id""
        },
        ""table_name"": ""breast_hetero_host""
    },
    ""retcode"": 0,
    ""retmsg"": ""success""
}

# python /home/fate/standalone-fate-master-1.5.0/python/fate_flow/fate_flow_client.py -f table_info -n experiment -t breast_hetero_guest
{
    ""data"": {
        ""count"": 570,
        ""namespace"": ""experiment"",
        ""partition"": 1,
        ""schema"": {
            ""header"": ""y,x0,x1,x2,x3,x4,x5,x6,x7,x8,x9"",
            ""sid"": ""id""
        },
        ""table_name"": ""breast_hetero_guest""
    },
    ""retcode"": 0,
    ""retmsg"": ""success""
}

conf文件如下，没有改，直接从文件中拿的
{
    ""initiator"": {
        ""role"": ""guest"",
        ""party_id"": 10000
    },
    ""job_parameters"": {
        ""work_mode"": 0
    },
    ""role"": {
        ""guest"": [
            10000
        ],
        ""host"": [
            10000
        ],
        ""arbiter"": [
            10000
        ]
    },
    ""role_parameters"": {
        ""guest"": {
            ""args"": {
                ""data"": {
                    ""train_data"": [
                        {
                            ""name"": ""breast_hetero_guest"",
                            ""namespace"": ""experiment""
                        }
                    ],
                    ""eval_data"": [
                        {
                            ""name"": ""breast_hetero_guest"",
                            ""namespace"": ""experiment""
                        }
                    ]
                }
            },
            ""dataio_0"": {
                ""with_label"": [
                    true
                ],
                ""label_name"": [
                    ""y""
                ],
                ""label_type"": [
                    ""int""
                ],
                ""output_format"": [
                    ""dense""
                ],
                ""missing_fill"": [
                    true
                ],
                ""outlier_replace"": [
                    true
                ]
            },
            ""evaluation_0"": {
                ""eval_type"": [
                    ""binary""
                ],
                ""pos_label"": [
                    1
                ]
            }
        },
        ""host"": {
            ""args"": {
                ""data"": {
                    ""train_data"": [
                        {
                            ""name"": ""breast_hetero_host"",
                            ""namespace"": ""experiment""
                        }
                    ],
                    ""eval_data"": [
                        {
                            ""name"": ""breast_hetero_host"",
                            ""namespace"": ""experiment""
                        }
                    ]
                }
            },
            ""dataio_0"": {
                ""with_label"": [
                    false
                ],
                ""output_format"": [
                    ""dense""
                ],
                ""outlier_replace"": [
                    true
                ]
            },
            ""evaluation_0"": {
                ""need_run"": [
                    false
                ]
            }
        }
    },
    ""algorithm_parameters"": {
        ""hetero_lr_0"": {
            ""penalty"": ""L2"",
            ""optimizer"": ""rmsprop"",
            ""tol"": 0.0001,
            ""alpha"": 0.01,
            ""max_iter"": 30,
            ""early_stop"": ""diff"",
            ""batch_size"": -1,
            ""learning_rate"": 0.15,
            ""init_param"": {
                ""init_method"": ""zeros""
            },
            ""sqn_param"": {
                ""update_interval_L"": 3,
                ""memory_M"": 5,
                ""sample_size"": 5000,
                ""random_seed"": null
            },
            ""cv_param"": {
                ""n_splits"": 5,
                ""shuffle"": false,
                ""random_seed"": 103,
                ""need_cv"": false
            }
        },
        ""intersect_0"": {
            ""intersect_method"": ""rsa"",
            ""sync_intersect_ids"": true,
            ""only_output_key"": false
        }
    }
}

dsl文件也是
{
    ""components"" : {
        ""dataio_0"": {
            ""module"": ""DataIO"",
            ""input"": {
                ""data"": {
                    ""data"": [
                        ""args.train_data""
                    ]
                }
            },
            ""output"": {
                ""data"": [""train""],
                ""model"": [""dataio""]
            }
         },
         ""intersection_0"": {
             ""module"": ""Intersection"",
             ""input"": {
                 ""data"": {
                     ""data"": [
                         ""dataio_0.train""
                     ]
                 }
             },
             ""output"": {
                 ""data"": [""train""]
             }
         },
        ""hetero_lr_0"": {
            ""module"": ""HeteroLR"",
            ""input"": {
                ""data"": {
                    ""train_data"": [""intersection_0.train""]
                }
            },
            ""output"": {
                ""data"": [""train""],
                ""model"": [""hetero_lr""]
            }
        },
        ""evaluation_0"": {
            ""module"": ""Evaluation"",
            ""input"": {
                ""data"": {
                    ""data"": [""hetero_lr_0.train""]
                }
            }
        }
    }
}

运行后日志报错

[ERROR] [2021-01-04 16:58:58,271] [23915:139688257656640] - task_executor.py[line:196]: input data's value is empty, it does not contain a label
2
Traceback (most recent call last):
3
  File ""./fate/python/fate_flow/operation/task_executor.py"", line 174, in run_task
4
    run_object.run(component_parameters_on_party, task_run_args)
5
  File ""./fate/python/federatedml/model_base.py"", line 98, in run
6
    this_data_output = func(*real_param)
7
  File ""./fate/python/federatedml/util/data_io.py"", line 885, in fit
8
    data_inst = self.reader.read_data(data_inst, ""fit"")
9
  File ""./fate/python/federatedml/util/data_io.py"", line 133, in read_data
10
    raise ValueError(""input data's value is empty, it does not contain a label"")
11
ValueError: input data's value is empty, it does not contain a label

大神们帮帮我吧，菜鸟一个 ，只想先按照文档运行成功一次已解决 ，是数据文件的问题，直接copy整个文件 就解决了你好，我和你同样的问题，都是： input data's value is empty, it does not contain a label
我之前都能运行成功，但不知道我动了哪里，现在每次提交任务后都显示这样的失败结果
我目前从新上传了数据还是这样
请问是配置文件的问题吗你好，我和你同样的问题，都是： input data's value is empty, it does not contain a label
我之前都能运行成功，但不知道我动了哪里，现在每次提交任务后都显示这样的失败结果
我目前从新上传了数据还是这样
请问是配置文件的问题吗> 你好，我和你同样的问题，都是： input data's value is empty, it does not contain a label
> 我之前都能运行成功，但不知道我动了哪里，现在每次提交任务后都显示这样的失败结果
> 我目前从新上传了数据还是这样
> 请问是配置文件的问题吗

就是数据格式的原因，你用包里的案例中的数据 验证下",4,2021-01-04 09:48:45,2021-05-15 15:13:10,2021-01-05 05:55:23
https://github.com/FederatedAI/FATE/issues/2423,[],Fate1.5版本交集组件报错(get_commit_id),"Fate1.5版本交集组件报错(get_commit_id)**Describe the bug**
Fate1.5版本读取HDFS文件，执行纵向逻辑回归的交集组件时报错
ImportError: cannot import name 'get_commit_id'
![image](https://user-images.githubusercontent.com/41407129/103127635-7c256e80-46cd-11eb-9de4-41165bce61df.png)


经过分析源码，确实没有定义该方法。
通过修改代码，组件能够正常跑通。请官方更新代码。
base_utils.py文件增加方法定义：
def get_commit_id():
    # the model may be larger, SHA1 is not used
    return fate_uuid()

![image](https://user-images.githubusercontent.com/5196427/103132597-3920c680-46e0-11eb-9331-193abf7c2497.png)
Please check to see if your code is in release 1.5.0.
Incorrect code may have other problems.",1,2020-12-25 08:24:16,2021-04-23 02:55:11,2021-04-23 02:55:11
https://github.com/FederatedAI/FATE/issues/2412,[],集群部署的情况下，跑quick测试，任务创建成功之后一直wait,"集群部署的情况下，跑quick测试，任务创建成功之后一直wait**Describe the bug**
FATE集群部署的情况下，单边测试和双边测试都跑过了，但是quick测试，可以创建job，然后之后一直是wait，查看日志说是向eggroll申请资源失败，请赐教怎么解决谢谢

![image](https://user-images.githubusercontent.com/48507141/102451934-6d5b0e00-4074-11eb-9d7c-68578307051d.png)

我这边也是这个问题，请问您解决了吗：
""f_status"": ""success"",
            ""f_status_code"": null,
            ""f_status"": ""waiting"",
            ""f_status_code"": null,
            ""f_status"": ""waiting"",
            ""f_status_code"": null,
            ""f_status"": ""success"",
            ""f_status_code"": null,
            ""f_status"": ""waiting"",
            ""f_status_code"": null,
            ""f_status"": ""success"",
            ""f_status_code"": null,
            ""f_status"": ""waiting"",
            ""f_status_code"": null,
            ""f_status"": ""success"",
            ""f_status_code"": null,
            ""f_status"": ""waiting"",
            ""f_status_code"": null,
            ""f_status"": ""waiting"",
            ""f_status_code"": null,
            ""f_status"": ""success"",
            ""f_status_code"": null,
            ""f_status"": ""waiting"",
            ""f_status_code"": null,
            ""f_status"": ""success"",
            ""f_status_code"": null,
            ""f_status"": ""waiting"",
            ""f_status_code"": null,
            ""f_status"": ""success"",
            ""f_status_code"": null,
            ""f_status"": ""waiting"",
            ""f_status_code"": null,
> 我这边也是这个问题，请问您解决了吗：
> ""f_status"": ""success"",
> ""f_status_code"": null,
> ""f_status"": ""waiting"",
> ""f_status_code"": null,
> ""f_status"": ""waiting"",
> ""f_status_code"": null,
> ""f_status"": ""success"",
> ""f_status_code"": null,
> ""f_status"": ""waiting"",
> ""f_status_code"": null,
> ""f_status"": ""success"",
> ""f_status_code"": null,
> ""f_status"": ""waiting"",
> ""f_status_code"": null,
> ""f_status"": ""success"",
> ""f_status_code"": null,
> ""f_status"": ""waiting"",
> ""f_status_code"": null,
> ""f_status"": ""waiting"",
> ""f_status_code"": null,
> ""f_status"": ""success"",
> ""f_status_code"": null,
> ""f_status"": ""waiting"",
> ""f_status_code"": null,
> ""f_status"": ""success"",
> ""f_status_code"": null,
> ""f_status"": ""waiting"",
> ""f_status_code"": null,
> ""f_status"": ""success"",
> ""f_status_code"": null,
> ""f_status"": ""waiting"",
> ""f_status_code"": null,

我当时是因为在虚拟机部署的集群，跳过了128G交换空间的分配，在配置足够的真实机器上部署，没出现这个问题好的，谢谢了




------------------&nbsp;原始邮件&nbsp;------------------
发件人: ""HunDeMingMingBaiBai""<notifications@github.com&gt;; 
发送时间: 2021年1月25日(星期一) 下午4:37
收件人: ""FederatedAI/FATE""<FATE@noreply.github.com&gt;; 
抄送: ""李因新""<1034113547@qq.com&gt;; ""Comment""<comment@noreply.github.com&gt;; 
主题: Re: [FederatedAI/FATE] 集群部署的情况下，跑quick测试，任务创建成功之后一直wait (#2412)





  
我这边也是这个问题，请问您解决了吗：
 ""f_status"": ""success"",
 ""f_status_code"": null,
 ""f_status"": ""waiting"",
 ""f_status_code"": null,
 ""f_status"": ""waiting"",
 ""f_status_code"": null,
 ""f_status"": ""success"",
 ""f_status_code"": null,
 ""f_status"": ""waiting"",
 ""f_status_code"": null,
 ""f_status"": ""success"",
 ""f_status_code"": null,
 ""f_status"": ""waiting"",
 ""f_status_code"": null,
 ""f_status"": ""success"",
 ""f_status_code"": null,
 ""f_status"": ""waiting"",
 ""f_status_code"": null,
 ""f_status"": ""waiting"",
 ""f_status_code"": null,
 ""f_status"": ""success"",
 ""f_status_code"": null,
 ""f_status"": ""waiting"",
 ""f_status_code"": null,
 ""f_status"": ""success"",
 ""f_status_code"": null,
 ""f_status"": ""waiting"",
 ""f_status_code"": null,
 ""f_status"": ""success"",
 ""f_status_code"": null,
 ""f_status"": ""waiting"",
 ""f_status_code"": null,
  
我当时是因为在虚拟机部署的集群，跳过了128G交换空间的分配，在配置足够的真实机器上部署，没出现这个问题
 
—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub, or unsubscribe.",3,2020-12-17 06:30:54,2021-03-19 10:41:58,2021-03-19 10:41:58
https://github.com/FederatedAI/FATE/issues/2355,[],Failed to install fate_flow while running pip3 install fate-client,"Failed to install fate_flow while running pip3 install fate-client**Describe the bug**
Failed to install fate_flow while running pip3 install fate-client. Only fate_client is installed.

**To Reproduce**
Steps to reproduce the behavior:
1. Run pip3 install fate-client
2. Run pip3 list to check
3. Run /data/projects/fate/python/fate_flow/fate_flow_server.py
4. ModuleNotFoundError: No module named 'fate_flow'

**Expected behavior**
fate_flow is installed

**Screenshots**
![image](https://user-images.githubusercontent.com/13084622/100074504-cbcc0c80-2e79-11eb-8a40-deeb1a18e65e.png)

![image](https://user-images.githubusercontent.com/13084622/100074541-d686a180-2e79-11eb-8f25-62eba3e8dcc2.png)


**Desktop (please complete the following information):**
 - OS: Ubuntu 18.04
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
FATE Flow is part of FATE package, and it is not included in FATE-Client. 
As for the described behavior, it appears to be an environment issue. Make sure to first set pythonpath to /data/projects/fate/python (or the directory where fate_flow and federatedml locate), and then execute service.sh or fate_flow_server.py inside the fate_flow directory.",1,2020-11-24 09:24:17,2020-12-02 09:26:04,2020-12-02 09:26:04
https://github.com/FederatedAI/FATE/issues/2345,[],求大佬告知应该如何写预测请求,"求大佬告知应该如何写预测请求**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.

对于随便写官网案例能出结果，自己写请求全是RPC105错误
![image](https://user-images.githubusercontent.com/40295475/99971016-9969d300-2dd7-11eb-9034-6c612273aed8.png)


![image](https://user-images.githubusercontent.com/40295475/99971105-b7cfce80-2dd7-11eb-9eb2-28a0b69e99c6.png)
一旦自己写预测请求就出错
![image](https://user-images.githubusercontent.com/40295475/99971321-f82f4c80-2dd7-11eb-8a41-f92c96a94e96.png)
对于预测文件，我按照官网的格式，无法预测secureboost模型，求大佬告知如何预测
![image](https://user-images.githubusercontent.com/40295475/99971581-552b0280-2dd8-11eb-8070-0469d412ab36.png)
nothing",3,2020-11-23 14:03:35,2020-11-25 06:25:55,2020-11-25 06:25:41
https://github.com/FederatedAI/FATE/issues/2332,[],Fate 1.5 run /fate_flow/examples/test_predict_conf.json,"Fate 1.5 run /fate_flow/examples/test_predict_conf.jsonwhen run this command 
```
python /data/projects/fate/python/fate_flow/fate_flow_client.py -f submit_job -c /data/projects/fate/python/fate_flow/examples/test_predict_conf.json
```

i get below error
```
{
    ""retcode"": 100,
    ""retmsg"": ""__init__() missing 1 required positional argument: 'msg'""
}

```


content of test_predict_conf.json
```
{
    ""initiator"": {
        ""role"": ""guest"",
        ""party_id"": 9999
    },
    ""job_parameters"": {
        ""work_mode"": 1,
        ""job_type"": ""predict"",
        ""model_id"": ""arbiter-10000#guest-9999#host-9998#model"",
        ""model_version"": ""202011180908219381005"",
        ""dsl_version"": ""2""
    },
    ""role"": {
        ""guest"": [9999],
        ""host"": [9998],
        ""arbiter"": [10000]
    },
    ""role_parameters"": {
        ""guest"": {
            ""0"": {
                ""reader_0"": {
                    ""table"": {""name"": ""breast_hetero_guest"", ""namespace"": ""experiment""}
                },
                ""dataio_0"":{
                    ""with_label"": true,
                    ""label_name"": ""y"",
                    ""label_type"": ""int"",
                    ""output_format"": ""dense""
                }
            }
        },
        ""host"": {
            ""0"": {
                ""reader_0"": {
                    ""table"": {""name"": ""breast_hetero_host"", ""namespace"": ""experiment""}
                },
                ""dataio_0"":{
                    ""with_label"": false,
                    ""output_format"": ""dense""
                }
            }
        }
    }
}
```

for your info, i have no issue run on 1.4.3@cometta There exists some errors in ""job_parameter"" part of your conf file. Please refer to this doc for more details:
https://github.com/FederatedAI/FATE/blob/master/examples/experiment_template/user_usage/dsl_v2_predict_tutorial.md@tanmc123 thanks, i read through the link you shared but can't find any issue with the code. The conf file is from https://github.com/FederatedAI/FATE/blob/master/python/fate_flow/examples/test_predict_conf.json , i only modified model_id, version and the role id. For your info, i did successfully ran similar codes for older version 1.4.3 and below successfully.  Can you elaborate more on what is possibility cause the issue? i am sure the model id, version and role id are correct. @tanmc123 i successfully run https://github.com/FederatedAI/FATE/blob/master/examples/experiment_template/user_usage/dsl_v2_predict_tutorial.md  example, but what about  https://github.com/FederatedAI/FATE/blob/master/python/fate_flow/examples/test_predict_conf.json ? does it still work in 1.5 ?issue resolved after done minor changes to test_predict_conf.json

```
{
    ""dsl_version"": 2,
    ""initiator"": {
        ""role"": ""guest"",
        ""party_id"": 9999
    },
    ""job_parameters"": {
      ""common"":{
        ""work_mode"": 1,
        ""job_type"": ""predict"",
        ""model_id"": ""<id>"",
        ""model_version"": ""<id>"",
        ""dsl_version"": ""2""
     }
    },

```",4,2020-11-18 09:21:30,2020-11-19 07:31:25,2020-11-19 07:31:25
https://github.com/FederatedAI/FATE/issues/2331,[],Find a typo: sparse_optmization should be sparse_optimization,"Find a typo: sparse_optmization should be sparse_optimization**Describe the bug**
Typo in [python/federatedml/param/boosting_param.py](../blob/master/python/federatedml/param/boosting_param.py#L415) line 415, 471, 495.
Got exception ""__init__() got an unexpected keyword argument 'sparse_optmization'""

**To Reproduce**
Steps to reproduce the behavior:
1. from pipeline.component.hetero_fast_secureboost import HeteroFastSecureBoost
2. Instantiate HeteroFastSecureBoost
3. Add this component to a pipeline and call .fit()
4. See error

**Expected behavior**
sparse_optmization should be sparse_optimization.
I had the same problem. Have you solved this problem?@furuifr hetero fast secure boost typo `sparse_optmization` problem has been solved in FATE-1.6
ref to #2619 

# FATE-1.5
![fate_old_1 5](https://user-images.githubusercontent.com/1425792/133391832-f46ff3ae-5b57-4d4e-9ce1-0e90788f4bb3.png)

# FATE-1.6
![fate_new_1 6](https://user-images.githubusercontent.com/1425792/133392080-58132ef1-42d7-4f4a-99e8-62ff270242c1.png)",2,2020-11-17 10:55:50,2021-09-15 08:04:58,2021-09-15 08:04:58
https://github.com/FederatedAI/FATE/issues/2325,[],"使用1.5.0 All in one 部署FATE集群，Mysqld无法启动【2003, ""Can't connect to MySQL server】","使用1.5.0 All in one 部署FATE集群，Mysqld无法启动【2003, ""Can't connect to MySQL server】环境：ubuntu18
原本已安装过mysqld，手动停止了原mysql的服务。

在执行`deploy.sh`时，遇到了这样的提示：
```bash
java -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:logs/eggroll/rollsite.gc.log -Dlog4j.configurationFile=/data/projects/fate/eggroll/conf/log4j2.properties -cp /data/projects/fate/eggroll/lib/*: com.webank.eggroll.core.Bootstrap --bootstraps com.webank.eggroll.rollsite.EggSiteBootstrap -c /data/projects/fate/eggroll/conf/eggroll.properties -p 9370 -s eggroll-host
service start failed
service not running

service not running
service start failed, please check /data/projects/fate/logs/error.log and /data/projects/fate/logs/console.log
service not running
```

errorlog内容如下
```bash
Traceback (most recent call last):
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/connections.py"", line 583, in connect
    **kwargs)
  File ""/data/projects/fate/common/miniconda3/lib/python3.6/socket.py"", line 724, in create_connection
    raise err
  File ""/data/projects/fate/common/miniconda3/lib/python3.6/socket.py"", line 713, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 2817, in connect
    self._state.set_connection(self._connect())
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/playhouse/pool.py"", line 155, in _connect
    conn = super(PooledDatabase, self)._connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 3642, in _connect
    conn = mysql.connect(db=self.database, **self.connect_params)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/__init__.py"", line 94, in Connect
    return Connection(*args, **kwargs)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/connections.py"", line 325, in __init__
    self.connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/connections.py"", line 630, in connect
    raise exc
pymysql.err.OperationalError: (2003, ""Can't connect to MySQL server on ''xxx.xxx.xxx.xxx'' ([Errno 111] Connection refused)"")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/python/fate_flow/fate_flow_server.py"", line 91, in <module>
    init_flow_db()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 393, in inner
    with self:
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 2724, in __enter__
    self.db.connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/playhouse/pool.py"", line 108, in connect
    return super(PooledDatabase, self).connect(reuse_if_open)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 2820, in connect
    self._initialize_connection(self._state.conn)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 2666, in __exit__
    reraise(new_type, new_type(*exc_args), traceback)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 179, in reraise
    raise value.with_traceback(tb)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 2817, in connect
    self._state.set_connection(self._connect())
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/playhouse/pool.py"", line 155, in _connect
    conn = super(PooledDatabase, self)._connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 3642, in _connect
    conn = mysql.connect(db=self.database, **self.connect_params)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/__init__.py"", line 94, in Connect
    return Connection(*args, **kwargs)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/connections.py"", line 325, in __init__
    self.connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/connections.py"", line 630, in connect
    raise exc
peewee.OperationalError: (2003, ""Can't connect to MySQL server on ''xxx.xxx.xxx.xxx'' ([Errno 111] Connection refused)"")
Traceback (most recent call last):
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/connections.py"", line 583, in connect
    **kwargs)
  File ""/data/projects/fate/common/miniconda3/lib/python3.6/socket.py"", line 724, in create_connection
    raise err
  File ""/data/projects/fate/common/miniconda3/lib/python3.6/socket.py"", line 713, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 2817, in connect
    self._state.set_connection(self._connect())
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/playhouse/pool.py"", line 155, in _connect
    conn = super(PooledDatabase, self)._connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 3642, in _connect
    conn = mysql.connect(db=self.database, **self.connect_params)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/__init__.py"", line 94, in Connect
    return Connection(*args, **kwargs)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/connections.py"", line 325, in __init__
    self.connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/connections.py"", line 630, in connect
    raise exc
pymysql.err.OperationalError: (2003, ""Can't connect to MySQL server on 'xxx.xxx.xxx.xxx' ([Errno 111] Connection refused)"")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/python/fate_flow/fate_flow_server.py"", line 91, in <module>
    init_flow_db()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 393, in inner
    with self:
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 2724, in __enter__
    self.db.connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/playhouse/pool.py"", line 108, in connect
    return super(PooledDatabase, self).connect(reuse_if_open)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 2820, in connect
    self._initialize_connection(self._state.conn)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 2666, in __exit__
    reraise(new_type, new_type(*exc_args), traceback)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 179, in reraise
    raise value.with_traceback(tb)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 2817, in connect
    self._state.set_connection(self._connect())
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/playhouse/pool.py"", line 155, in _connect
    conn = super(PooledDatabase, self)._connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/peewee.py"", line 3642, in _connect
    conn = mysql.connect(db=self.database, **self.connect_params)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/__init__.py"", line 94, in Connect
    return Connection(*args, **kwargs)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/connections.py"", line 325, in __init__
    self.connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/pymysql/connections.py"", line 630, in connect
    raise exc
peewee.OperationalError: (2003, ""Can't connect to MySQL server on 'xxx.xxx.xxx.xxx' ([Errno 111] Connection refused)"")

```
mysqld.log内容如下
```
2020-11-13T09:14:03.945889Z mysqld_safe Logging to '/data/projects/fate/common/mysql/mysql-8.0.13/logs/mysqld.log'.
2020-11-13T09:14:03.994002Z mysqld_safe Starting mysqld daemon with databases from /data/projects/fate/data/mysql
2020-11-13T09:14:04.987046Z 0 [System] [MY-010116] [Server] /data/projects/fate/common/mysql/mysql-8.0.13/bin/mysqld (mysqld 8.0.13) starting as process 10147
2020-11-13T09:14:04.997328Z 0 [Warning] [MY-013242] [Server] --character-set-server: 'utf8' is currently an alias for the character set UTF8MB3, but will be an alias for UTF8MB4 in a future release. Please consider using UTF8MB4 in order to be unambiguous.
2020-11-13T09:14:05.036838Z 1 [ERROR] [MY-011011] [Server] Failed to find valid data directory.
2020-11-13T09:14:05.037345Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
2020-11-13T09:14:05.037399Z 0 [ERROR] [MY-010119] [Server] Aborting
2020-11-13T09:14:05.038999Z 0 [System] [MY-010910] [Server] /data/projects/fate/common/mysql/mysql-8.0.13/bin/mysqld: Shutdown complete (mysqld 8.0.13)  MySQL Community Server - GPL.
2020-11-13T09:14:05.081642Z mysqld_safe mysqld from pid file /data/projects/fate/common/mysql/mysql-8.0.13/run/mysqld.pid ended
```

经检查，在my.cnf里头实际上是有
`basedir=/data/projects/fate/common/mysql/mysql-8.0.13`
`datadir=/data/projects/fate/data/mysql`
经检查该路径存在
ls /data/projects/fate/data/mysql
仅有一个文件`binlog.index`

尝试使用了`sudo bin/mysqld --initialize-insecure --user=xxx`指令,无果。不使用sudo结果相同，没有错误提示，也没有变好。
虽然用户名用的不是app，但是权限是按照前文的要求设置的，并且改动了init和deploy脚本里头
`--user=app`的内容。workaround：
apt-get 卸载掉原先安装的mysql
另外后续会因为本机装过openjdk11而再报一个不能启动jvm的错误
卸载后也能暂时解决这个问题请问，这里说的All in one是指KubeFATE吗？> 请问，这里说的All in one是指KubeFATE吗？

不是，native部署方式。> > 请问，这里说的All in one是指KubeFATE吗？
> 
> 不是，native部署方式。

按照cluster方式部署，可以指定每台机器的IP而不是使用系统默认的192.168.0.x的IP吗？
我看部署脚本里还有192.168.0.88和192.168.0.99，是不是至少需要四台机器，才可以配集群？你怎么解决的啊，我也遇到同样的问题。卸了原装MYSQL，只用集成的。



------------------&nbsp;原始邮件&nbsp;------------------
发件人: ""WOW""<notifications@github.com&gt;; 
发送时间: 2021年1月12日(星期二) 下午3:47
收件人: ""FederatedAI/FATE""<FATE@noreply.github.com&gt;; 
抄送: ""倪彧祺""<nnnnnn_2657@qq.com&gt;; ""State change""<state_change@noreply.github.com&gt;; 
主题: Re: [FederatedAI/FATE] 使用1.5.0 All in one 部署FATE集群，Mysqld无法启动【2003, &quot;Can&#39;t connect to MySQL server】 (#2325)





 
你怎么解决的啊，我也遇到同样的问题。
 
—
You are receiving this because you modified the open/close state.
Reply to this email directly, view it on GitHub, or unsubscribe.> 卸了原装MYSQL，只用集成的。
> […](#)
> ------------------&nbsp;原始邮件&nbsp;------------------ 发件人: ""WOW""<notifications@github.com&gt;; 发送时间: 2021年1月12日(星期二) 下午3:47 收件人: ""FederatedAI/FATE""<FATE@noreply.github.com&gt;; 抄送: ""倪彧祺""<nnnnnn_2657@qq.com&gt;; ""State change""<state_change@noreply.github.com&gt;; 主题: Re: [FederatedAI/FATE] 使用1.5.0 All in one 部署FATE集群，Mysqld无法启动【2003, &quot;Can&#39;t connect to MySQL server】 (#2325) 你怎么解决的啊，我也遇到同样的问题。 — You are receiving this because you modified the open/close state. Reply to this email directly, view it on GitHub, or unsubscribe.

谢谢回复，不过还是不行啊，还是遇到同样的问题，郁闷啊。我用的是kubefate在多个机器上安装的。我遇到了同样的问题：
pymysql.err.OperationalError: (2003, ""Can't connect to MySQL server on  ‘my ip’  (timed out)"")
peewee.OperationalError: (2003, ""Can't connect to MySQL server on ‘my ip’  (timed out)"")
已经重新卸载mysql 并重新部署安装，查看端口3306正常运行，但是仍出现上述错误。感谢提供解决思路！
![image](https://user-images.githubusercontent.com/17609160/124580809-14718c80-de83-11eb-95c7-b9ce16dcb53f.png)
> 我遇到了同样的问题：
> pymysql.err.OperationalError: (2003, ""Can't connect to MySQL server on ‘my ip’ (timed out)"")
> peewee.OperationalError: (2003, ""Can't connect to MySQL server on ‘my ip’ (timed out)"")
> 已经重新卸载mysql 并重新部署安装，查看端口3306正常运行，但是仍出现上述错误。感谢提供解决思路！
> ![image](https://user-images.githubusercontent.com/17609160/124580809-14718c80-de83-11eb-95c7-b9ce16dcb53f.png)

偶尔也会在部署成功后遇到这类问题
基本上是停止mysqld相关进程、重启服务，懵懵懂懂地恢复了。
不过这里需要强调的是，出现类似问题用service.sh带的stop是不太够的
最好参考一下官方部署文档里用的那些ps -ef清除所有相关进程",9,2020-11-16 01:18:44,2021-08-03 06:29:12,2020-11-16 08:57:42
https://github.com/FederatedAI/FATE/issues/2310,[],Deploy ansible-fate1.5 failed,"Deploy ansible-fate1.5 failed**Describe the bug**
A clear and concise description of what the bug is.
When I try to deploy fateboard with 3 remote servers, I got this note:
![image](https://user-images.githubusercontent.com/34049754/98496311-5b6b9b80-227c-11eb-94a5-76d3ec64376d.png)

![image](https://user-images.githubusercontent.com/34049754/98496314-5eff2280-227c-11eb-90c3-fd217a2febd5.png)


Actually, I had already install supervisor 

I got these problems in ubuntu 18.04, and I wanna how to fix it

![image](https://user-images.githubusercontent.com/34049754/98496367-776f3d00-227c-11eb-818b-64126455f8ec.png)


**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
![image](https://user-images.githubusercontent.com/34049754/98497039-f749d700-227d-11eb-9d8c-857591c6801d.png)

ulimit is 65535",1,2020-11-09 03:13:22,2020-11-09 07:13:31,2020-11-09 07:13:31
https://github.com/FederatedAI/FATE/issues/2202,[],pipeline API不兼容：'Reader' object has no attribute 'algorithm_param',"pipeline API不兼容：'Reader' object has no attribute 'algorithm_param'**Describe the bug**
安装最新的FATE 镜像，按照指导文档：https://github.com/FederatedAI/FATE/blob/master/examples/pipeline/README.rst操作。在执行：python demo/pipeline-mini-demo.py
报错：
Traceback (most recent call last):
  File ""fate/examples/pipeline/demo/pipeline-mini-demo.py"", line 129, in <module>
    main()
  File ""fate/examples/pipeline/demo/pipeline-mini-demo.py"", line 57, in main
    reader_0.get_party_instance(role=""guest"", party_id=guest).algorithm_param(table=guest_train_data)
AttributeError: 'Reader' object has no attribute 'algorithm_param'

> **Describe the bug**
> 安装最新的FATE 镜像，按照指导文档：[https://github.com/FederatedAI/FATE/blob/master/examples/pipeline/README.rst操作。在执行：python](https://github.com/FederatedAI/FATE/blob/master/examples/pipeline/README.rst%E6%93%8D%E4%BD%9C%E3%80%82%E5%9C%A8%E6%89%A7%E8%A1%8C%EF%BC%9Apython) demo/pipeline-mini-demo.py
> 报错：
> Traceback (most recent call last):
> File ""fate/examples/pipeline/demo/pipeline-mini-demo.py"", line 129, in 
> main()
> File ""fate/examples/pipeline/demo/pipeline-mini-demo.py"", line 57, in main
> reader_0.get_party_instance(role=""guest"", party_id=guest).algorithm_param(table=guest_train_data)
> AttributeError: 'Reader' object has no attribute 'algorithm_param'

There are some api modify in final release and we has publish rc2 to pypi for test

temporary, try 
install fate_client==0.1rc1
Or
install - e python/fate_client",1,2020-10-25 15:39:34,2020-10-27 04:20:59,2020-10-27 04:20:59
https://github.com/FederatedAI/FATE/issues/2153,[],"socket.gethostbyname(socket.getfqdn()) get gaierror: [Errno 8] nodename nor servname provided, or not known","socket.gethostbyname(socket.getfqdn()) get gaierror: [Errno 8] nodename nor servname provided, or not knownI use Macos and standalone-fate-master-1.4.5.tar.gz

when I run fate_flow_server.py , In core_utils.py line 97 
```ip = socket.gethostbyname(socket.getfqdn())``` get error.
socket.getfqdn() get : ""1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa""

but my ```/etc/hosts``` is 
```
127.0.0.1       localhost
255.255.255.255 broadcasthost
::1             localhost
# Added by Docker Desktop
# To allow the same kube context to work on the host and the container:
127.0.0.1 kubernetes.docker.internal
# End of section
127.0.0.1       X-QdeMacBook-Pro.local
127.0.0.1       X-QdeMacBook-Pro
0.0.0.0         X-QdeMacBook-Pro
0.0.0.0         X-QdeMacBook-Pro.local
```
try rename getfqdn to gethostname, which works for me in same situation.

by the way,  version 1.5.0 has better support for macos using standalone  deploy.This issue is closed due to not being active. Please feel free to open it again",2,2020-10-18 01:56:42,2020-11-11 12:35:51,2020-11-11 12:35:51
https://github.com/FederatedAI/FATE/issues/2082,[],Failed to release resource after fate_flow server crashed,"Failed to release resource after fate_flow server crashed**Describe the bug**
The `nodemanager` fails to release `roll_pair.py` process after fate_flow server crashed.

**To Reproduce**
Steps to reproduce the behavior:
1. Start FATE Cluster
2. Kill fate_flow server without `KeyboardInterrupt`
3. Check process in ""nodemanager"",  the resource occupied by the session will not be released.

**Expected behavior**
Once fate_flow server restart, it should reuse the previous session of delete it before create a new one.
Close the issue cuz fixed in v1.5.1",1,2020-10-13 09:11:58,2021-06-23 06:23:29,2021-06-23 06:23:29
https://github.com/FederatedAI/FATE/issues/2029,[],逻辑回归对稀疏数据的训练结果很差,"逻辑回归对稀疏数据的训练结果很差**求大佬指点 我运行稀疏数据的时候 效果总是很差 一筹莫展 是参数没配置好吗** 





                   ------如果我解决了这个问题我会在这回复的 如果没有回复那就是需要你的帮助 兄弟们 做一下特征处理 训练的结果 就没那么糟糕了  ",1,2020-10-05 14:42:22,2021-03-18 06:16:18,2021-03-18 06:16:18
https://github.com/FederatedAI/FATE/issues/1983,[],自定义算法的时候，上传错误脚本的缓存好像无法清空,"自定义算法的时候，上传错误脚本的缓存好像无法清空**Describe the bug**
自定义算法运行的时候，在第 1 步：定义此模块将使用的参数对象的时候填错了，修改过后，运行还是报原来的错误，怀疑是缓存的原因。如何清除，运行更新后的参数对象脚本？

**To Reproduce**
```
# 第一次，粗心写错了
[ERROR] [2020-09-25 06:32:31,022] [9:140147291039488] - job_app.py[line:37]: name 'GMMParam' is not defined
Traceback (most recent call last):
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise
    raise value
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/data/projects/fate/python/fate_flow/apps/job_app.py"", line 46, in submit_job
    job_id, job_dsl_path, job_runtime_conf_path, logs_directory, model_info, board_url = JobController.submit_job(request.json)
  File ""/data/projects/fate/python/fate_flow/driver/job_controller.py"", line 94, in submit_job
    train_runtime_conf=train_runtime_conf)
  File ""/data/projects/fate/python/fate_flow/utils/job_utils.py"", line 190, in get_job_dsl_parser
    mode=job_type)
  File ""/data/projects/fate/python/fate_flow/driver/dsl_parser.py"", line 642, in run
    self._init_component_setting(setting_conf_prefix, self.runtime_conf, default_runtime_conf_prefix)
  File ""/data/projects/fate/python/fate_flow/driver/dsl_parser.py"", line 165, in _init_component_setting
    name)
  File ""/data/projects/fate/python/fate_flow/utils/parameter_util.py"", line 41, in override_parameter
    param_obj = getattr(param_module, param_class)()
  File ""/data/projects/fate/python/federatedml/param/hetero_gmm_param.py"", line 78, in __init__
    super(GMMParam, self).__init__()
NameError: name 'GMMParam' is not defined

# 第二次，修改后，还是报第一次的错，但是最后一行super那里已经改过来了
[ERROR] [2020-09-25 07:04:01,956] [9:140147291039488] - job_app.py[line:37]: name 'GMMParam' is not defined
Traceback (most recent call last):
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise
    raise value
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/data/projects/python/venv/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/data/projects/fate/python/fate_flow/apps/job_app.py"", line 46, in submit_job
    job_id, job_dsl_path, job_runtime_conf_path, logs_directory, model_info, board_url = JobController.submit_job(request.json)
  File ""/data/projects/fate/python/fate_flow/driver/job_controller.py"", line 94, in submit_job
    train_runtime_conf=train_runtime_conf)
  File ""/data/projects/fate/python/fate_flow/utils/job_utils.py"", line 190, in get_job_dsl_parser
    mode=job_type)
  File ""/data/projects/fate/python/fate_flow/driver/dsl_parser.py"", line 642, in run
    self._init_component_setting(setting_conf_prefix, self.runtime_conf, default_runtime_conf_prefix)
  File ""/data/projects/fate/python/fate_flow/driver/dsl_parser.py"", line 165, in _init_component_setting
    name)
  File ""/data/projects/fate/python/fate_flow/utils/parameter_util.py"", line 41, in override_parameter
    param_obj = getattr(param_module, param_class)()
  File ""/data/projects/fate/python/federatedml/param/hetero_gmm_param.py"", line 78, in __init__
    super(HeteroGMMParam, self).__init__()
NameError: name 'GMMParam' is not defined
```

**Additional context**
Add any other context about the problem here.
try restart fateflow service",1,2020-09-25 07:16:15,2020-11-03 09:53:01,2020-11-03 09:53:01
https://github.com/FederatedAI/FATE/issues/1824,[],"version 1.4.4, I can pass the Unit Test, but when I run 'python quick_run.py', get an error","version 1.4.4, I can pass the Unit Test, but when I run 'python quick_run.py', get an error
when I run 'python quick_run.py', get error:

Upload data config json: {'file': 'examples/data/breast_b.csv', 'head': 1, 'partition': 16, 'work_mode': 0, 'table_name': 'breast_b', 'namespace': 'breast_b_guest'}
stdout:{
    ""retcode"": 100,
    ""retmsg"": ""The file is obtained from the fate flow client machine, but it does not exist, please check the path: /fate/examples/data/breast_b.csv"",
    ""traceback"": [
        ""Traceback (most recent call last):\n"",
        ""  File \""/fate/examples/federatedml-1.x-examples/../../fate_flow/fate_flow_client.py\"", line 298, in <module>\n    response = call_fun(args.function, config_data, dsl_path, config_path)\n"",
        ""  File \""/fate/examples/federatedml-1.x-examples/../../fate_flow/fate_flow_client.py\"", line 183, in call_fun\n    'please check the path: {}'.format(file_name))\n"",
        ""Exception: The file is obtained from the fate flow client machine, but it does not exist, please check the path: /fate/examples/data/breast_b.csv\n""
    ]
}


{
    ""retcode"": 100,
    ""retmsg"": [
        ""Traceback (most recent call last):\n"",
        ""  File \""quick_run.py\"", line 374, in <module>\n    upload_data()\n"",
        ""  File \""quick_run.py\"", line 354, in upload_data\n    upload(GUEST)\n"",
        ""  File \""quick_run.py\"", line 245, in upload\n    stdout = exec_upload_task(json_info, role)\n"",
        ""  File \""quick_run.py\"", line 126, in exec_upload_task\n    \""[Upload task]exec fail, status:{}, stdout:{}\"".format(status, stdout))\n"",
        ""ValueError: [Upload task]exec fail, status:100, stdout:{'retcode': 100, 'retmsg': 'The file is obtained from the fate flow client machine, but it does not exist, please check the path: /fate/examples/data/breast_b.csv', 'traceback': ['Traceback (most recent call last):\\n', '  File \""/fate/examples/federatedml-1.x-examples/../../fate_flow/fate_flow_client.py\"", line 298, in <module>\\n    response = call_fun(args.function, config_data, dsl_path, config_path)\\n', '  File \""/fate/examples/federatedml-1.x-examples/../../fate_flow/fate_flow_client.py\"", line 183, in call_fun\\n    \\'please check the path: {}\\'.format(file_name))\\n', 'Exception: The file is obtained from the fate flow client machine, but it does not exist, please check the path: /fate/examples/data/breast_b.csv\\n']}\n""
    ]
}

I can't find that file, and don't know where to get it.

I made a mistake, it should be generated by myself or using exiting official files.",1,2020-09-07 06:34:30,2020-09-08 04:17:05,2020-09-08 04:15:49
https://github.com/FederatedAI/FATE/issues/1792,[],官方data文件ionosphere_scale_hetero_host.csv是不是少了label？,"官方data文件ionosphere_scale_hetero_host.csv是不是少了label？跑官方demo中homo_secureboost\test_secureboost_train_binary_with_missing_value_conf.json横向场景时host报了如下错误：
![image](https://user-images.githubusercontent.com/62371712/91927897-3d7a3b80-ed0d-11ea-9cc1-82c7c07fc71b.png)
追溯到源码中的data：ionosphere_scale_hetero_host.csv发现的确不含label列已发现解决方案",1,2020-09-02 03:13:32,2020-09-02 03:14:43,2020-09-02 03:14:21
https://github.com/FederatedAI/FATE/issues/1699,[],单机部署时训练超过7k的数据量在hetero-lr组件会出现lmdb.Error,"单机部署时训练超过7k的数据量在hetero-lr组件会出现lmdb.Error**Describe the bug**
使用fate-1.4.2在单机部署时更换进行示例的训练时，小于7k的数据量都可以成功训练和测试，但是更换为大于7k的数据量时在hetero-lr-0组件failed，出现lmdb.Error：mdb_txn_commit:Input/output error 。但是我查看了服务器磁盘空间足够。这应该是什么原因造成的？我使用的数据集是fate/data/default_credit_hetero_guest.csv和default_credit_hetero_host.csv.

>- task_executor.py[line:144]: mdb_txn_commit: Input/output error 
concurrent.futures.process._RemoteTraceback:  
"""""" 
Traceback (most recent call last): 
  File ""/usr/local/lib/python3.6/concurrent/futures/process.py"", line 175, in _process_worker 
    r = call_item.fn(*call_item.args, **call_item.kwargs) 
  File ""/fate/arch/standalone/standalone/eggroll.py"", line 343, in do_join 
    dst_txn.put(k_bytes, serialize(v3)) 
lmdb.Error: mdb_txn_commit: Input/output error 
""""""   
  The above exception was the direct cause of the following exception:
 Traceback (most recent call last): 
 File ""/fate/fate_flow/driver/task_executor.py"", line 135, in run_task 
    run_object.run(component_parameters, task_run_args) 
File ""/fate/federatedml/model_base.py"", line 91, in run 
    this_data_output = func(*params)
File ""/fate/federatedml/linear_model/logistic_regression/hetero_logistic_regression/hetero_lr_guest.py"", line 77, in fit
    self.fit_binary(data_instances, validate_data)
File ""/fate/federatedml/linear_model/logistic_regression/hetero_logistic_regression/hetero_lr_guest.py"", line 123, in fit_binary
    batch_index
File ""/fate/federatedml/optim/gradient/hetero_linear_model_gradient.py"", line 177, in compute_gradient_procedure
    model_weights.fit_intercept)
 File ""/fate/federatedml/optim/gradient/hetero_linear_model_gradient.py"", line 112, in compute_gradient 
    lambda d, g: (d.features, g))
File ""/fate/arch/api/utils/profile_util.py"", line 32, in _fn 
    rtn = func(*args, **kwargs) 
  File ""/fate/arch/api/impl/based_1x/table.py"", line 140, in join 
    return DTable(self._dtable.join(other._dtable, func=func), session_id=self._session_id) 
  File ""/fate/arch/standalone/standalone/eggroll.py"", line 840, in join 
    result = r.result() 
  File ""/usr/local/lib/python3.6/concurrent/futures/_base.py"", line 425, in result 
    return self.__get_result() 
  File ""/usr/local/lib/python3.6/concurrent/futures/_base.py"", line 384, in __get_result 
    raise self._exception 
lmdb.Error: mdb_txn_commit: Input/output error

运行配置文件算法参数如下：
>- {'HeteroLogisticParam': {'penalty': 'L2', 'tol': 0.0001, 'alpha': 0.01, 'optimizer': 'nesterov_momentum_sgd', 'batch_size': -1, 'learning_rate': 0.15, 'init_param': {'init_method': 'random_uniform', 'init_const': 1, 'fit_intercept': True, 'random_seed': None}, 'max_iter': 30, 'early_stop': 'weight_diff', 'encrypt_param': {'method': 'Paillier', 'key_length': 1024}, 'predict_param': {'threshold': 0.5}, 
'cv_param': {'n_splits': 5, 'mode': 'hetero', 'role': 'guest', 'shuffle': False, 'random_seed': 103, 'need_cv': False}, 'decay': 1, 'decay_sqrt': True, 'multi_class': 'ovr', 'validation_freqs': None, 'stepwise_param': {'score_name': 'AIC', 'mode': 'hetero', 'role': 'guest', 'direction': 'both', 'max_step': 10, 'nvmin': 2, 'nvmax': None, 'need_stepwise': False}, 'early_stopping_rounds': None, 'metrics': ['auc', 'ks'], 'use_first_metric_only': False, 'encrypted_mode_calculator_param': {'mode': 'strict', 're_encrypted_rate': 1}, 
'sqn_param': {'update_interval_L': 3, 'memory_M': 5, 'sample_size': 5000, 'random_seed': None}}, 'initiator': {'role': 'guest', 'party_id': 10000}, 'job_parameters': {'work_mode': 0, 'model_id': 'arbiter-10000#guest-10000#host-10000#model', 'model_version': '20200813032518087622203'}, 'role': {'guest': [10000], 'host': [10000], 'arbiter': [10000]}, 'config': '/fate/examples/federatedml-1.x-examples/hetero_logistic_regression/test_hetero_lr_train_job_conf.json', 'dsl': 'federatedml-1.x-examples/hetero_logistic_regression/test_hetero_lr_train_job_dsl.json', 'function': 'submit_job', 'local': {'role': 'guest', 'party_id': 10000}, 'CodePath': 'federatedml/linear_model/logistic_regression/hetero_logistic_regression/hetero_lr_guest.py/HeteroLRGuest', 'module': 'HeteroLR'}



虽然不太清楚具体原因，但是看这个报错会不会是eggroll的容器空间不够导致的呢？ I find it's my own fault because I solved the problem after rebuilding the  software environment.",2,2020-08-13 05:11:00,2020-10-07 06:16:47,2020-10-07 06:16:47
https://github.com/FederatedAI/FATE/issues/1670,[],1.4.2版本Upload接口报400错误,"1.4.2版本Upload接口报400错误![微信图片_20200811095504](https://user-images.githubusercontent.com/41407129/89848471-cd1e4580-dbb8-11ea-81d0-f71b7675730c.jpg)

数据和flow client是在同一台机器的，尝试使用use_local_data和module参数，也是一样的效果。
请帮忙分析，谢谢。use_local_data 属性的取值不是String型，而是Integer(0,1)或者Boolean(true,false)类型。我同样遇到这个问题，请问您是怎样解决的？输入参数与您的一样",2,2020-08-11 01:56:59,2021-03-30 06:53:56,2020-08-12 10:00:41
https://github.com/FederatedAI/FATE/issues/1624,[],特征分箱仅支持等频和等距分箱吗？跑最优分箱会报错,"特征分箱仅支持等频和等距分箱吗？跑最优分箱会报错报错如下: - ERROR: hetero binning param's optimal is not supported, it should be in ['quantile', 'bucket']""
![image](https://user-images.githubusercontent.com/62371712/88898332-24e6b380-d27f-11ea-9295-4bd7f67d0145.png)

组件参数配置为：
{
""method"": ""optimal"",
 ""optimal_binning_param"": {
 ""metric_method"": ""iv"",
 ""min_bin_pct"": 0.05,
 ""max_bin_pct"": 0.8,
 ""init_bucket_method"": ""quantile"",
 ""init_bin_nums"": 100,
 ""mixture"": true
 },
 ""compress_thres"": 10000,
 ""head_size"": 10000,
 ""error"": 0.001,
 ""bin_num"": 10,
 ""bin_indexes"": -1,
 ""bin_names"": null,
 ""category_indexes"": [0,1,2],
 ""category_names"": null,
 ""adjustment_factor"": 0.5,
 ""local_only"": false,
 ""transform_param"": {
 ""transform_cols"": -1,
 ""transform_names"": null,
 ""transform_type"": ""bin_num""
 }
 }
从1.4版本开始，是支持最优分箱的。",1,2020-07-30 08:15:58,2021-07-02 08:18:54,2021-07-02 08:18:54
https://github.com/FederatedAI/FATE/issues/1608,[],please start fate flow server.......,"please start fate flow server.......when i execute fate_flow_client -f submit_job.....

i got an error :
{
    ""retcode"": 999,
    ""retmsg"": ""please start fate flow server: 192.167.0.6:9381""
}

but the service already started : 
![image](https://user-images.githubusercontent.com/30582212/88609277-4b0d2780-d0b6-11ea-8253-bfba28a32cf9.png)



Hello. So, how did you solve it?> Hello. So, how did you solve it?
I apologize. I can't remember why......
I don't use fate every day. Some problems have been solved but I forget",2,2020-07-28 01:40:00,2020-08-18 00:26:13,2020-07-28 01:41:41
https://github.com/FederatedAI/FATE/issues/1572,[],Fail to deploy stand-alone installation with Docker,"Fail to deploy stand-alone installation with Docker**Describe the bug**
Following the command [here](https://github.com/FederatedAI/FATE/tree/master/standalone-deploy),
after running `bash install_standalone_docker.sh`, `fate_fateboard` is started but `fate_python` fails to start.

**To Reproduce**
Just follow the command [here](https://github.com/FederatedAI/FATE/tree/master/standalone-deploy)

**Expected behavior**
A container named `fate_python` should be started.

**Screenshots**
![image](https://user-images.githubusercontent.com/35265996/87758557-3ec7d580-c83f-11ea-9a02-fa4c9cfbcfd3.png)
![image](https://user-images.githubusercontent.com/35265996/87758599-54d59600-c83f-11ea-9c51-2b9b81172c00.png)

**Desktop (please complete the following information):**
 - OS: Linux 5.3.0-51-generic #44~18.04.2-Ubuntu
 - Docker: Docker version 19.03.8
 - Docker Compose: docker-compose version 1.26.2

**Additional context**
I have found `fate_flow_server.py` in `fate/fate_flow`, but why it can not find it?

Thanks very much!I updated to the latest version of docker file and the problem vanishes.",1,2020-07-17 07:12:30,2020-08-17 06:21:55,2020-08-17 06:21:42
https://github.com/FederatedAI/FATE/issues/1551,['bug'],[SPDZ] Table-based implementation does not support multiple times’ operations,"[SPDZ] Table-based implementation does not support multiple times’ operationsIf I multiply under the SPDZ environment two tensors of class `fixedpoint_table.FixedPointTensor`, say *A* and *B*, the resulting product *C = A.dot(B)* will be of class `fixedpoint_numpy.FixedPointTensor`. While there is no problem if I immediately extract the result and leave the environment, errors will occur if I want to use the result to do further computation with tensors of type `fixedpoint_table.FixedPointTensor` because they will have different types. For example, if I want to perform *C+D* where *D* is of type `fixedpoint_table.FixedPointTensor`, either doing *C+D* or *D+C* does not work (with different errors prompting, though).As a user, currently I may sidestep this issue by separating the computation in pieces. Of course it will induce remarkable performance overhead since unnecessary “get()” and “from_source()” have to be performed.track in #2312",2,2020-07-11 17:10:25,2020-11-11 12:12:22,2020-11-11 12:12:21
https://github.com/FederatedAI/FATE/issues/1535,[],Fate flow service can't be connected with standalone-fate-master-1.4.1,"Fate flow service can't be connected with standalone-fate-master-1.4.1**Describe the bug**
FATE Flow server starts successfully but failed to connect to it with 9380 port. `ValueError: failed to exec task, status:100, stderr is None stdout:{'retcode': 100, 'retmsg': 'Connection refused, Please check if the fate flow service is started'}`

**To Reproduce**
Steps to reproduce the behavior:
1. Run ```source init.sh init``` under standalone-fate-master-1.4.1
![image](https://user-images.githubusercontent.com/4393234/86312087-76862900-bc54-11ea-8a71-d1804087a514.png)

2. Run `python ./examples/toy_example/run_toy_example.py 10000 10000 0`
![image](https://user-images.githubusercontent.com/4393234/86312225-d381df00-bc54-11ea-8c36-5f7c8aeea8a9.png)



I found it's caused by my wrong hostname which is the same with a website.",1,2020-07-02 03:23:49,2020-07-02 10:26:49,2020-07-02 10:26:49
https://github.com/FederatedAI/FATE/issues/1430,[],"""Hetero Feature Binning"" leaks extra information","""Hetero Feature Binning"" leaks extra informationAs described in https://github.com/FederatedAI/FATE/tree/master/federatedml/feature , to calculate WOE,   B party who has the data labels encrypt its labels with homomorphic encryption and then send them to A.  A then calculate the sum of each bin's label and send back. During this process, party A leaks extra information: the number of samples in each bin.

For example: A and B want to compute this function together:  

                    What is the WOE if 30>= age >= 20 ?

But if they use FATE, B will know **how many A's client are between 20~30 years old !** I don't think this is always acceptable for A. @vincehong That is not true. Actually, party B cannot obtain the feature information and bin split points at all. What party B know is that number of some bins of some features. That means in your case, party B cannot know it is a feature means age and the split points are 20 and 30 neither. 

Hope that can make you more clear about it. Thank you.@tanmc123 I know what you mean. What B can know is : 

                 how many A's client are with the same feature X

while B does not know what X means. 

Actually, given the distribution, and the numbers X_1, X_2, ...X_n, B could figure out these features by a high chance.

The point is: A leaks the number of samples in the bin, beyond the WOE.@vincehong I didn't get how to figure out these features through the number x_1, x_2...etc. Please note it is not the distribution of X, but number of each bins.  Take quantile binning as an example, the number is supposed to be almost same in each bean with the condition of which total amount is not a secret among the parties. Is there any material supporting that?

As for what you meantion so call a ""leak"", if this information cannot benefit the other party, it makes no sense to define it as a ""leak"". Doesn't it?
@tanmc123  Think about categorical features.   Suppose A has a feature X with 3 possible values X_1, X_2, X_3,  then B could know the exact number of samples for each value of X.  If this could not be a leak, then people would doubt what could be called a leak.> @tanmc123 Think about categorical features. Suppose A has a feature X with 3 possible values X_1, X_2, X_3, then B could know the exact number of samples for each value of X. If this could not be a leak, then people would doubt what could be called a leak.

First of all, Party B should have a chance to know it is a categorical feature which is not so obviously through the binning. Then, party B cannot obtain what the feature means even if they could know that. So I still cannot get how to use this information to benefit party B. 

I don't think it's fair to say an algorithm is not safe just because ""if we could know something else, it is not safe"". 

As an engineer, we should always make trade off between it is completely safe and great usablity. Calculating WOE is very meaningful in practice but the risk is low enough. If you can provide any material  supporting it is not safe enough or there is some new knowledge saying that, FATE is open to fix it and making progress through the process. 

Thank you for participating in the construction of FATE. First of all , it's easy to know it's a categorical feature: if X_1 + X_2 + X_3 = total_sum, X is a categorical feature. If total_sum is big (e.g. 100,000+), there's little chance of false positives using this method.

Second, if the security comes from ""B not knowing the feature meanings"", why doesn't A just simply wipe the feature headers and send the data to B?  After all, B does not know what kind of features A have, and we also cannot provide any material supporting ""sending raw data without feature name is not safe enough"". 

And we do have a safe way which only outputs the WOE without leaking the number of samples in the bin: it's just a secure inner product and secure division using MPC (which is already included in  https://github.com/FederatedAI/FATE/tree/master/federatedml/secureprotol/spdz).  

FATE put GDPR at its first page, and includes fancy stuff such as homomorphic encryption, so we would naturally think that it was considering security and privacy in a serious way. Please don't discourage us. We do know it's not easy to maintain such a big open-source project, but we should show better respect to security , privacy , and cryptography.

Thanks.

> First of all , it's easy to know it's a categorical feature: if X_1 + X_2 + X_3 = total_sum, X is a categorical feature. If total_sum is big (e.g. 100,000+), there's little chance of false positives using this method.
> 
> Second, if the security comes from ""B not knowing the feature meanings"", why doesn't A just simply wipe the feature headers and send the data to B? After all, B does not know what kind of features A have, and we also cannot provide any material supporting ""sending raw data without feature name is not safe enough"".
> 
> And we do have a safe way which only outputs the WOE without leaking the number of samples in the bin: it's just a secure inner product and secure division using MPC (which is already included in https://github.com/FederatedAI/FATE/tree/master/federatedml/secureprotol/spdz).
> 
> FATE put GDPR at its first page, and includes fancy stuff such as homomorphic encryption, so we would naturally think that it was considering security and privacy in a serious way. Please don't discourage us. We do know it's not easy to maintain such a big open-source project, but we should show better respect to security , privacy , and cryptography.
> 
> Thanks.

At your first point, even if it is not a categorical feature, it still satisfy  X_1 + X_2 + X_3 = total_sum. I cannot get why this could be a proof of a feature being categorical. 

Well, as for the second point, the number in each bin is totally different with raw data. We are just saying, without the meaning of features, only having the number in each bin is safe. Please do not steal the concept. 

As for the MPC way of calculating WOE, that may be a good suggestion, we will try to think about it more detailly. 

FATE do put GDPR at its first page and that's why FATE is an open-source project. FATE open to discuss with our users and to make progress in every possible way. Thanks for you suggestion.",7,2020-06-05 03:08:10,2021-07-02 08:41:32,2021-07-02 08:41:32
https://github.com/FederatedAI/FATE/issues/1410,[],the model and the predict score has not been  displayed  in the host fateboard for the homo_lr algorithm,"the model and the predict score has not been  displayed  in the host fateboard for the homo_lr algorithm![image](https://user-images.githubusercontent.com/13013299/83101713-8b1b5280-a0e5-11ea-82ff-f8998162c5e5.png)
![image](https://user-images.githubusercontent.com/13013299/83101744-98d0d800-a0e5-11ea-83bf-20bf9cabc218.png)
![image](https://user-images.githubusercontent.com/13013299/83101816-c61d8600-a0e5-11ea-9b3d-ff14988ffcf7.png)
@ZhanqiLiu Do you set Paillier encrypt method for homo_lr? If so, this is what they are expected to be. 

Since if you set the model as encrypted, that means you don't want host obtain the final model. Thus model will not be shown. What's more, if the predict score is provided, host can also infer the model. Thus, predict score will not be transferred back neither. thank you , this issue has been solved according to your guidance",2,2020-05-28 05:21:50,2020-06-16 07:48:43,2020-06-16 07:48:43
https://github.com/FederatedAI/FATE/issues/1409,[],"The error after the command ""docker-compose down and docker-compose up -d"" for the KubeFATE 1.4","The error after the command ""docker-compose down and docker-compose up -d"" for the KubeFATE 1.4When the container is restarted, the table which is uploaded in the last container cann't be accessed  again in the new job, and the job which was run in the last container cann't be readed from the fateboard also.

![image](https://user-images.githubusercontent.com/13013299/83089937-1f77bc00-a0ca-11ea-93cc-c1dc226392c1.png)
![image](https://user-images.githubusercontent.com/13013299/83089959-269eca00-a0ca-11ea-8676-87db8fee6806.png)
![image](https://user-images.githubusercontent.com/13013299/83089976-2c94ab00-a0ca-11ea-9ad4-38756c46f326.png)
@ZhanqiLiu hi thank you for your feedback, this issue has been addressed by this [PR](https://github.com/FederatedAI/KubeFATE/pull/146)",1,2020-05-28 02:13:38,2020-06-01 11:25:17,2020-06-01 11:25:17
https://github.com/FederatedAI/FATE/issues/1408,['bug'],homo nn: problem with sparse_categorical_crossentropy loss in v1.4.0,"homo nn: problem with sparse_categorical_crossentropy loss in v1.4.0**Describe the bug**
In v1.4.0 homo nn, when `softmax` activation and `sparse_categorical_crossentropy` loss are used, the performance of the model is extremely poor.

**To Reproduce**
Steps to reproduce the behavior:
1. Upload data

<details>
  <summary>Guest (click to expand)</summary>

```
{
    ""file"": ""examples/data/breast_homo_guest.csv"",
    ""head"": 1,
    ""partition"": 16,
    ""work_mode"": 1,
    ""table_name"": ""homo_breast_guest"",
    ""namespace"": ""homo_breast_guest""
}
```
</details>

<details>
  <summary>Host (click to expand)</summary>

```
{
    ""file"": ""examples/data/breast_homo_host.csv"",
    ""head"": 1,
    ""partition"": 16,
    ""work_mode"": 1,
    ""table_name"": ""homo_breast_host"",
    ""namespace"": ""homo_breast_host""
}
```
</details>

2. Submit job

<details>
  <summary>dsl (click to expand)</summary>

```
{
  ""components"": {
    ""dataio_0"": {
      ""module"": ""DataIO"",
      ""input"": {
        ""data"": {
          ""data"": [
            ""args.train_data""
          ]
        }
      },
      ""output"": {
        ""data"": [
          ""train""
        ],
        ""model"": [
          ""dataio""
        ]
      }
    },
    ""homo_nn_0"": {
      ""module"": ""HomoNN"",
      ""input"": {
        ""data"": {
          ""train_data"": [
            ""dataio_0.train""
          ]
        }
      },
      ""output"": {
        ""data"": [
          ""train""
        ],
        ""model"": [
          ""homo_nn""
        ]
      }
    },
    ""homo_nn_1"": {
      ""module"": ""HomoNN"",
      ""input"": {
        ""data"": {
          ""eval_data"": [
            ""dataio_0.train""
          ]
        },
        ""model"": [
          ""homo_nn_0.homo_nn""
        ]
      },
      ""output"": {
        ""data"": [
          ""train2""
        ],
        ""model"": [
          ""homo_nn2""
        ]
      }
    },
    ""evaluation_0"": {
      ""module"": ""Evaluation"",
      ""input"": {
        ""data"": {
          ""data"": [
            ""homo_nn_1.train2""
          ]
        }
      },
      ""output"": {
        ""data"": [
          ""evaluate""
        ]
      }
    }
  }
}
```
</details>

<details>
  <summary>conf (click to expand)</summary>

```
{
  ""initiator"": {
    ""role"": ""guest"",
    ""party_id"": 10000
  },
  ""job_parameters"": {
    ""work_mode"": 1
  },
  ""role"": {
    ""guest"": [
      10000
    ],
    ""host"": [
      9999
    ],
    ""arbiter"": [
      10000
    ]
  },
  ""role_parameters"": {
    ""guest"": {
      ""args"": {
        ""data"": {
          ""train_data"": [
            {
              ""name"": ""homo_breast_guest"",
              ""namespace"": ""homo_breast_guest""
            }
          ]
        }
      },
      ""dataio_0"": {
        ""with_label"": [
          true
        ],
        ""label_name"": [
          ""y""
        ],
        ""label_type"": [
          ""int""
        ],
        ""output_format"": [
          ""dense""
        ]
      }
    },
    ""host"": {
      ""args"": {
        ""data"": {
          ""train_data"": [
            {
              ""name"": ""homo_breast_host"",
              ""namespace"": ""homo_breast_host""
            }
          ]
        }
      },
      ""dataio_0"": {
        ""with_label"": [
          true
        ],
        ""label_name"": [
          ""y""
        ],
        ""label_type"": [
          ""int""
        ],
        ""output_format"": [
          ""dense""
        ]
      }
    }
  },
  ""algorithm_parameters"": {
    ""homo_nn_0"": {
      ""config_type"": ""nn"",
      ""nn_define"": [
        {
          ""layer"": ""Dense"",
          ""units"": 6,
          ""activation"": ""relu""
        },
        {
          ""layer"": ""Dense"",
          ""units"": 2,
          ""activation"": ""softmax""
        }
      ],
      ""batch_size"": -1,
      ""optimizer"": {
        ""optimizer"": ""Adam"",
        ""learning_rate"": 0.05
      },
      ""early_stop"": {
        ""early_stop"": ""diff"",
        ""eps"": 1e-4
      },
      ""loss"": ""sparse_categorical_crossentropy"",
      ""metrics"": [
        ""accuracy""
      ],
      ""max_iter"": 5
    },
    ""evaluation_0"": {
      ""eval_type"": ""binary""
    }
  }
}
```
</details>

3. Check data output and model performance

![softmax_data_output_v14](https://user-images.githubusercontent.com/10785873/82991928-0111b200-a031-11ea-92c2-5d94d7268e38.PNG)

![softmax_eval_v14](https://user-images.githubusercontent.com/10785873/82991996-17b80900-a031-11ea-9c91-600f7c847ef6.PNG)

**Expected behavior**
Performance should be consistent across different versions of FATE when the same algorithm are used on the same data.

**Screenshots**
As attached.

**Desktop (please complete the following information):**
 - OS: Ubuntu 18.04 LTS
 - Browser: Chrome

**Smartphone (please complete the following information):**
 - N.A.

**Additional context**
- In `v1.3.0`, with the same data, dsl, and conf, the results are like this:

![softmax_data_output_v13](https://user-images.githubusercontent.com/10785873/82992059-31595080-a031-11ea-9af6-068e9fc6e561.PNG)

![softmax_eval_v13](https://user-images.githubusercontent.com/10785873/82992074-361e0480-a031-11ea-8e87-5d4078bb6684.PNG)
> **Describe the bug**
> In v1.4.0 homo nn, when `softmax` activation and `sparse_categorical_crossentropy` loss are used, the performance of the model is extremely poor.
> 
> **To Reproduce**
> Steps to reproduce the behavior:
> 
> 1. Upload data
> 
> Guest (click to expand)
> Host (click to expand)
> 1. Submit job
> 
> dsl (click to expand)
> conf (click to expand)
> 1. Check data output and model performance
> 
> ![softmax_data_output_v14](https://user-images.githubusercontent.com/10785873/82991928-0111b200-a031-11ea-92c2-5d94d7268e38.PNG)
> 
> ![softmax_eval_v14](https://user-images.githubusercontent.com/10785873/82991996-17b80900-a031-11ea-9c91-600f7c847ef6.PNG)
> 
> **Expected behavior**
> Performance should be consistent across different versions of FATE when the same algorithm are used on the same data.
> 
> **Screenshots**
> As attached.
> 
> **Desktop (please complete the following information):**
> 
> * OS: Ubuntu 18.04 LTS
> * Browser: Chrome
> 
> **Smartphone (please complete the following information):**
> 
> * N.A.
> 
> **Additional context**
> 
> * In `v1.3.0`, with the same data, dsl, and conf, the results are like this:
> 
> ![softmax_data_output_v13](https://user-images.githubusercontent.com/10785873/82992059-31595080-a031-11ea-9af6-068e9fc6e561.PNG)
> 
> ![softmax_eval_v13](https://user-images.githubusercontent.com/10785873/82992074-361e0480-a031-11ea-8e87-5d4078bb6684.PNG)

Thanks for the detailed report!  
We fix this issue with pr #1419 problem which will be released alone with next mirror version.
Thanks! Looking forward to your next release.",2,2020-05-27 07:56:30,2020-06-29 07:37:48,2020-06-29 07:37:48
https://github.com/FederatedAI/FATE/issues/1399,[],a little bug in hetero_decision_tree_guest.py file,"a little bug in hetero_decision_tree_guest.py fileIn the 315 line, which is in the function of  find_best_split_guest_and_host(), the  value of 1 should be modified as 2.
![image](https://user-images.githubusercontent.com/13013299/82445526-8d732080-9ad7-11ea-8c7d-d1c8dba016e3.png)

It wastes one time of comparision, but will not affect the result correctnessyes, it will not affects the  result correctness",2,2020-05-20 12:26:47,2020-06-16 07:50:33,2020-06-16 07:50:32
https://github.com/FederatedAI/FATE/issues/1377,[],单机版fate1.4在运行推荐模型时报错,"单机版fate1.4在运行推荐模型时报错版本：单机版1.4
运行用例shell脚本如下：
```
#!/bin/bash

MODEL_DIR=""examples/federatedrec-examples/hetero_svd""
python fate_flow/fate_flow_client.py -f upload -c ${MODEL_DIR}/upload_data_guest.json
sleep 3s
python fate_flow/fate_flow_client.py -f upload -c ${MODEL_DIR}/upload_data_host.json
sleep 3s
python fate_flow/fate_flow_client.py -f submit_job -d ${MODEL_DIR}/test_hetero_svd_train_job_dsl.json -c ${MODEL_DIR}/test_hetero_svd_train_job_conf.json
```
报如下错误

> [ERROR] [2020-05-18 06:56:00,745] [6994:140016906028864] - task_executor.py[line:140]: Attempt to operate on closed/deleted/dropped object.
> Traceback (most recent call last):
> File ""/fate/fate_flow/driver/task_executor.py"", line 127, in run_task
> input_dsl=task_input_dsl)
> File ""/fate/fate_flow/driver/task_executor.py"", line 203, in get_task_run_args
> name=data_table.get_name(), options=save_as_options)
> File ""/fate/arch/api/utils/profile_util.py"", line 32, in _fn
> rtn = func(*args, **kwargs)
> File ""/fate/arch/api/impl/based_1x/table.py"", line 58, in save_as
> persistent_engine=persistent_engine)
> File ""/fate/arch/standalone/standalone/eggroll.py"", line 696, in save_as
> dup.put_all(self.collect(use_serialize=use_serialize), use_serialize=use_serialize)
> File ""/fate/arch/standalone/standalone/eggroll.py"", line 650, in put_all
> for k, v in kv_list:
> File ""/fate/arch/standalone/standalone/eggroll.py"", line 728, in _merge
> if it.next():
> lmdb.Error: Attempt to operate on closed/deleted/dropped object.您好 

> 版本：单机版1.4
> 运行用例shell脚本如下：
> 
> ```
> #!/bin/bash
> 
> MODEL_DIR=""examples/federatedrec-examples/hetero_svd""
> python fate_flow/fate_flow_client.py -f upload -c ${MODEL_DIR}/upload_data_guest.json
> sleep 3s
> python fate_flow/fate_flow_client.py -f upload -c ${MODEL_DIR}/upload_data_host.json
> sleep 3s
> python fate_flow/fate_flow_client.py -f submit_job -d ${MODEL_DIR}/test_hetero_svd_train_job_dsl.json -c ${MODEL_DIR}/test_hetero_svd_train_job_conf.json
> ```
> 
> 报如下错误
> 
> > [ERROR] [2020-05-18 06:56:00,745] [6994:140016906028864] - task_executor.py[line:140]: Attempt to operate on closed/deleted/dropped object.
> > Traceback (most recent call last):
> > File ""/fate/fate_flow/driver/task_executor.py"", line 127, in run_task
> > input_dsl=task_input_dsl)
> > File ""/fate/fate_flow/driver/task_executor.py"", line 203, in get_task_run_args
> > name=data_table.get_name(), options=save_as_options)
> > File ""/fate/arch/api/utils/profile_util.py"", line 32, in _fn
> > rtn = func(*args, **kwargs)
> > File ""/fate/arch/api/impl/based_1x/table.py"", line 58, in save_as
> > persistent_engine=persistent_engine)
> > File ""/fate/arch/standalone/standalone/eggroll.py"", line 696, in save_as
> > dup.put_all(self.collect(use_serialize=use_serialize), use_serialize=use_serialize)
> > File ""/fate/arch/standalone/standalone/eggroll.py"", line 650, in put_all
> > for k, v in kv_list:
> > File ""/fate/arch/standalone/standalone/eggroll.py"", line 728, in _merge
> > if it.next():
> > lmdb.Error: Attempt to operate on closed/deleted/dropped object.

您好，我遇到同样的问题，请问您解决了么？
没有解决，可以试试最新的1.6.0版本> 没有解决，可以试试最新的1.6.0版本

您好，使用1.3.1版本就可以解决了，应该是1.4没注册那个算法",3,2020-05-18 07:20:58,2021-04-16 10:16:24,2021-04-14 08:16:49
https://github.com/FederatedAI/FATE/issues/1376,['bug'],使用全连接神经网络横向联邦学习进行手写体识别任务，报错,"使用全连接神经网络横向联邦学习进行手写体识别任务，报错**Describe the bug**

使用全连接神经网络横向联邦学习进行手写体识别任务。数据集使用mnist，按标签01234和56789划分为两个数据集，分别上传。每个角色没有其他类别标签的数据。分类任务设为5或者10均报错。
对于数据特征相同的分类任务，不同角色拥有的数据标签完全不同时，推断报错。

**To Reproduce**
Steps to reproduce the behavior:
1. guest、host分别上传数据
2. 运行相应的配置文件、dsl文件
3.fateboard查看运行状态


**Expected behavior**

算法正常运行。




在横向任务中，每个本地节点，数据label必须一样，比如guest{0123456789}host{0123456789}It works in 1.5.0, close issue.> It works in 1.5.0, close issue.

OK，looking forward to it.",3,2020-05-18 07:13:36,2020-11-02 07:35:26,2020-11-02 07:32:44
https://github.com/FederatedAI/FATE/issues/1374,[],component_output_data api 导出的数据量问题,"component_output_data api 导出的数据量问题**Describe the bug**
参考文档 https://fate.readthedocs.io/en/latest/fate_flow/doc/fate_flow_cli.html
使用 component_output_data  api时
1. 数据量不一致的问题
总数据量（训练+验证）1万时，发现导出数据量
    output_data.csv 数据量 == train + eval
总数据量（训练+验证）10万时，发现导出数据量（有时）
    output_data.csv 数据量 == train
总数据量 （训练+验证）> 5万时，发现数据量有一定波动性，但是均为
    output_data.csv 数据量 < train + eval
 2. 重启fate后数据不见了的问题
    重启之后发现无法导出了

**To Reproduce**
Steps to reproduce the behavior:
1. 训练一个secureboost任务
2. 使用api：python fate_flow_client.py -f component_output_data -j $job_id -r $role -p $party_id -cpn $component_name -o $output_path
导出 out_data.csv
3. cat output_data.csv |wc -l
4. 数量不对劲

**Expected behavior**
希望导出数据量能与 训练+验证 一致
不排除是训练过程中丢包什么导致的数据量不一致，重复使用out_data.csv 导出数据量一致
使用 submit_job api 中 predict 的方法，多试 predict 的 out_data.csv 的数据量有一定随机性且很少与 输入的数据量一致
由于没有报错信息，不知道数据量不一致具体原因

**Screenshots**
1万数据量一致
![1万数据量一致](https://user-images.githubusercontent.com/20585352/82003959-4a0e5180-9694-11ea-8471-b05a27672c41.png)
10万数据量只有训练集
![10万数据量只有训练集](https://user-images.githubusercontent.com/20585352/82003967-51355f80-9694-11ea-98eb-3a2b0aae962b.png)
重启之后再无法导出
![重启之后再无法导出](https://user-images.githubusercontent.com/20585352/82003982-58f50400-9694-11ea-9c76-f484609c1aba.png)


**Desktop (please complete the following information):**
 - OS: centos7
 - Version fate1.3

经过排查大概率可能是 训练过程的丢包导致的，对于这种情况能否处理一下，重新打分也好，保证数据量一致的，否则线下很难评估FATE v1.4 已经fixed这个bug，也做了检测。可以试验下。> FATE v1.4 已经fixed这个bug，也做了检测。可以试验下。

好的 这两天会试一下> FATE v1.4 已经fixed这个bug，也做了检测。可以试验下。

v1.4 output_data.csv 数据量与训练+测试已一致",4,2020-05-15 02:11:04,2020-05-27 02:03:13,2020-05-27 02:03:13
https://github.com/FederatedAI/FATE/issues/1373,[],fate_flow service fail to start,"fate_flow service fail to startFor Fate1.4.0 cluster-deploy, fate_flow service failed to start on host.

Logs from /data/projects/fate/python/logs/error.log:
![f01-2020-05-14-16-17-00](https://user-images.githubusercontent.com/24994791/81995778-1da8f580-9600-11ea-9bcb-83147672fd86.png)
![f01-2020-05-14-16-17-36](https://user-images.githubusercontent.com/24994791/81995782-1eda2280-9600-11ea-8c01-db80f6bbd46c.png)

How can I solve it?Solved after re-deployed eggroll&fate",1,2020-05-14 23:35:14,2020-05-15 00:07:29,2020-05-15 00:06:46
https://github.com/FederatedAI/FATE/issues/1328,[],intersection 匹配率不为1且不固定,"intersection 匹配率不为1且不固定**Describe the bug**
intersection 会匹配率不为1且不固定
已检查id一致，匹配率也不是一个固定的值，在0-1之间的一个浮点数

**To Reproduce**
Steps to reproduce the behavior:
1.提交任务
2.检查intersection

**Expected behavior**
希望匹配率稳定为1，且增加丢包检查

**Screenshots**
![微信图片_20200507101212](https://user-images.githubusercontent.com/20585352/81247311-3cc9e500-904c-11ea-9158-d6e6699a809f.png)

![微信图片_20200507101137](https://user-images.githubusercontent.com/20585352/81246917-574f8e80-904b-11ea-9910-58c9e2cd5ce3.png)



**Desktop (please complete the following information):**
 - fate1.3

**Additional context**
看了 issue 里面好像没有相关问题，不知道我这种是不是个例我这也有相同的问题，没找到解决方法FATE v1.4 已经解决，可以试用下> FATE v1.4 已经解决，可以试用下

谢谢啦 最近试试> FATE v1.4 已经解决，可以试用下

已尝试，数据量问题1.4 已解决",4,2020-05-07 02:18:54,2020-05-27 01:56:18,2020-05-27 01:56:18
https://github.com/FederatedAI/FATE/issues/1247,['bug'],Wrong Party ID in Feature Names on Board for Stepwise,"Wrong Party ID in Feature Names on Board for Stepwise**Describe the bug**
FATE Board always uses local party's id when making feature name anonyms. 


**To Reproduce**
Steps to reproduce the behavior:
1. Set host/guest party id to different numbers in stepwise job configuration 
2. Run the stepwise job and check Board for where feature anonyms are presented. 

Fixed in commit c10906e",1,2020-04-27 07:31:53,2020-04-27 07:36:05,2020-04-27 07:36:05
https://github.com/FederatedAI/FATE/issues/1162,[], hetero_nn model : the  error of Backward Propagation Part II in README.md ,"hetero_nn model : the  error of Backward Propagation Part II in README.md According to the papers and code, the description should be modified as follows， @ is the modification location, the total error number is 16 :
1、Party B calculates the error delta_act of activation function's output by delta.
2、Party B propagates delta_bottomB = delta_act * W_B to bottom model, then updates W_B(W_B -= eta * delta_act * alpha_B).
3、Party B generates noise epsilon_B, calculates [delta_act* @alpha_A + epsilon_B] and sends it to party A.
4、Party A encrypts epsilon_acc, sends [epsilon_acc] to party B.
Then party @A decrypts the received value, generates noise epsilon_A, adds epsilon_A / eta to decrypted result(delta_act * @alpha_A  + epsilon_B + epsilon_A / eta) and epsilon_A to accumulate noise epsilon_acc(epsilon_acc += epsilon_A). Party A sends the addition result to party B. (delta_act * @alpha_A + epsilon_B + epsilon_A / eta)
5、Party B receives [epsilon_acc] and delta_act * @alpha_A  + epsilon_B + epsilon_A / eta. Firstly it sends party A's bottom model output' error [ @delta_act * @(W_A + epsilon_acc)] to host. Secondly updates W_A -= eta * (delta_act * alpha_A + epsilon_B + epsilon_A / eta - epsilon_B @) = eta * delta_act * @alpha_A  @+ @epsilon_A,  @ W_A =W_TRUE - epsilon_acc. Where W_TRUE represents the actually weights.
6、Party A decrypts [@delta_act * (W_A + @epsilon_acc)] and passes @delta_act * (W_A + @epsilon_acc) to its bottom model.
@ZhanqiLiu Thanks for your detailed modification comments，we will fix it soon. Sorry for misleading,  refer to the attached flow diagram will be much better.",1,2020-04-15 02:39:06,2020-06-16 12:06:03,2020-06-16 12:06:03
https://github.com/FederatedAI/FATE/issues/1159,[],跑横向逻辑回归训练模型时arbiter方出现错误,"跑横向逻辑回归训练模型时arbiter方出现错误fate版本1.3，提交test_homolr_train_job.conf.json任务在homo_lr_0出错
在fateboard 界面host和guest方log显示无误，arbiter 端出现error，将算法配置中CV_param的shuffle由原来的true改成false后成功跑通。请教一下各位大神，这是什么原因？
log如下：
 ERROR: <_Rendezvous of RPC that terminated with:
2 status = StatusCode.INTERNAL
3 details = ""192.168.22.141:8011: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
4 at com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:69)
5 at com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:44)
6 at com.webank.ai.eggroll.framework.roll.manager.RollSessionManager.initializeEgg(RollSessionManager.java:112)
7 at com.webank.ai.eggroll.framework.roll.api.grpc.server.RollProcessServiceImpl$ProcessServiceTemplate.run(RollProcessServiceImpl.java:344)
8 at com.webank.ai.eggroll.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:52)
9 at com.webank.ai.eggroll.framework.roll.api.grpc.server.RollProcessServiceImpl.mapValues(RollProcessServiceImpl.java:117)
10 at com.webank.ai.eggroll.api.computing.processor.ProcessServiceGrpc$MethodHandlers.invoke(ProcessServiceGrpc.java:976)
11 at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
12 at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
13 at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
14 at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
15 at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
16 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
17 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
18 at java.lang.Thread.run(Thread.java:748)
19 Caused by: java.lang.reflect.InvocationTargetException
20 at com.webank.ai.eggroll.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:154)
21 at com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:67)
22 ... 14 more
23 Caused by: io.grpc.StatusRuntimeException: INTERNAL: 192.168.22.141:7888: java.lang.IllegalStateException: The attempt resulted in a result. There is no exception
24 at com.webank.ai.eggroll.core.retry.impl.attempt.context.ResultAttemptContext.getExceptionCause(ResultAttemptContext.java:63)
25 at com.webank.ai.eggroll.core.retry.RetryException.<init>(RetryException.java:29)
26 at com.webank.ai.eggroll.core.retry.RetryException.<init>(RetryException.java:24)
27 at com.webank.ai.eggroll.core.retry.Retryer.call(Retryer.java:106)
28 at com.webank.ai.eggroll.framework.egg.manager.EggSessionManager.getOrCreateSession(EggSessionManager.java:126)
29 at com.webank.ai.eggroll.framework.egg.api.grpc.server.EggSessionServiceImpl.lambda$getOrCreateSession$0(EggSessionServiceImpl.java:38)
30 at com.webank.ai.eggroll.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:52)
31 at com.webank.ai.eggroll.framework.egg.api.grpc.server.EggSessionServiceImpl.getOrCreateSession(EggSessionServiceImpl.java:36)
32 at com.webank.ai.eggroll.api.framework.egg.SessionServiceGrpc$MethodHandlers.invoke(SessionServiceGrpc.java:349)
33 at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
34 at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
35 at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
36 at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
37 at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
38 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
39 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
40 at java.lang.Thread.run(Thread.java:748)
41  
42 at io.grpc.Status.asRuntimeException(Status.java:526)
43 at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
44 at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
45 at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
46 at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
47 at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
48 at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
49 at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
50 at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
51 at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
52 at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
53 at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
54 at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
55 at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
56 at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
57 ... 5 more
58 ""
59 debug_error_string = ""{""created"":""@1586767213.483875340"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1017,""grpc_message"":""192.168.22.141:8011: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n\tat com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:69)\n\tat com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:44)\n\tat com.webank.ai.eggroll.framework.roll.manager.RollSessionManager.initializeEgg(RollSessionManager.java:112)\n\tat com.webank.ai.eggroll.framework.roll.api.grpc.server.RollProcessServiceImpl$ProcessServiceTemplate.run(RollProcessServiceImpl.java:344)\n\tat com.webank.ai.eggroll.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:52)\n\tat com.webank.ai.eggroll.framework.roll.api.grpc.server.RollProcessServiceImpl.mapValues(RollProcessServiceImpl.java:117)\n\tat com.webank.ai.eggroll.api.computing.processor.ProcessServiceGrpc$MethodHandlers.invoke(ProcessServiceGrpc.java:976)\n\tat io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)\n\tat io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)\n\tat io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)\n\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat com.webank.ai.eggroll.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:154)\n\tat com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:67)\n\t... 14 more\nCaused by: io.grpc.StatusRuntimeException: INTERNAL: 192.168.22.141:7888: java.lang.IllegalStateException: The attempt resulted in a result. There is no exception\n\tat com.webank.ai.eggroll.core.retry.impl.attempt.context.ResultAttemptContext.getExceptionCause(ResultAttemptContext.java:63)\n\tat com.webank.ai.eggroll.core.retry.RetryException.<init>(RetryException.java:29)\n\tat com.webank.ai.eggroll.core.retry.RetryException.<init>(RetryException.java:24)\n\tat com.webank.ai.eggroll.core.retry.Retryer.call(Retryer.java:106)\n\tat com.webank.ai.eggroll.framework.egg.manager.EggSessionManager.getOrCreateSession(EggSessionManager.java:126)\n\tat com.webank.ai.eggroll.framework.egg.api.grpc.server.EggSessionServiceImpl.lambda$getOrCreateSession$0(EggSessionServiceImpl.java:38)\n\tat com.webank.ai.eggroll.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:52)\n\tat com.webank.ai.eggroll.framework.egg.api.grpc.server.EggSessionServiceImpl.getOrCreateSession(EggSessionServiceImpl.java:36)\n\tat com.webank.ai.eggroll.api.framework.egg.SessionServiceGrpc$MethodHandlers.invoke(SessionServiceGrpc.java:349)\n\tat io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)\n\tat io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)\n\tat io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)\n\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat io.grpc.Status.asRuntimeException(Status.java:526)\n\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)\n\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)\n\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)\n\tat io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)\n\tat io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)\n\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)\n\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)\n\tat io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)\n\tat io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)\n\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)\n\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)\n\t... 5 more\n"",""grpc_status"":13}""
60 >""
61 Traceback (most recent call last):
62 File ""/home/fate/projects/fate/python/fate_flow/driver/task_executor.py"", line 119, in run_task
63 run_object.run(parameters, task_run_args)
64 File ""/home/fate/projects/fate/python/federatedml/model_base.py"", line 91, in run
65 this_data_output = func(*params)
66 File ""/home/fate/projects/fate/python/federatedml/linear_model/logistic_regression/homo_logsitic_regression/homo_lr_arbiter.py"", line 116, in predict
67 predict_wx = cipher.distribute_decrypt(encrypted_predict_wx)
68 File ""/home/fate/projects/fate/python/federatedml/secureprotol/encrypt.py"", line 68, in distribute_decrypt
69 decrypt_table = X.mapValues(lambda x: self.decrypt(x))
70 File ""/home/fate/projects/fate/python/arch/api/utils/profile_util.py"", line 31, in _fn
71 rtn = func(*args, **kwargs)
72 File ""/home/fate/projects/fate/python/arch/api/table/eggroll/table_impl.py"", line 113, in mapValues
73 return DTable(self._dtable.mapValues(func), session_id=self._session_id)
74 File ""/home/fate/projects/fate/eggroll/python/eggroll/api/cluster/eggroll.py"", line 271, in mapValues
75 return _EggRoll.get_instance().map_values(self, func)
76 File ""/home/fate/projects/fate/eggroll/python/eggroll/api/cluster/eggroll.py"", line 600, in map_values
77 return self.__do_unary_process_and_create_table(table=_table, user_func=func, stub_func=self.proc_stub.mapValues)
78 File ""/home/fate/projects/fate/eggroll/python/eggroll/api/cluster/eggroll.py"", line 680, in __do_unary_process_and_create_table
79 resp = self.__do_unary_process(table=table, user_func=user_func, stub_func=stub_func)
80 File ""/home/fate/projects/fate/eggroll/python/eggroll/api/cluster/eggroll.py"", line 677, in __do_unary_process
81 return stub_func(process)
82 File ""/home/fate/projects/fate/common/python/venv/lib/python3.6/site-packages/grpc/_channel.py"", line 533, in __call__
83 return _end_unary_response_blocking(state, call, False, None)
84 File ""/home/fate/projects/fate/common/python/venv/lib/python3.6/site-packages/grpc/_channel.py"", line 467, in _end_unary_response_blocking
85 raise _Rendezvous(state, None, None, deadline)
86 grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:
87 status = StatusCode.INTERNAL
88 details = ""192.168.22.141:8011: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
89 at com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:69)
90 at com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:44)
91 at com.webank.ai.eggroll.framework.roll.manager.RollSessionManager.initializeEgg(RollSessionManager.java:112)
92 at com.webank.ai.eggroll.framework.roll.api.grpc.server.RollProcessServiceImpl$ProcessServiceTemplate.run(RollProcessServiceImpl.java:344)
93 at com.webank.ai.eggroll.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:52)
94 at com.webank.ai.eggroll.framework.roll.api.grpc.server.RollProcessServiceImpl.mapValues(RollProcessServiceImpl.java:117)
95 at com.webank.ai.eggroll.api.computing.processor.ProcessServiceGrpc$MethodHandlers.invoke(ProcessServiceGrpc.java:976)
96 at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
97 at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
98 at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
99 at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
100 at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
101 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
102 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
103 at java.lang.Thread.run(Thread.java:748)
104 Caused by: java.lang.reflect.InvocationTargetException
105 at com.webank.ai.eggroll.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:154)
106 at com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:67)
107 ... 14 more
108 Caused by: io.grpc.StatusRuntimeException: INTERNAL: 192.168.22.141:7888: java.lang.IllegalStateException: The attempt resulted in a result. There is no exception
109 at com.webank.ai.eggroll.core.retry.impl.attempt.context.ResultAttemptContext.getExceptionCause(ResultAttemptContext.java:63)
110 at com.webank.ai.eggroll.core.retry.RetryException.<init>(RetryException.java:29)
111 at com.webank.ai.eggroll.core.retry.RetryException.<init>(RetryException.java:24)
112 at com.webank.ai.eggroll.core.retry.Retryer.call(Retryer.java:106)
113 at com.webank.ai.eggroll.framework.egg.manager.EggSessionManager.getOrCreateSession(EggSessionManager.java:126)
114 at com.webank.ai.eggroll.framework.egg.api.grpc.server.EggSessionServiceImpl.lambda$getOrCreateSession$0(EggSessionServiceImpl.java:38)
115 at com.webank.ai.eggroll.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:52)
116 at com.webank.ai.eggroll.framework.egg.api.grpc.server.EggSessionServiceImpl.getOrCreateSession(EggSessionServiceImpl.java:36)
117 at com.webank.ai.eggroll.api.framework.egg.SessionServiceGrpc$MethodHandlers.invoke(SessionServiceGrpc.java:349)
118 at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
119 at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
120 at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
121 at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
122 at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
123 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
124 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
125 at java.lang.Thread.run(Thread.java:748)
126  
127 at io.grpc.Status.asRuntimeException(Status.java:526)
128 at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
129 at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
130 at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
131 at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
132 at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
133 at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
134 at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
135 at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
136 at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
137 at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
138 at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
139 at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
140 at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
141 at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
142 ... 5 more
143 ""
144 debug_error_string = ""{""created"":""@1586767213.483875340"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1017,""grpc_message"":""192.168.22.141:8011: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n\tat com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:69)\n\tat com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:44)\n\tat com.webank.ai.eggroll.framework.roll.manager.RollSessionManager.initializeEgg(RollSessionManager.java:112)\n\tat com.webank.ai.eggroll.framework.roll.api.grpc.server.RollProcessServiceImpl$ProcessServiceTemplate.run(RollProcessServiceImpl.java:344)\n\tat com.webank.ai.eggroll.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:52)\n\tat com.webank.ai.eggroll.framework.roll.api.grpc.server.RollProcessServiceImpl.mapValues(RollProcessServiceImpl.java:117)\n\tat com.webank.ai.eggroll.api.computing.processor.ProcessServiceGrpc$MethodHandlers.invoke(ProcessServiceGrpc.java:976)\n\tat io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)\n\tat io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)\n\tat io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)\n\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat com.webank.ai.eggroll.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:154)\n\tat com.webank.ai.eggroll.framework.roll.api.grpc.client.EggSessionServiceClient.getOrCreateSession(EggSessionServiceClient.java:67)\n\t... 14 more\nCaused by: io.grpc.StatusRuntimeException: INTERNAL: 192.168.22.141:7888: java.lang.IllegalStateException: The attempt resulted in a result. There is no exception\n\tat com.webank.ai.eggroll.core.retry.impl.attempt.context.ResultAttemptContext.getExceptionCause(ResultAttemptContext.java:63)\n\tat com.webank.ai.eggroll.core.retry.RetryException.<init>(RetryException.java:29)\n\tat com.webank.ai.eggroll.core.retry.RetryException.<init>(RetryException.java:24)\n\tat com.webank.ai.eggroll.core.retry.Retryer.call(Retryer.java:106)\n\tat com.webank.ai.eggroll.framework.egg.manager.EggSessionManager.getOrCreateSession(EggSessionManager.java:126)\n\tat com.webank.ai.eggroll.framework.egg.api.grpc.server.EggSessionServiceImpl.lambda$getOrCreateSession$0(EggSessionServiceImpl.java:38)\n\tat com.webank.ai.eggroll.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:52)\n\tat com.webank.ai.eggroll.framework.egg.api.grpc.server.EggSessionServiceImpl.getOrCreateSession(EggSessionServiceImpl.java:36)\n\tat com.webank.ai.eggroll.api.framework.egg.SessionServiceGrpc$MethodHandlers.invoke(SessionServiceGrpc.java:349)\n\tat io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)\n\tat io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)\n\tat io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)\n\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat io.grpc.Status.asRuntimeException(Status.java:526)\n\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)\n\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)\n\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)\n\tat io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)\n\tat io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)\n\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)\n\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)\n\tat io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)\n\tat io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)\n\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)\n\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)\n\t... 5 more\n"",""grpc_status"":13}""
I suspect it was because some services are down. Please have a check if roll service(whose port number is 8011) is still here or not. And restart all the services before another try. > I suspect it was because some services are down. Please have a check if roll service(whose port number is 8011) is still here or not. And restart all the services before another try.

好的，谢谢> > I suspect it was because some services are down. Please have a check if roll service(whose port number is 8011) is still here or not. And restart all the services before another try.
> 
> 好的，谢谢

请问在跑横向逻辑回归时有遇到list index out of range这个错误吗？host guest 第一步dataio就报错了……",3,2020-04-14 02:18:23,2022-05-19 02:41:46,2021-11-18 09:31:53
https://github.com/FederatedAI/FATE/issues/1157,"['bug', 'federatedml']","横向联邦100w数据量在做predict的时候报错【ERROR: 'NoneType' object has no attribute 'unboxed'"" 】","横向联邦100w数据量在做predict的时候报错【ERROR: 'NoneType' object has no attribute 'unboxed'"" 】""2020-04-13 09:31:55,560 - homo_lr_arbiter.py[line:95] - INFO: Start predict task""
82""2020-04-13 09:31:55,901 - task_executor.py[line:132] - ERROR: 'NoneType' object has no attribute 'unboxed'""
83Traceback (most recent call last):
84File ""/home/fate/fate/python/fate_flow/driver/task_executor.py"", line 119, in run_task
85run_object.run(parameters, task_run_args)
86File ""/home/fate/fate/python/federatedml/model_base.py"", line 91, in run
87this_data_output = func(*params)
88File ""/home/fate/fate/python/federatedml/linear_model/logistic_regression/homo_logsitic_regression/homo_lr_arbiter.py"", line 100, in predict
89LOGGER.debug(""Loaded arbiter model: {}"".format(self.model_weights.unboxed))
90AttributeError: 'NoneType' object has no attribute 'unboxed'
91""2020-04-13 09:31:55,951 - api_utils.py[line:69] - INFO: grpc api response: header {@JackLi529 According to the parameters settings, max_iter has been set as 1 which may cause fail to aggregate model in arbiter. Therefore, arbiter cannot access the aggregated model yet before entering predict process. However, this is still a problem for arbiter fail to aggregate. This will be fix in FATE-1.4.",2,2020-04-13 02:02:13,2020-06-15 05:49:54,2020-06-15 05:49:54
https://github.com/FederatedAI/FATE/issues/1156,[],有虚拟网桥的离线环境下安装集群模式后 toy example 不能正常运行,"有虚拟网桥的离线环境下安装集群模式后 toy example 不能正常运行**Describe the bug**
离线环境下，两台 Centos 7.6 的虚拟机，利用提供的脚本安装后，在 guset 节点上执行 `python run_toy_example.py 9999 10000 1` 无法正常运行，提示的日志如下：

![image](https://user-images.githubusercontent.com/8924378/79070752-62dcbd80-7d0a-11ea-9194-4a4205d5e86f.png)

后来我们发现图片里面显示的 IP 192.168.122.1 ，这个是虚拟网桥 virbr0 的 IP 地址，因此我们将这个虚拟网桥关闭后重新安装了一次，但仍然无法正常运行 toy_example 提示的问题如下：

![image](https://user-images.githubusercontent.com/8924378/79070963-83f1de00-7d0b-11ea-9e25-83f78b3cf325.png)

抛出的错误提示从 192.168.122.1:8011 变成了 null:8011。

**Expected behavior**
Toy Example 能够正常运行。

**Desktop (please complete the following information):**
 - OS: Centos
 - Browser chrome
 - Version v1.3.0

最终定位问题是 eggroll 1.x 版本的一个 BUG，在 [RuntimeUtils.java](https://github.com/WeBankFinTech/eggroll/blob/v1.x/core/src/main/java/com/webank/ai/eggroll/core/utils/RuntimeUtils.java) 的47~67行里有如下的代码：

```java
public String getMySiteLocalAddress() {
        if (siteLocalAddress == null) {
            Enumeration<NetworkInterface> networkInterfaces = null;
            try {
                networkInterfaces = NetworkInterface.getNetworkInterfaces();

                for (NetworkInterface ni : Collections.list(networkInterfaces)) {
                    Enumeration<InetAddress> inetAddresses = ni.getInetAddresses();
                    for (InetAddress ia : Collections.list(inetAddresses)) {
                        if (ia.isSiteLocalAddress()) {
                            siteLocalAddress = StringUtils.substringAfterLast(ia.toString(), ""/"");
                        }
                    }
                }
            } catch (SocketException e) {
                siteLocalAddress = ""127.0.0.1"";
            }
        }

        return siteLocalAddress;
    }
```

里面调用了 `isSiteLocalAddress()` 这个方法，这个方法无法把我使用的虚拟机的 ip 地址 128.x.x.x 识别为本地地址，而会把 192.168 开头的虚拟网桥地址识别为 LocalAddress，从而导致 grpc 服务监听的地址错误。

 eggroll 2.0 版本已经修复了这个 bug， 而 FATE 1.3.0 安装包里面使用的还是 eggroll 1.1.4 的版本，因而出现了这个问题。",1,2020-04-12 14:20:53,2020-04-14 07:50:44,2020-04-14 07:50:44
https://github.com/FederatedAI/FATE/issues/1127,[],"standalone-deploy install FATE in host, error in init.sh script","standalone-deploy install FATE in host, error in init.sh script- Went over the process described at https://github.com/FederatedAI/FATE/tree/master/standalone-deploy
Option 2) install FATE in host
Running the command `source init.sh init` caused errors looking like this :
`service.sh: 65: service.sh: [[: not found` and services not starting.

- It is because string conditions like `[[ cond ]]` are bash specific and the script init.sh calls the script service.sh like this `sh  service.sh restart`.
sh is not bash and it does not implements `[[. which causes errors like the one I mentioned above.

- Could complete the install by replacing ocurences of `sh` calls by `bash`
Excute commonds:
yum -y install dos2unix
dos2unix  service.sh",1,2020-03-26 16:55:34,2020-04-15 06:12:24,2020-04-15 06:12:23
https://github.com/FederatedAI/FATE/issues/1124,[],hetero secureboost: large disk usage by egg lmdb,"hetero secureboost: large disk usage by egg lmdb**Describe the bug**
The disk usage of hetero secureboost is around 500GB during training.

**To Reproduce**
Steps to reproduce the behavior:
1. Train a hetero secureboost model with the following parameters:
    - ""num_trees"": 200
    - ""encrypt_param"": {""method"": ""IterativeAffine""}
    - ""validation_freqs"": null
2. After training finishes, check the disk usage of egg service from both guest and host sides:
    - guest:
    ```bash
    $ cd /data/projects/fate/confs-10000/confs/egg/data-dir/lmdb
    $ du -hs 202003230126391918532_secureboost_0_guest_10000/
    507G    202003230126391918532_secureboost_0_guest_10000/
    ```
    - host:
    ```bash
    $ cd /data/projects/fate/confs-9999/confs/egg/data-dir/lmdb
    $ du -hs 202003230126391918532_secureboost_0_host_9999/
    474G    202003230126391918532_secureboost_0_host_9999/
    ```

**Expected behavior**
Is this expected?

**Screenshots**
N.A.

**Desktop (please complete the following information):**
 - OS: Ubuntu 18.04
 - Browser: Chrome

**Smartphone (please complete the following information):**
 N.A.

**Additional context**
- FATE is deployed with `docker-compose` on one guest and one host
- Data size: guest: `454,902 x 17`; host: `454,902 x 16`
- FATE version: v1.3.0
- Related issue: #648 (which was closed and reopened)After the task, whether the space has been released？Unfortunately no. I had to remove the folders manually.@dylan-fan Looks like this has been fixed. Shall I go ahead and close this issue?fix already, close issue",4,2020-03-24 02:16:26,2021-12-17 03:26:44,2021-12-17 03:26:44
https://github.com/FederatedAI/FATE/issues/1120,[],when I build docker meet a problem 部署docker的时候遇到一个问题,"when I build docker meet a problem 部署docker的时候遇到一个问题when I build the Docker images of FATE components, just use this command ""bash build_cluster_docker.sh all"", I meet this problem ""Unable to find image 'maven:3.6-jdk-8' locally‘

我在单机部署docker时遇到一个问题“Unable to find image 'maven:3.6-jdk-8' locally‘

![8ECCB4536B1266A52860AD31BC41ADE9](https://user-images.githubusercontent.com/29162161/77231167-38e51f00-6bd4-11ea-855f-1cf42c34813c.jpg)

Not issue related to FATE, Google Docker usage first please!",1,2020-03-21 16:32:04,2021-12-13 04:14:13,2021-12-13 04:14:13
https://github.com/FederatedAI/FATE/issues/1089,"['bug', 'enhancement', 'federatedml']",Cannot Evaluate Multi-Class Prediction from Local Baseline,"Cannot Evaluate Multi-Class Prediction from Local Baseline**Describe the bug**
When trying to evaluate multi-class results from Local Baseline module, an Error is raised. It appears to be due to prediction result being type ""str"", different from ""int"" type as in data set.

**To Reproduce**
Steps to reproduce the behavior:
1. siting evaluation type to ""multi""
2. run a local baseline case with multi-class instead of binary labels. 

**Expected behavior**
The Evaluation module should present the multi-class evaluation results. 
This issue is due to a type misspecification when converting between data instance table and numpy array. In addition, Local Baseline module was not designed to also support multi-class cases. Support for multi-class will be added in a newer release.",1,2020-03-11 07:43:32,2020-06-15 05:48:10,2020-06-15 05:48:10
https://github.com/FederatedAI/FATE/issues/1080,[],execute the fate_flow  -f download  have a Nonetype error ,"execute the fate_flow  -f download  have a Nonetype error **Describe the bug**
According to documents the fate_flow -f download need four parameter . the doc example :

#### 7.How to download a table which has been uploaded before
 > python fate_flow_client.py -f download -n table_namespace -t table_name -w work_mode -o save_file  

but if execute this script ,will return a error :

![微信图片_20200308223633](https://user-images.githubusercontent.com/10710148/76164886-4d192d00-618d-11ea-9fb6-f730aae6953a.png)



**To Reproduce**
Steps to reproduce the behavior:
1. execute  'python fate_flow_client.py -f download -n table_namespace -t table_name -w work_mode -o save_file  '
2 See error

**Expected behavior**
Expected behavior is download data to outputdata .because the python scrip ""FATE\fate_flow\utils\download.py"" lines 49 :
  
fout.write(key + self.parameters.get(""delimitor"", "","") + value + ""\n"")

the  self.parameters.get(""delimitor"", "","")  there is no judgment that the variable delimitor is None 



**Desktop (please complete the following information):**
 - OS: [e.g. linux]
 - Browser [e.g. chrome]
 - Version [e.g. 1.2]


Please refer to the document:https://github.com/FederatedAI/FATE/blob/master/fate_flow/doc/fate_flow_cli.md#download",1,2020-03-08 14:43:39,2021-12-13 04:09:50,2021-12-13 04:09:35
https://github.com/FederatedAI/FATE/issues/1078,[],modify fate_flow chinese doc ,"modify fate_flow chinese doc **Describe the bug**
The doc have a mistake 
**To Reproduce**
Steps to reproduce the behavior:
1. Go to 'https://github.com/FederatedAI/FATE/blob/master/fate_flow/doc/fate_flow_cli.md'
2.   Scroll down to  'component_output_data' section 
3. The sample 'python fate_flow_client.py -f component_output_model -j $job_id -r $role -p $party_id -cpn $component_name -o $output_path' have a mistake 
4. See error is  'component_output_model' it's should ' component_output_data' 

**Expected behavior**
modify 'component_output_model'  to  ' component_output_data' 

**Screenshots**
![image](https://user-images.githubusercontent.com/10710148/76164197-ecd3bc80-6187-11ea-885d-0a0fedf7573b.png)


**Desktop (please complete the following information):**
 - OS: [e.g. linux]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 1.2]


Thank you for your correction.",1,2020-03-08 13:59:42,2021-12-27 07:55:04,2021-12-27 07:55:04
https://github.com/FederatedAI/FATE/issues/1060,[],homo nn: Object of type 'MetricType' is not JSON serializable,"homo nn: Object of type 'MetricType' is not JSON serializable**Describe the bug**
Cannot see model output of homo nn in FATE Board

**To Reproduce**
Steps to reproduce the behavior:
1. Start a homo nn job and wait for its completion
2. In FATE Board, view the job id with arbiter role
3. Choose `homo_nn_0` and click on `view the outputs` 
4. Have an error dialog with message `Object of type 'MetricType' is not JSON serializable`

**Expected behavior**
Model output similar to that of homo lr

**Desktop (please complete the following information):**
 - OS: [Ubuntu 18.04]
 - Browser [Chrome]

**Additional context**

In file `federatedml/linear_model/linear_model_base.py` it is `metric_type=""LOSS""` ([link](https://github.com/FederatedAI/FATE/blob/master/federatedml/linear_model/linear_model_base.py#L103))
but
in file `federatedml/nn/homo_nn/enter_point.py` it is `metric_type=MetricType.LOSS` ([link](https://github.com/FederatedAI/FATE/blob/5a614edf7814f20a4a34151ac7b19d2b0eb39b12/federatedml/nn/homo_nn/enter_point.py#L85))

Looks like some refactoring work for `MetricType` is still in-progress...?
> **Describe the bug**
> Cannot see model output of homo nn in FATE Board
> 
> **To Reproduce**
> Steps to reproduce the behavior:
> 
> 1. Start a homo nn job and wait for its completion
> 2. In FATE Board, view the job id with arbiter role
> 3. Choose `homo_nn_0` and click on `view the outputs`
> 4. Have an error dialog with message `Object of type 'MetricType' is not JSON serializable`
> 
> **Expected behavior**
> Model output similar to that of homo lr
> 
> **Desktop (please complete the following information):**
> 
> * OS: [Ubuntu 18.04]
> * Browser [Chrome]
> 
> **Additional context**
> 
> In file `federatedml/linear_model/linear_model_base.py` it is `metric_type=""LOSS""` ([link](https://github.com/FederatedAI/FATE/blob/master/federatedml/linear_model/linear_model_base.py#L103))
> but
> in file `federatedml/nn/homo_nn/enter_point.py` it is `metric_type=MetricType.LOSS` ([link](https://github.com/FederatedAI/FATE/blob/5a614edf7814f20a4a34151ac7b19d2b0eb39b12/federatedml/nn/homo_nn/enter_point.py#L85))
> 
> Looks like some refactoring work for `MetricType` is still in-progress...?


Thank you for your detailed description. This bug is related to another bug of fateflow. You can temporarily change `MetricType.LOSS` to ""LOSS""  to make it workaround.> You can temporarily change `MetricType.LOSS` to ""LOSS"" to make it workaround.

Yes already did this from my side. Thanks for your reply and look forward to your future releases!

Closing the issue now.",2,2020-03-04 02:29:51,2020-03-05 06:51:27,2020-03-05 06:51:27
https://github.com/FederatedAI/FATE/issues/1055,[],table info - the count of upload data is 0,"table info - the count of upload data is 0when I deployed the cluster version, and test the min_test_task as follow codes:
```
source /data/projects/fate/init_env.sh
cd /data/projects/fate/python/examples/min_test_task/
sh run.sh host fast 
```
I meet a problem that **the count of upload data is 0**, the console is outputed as follows. I don't know how to handle with this problem.
```
(venv) [app@hadoop-slave1 min_test_task]$ sh run.sh host fast
role is host
task is fast
upload_task, table_name:host_table_name_1583256848_2084
upload_task, namespace:host_table_namespace_1583256848_2084
Upload data config json: {'file': '/data/projects/fate/python/examples/min_test_task/../data/breast_a.csv', 'head': 1, 'partition': 10, 'work_mode': 1, 'table_name': 'host_table_name_1583256848_2084', 'namespace': 'host_table_namespace_1583256848_2084'}
Start task: ['python', '/data/projects/fate/python/examples/min_test_task/../../fate_flow/fate_flow_client.py', '-f', 'upload', '-c', '/data/projects/fate/python/examples/min_test_task/test/upload_host.config_1583256848_3983']
Upload output is {'data': {'board_url': 'http://192.168.1.192:8080/index.html#/dashboard?job_id=2020030401340924436620&role=local&party_id=0', 'job_dsl_path': '/data/projects/fate/python/jobs/2020030401340924436620/job_dsl.json', 'job_runtime_conf_path': '/data/projects/fate/python/jobs/2020030401340924436620/job_runtime_conf.json', 'logs_directory': '/data/projects/fate/python/logs/2020030401340924436620', 'namespace': 'host_table_namespace_1583256848_2084', 'table_name': 'host_table_name_1583256848_2084'}, 'jobId': '2020030401340924436620', 'retcode': 0, 'retmsg': 'success'}
Start task: ['python', '/data/projects/fate/python/examples/min_test_task/../../fate_flow/fate_flow_client.py', '-f', 'table_info', '-t', 'host_table_name_1583256848_2084', '-n', 'host_table_namespace_1583256848_2084']
{'data': {'count': 0, 'namespace': 'host_table_namespace_1583256848_2084', 'partition': 1, 'table_name': 'host_table_name_1583256848_2084'}, 'retcode': 0, 'retmsg': 'success'}
Upload Data, role: host, count: {'data': {'count': 0, 'namespace': 'host_table_namespace_1583256848_2084', 'partition': 1, 'table_name': 'host_table_name_1583256848_2084'}, 'retcode': 0, 'retmsg': 'success'}
The table name and namespace is needed by GUEST. To start a modeling task, please inform GUEST with the table name and namespace.
finish upload intersect data
*********************
*******finish!*******
```Hello, you can check if there is an error message in ‘/data/projects/fate/python/logs/2020030401340924436620/local/0' or '/data/projects/fate/python/logs/fate_flow/ERROR.log'The error log is showed in '/data/projects/fate/python/logs/fate_flow/ERROR.log' as follows:
```
  self.on_connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/redis/connection.py"", line 570, in on_connect
    if nativestr(self.read_response()) != 'OK':
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/redis/connection.py"", line 638, in read_response
    raise response
redis.exceptions.ResponseError: Client sent AUTH, but no password is set
""2020-03-03 00:00:45,731 - data_access_app.py[line:32] - ERROR: push job into queue failed""
Traceback (most recent call last):
  File ""/data/projects/fate/python/fate_flow/driver/job_controller.py"", line 98, in submit_job
    RuntimeConfig.JOB_QUEUE.put_event(job_event)
  File ""/data/projects/fate/python/fate_flow/manager/queue_manager.py"", line 80, in put_event
    raise e
  File ""/data/projects/fate/python/fate_flow/manager/queue_manager.py"", line 75, in put_event
    ret = conn.lpush(self.queue_name, json.dumps(event))
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/redis/client.py"", line 1554, in lpush
    return self.execute_command('LPUSH', name, *values)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/redis/client.py"", line 754, in execute_command
    connection.send_command(*args)
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/redis/connection.py"", line 619, in send_command
    self.send_packed_command(self.pack_command(*args))
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/redis/connection.py"", line 594, in send_packed_command
    self.connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/redis/connection.py"", line 502, in connect
    self.on_connect()
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/redis/connection.py"", line 570, in on_connect
    if nativestr(self.read_response()) != 'OK':
  File ""/data/projects/fate/common/python/venv/lib/python3.6/site-packages/redis/connection.py"", line 638, in read_response
    raise response
redis.exceptions.ResponseError: Client sent AUTH, but no password is set
```
The redis process is showed as follows:
```
[app@hadoop-slave1 fate_flow]$ ps aux|grep redis
app       98604  0.0  0.0 112712   964 pts/1    S+   18:14   0:00 grep --color=auto redis
app      183604  0.1  0.0 153884  3300 pts/1    Sl   01:33   1:10 /data/projects/fate/common/redis/redis-5.0.2/bin/redis-server 0.0.0.0:6379
```
I don't know how to do next to deal with the redis problem, thanks very muchI kill all the process of fate and stop all the modules as follows
```
kill -s 9 `ps -aux | grep fate | awk '{print $2}'`
cd /data/projects/fate
sh services.sh all stop
```
and restart all the modules
```
cd /data/projects/fate
sh services.sh all start

```
it solve the problem. Thanks very much",3,2020-03-03 09:41:23,2020-03-04 03:54:33,2020-03-04 03:31:30
https://github.com/FederatedAI/FATE/issues/1047,"['bug', 'federatedml']",Transfer Variable Cleaned before Being Sent by LinR & Poisson Host,"Transfer Variable Cleaned before Being Sent by LinR & Poisson Host**Describe the bug**
The ""host_partial_prediction"" variable is cleaned before Host sending it to Guest during cross validation. 

**To Reproduce**
1. Set ""validation_freqs"" in configuration file to be real value. 
2. Run a poisson or linear regression cross validation task. 

**Expected behavior**
Guest keeps waiting on Host sending host_partial_prediction at the end of first fold, with no error message. 
Fixed & Merged .",1,2020-03-02 14:57:45,2020-03-03 02:34:58,2020-03-03 02:34:58
https://github.com/FederatedAI/FATE/issues/1009,"['bug', 'enhancement', 'federatedml']",Fix HeteroSecureBoost's error when host feature histogram is large.,"Fix HeteroSecureBoost's error when host feature histogram is large.**Is your feature request related to a problem? Please describe.**
In FATE-v1.2 or lower version, HeteroSecureBoost will raise an error when host size's encrypted feature histogram is larger than 32M.It's easy to reproduce this if host's feature shape is large (for example, 1k). In FATE-1.3, mapPartition2 api will be use to avoid this situation,  making training data with thousands features is possible using Paillier Encryption

In FATE-1.3， only host's feature histogram generation and finding split process will be optimize. In later version, guest's feature histogram, histogram accumulating process, host's split infos exchaning and federated finding processor  should also be optimize too.",1,2020-02-20 07:45:36,2020-03-06 12:25:55,2020-03-06 12:25:55
https://github.com/FederatedAI/FATE/issues/985,[],error when running quick_run.py,"error when running quick_run.py**Describe the bug**
Hi, I am new to FATE, and I'm trying to follow Guide. I install FATE in Host successfully, but when I execute `python standalone-fate-master-1.2.0/examples/federatedml-1.x-examples/quick_run.py`, I first get some output:
> Upload data config json: {'file': 'examples/data/default_credit_homo_guest.csv', 'head': 1, 'partition': 10, 'work_mode': 0, 'table_name': 'default_credit_homo_guest', 'namespace': 'default_credit_homo_guest_guest'}

But then I get an error:
> stdout:Traceback (most recent call last):
  File ""/home/rong/standalone-fate-master-1.2.0/examples/federatedml-1.x-examples/../../fate_flow/fate_flow_client.py"", line 27, in <module>
    from arch.api.utils import file_utils
ModuleNotFoundError: No module named 'arch.api'

But I had install arch model and it's version is 4.13, I don't know what happen. Please help~~

**To Reproduce**
Steps to reproduce the behavior:
1. Go to 'standalone-fate-master-1.2.0/examples/federatedml-1.x-examples/'
2. Click on 'python quick.py'
3. See error

**Expected behavior**
I want to have some suggestion!

**Desktop (please complete the following information):**
 - OS: Linux [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
 - Version python3.6.0

please check your pythonpath> please check your pythonpath

I had doublecheck my pythonpath. 
I find that I can import arch in python environment, but I **cannot** from arch.api.utils import file_utils, it also error:
`No module named 'arch.api'`

I wonder if I installed wrong version of model arch? I use `pip install arch` to install model arch, is that correct?I think the **arch model** I had installed is : https://github.com/bashtage/arch
is it right?@Mantj  execute ""echo $PYTHONPATH"" , and check if this includes the parent directory of arch(where you put your code of FATE projects"". Hope this will be helpful.@mgqa34 it's helpful, thanks a lot!",5,2020-02-14 09:45:32,2020-02-19 09:59:12,2020-02-19 09:59:12
https://github.com/FederatedAI/FATE/issues/980,['bug'],heter_logic_regression failed when use spark backend,"heter_logic_regression failed when use spark backend**Describe the bug**
There are two bugs in this situation:
1. an error occurred in dataio stage, and error log is as follows
```
20/02/11 16:24:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
process 12465 thread 140369892562752 run __init__ init table name:__gc_e9eeb622-4ca7-11ea-b80d-525400cb55fa, namespace:e9eeb622-4ca7-11ea-b80d-525400cb55fa
created table: storage_type: LMDB, namespace: e9eeb622-4ca7-11ea-b80d-525400cb55fa, name: __gc_e9eeb622-4ca7-11ea-b80d-525400cb55fa, partitions: 1, in_place_computing: False
Traceback (most recent call last):
  File ""/data/projects/fate/python/fate_flow/driver/task_executor.py"", line 79, in run_task
    session.init(job_id='{}_{}_{}'.format(task_id, role, party_id), mode=RuntimeConfig.WORK_MODE, backend=RuntimeConfig.BACKEND)
  File ""/data/projects/fate/python/arch/api/session.py"", line 50, in init
    session = build_session(job_id=job_id, work_mode=mode, backend=backend)
  File ""/data/projects/fate/python/arch/api/table/session.py"", line 38, in build_session
    session = session_impl.FateSessionImpl(eggroll_session, work_mode)
  File ""/data/projects/fate/python/arch/api/table/pyspark/session_impl.py"", line 38, in __init__
    eggroll_util.broadcast_eggroll_session(self._sc, work_mode, eggroll_session)
  File ""/data/projects/fate/python/arch/api/table/eggroll_util.py"", line 50, in broadcast_eggroll_session
    pickled_client = pickle.dumps((work_mode.value, eggroll_session)).hex()
  File ""stringsource"", line 2, in grpc._cython.cygrpc.Channel.__reduce_cython__
TypeError: no default __reduce__ due to non-trivial __cinit__
Traceback (most recent call last):
  File ""/data/projects/fate/python/fate_flow/driver/task_executor.py"", line 132, in run_task
    session.stop()
  File ""/data/projects/fate/python/arch/api/session.py"", line 179, in stop
    RuntimeInstance.SESSION.stop()
AttributeError: 'NoneType' object has no attribute 'stop'
finish 202002111624222006152 dataio_0 202002111624222006152_dataio_0 host 10000 failed task
```
2. job status in fateboard was remaining running after the error occurred in both guest and host.

**job_parameters config**
```
""job_parameters"": {
    ""work_mode"": 1,
    ""backend"": 1,
    ""spark_submit_config"": {
        ""driver-memory"": ""1g"",
        ""num-executors"": 10,
        ""executor-memory"": ""1g"",
        ""executor-cores"": 1
    }
  }
```After we update version to Release 1.2.0, the error changed, details as follows:
```
1
""2020-02-25 22:47:38,212 - task_executor.py[line:132] - ERROR: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
2
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 21, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
3
File ""/usr/local/service/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 253, in main
4
process()
5
File ""/usr/local/service/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 248, in process
6
serializer.dump_stream(func(split_index, iterator), outfile)
7
File ""/data/projects/fate/python/arch/api/table/pyspark/rdd_func.py"", line 30, in _func
8
eggroll_util.maybe_create_eggroll_client()
9
File ""/data/projects/fate/python/arch/api/table/eggroll_util.py"", line 63, in maybe_create_eggroll_client
10
mode, eggroll_session = pickle.loads(bytes.fromhex(TaskContext.get().getLocalProperty(_EGGROLL_CLIENT)))
11
AttributeError: 'TaskContext' object has no attribute 'getLocalProperty'
12
13
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)
14
at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)
15
at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)
16
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)
17
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
18
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
19
at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
20
at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
21
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
22
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
23
at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
24
at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
25
at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
26
at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
27
at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
28
at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
29
at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)
30
at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)
31
at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
32
at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
33
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
34
at org.apache.spark.scheduler.Task.run(Task.scala:109)
35
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
36
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
37
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
38
at java.lang.Thread.run(Thread.java:748)
39
40
Driver stacktrace:
41
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
42
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
43
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
44
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
45
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
46
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
47
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
48
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
49
at scala.Option.foreach(Option.scala:257)
50
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
51
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)
52
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
53
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
54
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
55
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
56
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
57
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
58
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
59
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
60
at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
61
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
62
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
63
at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
64
at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
65
at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)
66
at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
67
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
68
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
69
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
70
at java.lang.reflect.Method.invoke(Method.java:498)
71
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
72
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
73
at py4j.Gateway.invoke(Gateway.java:282)
74
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
75
at py4j.commands.CallCommand.execute(CallCommand.java:79)
76
at py4j.GatewayConnection.run(GatewayConnection.java:238)
77
at java.lang.Thread.run(Thread.java:748)
78
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
79
File ""/usr/local/service/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 253, in main
80
process()
81
File ""/usr/local/service/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 248, in process
82
serializer.dump_stream(func(split_index, iterator), outfile)
83
File ""/data/projects/fate/python/arch/api/table/pyspark/rdd_func.py"", line 30, in _func
84
eggroll_util.maybe_create_eggroll_client()
85
File ""/data/projects/fate/python/arch/api/table/eggroll_util.py"", line 63, in maybe_create_eggroll_client
86
mode, eggroll_session = pickle.loads(bytes.fromhex(TaskContext.get().getLocalProperty(_EGGROLL_CLIENT)))
87
AttributeError: 'TaskContext' object has no attribute 'getLocalProperty'
88
89
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)
90
at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)
91
at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)
92
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)
93
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
94
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
95
at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
96
at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
97
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
98
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
99
at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
100
at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
101
at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
102
at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
103
at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
104
at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
105
at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)
106
at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)
107
at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
108
at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
109
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
110
at org.apache.spark.scheduler.Task.run(Task.scala:109)
111
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
112
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
113
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
114
... 1 more
115
""
116
Traceback (most recent call last):
117
File ""/data/projects/fate/python/fate_flow/driver/task_executor.py"", line 125, in run_task
118
tracker.save_output_data_table(output_data, task_output_dsl.get('data')[0] if task_output_dsl.get('data') else 'component')
119
File ""/data/projects/fate/python/fate_flow/manager/tracking.py"", line 179, in save_output_data_table
120
name='{}_persistent'.format(data_table._name))
121
File ""/data/projects/fate/python/arch/api/utils/profile_util.py"", line 31, in _fn
122
rtn = func(*args, **kwargs)
123
File ""/data/projects/fate/python/arch/api/table/pyspark/table_impl.py"", line 287, in save_as
124
return _save_as_func(self._rdd, name=name, namespace=namespace, partition=partition, persistent=persistent)
125
File ""/data/projects/fate/python/arch/api/table/pyspark/rdd_func.py"", line 34, in _save_as_func
126
rdd.mapPartitionsWithIndex(_func, preservesPartitioning=False).collect()
127
File ""/usr/local/service/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 814, in collect
128
sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
129
File ""/usr/local/service/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
130
answer, self.gateway_client, self.target_id, self.name)
131
File ""/usr/local/service/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
132
format(target_id, ""."", name), value)
133
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
134
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 21, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
135
File ""/usr/local/service/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 253, in main
136
process()
137
File ""/usr/local/service/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 248, in process
138
serializer.dump_stream(func(split_index, iterator), outfile)
139
File ""/data/projects/fate/python/arch/api/table/pyspark/rdd_func.py"", line 30, in _func
140
eggroll_util.maybe_create_eggroll_client()
141
File ""/data/projects/fate/python/arch/api/table/eggroll_util.py"", line 63, in maybe_create_eggroll_client
142
mode, eggroll_session = pickle.loads(bytes.fromhex(TaskContext.get().getLocalProperty(_EGGROLL_CLIENT)))
143
AttributeError: 'TaskContext' object has no attribute 'getLocalProperty'
144
145
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)
146
at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)
147
at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)
148
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)
149
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
150
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
151
at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
152
at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
153
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
154
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
155
at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
156
at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
157
at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
158
at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
159
at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
160
at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
161
at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)
162
at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)
163
at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
164
at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
165
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
166
at org.apache.spark.scheduler.Task.run(Task.scala:109)
167
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
168
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
169
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
170
at java.lang.Thread.run(Thread.java:748)
171
172
Driver stacktrace:
173
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
174
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
175
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
176
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
177
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
178
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
179
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
180
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
181
at scala.Option.foreach(Option.scala:257)
182
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
183
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)
184
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
185
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
186
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
187
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
188
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
189
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
190
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
191
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
192
at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
193
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
194
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
195
at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
196
at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
197
at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)
198
at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
199
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
200
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
201
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
202
at java.lang.reflect.Method.invoke(Method.java:498)
203
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
204
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
205
at py4j.Gateway.invoke(Gateway.java:282)
206
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
207
at py4j.commands.CallCommand.execute(CallCommand.java:79)
208
at py4j.GatewayConnection.run(GatewayConnection.java:238)
209
at java.lang.Thread.run(Thread.java:748)
210
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
211
File ""/usr/local/service/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 253, in main
212
process()
213
File ""/usr/local/service/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 248, in process
214
serializer.dump_stream(func(split_index, iterator), outfile)
215
File ""/data/projects/fate/python/arch/api/table/pyspark/rdd_func.py"", line 30, in _func
216
eggroll_util.maybe_create_eggroll_client()
217
File ""/data/projects/fate/python/arch/api/table/eggroll_util.py"", line 63, in maybe_create_eggroll_client
218
mode, eggroll_session = pickle.loads(bytes.fromhex(TaskContext.get().getLocalProperty(_EGGROLL_CLIENT)))
219
AttributeError: 'TaskContext' object has no attribute 'getLocalProperty'
220
221
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)
222
at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)
223
at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)
224
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)
225
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
226
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
227
at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
228
at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
229
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
230
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
231
at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
232
at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
233
at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
234
at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
235
at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
236
at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
237
at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)
238
at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)
239
at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
240
at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
241
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
242
at org.apache.spark.scheduler.Task.run(Task.scala:109)
243
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
244
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
245
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
246
... 1 more
```This msg: indicate that you are using spark version under 2.4: ""AttributeError: 'TaskContext' object has no attribute 'getLocalProperty'"" , which is not supported or testedI'm closing issue now, please reopen this if issue not resolved",3,2020-02-11 09:05:52,2020-05-11 14:12:31,2020-05-11 14:12:30
https://github.com/FederatedAI/FATE/issues/938,[],SPARK_HOME not found,"SPARK_HOME not foundHi, I deployed FATE 1.2 using docker deploy. When I test the spark backend it raise an error:

Traceback (most recent call last):
  File ""/data/projects/fate/python/fate_flow/driver/task_scheduler.py"", line 318, in start_task
    raise EnvironmentError(""SPARK_HOME not found"")
OSError: SPARK_HOME not found

Dose it mean that I should deploy spark+hadoop manually to use the spark backend?
Thanks!
If Yes, should I deploy spark+hadoop in the containers of ""federatedai/python:1.2.0-release"" in both guest and host devices, or other containers?@linjieccc you should deploy spark manually, both for guest and  and host devices. no further response, close this issue",3,2020-01-14 03:18:00,2020-04-20 07:03:41,2020-04-20 07:03:41
https://github.com/FederatedAI/FATE/issues/928,[],[logistic_regression_param] Make it More Elegant,"[logistic_regression_param] Make it More Elegant**Is your feature request related to a problem? Please describe.**
Just a subtle engineering problem.
Have a look at [Line 111](https://github.com/FederatedAI/FATE/blob/master/federatedml/param/logistic_regression_param.py#L111) of the file `federatedml/param/logistic_regression_param`. You have defined a prefix, however, you have not used it at most of the rest of the code. Instead, you would rather code ""logistic_param's ..."" in the exception raise clause. Like [Line 117](https://github.com/FederatedAI/FATE/blob/master/federatedml/param/logistic_regression_param.py#L117) of the same file, and so as most of them.


**Describe the solution you'd like**
Clearly just substitude those `""logistic_param's` with `descr + ""`. If you like, also do the same thing for the class `HomoLogisticParam` which follows the definition of `LogisticParam`, though the appearances of `logistic_param's` are not too many.

**Additional context**
Thanks for the attention.
@SamuelGong OK, good suggestion, thanks.",1,2020-01-09 01:17:13,2021-11-18 09:27:51,2021-11-18 09:27:51
https://github.com/FederatedAI/FATE/issues/927,[],can't run job in 1.2,"can't run job in 1.2First, i have no issue to run a job on a cluster mode for 1.1 . For 1.2, I able to run a job on a single machine  (guest and host on the same machine). However, when host and guest are on different machines, i get below error in egg log.

```

 [ERROR] 2020-01-08T07:39:32,907 [grpcServiceExecutor-9] [GrpcServerWrapper:54] - java.lang.IllegalStateException: sessionId 202001080739241686922_dataio_0_guest_10000 does not exist
	at com.webank.ai.eggroll.framework.egg.manager.EggSessionManager.getComputeEngine(EggSessionManager.java:163)
	at com.webank.ai.eggroll.framework.egg.api.grpc.server.EggSessionServiceImpl.lambda$getComputingEngine$2(EggSessionServiceImpl.java:72)
	at com.webank.ai.eggroll.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:52)
	at com.webank.ai.eggroll.framework.egg.api.grpc.server.EggSessionServiceImpl.getComputingEngine(EggSessionServiceImpl.java:70)
	at com.webank.ai.eggroll.api.framework.egg.SessionServiceGrpc$MethodHandlers.invoke(SessionServiceGrpc.java:357)
	at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
	at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)



	at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```
additional info: 202001080739241686922_dataio_0_guest_10000 folder exist in /data/projects/fate/data-dir/lmdb/ 


Also, i notice the route table is different from 1.1 . On 1.2 , another machine info is inside the route table. On 1.1, the route table shows only info about the exchange serveri able to get it running, after making sure egg is running on port 7778",1,2020-01-08 08:33:48,2020-01-10 06:40:53,2020-01-10 06:40:53
https://github.com/FederatedAI/FATE/issues/925,[],Again. Can't predict,"Again. Can't predicthetero_lr_task.
guest_tarin_data: breast_b((id:0-499) total:500) host_train_data:breast_a((id:0-499) total:500)
guest_eval_data: breast_b((id:500-568) total:69) host_eval_data:breast_a((id:500-568) total:69)
My Step:

upload_guest_train_data
upload_host_train_data
start modeling (I add the 'need_deploy'=[true] to the xxx_dsl.json)
upload_guest_eval_data
upload_host_eval_data
predict
step 1-5 is success in fate_board.
I checked the the error log of 'step6.predict' and found there is an error about ""input data's schema for fit and transform should be same"".
Can anybody help me to solve this problem?
Here are data and config files. 链接: https://pan.baidu.com/s/138eq7yhkonFbBWs2U_DwrQ 提取码: 413h@JustcallmeTuner 
After split the data, have you provided headers which is same between train and eval data for all data files? That might be a reason for that.",1,2020-01-06 14:39:17,2021-11-18 09:28:48,2021-11-18 09:28:48
https://github.com/FederatedAI/FATE/issues/897,[],"[hetero Poisson regression] The ""aggregate_forward"" is suspected to be mistakenly computed","[hetero Poisson regression] The ""aggregate_forward"" is suspected to be mistakenly computed**Describe the bug and Expected Behavior**
Have a look at [Line 57](https://github.com/FederatedAI/FATE/blob/master/federatedml/optim/gradient/hetero_poisson_gradient_and_loss.py#L57) of File [hetero_poisson_gradient_and_loss](https://github.com/FederatedAI/FATE/blob/master/federatedml/optim/gradient/hetero_poisson_gradient_and_loss.py). This is where the Guest compute the ""aggregated forward"".

To point out a potential bug, let's follow the annotations and process depicted in your illustration, i.e., File [HeteroPoisson.png](https://github.com/FederatedAI/FATE/blob/master/federatedml/linear_model/poisson_regression/images/HeteroPoisson.png).

As we can see, in Line 57 of the code file, `self.host_forwards[0]` contains `[[ exp(W^H X^H) ]]` of each sample, while `self.forwards`t consists of `exp(W^G X^G)` of each sample. To my knowledge, you have already overridden `+` operation for the encrypted elements in `self.host_forwards[0]`. Thus, to obtain `[[ exp(W^H X^H) + exp(W^G X^G) ]]`, one should simple ""add"" `exp(W^G X^G)` to `[[ exp(W^H X^H) ]]`, i.e., I think maybe the line should be

```
self.aggregated_forwards = self.forwards.join(self.host_forwards[0], lambda g, h: g + h)
```

instead of

```
self.aggregated_forwards = self.forwards.join(self.host_forwards[0], lambda g, h: g * h)
```

I think this issue is just a theoretical discussion and therefore have not found any way to reproduce this ""bug"". Hope that I have made things clear enough for you.Thank you for pointing out a mistake in the README doc. We will correct this illustration mistake in the next release. The intermediate result should be [[exp(W^H X^H + W^G X^G)]]. By product rule of exponents, the computation is thus equivalently [[exp(W^H X^H ]] * exp(W^G X^G).Solved in latest release 357af97",2,2019-12-27 07:03:41,2020-01-16 02:15:49,2020-01-16 02:08:18
https://github.com/FederatedAI/FATE/issues/887,[],Can't execute the predict task,"Can't execute the predict taskhetero_lr_task.
guest_tarin_data: breast_b((id:0-499) total:500) host_train_data:breast_a((id:0-499) total:500)
guest_eval_data: breast_b((id:500-568) total:69) host_eval_data:breast_a((id:500-568) total:69)
My Step:

upload_guest_train_data
upload_host_train_data
start modeling (I add the 'need_deploy'=[true] to the xxx_dsl.json)
upload_guest_eval_data
upload_host_eval_data
predict
step 1-5 is success in fate_board.
I checked the the error log of 'step6.predict' and found there is an error about ""input data's schema for fit and transform should be same"".
Can anybody help me to solve this problem?
Here are data and config files. 链接: https://pan.baidu.com/s/138eq7yhkonFbBWs2U_DwrQ 提取码: 413hSame with #925",1,2019-12-25 06:27:07,2020-02-13 10:09:31,2020-02-13 10:09:30
https://github.com/FederatedAI/FATE/issues/872,[],Onehot moduel error when set need_run is false,"Onehot moduel error when set need_run is false**Describe the bug**
When need_run parameter is set as false in onehot module, the export model function cannot access empty model meta. Therefore, it occurs error in this case. 
Already fix in v1.1.2",1,2019-12-23 03:41:33,2020-02-17 04:05:16,2020-02-17 04:05:16
https://github.com/FederatedAI/FATE/issues/869,[],"[hetero linear regression] The host sends its ""forwards"" and ""loss_regular"" directly without encryption","[hetero linear regression] The host sends its ""forwards"" and ""loss_regular"" directly without encryption**Describe the bug**
Have a look at [Line 131](https://github.com/FederatedAI/FATE/blob/efae2b1add20d9f98ac05a669298e36369f91497/federatedml/optim/gradient/hetero_linr_gradient_and_loss.py#L131) of File [hetero_linr_gradient_and_loss](https://github.com/FederatedAI/FATE/blob/master/federatedml/optim/gradient/hetero_linr_gradient_and_loss.py). This is where the Host compute part of the loss (in your word, loss intermediate). Note that **""forwards"" is plain text**, and hence the result ""self_wx_square"" is also plain text. As a result, the guest will know the intermediate loss of the host.

Similar also happen at [Line 134](https://github.com/FederatedAI/FATE/blob/efae2b1add20d9f98ac05a669298e36369f91497/federatedml/optim/gradient/hetero_linr_gradient_and_loss.py#L134) of the same file. Note that **""loss_regular"" is plain text as well**.

**To Reproduce**
One can simply add a line following [Line 131](https://github.com/FederatedAI/FATE/blob/efae2b1add20d9f98ac05a669298e36369f91497/federatedml/optim/gradient/hetero_linr_gradient_and_loss.py#L131) to print the type of the variable `self_wx_square`. According to my test, it is `numpy.float64`. The same thing happens with the variable `loss_regular` when I added a line following [Line 134](https://github.com/FederatedAI/FATE/blob/efae2b1add20d9f98ac05a669298e36369f91497/federatedml/optim/gradient/hetero_linr_gradient_and_loss.py#L134) to check its type.

**Expected behavior**
We should **encrypt ""self_wx_square"" and ""loss_regular"" before sending it to the guest**. In other words, the type of `self_wx_square` and `loss_regular` are both expected to be `Federatedml.secureprotol.fate_paillier.PaillierEncryptedNumber` (or something like that depending on the encryption method we use).

**Additional context**
You may not notice the problem before, as these bugs **do not harm the precision of the loss**. This is because the intermediate loss of the host, though not encrypted at the host, will be encrypted at the guest before producing the final encrypted loss.
@SamuelGong Thanks for you feedback. We have checked this bug report and find that the ""self_wx_square"" and ""loss_regular"" are not encrypted indeed. These two values are two statistical intermediate variables which leak few data information. 

However, it still makes more sense to encrypt them before transmission. We will fix this problem in next version. Thank you again for pointing out this problem for us. Fixed in FATE-1.2 357af97",2,2019-12-19 13:07:09,2020-02-17 03:49:22,2020-02-17 03:49:21
https://github.com/FederatedAI/FATE/issues/852,[], ERROR: <_Rendezvous of RPC that terminated with: status = StatusCode.INTERNAL,"ERROR: <_Rendezvous of RPC that terminated with: status = StatusCode.INTERNALI am using the cluster version FATE on the Centos 7 OS. I have already successfully passed the toy_example test and min test in both the fast and normal mode.
 When I was trying to use the quick_run.py to run the secureboost example, I successfully completed the train task but failed in the predict task.  
There was no error.log in the eggroll logs but an error.log in logs/fate_flow.  You can see the ERROR.log and the fate_flow ERROR.log in the attachments.
Actually, **I have seen the same error message** when I run the toy_example test and min test. And it seems that once this error occurs, whatever test I run the system will report the same error. 
I have checked the netstat and the status of the service and there reports no error.  Also, I have restarted the service several times but it doesn't help.
I wonder whether anyone can help me locate the problem or solve this problem, thank you so much.

The ERROR message are as follows:
ERROR.log
[ERROR.log](https://github.com/FederatedAI/FATE/files/3965842/ERROR.log)

fate_flow  ERROR.log
[fate_flow_ERROR.log](https://github.com/FederatedAI/FATE/files/3965843/fate_flow_ERROR.log)

The netstat on both sides are as follows:
![netstat](https://user-images.githubusercontent.com/48935149/70875580-5ad96f00-1ff1-11ea-88ad-368b68b4600a.png)

I have met the same problem, could anyone give us a hand?I have met the same problem using docker deployment.
Version: FATE 1.1.1 Same too. It cost me almost two days to handle it . All container needed is UP. Still got:

stdout:{
    ""data"": {
        ""board_url"": ""http://fateboard:8080/index.html#/dashboard?job_id=201912190250066120611&role=guest&party_id=10000"",
        ""job_dsl_path"": ""/data/projects/fate/python/jobs/201912190250066120611/job_dsl.json"",
        ""job_runtime_conf_path"": ""/data/projects/fate/python/jobs/201912190250066120611/job_runtime_conf.json"",
        ""logs_directory"": ""/data/projects/fate/python/logs/201912190250066120611"",
        ""model_info"": {
            ""model_id"": ""guest-10000#host-9999#model"",
            ""model_version"": ""201912190250066120611""
        }
    },
    ""jobId"": ""201912190250066120611"",
    ""retcode"": 0,
    ""retmsg"": ""success""
}


job status is running
job status is running
""2019-12-19 02:50:13,175 - task_executor.py[line:127] - ERROR: <_Rendezvous of RPC that terminated with:
status = StatusCode.INTERNAL
details = ""172.22.0.8:8011: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
at com.webank.ai.eggroll.framework.roll.api.grpc.client.StorageServiceClient.get(StorageServiceClient.java:223)
at com.webank.ai.eggroll.framework.roll.api.grpc.server.RollKvServiceImpl.lambda$get$5(RollKvServiceImpl.java:240)
at com.webank.ai.eggroll.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:52)
at com.webank.ai.eggroll.framework.roll.api.grpc.server.RollKvServiceImpl.get(RollKvServiceImpl.java:235)
at com.webank.ai.eggroll.api.storage.KVServiceGrpc$MethodHandlers.invoke(KVServiceGrpc.java:959)
at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
at io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
at io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
at io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)
at io.grpc.Contexts$ContextualizedServerCallListener.onHalfClose(Contexts.java:86)
at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
at com.webank.ai.eggroll.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:154)
at com.webank.ai.eggroll.framework.roll.api.grpc.client.StorageServiceClient.get(StorageServiceClient.java:219)
... 16 more
Caused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
at io.grpc.Status.asRuntimeException(Status.java:526)
at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
... 5 more
Caused by: io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: egg/172.22.0.6:7778
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
at io.grpc.netty.shaded.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
at io.grpc.netty.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
... 1 more
Caused by: java.net.ConnectException: Connection refused
... 11 more
""


> I am using the cluster version FATE on the Centos 7 OS. I have already successfully passed the toy_example test and min test in both the fast and normal mode.
> When I was trying to use the quick_run.py to run the secureboost example, I successfully completed the train task but failed in the predict task.
> There was no error.log in the eggroll logs but an error.log in logs/fate_flow. You can see the ERROR.log and the fate_flow ERROR.log in the attachments.
> Actually, **I have seen the same error message** when I run the toy_example test and min test. And it seems that once this error occurs, whatever test I run the system will report the same error.
> I have checked the netstat and the status of the service and there reports no error. Also, I have restarted the service several times but it doesn't help.
> I wonder whether anyone can help me locate the problem or solve this problem, thank you so much.
> 
> The ERROR message are as follows:
> ERROR.log
> [ERROR.log](https://github.com/FederatedAI/FATE/files/3965842/ERROR.log)
> 
> fate_flow ERROR.log
> [fate_flow_ERROR.log](https://github.com/FederatedAI/FATE/files/3965843/fate_flow_ERROR.log)
> 
> The netstat on both sides are as follows:
> ![netstat](https://user-images.githubusercontent.com/48935149/70875580-5ad96f00-1ff1-11ea-88ad-368b68b4600a.png)

![image](https://user-images.githubusercontent.com/9403585/71142896-b86cf600-2253-11ea-81f0-db3ebfa99d2c.png)
I checked the logs and found msg ""connection refused"". The IP: ""egg/172.22.0.6"" is not a pure IP, could you please have a check if it is attachable？> I am using the cluster version FATE on the Centos 7 OS. I have already successfully passed the toy_example test and min test in both the fast and normal mode.
> When I was trying to use the quick_run.py to run the secureboost example, I successfully completed the train task but failed in the predict task.
> There was no error.log in the eggroll logs but an error.log in logs/fate_flow. You can see the ERROR.log and the fate_flow ERROR.log in the attachments.
> Actually, I have seen the same error message when I run the toy_example test and min test. And it seems that once this error occurs, whatever test I run the system will report the same error.
> I have checked the netstat and the status of the service and there reports no error. Also, I have restarted the service several times but it doesn't help.
> I wonder whether anyone can help me locate the problem or solve this problem, thank you so much.
> The ERROR message are as follows:
> ERROR.log
> ERROR.log
> fate_flow ERROR.log
> fate_flow_ERROR.log
> The netstat on both sides are as follows:
> 
> 
> 
> I checked the logs and found msg ""connection refused"". The IP: ""egg/172.22.0.6"" is not a pure IP, could you please have a check if it is attachable？

I have checked the status of egg using commad ""sh service.sh egg status"" and it was on.  Then I ping 172.22.0.6 but there was just no response, not showing timeout or anything .  I wonder if this means it is not attachable or what should I do next?> > I am using the cluster version FATE on the Centos 7 OS. I have already successfully passed the toy_example test and min test in both the fast and normal mode.
> > When I was trying to use the quick_run.py to run the secureboost example, I successfully completed the train task but failed in the predict task.
> > There was no error.log in the eggroll logs but an error.log in logs/fate_flow. You can see the ERROR.log and the fate_flow ERROR.log in the attachments.
> > Actually, I have seen the same error message when I run the toy_example test and min test. And it seems that once this error occurs, whatever test I run the system will report the same error.
> > I have checked the netstat and the status of the service and there reports no error. Also, I have restarted the service several times but it doesn't help.
> > I wonder whether anyone can help me locate the problem or solve this problem, thank you so much.
> > The ERROR message are as follows:
> > ERROR.log
> > ERROR.log
> > fate_flow ERROR.log
> > fate_flow_ERROR.log
> > The netstat on both sides are as follows:
> > I checked the logs and found msg ""connection refused"". The IP: ""egg/172.22.0.6"" is not a pure IP, could you please have a check if it is attachable？
> 
> I have checked the status of egg using commad ""sh service.sh egg status"" and it was on. Then I ping 172.22.0.6 but there was just no response, not showing timeout or anything . I wonder if this means it is not attachable or what should I do next?
Reply sorry late. Maybe you should ping egg/172.22.0.6, not only 172.22.0.6. Because if you are using kubeFATE, IP addr may as follow: egg/172.22.0.6.
> I am using the cluster version FATE on the Centos 7 OS. I have already successfully passed the toy_example test and min test in both the fast and normal mode.
> When I was trying to use the quick_run.py to run the secureboost example, I successfully completed the train task but failed in the predict task.
> There was no error.log in the eggroll logs but an error.log in logs/fate_flow. You can see the ERROR.log and the fate_flow ERROR.log in the attachments.
> Actually, I have seen the same error message when I run the toy_example test and min test. And it seems that once this error occurs, whatever test I run the system will report the same error.
> I have checked the netstat and the status of the service and there reports no error. Also, I have restarted the service several times but it doesn't help.
> I wonder whether anyone can help me locate the problem or solve this problem, thank you so much.
> The ERROR message are as follows:
> ERROR.log
> ERROR.log
> fate_flow ERROR.log
> fate_flow_ERROR.log
> The netstat on both sides are as follows:
> I checked the logs and found msg ""connection refused"". The IP: ""egg/172.22.0.6"" is not a pure IP, could you please have a check if it is attachable？
> 
> I have checked the status of egg using commad ""sh service.sh egg status"" and it was on. Then I ping 172.22.0.6 but there was just no response, not showing timeout or anything . I wonder if this means it is not attachable or what should I do next?
> Reply sorry late. Maybe you should ping egg/172.22.0.6, not only 172.22.0.6. Because if you are using kubeFATE, IP addr may as follow: egg/172.22.0.6.

I have tired the command ping egg/172.22.0.6 and I get the following information:
ping: egg/172.22.0.6: Unknown name or service.  Also, I want to point out that I am using the cluster deployments by Native installation , not the KubeFATE installation. Looking forward to your response !> I am using the cluster version FATE on the Centos 7 OS. I have already successfully passed the toy_example test and min test in both the fast and normal mode.
> When I was trying to use the quick_run.py to run the secureboost example, I successfully completed the train task but failed in the predict task.
> There was no error.log in the eggroll logs but an error.log in logs/fate_flow. You can see the ERROR.log and the fate_flow ERROR.log in the attachments.
> Actually, I have seen the same error message when I run the toy_example test and min test. And it seems that once this error occurs, whatever test I run the system will report the same error.
> I have checked the netstat and the status of the service and there reports no error. Also, I have restarted the service several times but it doesn't help.
> I wonder whether anyone can help me locate the problem or solve this problem, thank you so much.
> The ERROR message are as follows:
> ERROR.log
> ERROR.log
> fate_flow ERROR.log
> fate_flow_ERROR.log
> The netstat on both sides are as follows:
> I checked the logs and found msg ""connection refused"". The IP: ""egg/172.22.0.6"" is not a pure IP, could you please have a check if it is attachable？
> 
> I have checked the status of egg using commad ""sh service.sh egg status"" and it was on. Then I ping 172.22.0.6 but there was just no response, not showing timeout or anything . I wonder if this means it is not attachable or what should I do next?
> Reply sorry late. Maybe you should ping egg/172.22.0.6, not only 172.22.0.6. Because if you are using kubeFATE, IP addr may as follow: egg/172.22.0.6.

I have checked my logs that I attached at the beginning and I am sure there is no message like: connection refused: egg/172.22.0.6 . Maybe you have seen it in other people's comments not the logs I attached. Could you please check the logs again and help me locate this problem?I have got the same problem and solved the problem，I updated  egg.properties(filePath: /data/projects/fate/eggroll/egg/conf/egg.properties),transfer edeggroll.computing.processor.session.max.count=16 to eggroll.computing.processor.session.max.count=2.  restarted  all the services, in the end, it worked!!!> I have got the same problem and solved the problem，I updated  egg.properties(filePath: /data/projects/fate/eggroll/egg/conf/egg.properties),transfer edeggroll.computing.processor.session.max.count=16 to eggroll.computing.processor.session.max.count=2.  restarted  all the services, in the end, it worked!!!

I have also changed the edeggroll.computing.processor.session.max.count to the number of cpus of my computer  and it also works.  I think this is the right solution. Thanks for your reply!@RicLee0124 
Could you explain why this configuration `edeggroll.computing.processor.session.max.count` should be changed to 2 ? Is this a bug or something ?I met the same issue. And I find egg.properties in path /date/projects/confs-10000/confs/egg/conf ,But the  edeggroll.computing.processor.session.max.count  is already set 2 . Anybody have another  solution?I met the same issue, too. My FATE version is 1.3.0. I modified the edeggroll.computing.processor.session.max.count to my CPU number, which is 4 under the folder /date/projects/confs-10000/confs/egg/conf as @novaxiaohui suggested, but the ""connection refused"" error still exists. > I have got the same problem and solved the problem，I updated egg.properties(filePath: /data/projects/fate/eggroll/egg/conf/egg.properties),transfer edeggroll.computing.processor.session.max.count=16 to eggroll.computing.processor.session.max.count=2. restarted all the services, in the end, it worked!!!

what fate version? my fate is 1.4.4 and i not find eggroll.computing.processor.session.max.count",14,2019-12-16 02:45:07,2021-12-13 04:08:34,2021-12-13 04:08:34
https://github.com/FederatedAI/FATE/issues/843,[],Can't start modeling task.,"Can't start modeling task.1.I have uploaded data (Firstly  python ../fate_flow_client.py -f upload -c upload_data.json(guest) ;Then python ../fate_flow_client.py -f upload -c upload_data.json(host) ).  And the status is success in fate_board.
2. I started my modeling task, but failed.(python ../fate_flow_client.py -f Submit_job -d hetero_logistic_regression / test_hetero_lr_train_job_dsl.json -c hetero_logistic_regression / test_hetero_lr_train_job_conf.json) The retmsg is success in terminal, however the status is failed in fate_board. 
I checked the guest task's log, ERROR: Count of data_instance is 0"" 
In addition, when I run the ""python quick_run.py ""  the status is success in fate_board. 
Can anyone help me ???
FATE:standalone
task:hetero_logistic_regression
ubuntu 18.04
docker version 18.06.3-ce
docker-compose version 1.24.0
If the ""count of data"" instance is 0 ""appears in the log, it means that the role side has not imported data, or there is a problem with data configuration@KathyKing Thanks very much.  I checked my data configuration and found no problem. Can you tell me the correct steps to achieve the hetero_lr task without quick_run.py?@KathyKing  I finally solved the problem. The reason is that table_name and space_name are inconsistent with the config setting. Thanks again ~",3,2019-12-12 12:57:20,2019-12-14 07:54:27,2019-12-14 07:54:27
https://github.com/FederatedAI/FATE/issues/830,[],how to limit number of job running,how to limit number of job runninghow to configure each party to execute only one job at at time and the rest wait in queue. Currently multiple jobs will be executing at the same time and leads to out of memory and egg will crash.Modify the fate_flow/settings.py: MAX_CONCURRENT_JOB_RUN = 1 and restart fate_flow。,1,2019-12-09 07:58:03,2020-01-02 08:21:50,2020-01-02 08:21:49
https://github.com/FederatedAI/FATE/issues/827,[],query for all tables,"query for all tablesCan party_id = A (fate_flow_client) query party_id= B to get a list of all table name and namespace uploaded successfully?

can this https://github.com/FederatedAI/FATE/blob/9d1d9010cbad9943a7657eed248069f0ed6160ee/fate_flow/doc/fate_flow_rest_api.md  be used to query another party's information. 

I have a scenario where as a party A, I would like to know table,namespace uploaded by party B. What is your advice?At present, there is no interface for querying the list of successfully uploaded table names and namespaces. Version 1.2 only provides an interface for querying the list of table names and namespaces uploaded by itself. For the query of other parties, it does not currently provide an interface.This is an unsafe operation which party A  can query info<name, namespace> in party B.",2,2019-12-06 08:56:13,2020-01-02 08:32:05,2020-01-02 08:32:05
https://github.com/FederatedAI/FATE/issues/824,[],homo_breast_* example evaluation,"homo_breast_* example evaluationI able to run the homo_breast_* example with config file :test_multi_host_job_conf.json,  By default ""evaluation"" is run on the guest. 

When i change the config file evaluation_0 for ""Host"" to true in test_multi_host_job_conf.json, the job will fail to run. Can help to explain on this?

Also, where can we see the output of the model after ran the job successfully?

@cometta 

In the predict task, host send wx to arbiter and arbiter make the judgement of which class a sample belongs to. Therefore, a host has no way to get the predict score of samples which means the host cannot calculate the evaluation indicators such as auc etc. That's why we make evaluation module Inoperable. 

As for the second question, if you have deploy a fate-board, the model results will be avaiable in the visual interface. However, that is not your case, we recommend you obtain the model info by fate-flow cli whose documentation is located here:  https://github.com/FederatedAI/FATE/blob/master/fate_flow/doc/fate_flow_cli.md#component_output_model

Hope that is useful for you.",1,2019-12-05 09:17:51,2019-12-06 16:17:31,2019-12-06 16:17:31
https://github.com/FederatedAI/FATE/issues/823,[],"what happen when upload different csv with the same table_name, namespace","what happen when upload different csv with the same table_name, namespace1) May I know if we re-use the same table_name and namespace when uploading a CSV files. Will the latest upload replace the old file when we train a model?
For example,
first time upload, CSV looks like.  x1,x2,y
second time upload, CSV looks like, x1,x2,x3,x4,y

2) Does the egg service has any versioning system?

3) Is it possible to integrate egg service with Azure Blobstore so that the egg service will not get out of disk space? 

4) Anyway to archieve old data (csv) in egg service to external backup system  (external hard disk) and load back into egg service when require in the future?
 1. dtable is kv based. Currently no duplicate key is allowed. If you upload data with same namespace and name, and if the keys are the same (or keys in new table are a superset on the old table), old values will be overridden; If the keys or some of the keys are not the same, old data will still in the dtable.

2. The storage service is based on LMDB. LMDB has a MVCC (Multi-Version Concurrency Control) mechanism which solves concurrent access in runtime. We suggest users maintain their application data for specific needs.

3. Maybe networking overhead is quite high? 

4. Storage system in Eggroll is file based. If a dtable is properly closed, you can directly backup the data file. But as for future use, you may need to reconstruct the metadata (i.e. a eggroll.table call).",1,2019-12-05 09:03:20,2021-12-13 03:31:27,2021-12-13 03:31:27
https://github.com/FederatedAI/FATE/issues/822,[],fateboard system error,"fateboard system error之前用的1.0.2，build_standalone_docker.sh 方式安装的，看到出了新版本1.1，所以换成了最新版。但是在跑一些大规模或者是高维数据集的时候老是failed，卡在dataio那里，错误信息大概是security pool abruptly kill when the future什么什么的，忘了保存了。除此之外还会有一个错误，就是evaluation那里又只有train的结果，一直是有这俩个错误，也没找到其他解决办法。只能换回1.0.2，现在是container都激活了，python quick_run.py也能运行，但是fateboard(127.0.0.1:8080)一直是新版的界面，提示system error，重试了好几次都一样，现在是要改什么东西吗？> 之前用的1.0.2，build_standalone_docker.sh 方式安装的，看到出了新版本1.1，所以换成了最新版。但是在跑一些大规模或者是高维数据集的时候老是failed，卡在dataio那里，错误信息大概是security pool abruptly kill when the future什么什么的，忘了保存了。除此之外还会有一个错误，就是evaluation那里又只有train的结果，一直是有这俩个错误，也没找到其他解决办法。只能换回1.0.2，现在是container都激活了，python quick_run.py也能运行，但是fateboard(127.0.0.1:8080)一直是新版的界面，提示system error，重试了好几次都一样，现在是要改什么东西吗？

你好， 请问你切换不同版本的时候，docker容器和镜像是否清理干净？ 既然fateboard还是新版的界面，说明他还是用新版的容器和镜像启动的> > 之前用的1.0.2，build_standalone_docker.sh 方式安装的，看到出了新版本1.1，所以换成了最新版。但是在跑一些大规模或者是高维数据集的时候老是failed，卡在dataio那里，错误信息大概是security pool abruptly kill when the future什么什么的，忘了保存了。除此之外还会有一个错误，就是evaluation那里又只有train的结果，一直是有这俩个错误，也没找到其他解决办法。只能换回1.0.2，现在是container都激活了，python quick_run.py也能运行，但是fateboard(127.0.0.1:8080)一直是新版的界面，提示system error，重试了好几次都一样，现在是要改什么东西吗？
> 
> 你好， 请问你切换不同版本的时候，docker容器和镜像是否清理干净？ 既然fateboard还是新版的界面，说明他还是用新版的容器和镜像启动的

是清理干净的，docker 镜像 用的rmi命令删除的，container用的docker container prune删除的。fate_fateboard没法看修改日期，fate_python确实是9月24号的更新。
![image](https://user-images.githubusercontent.com/17498125/70257672-b7ed4d80-17c5-11ea-9ab0-718121c23c12.png)
docker container logs fate_fateboard一直报这个错误。这次是又重新装了一次。

> > 之前用的1.0.2，build_standalone_docker.sh 方式安装的，看到出了新版本1.1，所以换成了最新版。但是在跑一些大规模或者是高维数据集的时候老是failed，卡在dataio那里，错误信息大概是security pool abruptly kill when the future什么什么的，忘了保存了。除此之外还会有一个错误，就是evaluation那里又只有train的结果，一直是有这俩个错误，也没找到其他解决办法。只能换回1.0.2，现在是container都激活了，python quick_run.py也能运行，但是fateboard(127.0.0.1:8080)一直是新版的界面，提示system error，重试了好几次都一样，现在是要改什么东西吗？
> 
> 你好， 请问你切换不同版本的时候，docker容器和镜像是否清理干净？ 既然fateboard还是新版的界面，说明他还是用新版的容器和镜像启动的

feel free to reopen if issue still persist",4,2019-12-04 16:58:04,2021-12-17 03:33:18,2021-12-17 03:33:18
https://github.com/FederatedAI/FATE/issues/805,[],SBT CV模型运行中一方early stop成功 另一方继续执行 导致任务卡死,"SBT CV模型运行中一方early stop成功 另一方继续执行 导致任务卡死**Describe the bug**
背景：该bug理论上仅会出现在cv模式第2折及之后的训练/预测过程中。进行CV SBT建模时可能会出现任务卡死但是不报错，直到48小时超时的问题。



**To Reproduce**
假设： GUEST方在fold=2， round=8时达到early stop的条件，主动发起early stop
正常情况：HOST方根据生成规则生成本轮训练中获取stop_flag的key来获取获取到这个stop_flag 然后也中断后续执行过程。

问题复现：然而由于双方环境独立且不一致以及网络延迟，host方运行到get_stop_flag这一步骤时，guest发出的这个消息可能还到达，但是由于stop_flag的key的生成规则不包含折数这一变量，所以每一折中每轮训练的stop_flag的key完全一样，同时每折训练完成后中间变量应该没有被删除，这样一来host获取到的其实是前一折中guest发送的stop_flag, 导致两边状态不同步，出现任务卡死的问题。


If use cv，cv module will reset flowid every fold (like train.0\train.1)，so the tag will be new for every fold as its prefix is flowid. If you have more questions, please post below, thanks very much.> If use cv，cv module will reset flowid every fold (like train.0\train.1)，so the tag will be new for every fold as its prefix is flowid. If you have more questions, please post below, thanks very much.

Thank you for the reply. But I reviewed the relevant code and found that flowid wasn't used to generate tag, so every flow stayed the same.
![image](https://user-images.githubusercontent.com/52624066/69790099-cdc6a580-11fc-11ea-9006-6fc457bc125f.png)

There is a cv log as below, and we can see that the tag of stop_flag  remains same in each round of  
 every flow.
![image](https://user-images.githubusercontent.com/52624066/69790009-98ba5300-11fc-11ea-988b-fc330e44fd6b.png)
![image](https://user-images.githubusercontent.com/52624066/69790040-a7a10580-11fc-11ea-9046-a5fdbab5a898.png)
If I make mistakes in understanding and expressing， please point it out. Thanks again.
transfer_variable.remote api source code :
https://github.com/FederatedAI/FATE/blob/master/federatedml/transfer_variable/transfer_class/base_transfer_variable.py#L31

> tag=self._transfer_variable.generate_transferid(self, *suffix)

https://github.com/FederatedAI/FATE/blob/master/federatedml/transfer_variable/transfer_class/base_transfer_variable.py#L87
> transferid = transfer_var.name + ""."" + str(self.flowid)
   if suffix:
       transferid += ""."" + ""."".join(map(str, suffix))

We can see that flowid is used. 

We will check what happens in images show in your reply, thanks for your feedback@sookieqin We check the code and find out that because  secureboost's transfer_variable' name is 'transfer_inst' instead of 'transfer_variable', whose flowid  remain's zero forever, as it will not be change by modelbase' set_flowid method, and it will only influence stop_tag transfer.  
https://github.com/FederatedAI/FATE/blob/master/federatedml/model_base.py#L193

Thanks very much for pointing out this bug, it will be fixed in next version soon.",4,2019-11-28 02:53:35,2021-12-17 03:30:04,2021-12-17 03:30:04
https://github.com/FederatedAI/FATE/issues/797,[],启动服务异常,"启动服务异常背景：
操作系统：macos
docker：环境检测ok。
按照“使用Docker镜像安装FATE（推荐）” 进行安装。
执行部署：bash install_standalone_docker.sh
![image](https://user-images.githubusercontent.com/12758136/69597493-1fc2cc00-1041-11ea-96cd-04a13b97a065.png)



nohup日志报错：
[SQLITE_ERROR] SQL error or missing database (no such table: t_job)

SQL: select                   'true' as QUERYID,               f_job_id, f_role, f_party_id, f_name, f_tag, f_initiator_party_id, f_status, f_cu
rrent_steps,      f_current_tasks, f_progress, f_create_time, f_update_time, f_start_time, f_end_time,      f_elapsed, f_run_ip             ,               f_description, f_roles
, f_dsl, f_runtime_conf             from t_job                                  WHERE (  f_end_time between ? and ? )
Cause: org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (no such table: t_job); uncategorized SQLException for SQL []; SQL state [null]; error code [1]; [SQLITE_ERROR] SQL error or missing database (no such table: t_job); nested exception is org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (no such table: t_job)No database ignored in docker designSorry, I do not understand your answer. I just follow your doc to install it with docker. But it do not work. How can I fix this? @KathyKing t_job This table will be created automatically. Please check if there is any problem with your docker",3,2019-11-26 03:43:44,2019-12-23 06:36:41,2019-12-23 06:36:41
https://github.com/FederatedAI/FATE/issues/794,[],fail to run toy_example  test in Allinone mode,"fail to run toy_example  test in Allinone modeI rewrite the allinone_cluster_configurations.sh file as below:

#!/bin/bash
user=app
deploy_dir=/data/projects/fate
party_list=(10000 9999)
node_list=(192.168.0.1)
db_auth=(fate_dev fate_dev)
redis_password=fate_dev
cxx_compile_flag=false

then build the project , all services started and succeed to  run  run_test.sh, but when I tested toy_example by command: 
python run_toy_example.py 9999 10000 1
I got en error!

console output like this:
![image](https://user-images.githubusercontent.com/13956966/69608631-d2efed00-1062-11ea-8320-af0b7ca57658.png)
![image](https://user-images.githubusercontent.com/13956966/69608661-e1d69f80-1062-11ea-93a1-63c67b04da51.png)

and the proxy logs is :
![image](https://user-images.githubusercontent.com/13956966/69608752-0af73000-1063-11ea-917c-5e0809dff119.png)



It seems 10000 party failed to start, how should I solve this problem ?

It is currently not possible to deploy two parties on one machine. It is recommended to prepare two hosts, one for each host.",1,2019-11-26 02:46:09,2021-12-17 03:29:06,2021-12-17 03:29:06
https://github.com/FederatedAI/FATE/issues/792,[],logic in quick_run.py may not right.,"logic in quick_run.py may not right.In quick_run.py:if \__name\__ == '\__main\__'(),  the if/else statement below would print ValueError if the host only does `upload_data()'. However this is actually not an error which using a host to initiate a prediction. For example, if a user wants to do LR, he/she should use quick_run to upload host's data first. If doing so, the ValueError would be raised.

    try:
        args = parser.parse_args()
        upload_data()
        if TASK == 'train' and args.role == GUEST:
            submit_job()
        else:
            if args.role == HOST:
                raise ValueError(""Predict task can be initialed by guest only"")
            predict_task()


@TerrenceKao 

Thanks for your question. Quick_run is designed for user experience FATE quickly. What we designed in quick_run contains uploading data only in host (For training purpose), uploading and starting training in guest and starting predict task in guest. In predict task, we use the previews trained model as well as the trained data. That's why we have the logic you mentioned. 

However, it is really a good suggestion to develop scrips for easily using FATE which is not purpose on experiencement only. And we do working on it. Please look forward to our future releases. 

Thanks again for your question and hope that is useful for you. > @TerrenceKao
> 
> Thanks for your question. Quick_run is designed for user experience FATE quickly. What we designed in quick_run contains uploading data only in host (For training purpose), uploading and starting training in guest and starting predict task in guest. In predict task, we use the previews trained model as well as the trained data. That's why we have the logic you mentioned.
> 
> However, it is really a good suggestion to develop scrips for easily using FATE which is not purpose on experiencement only. And we do working on it. Please look forward to our future releases.
> 
> Thanks again for your question and hope that is useful for you.

got it. thanks!",2,2019-11-25 09:30:37,2019-11-26 11:05:28,2019-11-26 11:05:28
https://github.com/FederatedAI/FATE/issues/788,[],ERROR.log was not found on the guest side,"ERROR.log was not found on the guest side**Describe the bug**
On FATE v1.1, while running the toy example on the guest side, The fate flow server would report the `ERROR.log` was not found some time.

**Expected behavior**
Unknown, maybe it should not print this message.

**Screenshots**
The details of the error.
```
job status is running
Traceback (most recent call last):
  File ""run_toy_example.py"", line 196, in <module>
    exec_toy_example(runtime_config)
  File ""run_toy_example.py"", line 171, in exec_toy_example
    show_log(jobid, ""error"")
  File ""run_toy_example.py"", line 149, in show_log
    with open(error_log, ""r"") as fin:
FileNotFoundError: [Errno 2] No such file or directory: '/data/projects/fate/python/examples/toy_example/19112203250928712616_log/guest/10000/secure_add_example_0/ERROR.log'
```Thanks for your feedback: our tips will be more friendly in the future.
You can go to the corresponding job under fate_flow_schedule.log to see if there is an error log.Thank you for your reply, close now.",2,2019-11-23 07:54:43,2019-11-28 09:34:22,2019-11-28 09:34:22
https://github.com/FederatedAI/FATE/issues/777,[],homo_nn running failed,"homo_nn running failed**Describe the bug**
The dataIO is got, but the homo_nn failed.

**Screenshots**
![image](https://user-images.githubusercontent.com/44254161/68741272-25271c00-0628-11ea-93b4-5c21c01d0542.png)

Log:
""2019-11-13 06:57:10,201 - api_utils.py[line:80] - INFO: local api request: http://172.19.0.2:9380/v1/schedule/201911130656552137699/homo_nn_0/201911130656552137699_homo_nn_0/guest/10000/status""
2
""2019-11-13 06:57:10,201 - api_utils.py[line:80] - INFO: local api request: http://172.19.0.2:9380/v1/schedule/201911130656552137699/homo_nn_0/201911130656552137699_homo_nn_0/guest/10000/status""
3
""2019-11-13 06:57:10,302 - api_utils.py[line:83] - INFO: {""retcode"":0,""retmsg"":""success""}
4
""
5
""2019-11-13 06:57:10,302 - api_utils.py[line:83] - INFO: {""retcode"":0,""retmsg"":""success""}
6
""
7
""2019-11-13 06:57:10,302 - api_utils.py[line:85] - INFO: local api response: /v1/schedule/201911130656552137699/homo_nn_0/201911130656552137699_homo_nn_0/guest/10000/status {'retcode': 0, 'retmsg': 'success'}""
8
""2019-11-13 06:57:10,302 - api_utils.py[line:85] - INFO: local api response: /v1/schedule/201911130656552137699/homo_nn_0/201911130656552137699_homo_nn_0/guest/10000/status {'retcode': 0, 'retmsg': 'success'}""
9
""2019-11-13 06:57:10,303 - task_executor.py[line:115] - INFO: run 201911130656552137699 homo_nn_0 201911130656552137699_homo_nn_0 guest 10000 task""
10
""2019-11-13 06:57:10,303 - task_executor.py[line:115] - INFO: run 201911130656552137699 homo_nn_0 201911130656552137699_homo_nn_0 guest 10000 task""
11
""2019-11-13 06:57:10,304 - task_executor.py[line:116] - INFO: {'HomoNNParam': {'secure_aggregate': True, 'aggregate_every_n_epoch': 1, 'config_type': 'nn', 'nn_define': [{'layer': 'Dense', 'units': 3, 'use_bias': False, 'activation': 'relu'}, {'layer': 'Dense', 'units': 2, 'activation': 'selu'}, {'layer': 'Dense', 'units': 1, 'activation': 'sigmoid'}], 'batch_size': -1, 'max_iter': 10, 'early_stop': {'early_stop': 'diff', 'eps': 0.0001}, 'metrics': ['accuracy', 'AUC', 'Hinge'], 'optimizer': {'optimizer': 'Adadelta', 'learning_rate': 0.05, 'rho': 0.95}, 'loss': 'binary_crossentropy', 'predict_param': {'threshold': 0.5}, 'cv_param': {'n_splits': 5, 'mode': 'hetero', 'role': 'guest', 'shuffle': True, 'random_seed': 1, 'need_cv': False}}, 'initiator': {'role': 'guest', 'party_id': 10000}, 'job_parameters': {'work_mode': 0, 'model_id': 'arbiter-10000#guest-10000#host-10000#model', 'model_version': '201911130656552137699'}, 'role': {'guest': [10000], 'host': [10000], 'arbiter': [10000]}, 'config': '/fate/examples/federatedml-1.x-examples/homo_nn/test_homo_dnn_multi_layer.json', 'dsl': 'examples/federatedml-1.x-examples/homo_nn/test_homo_nn_train_then_predict.dsl', 'function': 'submit_job', 'local': {'role': 'guest', 'party_id': 10000}, 'CodePath': 'federatedml/nn/homo_nn/enter_point.py/HomoNNGuest', 'module': 'HomoNN'}""
12
""2019-11-13 06:57:10,304 - task_executor.py[line:116] - INFO: {'HomoNNParam': {'secure_aggregate': True, 'aggregate_every_n_epoch': 1, 'config_type': 'nn', 'nn_define': [{'layer': 'Dense', 'units': 3, 'use_bias': False, 'activation': 'relu'}, {'layer': 'Dense', 'units': 2, 'activation': 'selu'}, {'layer': 'Dense', 'units': 1, 'activation': 'sigmoid'}], 'batch_size': -1, 'max_iter': 10, 'early_stop': {'early_stop': 'diff', 'eps': 0.0001}, 'metrics': ['accuracy', 'AUC', 'Hinge'], 'optimizer': {'optimizer': 'Adadelta', 'learning_rate': 0.05, 'rho': 0.95}, 'loss': 'binary_crossentropy', 'predict_param': {'threshold': 0.5}, 'cv_param': {'n_splits': 5, 'mode': 'hetero', 'role': 'guest', 'shuffle': True, 'random_seed': 1, 'need_cv': False}}, 'initiator': {'role': 'guest', 'party_id': 10000}, 'job_parameters': {'work_mode': 0, 'model_id': 'arbiter-10000#guest-10000#host-10000#model', 'model_version': '201911130656552137699'}, 'role': {'guest': [10000], 'host': [10000], 'arbiter': [10000]}, 'config': '/fate/examples/federatedml-1.x-examples/homo_nn/test_homo_dnn_multi_layer.json', 'dsl': 'examples/federatedml-1.x-examples/homo_nn/test_homo_nn_train_then_predict.dsl', 'function': 'submit_job', 'local': {'role': 'guest', 'party_id': 10000}, 'CodePath': 'federatedml/nn/homo_nn/enter_point.py/HomoNNGuest', 'module': 'HomoNN'}""
13
""2019-11-13 06:57:10,306 - task_executor.py[line:117] - INFO: {'data': {'train_data': ['dataio_0.train']}}""
14
""2019-11-13 06:57:10,306 - task_executor.py[line:117] - INFO: {'data': {'train_data': ['dataio_0.train']}}""
15
""2019-11-13 06:57:10,307 - random_padding_cipher.py[line:59] - INFO: synchronizing uuid""
16
""2019-11-13 06:57:10,307 - random_padding_cipher.py[line:59] - INFO: synchronizing uuid""
17
""2019-11-13 06:57:11,249 - random_padding_cipher.py[line:61] - INFO: local uuid=d043f7e8-05e2-11ea-a958-0242ac130002""
18
""2019-11-13 06:57:11,249 - random_padding_cipher.py[line:61] - INFO: local uuid=d043f7e8-05e2-11ea-a958-0242ac130002""
19
""2019-11-13 06:57:11,251 - random_padding_cipher.py[line:63] - INFO: Diffie-Hellman keys exchanging""
20
""2019-11-13 06:57:11,251 - random_padding_cipher.py[line:63] - INFO: Diffie-Hellman keys exchanging""
21
""2019-11-13 06:57:11,518 - random_padding_cipher.py[line:65] - INFO: Diffie-Hellman exchanged keys {UUID('d0767452-05e2-11ea-81ea-0242ac130002'): mpz(43050761789211103074012414491935670190873937671344350864488469987501248411593968086665275227175744745675186101408107064462459944809639534889860496004791823226501632472603050470479029436011999398580844231368665449208919456101657714250502356902046714048087896213426489321167869548761094399216119470948586090681)}""
22
""2019-11-13 06:57:11,518 - random_padding_cipher.py[line:65] - INFO: Diffie-Hellman exchanged keys {UUID('d0767452-05e2-11ea-81ea-0242ac130002'): mpz(43050761789211103074012414491935670190873937671344350864488469987501248411593968086665275227175744745675186101408107064462459944809639534889860496004791823226501632472603050470479029436011999398580844231368665449208919456101657714250502356902046714048087896213426489321167869548761094399216119470948586090681)}""
23
""2019-11-13 06:57:19,620 - deprecation.py[line:506] - WARNING: From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
24
Instructions for updating:
25
Call initializer instance with the dtype argument instead of passing it to the constructor""
26
""2019-11-13 06:57:19,838 - deprecation.py[line:323] - WARNING: From /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4075: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
27
Instructions for updating:
28
Use tf.where in 2.0, which has the same broadcast rule as np.where""
29
""2019-11-13 06:57:19,973 - deprecation_wrapper.py[line:119] - WARNING: From /fate/federatedml/nn/homo_nn/backend/tf_keras/nn_model.py:59: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.
30
""
31
""2019-11-13 06:57:20,012 - deprecation_wrapper.py[line:119] - WARNING: From /fate/federatedml/nn/homo_nn/backend/tf_keras/nn_model.py:60: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.
32
""
33
""2019-11-13 06:57:20,834 - enter_point.py[line:161] - INFO: start 0_th aggregation""
34
""2019-11-13 06:57:21,485 - task_executor.py[line:127] - ERROR: A target array with shape (846, 4) was passed for an output of shape (None, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.""
35
Traceback (most recent call last):
36
File ""/fate/fate_flow/driver/task_executor.py"", line 118, in run_task
37
run_object.run(parameters, task_run_args)
38
File ""/fate/federatedml/model_base.py"", line 157, in run
39
self._run_data(args[""data""], stage)
40
File ""/fate/federatedml/model_base.py"", line 98, in _run_data
41
self.fit(train_data, eval_data)
42
File ""/fate/federatedml/nn/homo_nn/enter_point.py"", line 164, in fit
43
self.nn_model.train(data, aggregate_every_n_epoch=self.aggregate_every_n_epoch)
44
File ""/fate/federatedml/nn/homo_nn/backend/tf_keras/nn_model.py"", line 118, in train
45
self._model.fit(x=data, epochs=epochs, verbose=1, shuffle=True, **left_kwargs)
46
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 673, in fit
47
initial_epoch=initial_epoch)
48
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1433, in fit_generator
49
steps_name='steps_per_epoch')
50
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py"", line 264, in model_iteration
51
batch_outs = batch_function(*batch_data)
52
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1153, in train_on_batch
53
extract_tensors_from_dataset=True)
54
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2692, in _standardize_user_data
55
y, self._feed_loss_fns, feed_output_shapes)
56
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 549, in check_loss_and_target_compatibility
57
' while using as loss `' + loss_name + '`. '
58
ValueError: A target array with shape (846, 4) was passed for an output of shape (None, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.
59
""2019-11-13 06:57:21,485 - task_executor.py[line:127] - ERROR: A target array with shape (846, 4) was passed for an output of shape (None, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.""
60
Traceback (most recent call last):
61
File ""/fate/fate_flow/driver/task_executor.py"", line 118, in run_task
62
run_object.run(parameters, task_run_args)
63
File ""/fate/federatedml/model_base.py"", line 157, in run
64
self._run_data(args[""data""], stage)
65
File ""/fate/federatedml/model_base.py"", line 98, in _run_data
66
self.fit(train_data, eval_data)
67
File ""/fate/federatedml/nn/homo_nn/enter_point.py"", line 164, in fit
68
self.nn_model.train(data, aggregate_every_n_epoch=self.aggregate_every_n_epoch)
69
File ""/fate/federatedml/nn/homo_nn/backend/tf_keras/nn_model.py"", line 118, in train
70
self._model.fit(x=data, epochs=epochs, verbose=1, shuffle=True, **left_kwargs)
71
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 673, in fit
72
initial_epoch=initial_epoch)
73
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1433, in fit_generator
74
steps_name='steps_per_epoch')
75
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py"", line 264, in model_iteration
76
batch_outs = batch_function(*batch_data)
77
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1153, in train_on_batch
78
extract_tensors_from_dataset=True)
79
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2692, in _standardize_user_data
80
y, self._feed_loss_fns, feed_output_shapes)
81
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 549, in check_loss_and_target_compatibility
82
' while using as loss `' + loss_name + '`. '
83
ValueError: A target array with shape (846, 4) was passed for an output of shape (None, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.
84
""2019-11-13 06:57:21,491 - api_utils.py[line:80] - INFO: local api request: http://172.19.0.2:9380/v1/schedule/201911130656552137699/homo_nn_0/201911130656552137699_homo_nn_0/guest/10000/status""
85
""2019-11-13 06:57:21,491 - api_utils.py[line:80] - INFO: local api request: http://172.19.0.2:9380/v1/schedule/201911130656552137699/homo_nn_0/201911130656552137699_homo_nn_0/guest/10000/status""
86
""2019-11-13 06:57:21,615 - api_utils.py[line:83] - INFO: {""retcode"":0,""retmsg"":""success""}
87
""
88
""2019-11-13 06:57:21,615 - api_utils.py[line:83] - INFO: {""retcode"":0,""retmsg"":""success""}
89
""
90
""2019-11-13 06:57:21,616 - api_utils.py[line:85] - INFO: local api response: /v1/schedule/201911130656552137699/homo_nn_0/201911130656552137699_homo_nn_0/guest/10000/status {'retcode': 0, 'retmsg': 'success'}""
91
""2019-11-13 06:57:21,616 - api_utils.py[line:85] - INFO: local api response: /v1/schedule/201911130656552137699/homo_nn_0/201911130656552137699_homo_nn_0/guest/10000/status {'retcode': 0, 'retmsg': 'success'}""
92
""2019-11-13 06:57:21,617 - task_executor.py[line:146] - INFO: finish 201911130656552137699 homo_nn_0 201911130656552137699_homo_nn_0 guest 10000 failed task""
93
""2019-11-13 06:57:21,617 - task_executor.py[line:146] - INFO: finish 201911130656552137699 homo_nn_0 201911130656552137699_homo_nn_0 guest 10000 failed task""
Btw, a typo under /fate/federatedml/nn/homo_nn/backend/tf_keras/layers, it should be basic.py if I am not wrongas log `ValueError: A target array with shape (846, 4) was passed for an output of shape (None, 1) while using as loss binary_crossentropy. This loss expects targets to have the same shape as the output.` suggests,  binary_crossentropy accept output shape (None, 1), while you the data you provided has shape (846, 4). In other words, binary_crossentropy loss used for binary classification only, try ` categorical_crossentropy' for multi-class datas.> Btw, a typo under /fate/federatedml/nn/homo_nn/backend/tf_keras/layers, it should be basic.py if I am not wrong

to be fixed in next version, thanks for comment.Thank you for the rapid response, but I still failed. I changed the dataset into breast and try categorical_crossentropy, but the error is below:
self._run_data(args[""data""], stage)
22
File ""/fate/federatedml/model_base.py"", line 98, in _run_data
23
self.fit(train_data, eval_data)
24
File ""/fate/federatedml/nn/homo_nn/enter_point.py"", line 155, in fit
25
data = self.data_converter.convert(data_inst, batch_size=self.batch_size)
26
File ""/fate/federatedml/nn/homo_nn/backend/tf_keras/nn_model.py"", line 236, in convert
27
return KerasSequenceData(data, *args, **kwargs)
28
File ""/fate/federatedml/nn/homo_nn/backend/tf_keras/nn_model.py"", line 201, in __init__
29
self.x[index] = inst.features
30
ValueError: could not broadcast input array from shape (18) into shape (30)The old dataset error:
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 549, in check_loss_and_target_compatibility
49
' while using as loss `' + loss_name + '`. '
50
ValueError: A target array with shape (413, 4) was passed for an output of shape (None, 1) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",5,2019-11-13 07:16:03,2019-11-15 03:15:23,2019-11-15 03:15:23
https://github.com/FederatedAI/FATE/issues/773,[],Quick Start-Cluster Version，task failed,"Quick Start-Cluster Version，task failedIn the host party,I run this command:   python quick_run.py -r host
In the guest party,I run the command:  python quick_run.py -r guest
The in the logs file ,I find the following message,How should I do?
`""2019-11-13 03:37:06,755 - dag_scheduler.py[line:60] - ERROR: there are no components in dsl, please have a check!""
Traceback (most recent call last):
  File ""/data/projects/fate/python/fate_flow/driver/dag_scheduler.py"", line 58, in handle_event
    return TaskScheduler.run_job(**job_event)
  File ""/data/projects/fate/python/fate_flow/driver/task_scheduler.py"", line 74, in run_job
    train_runtime_conf=train_runtime_conf)
  File ""/data/projects/fate/python/fate_flow/utils/job_utils.py"", line 162, in get_job_dsl_parser
    mode=job_type)
  File ""/data/projects/fate/python/fate_flow/driver/dsl_parser.py"", line 632, in run
    self._init_components(pipeline_dsl, mode)
  File ""/data/projects/fate/python/fate_flow/driver/dsl_parser.py"", line 113, in _init_components
    raise ValueError(""there are no components in dsl, please have a check!"")
ValueError: there are no components in dsl, please have a check!
`Now you can not submit task in hostI have the same Error when  I run min_test_task, @dylan-fan  Can explain these problem more detailed? Thank you very much！",2,2019-11-12 10:52:35,2020-03-02 13:39:02,2020-03-02 13:39:02
https://github.com/FederatedAI/FATE/issues/755,[],fate_flow start failed,"fate_flow start failedI start fate_flow service failed on cluster mode 
""fate_flow_stat.log"" shows only ""init mysql database on cluster mode successfully"".
how should i do ？

ps : I connect fate to my own mysql installed earlier,but i found no table created after starting  fate_flow.Check the logs/error.log for error messages and check if the DATABASE configuration is correct under fate_flow/settings.py",1,2019-11-05 03:40:04,2020-01-02 08:29:38,2020-01-02 08:29:38
https://github.com/FederatedAI/FATE/issues/753,[],storage-service-cxx start failed,"storage-service-cxx start failedall modules are started except storage-service-cxx, error output:
storage-service-cxx/storage-service: error while loading shared libraries: libgrpc++.so.1: cannot open shared object file: No such file or directory

os: centos 7.2

there is different error:
Illegal instruction
after **export LD_LIBRARY_PATH=/data/projects/fate/eggroll/storage-service-cxx/third_party/lib/**
we have the same problem now
u have any soluations now?try reinstall storage-service-cxx module

```
cd FATE/cluster-deploy/scripts
bash deploy_cluster_multinode.sh binary storage-service-cxx
```> try reinstall storage-service-cxx module
> 
> ```
> cd FATE/cluster-deploy/scripts
> bash deploy_cluster_multinode.sh binary storage-service-cxx
> ```
I met the exactly same error After reinstalling remains the issue.

After export the necessary enviroment path, I take the code snippet from the deploy shell code from 
/data/projects/fate/eggroll/service.sh 

cd /data/projects/fate/eggroll/storage-service-cxx
./storage-service-1.1  -p 7778 -d /data/projects/fate/eggroll/data-dir

the result is : Illegal instruction

PS : everthing needed is installed. 




Excute commands:

wget https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/third_party_source.tar.gz

tar -xzf third_party_source.tar.gz

cd third_party/rocksdb

PORTABLE=1 make shared_lib

cp librocksdb.so.6.1.2 /data/projects/fate/eggroll/storage-service-cxx/third_party/rocksdb/librocksdb.so.6.1.2

Then，restart  storage-service-cxx  service.",4,2019-11-04 08:53:15,2020-04-15 06:16:09,2020-04-15 06:16:09
https://github.com/FederatedAI/FATE/issues/733,[],"The first time I run ”python quick_run.py“, I get an error when I run it again.","The first time I run ”python quick_run.py“, I get an error when I run it again.The first time I run **python quick_run.py**, I get an error when I run it again.

`Upload data config json: {'file': 'examples/data/breast_b.csv', 'head': 1, 'partition': 10, 'work_mode': 0, 'table_name': 'breast_b', 'namespace': 'breast_b_guest'}
stdout:{
    ""retcode"": 100,
    ""retmsg"": ""HTTPConnectionPool(host='127.0.0.1', port=9380): Max retries exceeded with url: /v1/data/upload (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f99233845f8>: Failed to establish a new connection: [Errno 111] Connection refused',))"",
    ""traceback"": [
        ""Traceback (most recent call last):\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/urllib3/connection.py\"", line 159, in _new_conn\n    (self._dns_host, self.port), self.timeout, **extra_kw)\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/urllib3/util/connection.py\"", line 80, in create_connection\n    raise err\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/urllib3/util/connection.py\"", line 70, in create_connection\n    sock.connect(sa)\n"",
        ""ConnectionRefusedError: [Errno 111] Connection refused\n"",
        ""\nDuring handling of the above exception, another exception occurred:\n\n"",
        ""Traceback (most recent call last):\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\"", line 600, in urlopen\n    chunked=chunked)\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\"", line 354, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n"",
        ""  File \""/usr/local/lib/python3.6/http/client.py\"", line 1254, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n"",
        ""  File \""/usr/local/lib/python3.6/http/client.py\"", line 1300, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n"",
        ""  File \""/usr/local/lib/python3.6/http/client.py\"", line 1249, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n"",
        ""  File \""/usr/local/lib/python3.6/http/client.py\"", line 1036, in _send_output\n    self.send(msg)\n"",
        ""  File \""/usr/local/lib/python3.6/http/client.py\"", line 974, in send\n    self.connect()\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/urllib3/connection.py\"", line 181, in connect\n    conn = self._new_conn()\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/urllib3/connection.py\"", line 168, in _new_conn\n    self, \""Failed to establish a new connection: %s\"" % e)\n"",
        ""urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f99233845f8>: Failed to establish a new connection: [Errno 111] Connection refused\n"",
        ""\nDuring handling of the above exception, another exception occurred:\n\n"",
        ""Traceback (most recent call last):\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/requests/adapters.py\"", line 449, in send\n    timeout=timeout\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n    _stacktrace=sys.exc_info()[2])\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py\"", line 399, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n"",
        ""urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=9380): Max retries exceeded with url: /v1/data/upload (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f99233845f8>: Failed to establish a new connection: [Errno 111] Connection refused',))\n"",
        ""\nDuring handling of the above exception, another exception occurred:\n\n"",
        ""Traceback (most recent call last):\n"",
        ""  File \""/usr/FATE/examples/federatedml-1.0-examples/../../fate_flow/fate_flow_client.py\"", line 213, in <module>\n    response = call_fun(args.function, config_data, dsl_path, config_path)\n"",
        ""  File \""/usr/FATE/examples/federatedml-1.0-examples/../../fate_flow/fate_flow_client.py\"", line 145, in call_fun\n    response = requests.post(\""/\"".join([server_url, \""data\"", func]), json=config_data)\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/requests/api.py\"", line 116, in post\n    return request('post', url, data=data, json=json, **kwargs)\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/requests/api.py\"", line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/requests/sessions.py\"", line 524, in request\n    resp = self.send(prep, **send_kwargs)\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/requests/sessions.py\"", line 637, in send\n    r = adapter.send(request, **kwargs)\n"",
        ""  File \""/usr/local/lib/python3.6/site-packages/requests/adapters.py\"", line 516, in send\n    raise ConnectionError(e, request=request)\n"",
        ""requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=9380): Max retries exceeded with url: /v1/data/upload (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f99233845f8>: Failed to establish a new connection: [Errno 111] Connection refused',))\n""
    ]
}


{
    ""retcode"": 100,
    ""retmsg"": [
        ""Traceback (most recent call last):\n"",
        ""  File \""quick_run.py\"", line 368, in <module>\n    upload_data()\n"",
        ""  File \""quick_run.py\"", line 348, in upload_data\n    upload(GUEST)\n"",
        ""  File \""quick_run.py\"", line 239, in upload\n    stdout = exec_upload_task(json_info, role)\n"",
        ""  File \""quick_run.py\"", line 120, in exec_upload_task\n    \""[Upload task]exec fail, status:{}, stdout:{}\"".format(status, stdout))\n"",
        ""ValueError: [Upload task]exec fail, status:100, stdout:{'retcode': 100, 'retmsg': \""HTTPConnectionPool(host='127.0.0.1', port=9380): Max retries exceeded with url: /v1/data/upload (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f99233845f8>: Failed to establish a new connection: [Errno 111] Connection refused',))\"", 'traceback': ['Traceback (most recent call last):\\n', '  File \""/usr/local/lib/python3.6/site-packages/urllib3/connection.py\"", line 159, in _new_conn\\n    (self._dns_host, self.port), self.timeout, **extra_kw)\\n', '  File \""/usr/local/lib/python3.6/site-packages/urllib3/util/connection.py\"", line 80, in create_connection\\n    raise err\\n', '  File \""/usr/local/lib/python3.6/site-packages/urllib3/util/connection.py\"", line 70, in create_connection\\n    sock.connect(sa)\\n', 'ConnectionRefusedError: [Errno 111] Connection refused\\n', '\\nDuring handling of the above exception, another exception occurred:\\n\\n', 'Traceback (most recent call last):\\n', '  File \""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\"", line 600, in urlopen\\n    chunked=chunked)\\n', '  File \""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\"", line 354, in _make_request\\n    conn.request(method, url, **httplib_request_kw)\\n', '  File \""/usr/local/lib/python3.6/http/client.py\"", line 1254, in request\\n    self._send_request(method, url, body, headers, encode_chunked)\\n', '  File \""/usr/local/lib/python3.6/http/client.py\"", line 1300, in _send_request\\n    self.endheaders(body, encode_chunked=encode_chunked)\\n', '  File \""/usr/local/lib/python3.6/http/client.py\"", line 1249, in endheaders\\n    self._send_output(message_body, encode_chunked=encode_chunked)\\n', '  File \""/usr/local/lib/python3.6/http/client.py\"", line 1036, in _send_output\\n    self.send(msg)\\n', '  File \""/usr/local/lib/python3.6/http/client.py\"", line 974, in send\\n    self.connect()\\n', '  File \""/usr/local/lib/python3.6/site-packages/urllib3/connection.py\"", line 181, in connect\\n    conn = self._new_conn()\\n', '  File \""/usr/local/lib/python3.6/site-packages/urllib3/connection.py\"", line 168, in _new_conn\\n    self, \""Failed to establish a new connection: %s\"" % e)\\n', 'urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f99233845f8>: Failed to establish a new connection: [Errno 111] Connection refused\\n', '\\nDuring handling of the above exception, another exception occurred:\\n\\n', 'Traceback (most recent call last):\\n', '  File \""/usr/local/lib/python3.6/site-packages/requests/adapters.py\"", line 449, in send\\n    timeout=timeout\\n', '  File \""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py\"", line 638, in urlopen\\n    _stacktrace=sys.exc_info()[2])\\n', '  File \""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py\"", line 399, in increment\\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\\n', \""urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=9380): Max retries exceeded with url: /v1/data/upload (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f99233845f8>: Failed to establish a new connection: [Errno 111] Connection refused',))\\n\"", '\\nDuring handling of the above exception, another exception occurred:\\n\\n', 'Traceback (most recent call last):\\n', '  File \""/usr/FATE/examples/federatedml-1.0-examples/../../fate_flow/fate_flow_client.py\"", line 213, in <module>\\n    response = call_fun(args.function, config_data, dsl_path, config_path)\\n', '  File \""/usr/FATE/examples/federatedml-1.0-examples/../../fate_flow/fate_flow_client.py\"", line 145, in call_fun\\n    response = requests.post(\""/\"".join([server_url, \""data\"", func]), json=config_data)\\n', '  File \""/usr/local/lib/python3.6/site-packages/requests/api.py\"", line 116, in post\\n    return request(\\'post\\', url, data=data, json=json, **kwargs)\\n', '  File \""/usr/local/lib/python3.6/site-packages/requests/api.py\"", line 60, in request\\n    return session.request(method=method, url=url, **kwargs)\\n', '  File \""/usr/local/lib/python3.6/site-packages/requests/sessions.py\"", line 524, in request\\n    resp = self.send(prep, **send_kwargs)\\n', '  File \""/usr/local/lib/python3.6/site-packages/requests/sessions.py\"", line 637, in send\\n    r = adapter.send(request, **kwargs)\\n', '  File \""/usr/local/lib/python3.6/site-packages/requests/adapters.py\"", line 516, in send\\n    raise ConnectionError(e, request=request)\\n', \""requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=9380): Max retries exceeded with url: /v1/data/upload (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f99233845f8>: Failed to establish a new connection: [Errno 111] Connection refused',))\\n\""]}\n""
    ]
}
`Execute **sh /FATE/fate_flow/service.sh restart**, it's OK",1,2019-10-29 12:17:15,2019-11-05 06:24:28,2019-11-05 06:24:28
https://github.com/FederatedAI/FATE/issues/724,[],An error occurred while running ‘sh build_standalone_docker.sh’,"An error occurred while running ‘sh build_standalone_docker.sh’- When using docker to install the standalone version of the FATE framework, run `**sh build_standalone_docker.sh**` with the following error:

`ERROR: Exception:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/pip/_vendor/urllib3/response.py"", line 425, in _error_catcher
    yield
  File ""/usr/local/lib/python3.6/site-packages/pip/_vendor/urllib3/response.py"", line 507, in read
    data = self._fp.read(amt) if not fp_closed else b""""
  File ""/usr/local/lib/python3.6/site-packages/pip/_vendor/cachecontrol/filewrapper.py"", line 62, in read
    data = self.__fp.read(amt)
  File ""/usr/local/lib/python3.6/http/client.py"", line 459, in read
    n = self.readinto(b)
  File ""/usr/local/lib/python3.6/http/client.py"", line 503, in readinto
    n = self.fp.readinto(b)
  File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto
    return self._sock.recv_into(b)
  File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into
    return self.read(nbytes, buffer)
  File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read
    return self._sslobj.read(len, buffer)
  File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read
    v = self._sslobj.read(len, buffer)
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/cli/base_command.py"", line 153, in _main
    status = self.run(options, args)
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/commands/install.py"", line 382, in run
    resolver.resolve(requirement_set)
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/legacy_resolve.py"", line 201, in resolve
    self._resolve_one(requirement_set, req)
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/legacy_resolve.py"", line 365, in _resolve_one
    abstract_dist = self._get_abstract_dist_for(req_to_install)
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/legacy_resolve.py"", line 313, in _get_abstract_dist_for
    req, self.session, self.finder, self.require_hashes
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/operations/prepare.py"", line 194, in prepare_linked_requirement
    progress_bar=self.progress_bar
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/download.py"", line 465, in unpack_url
    progress_bar=progress_bar
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/download.py"", line 316, in unpack_http_url
    progress_bar)
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/download.py"", line 551, in _download_http_url
    _download_url(resp, link, content_file, hashes, progress_bar)
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/download.py"", line 253, in _download_url
    hashes.check_against_chunks(downloaded_chunks)
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/utils/hashes.py"", line 80, in check_against_chunks
    for chunk in chunks:
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/download.py"", line 223, in written_chunks
    for chunk in chunks:
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/utils/ui.py"", line 160, in iter
    for x in it:
  File ""/usr/local/lib/python3.6/site-packages/pip/_internal/download.py"", line 212, in resp_read
    decode_content=False):
  File ""/usr/local/lib/python3.6/site-packages/pip/_vendor/urllib3/response.py"", line 564, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""/usr/local/lib/python3.6/site-packages/pip/_vendor/urllib3/response.py"", line 529, in read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File ""/usr/local/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.6/site-packages/pip/_vendor/urllib3/response.py"", line 430, in _error_catcher
    raise ReadTimeoutError(self._pool, None, ""Read timed out."")
pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='pypi.tuna.tsinghua.edu.cn', port=443): Read timed out.
ERROR: Service 'python' failed to build: The command '/bin/sh -c pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt' returned a non-zero code: 2
`HTTPSConnectionPool(host='pypi.tuna.tsinghua.edu.cn', port=443): Read timed out.
Should be unable to connect to Tsinghua's pip source, you can test which sources can be connected and modifiedThank you. I installed it successfully using the manual method. @paulbaogang",2,2019-10-28 10:43:04,2019-11-05 06:24:40,2019-11-05 06:24:40
https://github.com/FederatedAI/FATE/issues/701,[],Float Cannot be Used as Random Seed,"Float Cannot be Used as Random Seed**Describe the bug**
When using float as random seed, it fails.

**To Reproduce**
Give float as a random seed in a training model job configuration file.

**Expected behavior**
Error indicates that float cannot be used as a random seed.
Float is not accepted as a random seed for bumpy random number generator. The check is not updated to give error message when float random seed is given.",1,2019-10-21 10:52:01,2019-10-21 10:54:43,2019-10-21 10:54:43
https://github.com/FederatedAI/FATE/issues/700,[],Fix Encrypt Param Check,"Fix Encrypt Param Check**Describe the bug**
Encrypt param check fails to report error when invalid encrypt method is given.

**To Reproduce**
Set encrypt param's 'method' to any string value other than ""paillier."" Then submit training task. 

**Expected behavior**
Fails without error message on encrypt param.

Fix param check. Encrypt method check added. ",1,2019-10-21 10:05:25,2019-10-21 10:06:01,2019-10-21 10:06:01
https://github.com/FederatedAI/FATE/issues/699,[],Fix Linear Regression Param Check,"Fix Linear Regression Param Check**Describe the bug**
Param check fails when learning rate or decay set to integer value.

**To Reproduce**
Set learning rate or decay in job configuration file to an integer. Then submit a training or CV job task.

**Expected behavior**
The job fails and gives error message indicating that the value is illegal. 
Fixed. 'float' type is now acceptable type for learning rate & decay input. ",1,2019-10-21 09:58:17,2019-10-21 09:58:44,2019-10-21 09:58:44
https://github.com/FederatedAI/FATE/issues/698,['bug'],Fix param check for Poisson Regression ,"Fix param check for Poisson Regression **Describe the bug**
Param check fails when learning rate or decay set to integer value.

**To Reproduce**
Set learning rate or decay in job configuration file to an integer. Then submit a training or CV job task.

**Expected behavior**
The job fails and gives error message indicating that the value is illegal. 
'float' is now added as acceptable type of learning rate & decay rate input. ",1,2019-10-21 09:56:16,2019-10-21 09:57:31,2019-10-21 09:57:31
https://github.com/FederatedAI/FATE/issues/697,['bug'],Fix Poisson Regression Predict BUG,"Fix Poisson Regression Predict BUG**Describe the bug**

When calling predict for evaluation during training, dataset's header does not match with the data. The original header is edited during training. 

**To Reproduce**
Submit a Poisson Regression training task. Within the configuration file, specify the exposure column name. The exposure column actually exists in the dataset.  

**Expected behavior**
The eggroll reports dimension mismatch. 
Bug fixed by creating another copy of header during training. ",1,2019-10-21 09:52:30,2019-10-21 09:53:14,2019-10-21 09:53:14
https://github.com/FederatedAI/FATE/issues/695,['bug'],Fix secureboost export_model bug when run cross validation,"Fix secureboost export_model bug when run cross validation**Describe the bug**
return None instead of raise error.fix this since FATE-1.1",1,2019-10-21 09:50:38,2019-11-04 11:20:40,2019-11-04 11:20:40
https://github.com/FederatedAI/FATE/issues/693,['bug'],"Fix bugs of model_base's flowid, parameter check of some module, dsl_parser bug""","Fix bugs of model_base's flowid, parameter check of some module, dsl_parser bug""**Describe the bug**
Fix bugs of model_base's flowid, parameter check of some module(e.g. encrypted_model_calculator, encrypt, l, dsl_parser bug""
Fix this since FATE-1.1",1,2019-10-21 09:43:13,2019-11-04 11:22:03,2019-11-04 11:22:03
https://github.com/FederatedAI/FATE/issues/685,[],"Unreasonable time consumption, in an epoch of the hetero LR.","Unreasonable time consumption, in an epoch of the hetero LR.My input data size is 350,000, there are 6 features on the guest side, 6 features on the host side, running hetero LR on this data, it takes about 1 minute to run an epoch, we think the time is too long, not reasonable.

From the log, we counted the time consumption of each step:
（1）Guest compute the compute_forward (computation time): 15s （Is the “join” in compute_forward function consumes too long time?）
（2）Guest get host_forward from host (Communication time)：8s 
（3）Guest compute fore_gradient (computation time): 6s
（4）Host get fore_gradient from guest (Communication time)：16s
（5）Guest compute guest_gradient and loss (computation time)：13s
（6）Arbiter get guest_gradient (Communication time)：< 1s
（7）Guest get optim_guest_gradient from arbiter（Communication time）：< 1s

is the time consumption reasonable ,on the current data size? 

the dsl file : 
[test_hetero_lr_train_job_dsl.txt](https://github.com/FederatedAI/FATE/files/3743074/test_hetero_lr_train_job_dsl.txt)
the conf file ：
[test_hetero_lr_train_job_conf.txt](https://github.com/FederatedAI/FATE/files/3743086/test_hetero_lr_train_job_conf.txt)

the guest log ：
![image](https://user-images.githubusercontent.com/5690422/67081135-30259280-f1c9-11e9-97e9-a021963e09f5.png)
the host log ：
![image](https://user-images.githubusercontent.com/5690422/67081181-403d7200-f1c9-11e9-8bc1-2ca88a33a43d.png)
the arbiter log ：
![image](https://user-images.githubusercontent.com/5690422/67081220-51867e80-f1c9-11e9-96e0-b3922f7b0825.png)
@EmilyMu This time comsuming is normal. ""Join"" action is time comsuming for distributed computing. More importantly, the whole computation is finished in encryption mode. The Paillier encrypt method and computing through encryted number is time comsuming. 

Hope than is useful for you.@tanmc123 It seems that fake encryption is applied which costs no time
      ""encrypt_param"": {
         ""method"": ""Fake"",
         ""key_length"": 1024
      },
Both computation and communication takes more time than expected. Does the join op in computation cost much? It takes 8 seconds for the guest to receive 350k x 3 x 4=4.2MB info. from the host. Here, 3 is for tuple3 and 4 for bytes of float.",2,2019-10-18 09:04:47,2019-10-28 13:24:25,2019-10-28 11:18:10
https://github.com/FederatedAI/FATE/issues/683,[],"task failed, but task_executor.py on port 9380 still be alive","task failed, but task_executor.py on port 9380 still be alive**Describe the bug**
    当我的任务失败时，发现9380端口上的服务还存在，并没有随着任务失败被关闭。
    我第一次使用hetero_lr/rmsprop/batch_size=2000训练数据，auc0.76是成功的，所以进行第二次尝试，将optimizer改为sgd，但是这次运行到iter=27时出错了，查看arbiter的日志，发现错误是""ask_executor.py[line:125] - ERROR: Overflow detected in decode number""。这时guest和host上的任务都显示失败了，虽然没有错误原因，但确实都显示task failed。但我查看系统进程，发现guest上的9380的服务并没有被杀掉，“python3 /data/projects/fate/python/fate_flow/driver/task_executor.py -j 2019101719574797565013 -n hetero_lr_0 -t 2019101719574797565013_hetero_lr_0 -r guest -p 10000 -c /data/projects/fate/python/jobs/2019101719574797565013/guest/10000/hetero_lr_0/task_config.json --job_server 127.0.0.1:9380”，如果这个进程还在，将会影响下一个任务的运行。所以我觉得这可能是个bug，这里我只能自己手动kill掉这个进程了。
    （顺便说一下，我任务中报的这个错误，发现是因为loss值太大了，已经e134次幂了，这可能因为我的数据引起的，改了sgd后，损失计算的问题，需要我进一步分析了。）
    不知这是否是一个bug，多谢开源！
@bigcash 
Thanks for your question and I apology that the question need to be answered in English. 

As you mentioned, there indeed exists a bug in fate-flow for killing failure task. in FATE1.0. When running a federated task, if the task in one party, let say Host party, failed, the task in Guest party will not be killed simultaneously even though it is killed in host. This bug will be fix in FATE1.1. Sorry for the misinformation. 

As for the exposure of loss, this might be caused by the un-normalized input data. Since we use Talor expansion to fit original loss function in encryption mode, it is quite sensitive for the scale of input data. 

Hope that is useful for you.@tanmc123 thank you, hope for new release FATE1.1.",2,2019-10-18 02:28:17,2019-11-05 01:38:45,2019-11-05 01:38:45
https://github.com/FederatedAI/FATE/issues/679,[],wrong comments on function aggregate_forward() in hetero_lr_guest.py,"wrong comments on function aggregate_forward() in hetero_lr_guest.pyi doubt whether the comment on function aggregate_forward() in hetero_lr_guest.py is correct.
![image](https://user-images.githubusercontent.com/46129889/66902627-7c69aa80-eff0-11e9-838b-a6e2e075057c.png)


i think the context with red rectangle in above screenshot should be 
 [[ (wx_g + wx_h)^2 ]] = [[ wx_g^2 ]] + [[ wx_h^2 ]] + 2 * wx_g* [[ wx_h ]]

looking forward to your reply.Here, we use ""en_"" prefix to represent Paillier encryption. Therefore, the above formula is actually the same as yours. However, it is a good suggestion to replace them by [[ ]] symbol. Thank you for pointing out this problem.",1,2019-10-16 08:46:50,2019-10-18 08:46:36,2019-10-18 08:46:36
https://github.com/FederatedAI/FATE/issues/678,[],MySql install problems?,"MySql install problems?The default installation of centos7 is mariadb. After I started the service and installed mysql, I reported an error after deploying. So I want to ask if we want to uninstall the mariadb service to install the mysql service? Still have other solutions? Thank you very much.you can  found the issue #669",1,2019-10-15 03:41:01,2019-11-05 06:28:13,2019-11-05 06:28:13
https://github.com/FederatedAI/FATE/issues/669,[],Question about installing mysql when deploying cluster.,"Question about installing mysql when deploying cluster.When I  ran `sh install_mysql.sh`，it occurred a problem, telling there is no `mysql.sock` file. Then I found the issue https://github.com/FederatedAI/FATE/issues/364, and tried to start the mysql server, but it shows a error: `Failed to start mysqld.service: Unit not found.`
And I saw some blogs, saying centos7 is not support mysql, need I install mariaDB instead of common mysql?Solved this problem... 
I cleaned the whole history mysql files(including configuration files), and restarted the server(sometimes an important step, for many associated processes or services may still be running), then executed install_mysql.sh, it runs without error.",1,2019-10-14 10:15:02,2019-10-16 02:25:35,2019-10-16 02:25:35
https://github.com/FederatedAI/FATE/issues/660,[],Bin num not match bug,"Bin num not match bug**Describe the bug**
In binning module, if the bin number smaller than the parameter bin_num, extra useless bin will be created. 
Already Finished",1,2019-10-11 11:21:26,2019-10-14 08:52:15,2019-10-14 08:52:15
https://github.com/FederatedAI/FATE/issues/648,[],Too many temporary data save to lmdb_temporary lead to disk full when training secureboost,"Too many temporary data save to lmdb_temporary lead to disk full when training secureboost
when fit a certain depth of a tree during training secureboost model,  a lot of data will be used and stored to lmdb_temporary, such as data_bin_with_node_dispatch, data_bin_with_node_dispathc_join_grad_hess and so on. But when the fit end, the data won't be deleted. 
when fit next depth, the similar data also will be saved to lmdb_temporary...
After several iterations, more and more data will be stored in lmdb_temporary and the disk will be filled.

Is there any solution for this issue? thanks.Suffer the same problem...
Too much temp data is cached which costs huge amount of disk space.
Is there anything we can do to clean up the cache?In later version of FATE (FATE 1.1 or later)，eggroll will upgrade and temporary data generated each iteration will be automatically cleaned@mgqa34 The ""iteration"" used in my words above refers to fit every depth of a tree, not training every tree. Is the same meaning of yours?@mgqa34 , Nice, this will significantly improve the practicability of FATE. 
BTW, as the @pandeBJ pointed out, does the ""each iteration"" refer to ""each tree"" or ""each layer of the tree""? The latter would be more resource-friendly. @pandeBJ Yes, it will be every depth of the tree.@mgqa34 it's clear now. Thanks for your explanation and look forward to your changes.FATE-1.1 support cleaning  temporary data  and this issue was closed",7,2019-10-10 11:34:33,2021-12-17 03:26:51,2021-12-17 03:26:51
https://github.com/FederatedAI/FATE/issues/646,['bug'],Fix bugs of use same flowid in predicting when validating train and validate data during each training epoch,"Fix bugs of use same flowid in predicting when validating train and validate data during each training epoch**Describe the bug**
Use same flowid in predicting when validating train and validate data during each training epoch, which may cause some bugs in some algorithm.

fixed, see pr #647",1,2019-10-10 07:42:23,2019-10-10 10:23:59,2019-10-10 10:23:59
https://github.com/FederatedAI/FATE/issues/640,[],Cluster Host&Guest cannot receive pallier public key when running Hetero Logistic Regression training,"Cluster Host&Guest cannot receive pallier public key when running Hetero Logistic Regression trainingThe Guest and Hosts are waiting to get the keys but the Arbiter has already remote it. How do I go about debugging this?  Thank you
I suspect that the transferStatus is not updated, hence the Host/Guest keep waiting for the status to change from 0 to 3.

**Debug log from Guest (party id 10000), same for the Host as well(party id 90000)**
""2019-10-03 05:24:55,693 - task_executor.py[line:111] - INFO: {'data': {'data': ['intersection_0.train']}}""
""2019-10-03 05:24:55,694 - encrypt_param.py[line:65] - DEBUG: Finish encrypt parameter check!""
""2019-10-03 05:24:55,694 - model_base.py[line:61] - DEBUG: need_run: True, need_cv: False""
""2019-10-03 05:24:55,695 - model_base.py[line:223] - DEBUG: set flowid to transfer_variable, flowid: 201910030524353448356_hetero_lr_0.fit""
""2019-10-03 05:24:55,695 - hetero_lr_guest.py[line:102] - INFO: Enter hetero_lr_guest fit""
""2019-10-03 05:24:55,855 - federation.py[line:205] - DEBUG: [GET] guest 10000 :getting remote object HeteroLRTransferVariable.paillier_pubkey.201910030524353448356_hetero_lr_0.fit from arbiter [90000]""

**Debug log from Arbiter(party id 90000)**
""2019-10-03 05:22:00,112 - hetero_lr_arbiter.py[line:83] - INFO: Enter hetero_lr_arbiter fit""
""2019-10-03 05:22:00,120 - hetero_lr_arbiter.py[line:94] - INFO: public_key:<PaillierPublicKey ce341411cf>""
""2019-10-03 05:22:00,130 - eggroll.py[line:246] - DEBUG: created table: storage_type: LMDB, namespace: 201910030524353448356_hetero_lr_0, name: __federation__, partitions: 10, in_place_computing: False""
""2019-10-03 05:22:00,184 - federation.py[line:177] - DEBUG: [REMOTE] Sending 201910030524353448356_hetero_lr_0-HeteroLRTransferVariable.paillier_pubkey-HeteroLRTransferVariable.paillier_pubkey.201910030524353448356_hetero_lr_0.train-arbiter-90000-host-90000""
""2019-10-03 05:22:00,191 - federation.py[line:183] - DEBUG: [REMOTE] Sent HeteroLRTransferVariable.paillier_pubkey for job jobId: ""201910030524353448356_hetero_lr_0""
""2019-10-03 05:22:00,191 - hetero_lr_arbiter.py[line:100] - INFO: remote public_key to host""
""2019-10-03 05:22:00,206 - eggroll.py[line:246] - DEBUG: created table: storage_type: LMDB, namespace: 201910030524353448356_hetero_lr_0, name: __federation__, partitions: 10, in_place_computing: False""
""2019-10-03 05:22:00,266 - federation.py[line:177] - DEBUG: [REMOTE] Sending 201910030524353448356_hetero_lr_0-HeteroLRTransferVariable.paillier_pubkey-HeteroLRTransferVariable.paillier_pubkey.201910030524353448356_hetero_lr_0.train-arbiter-90000-guest-10000""
""2019-10-03 05:22:00,268 - federation.py[line:183] - DEBUG: [REMOTE] Sent HeteroLRTransferVariable.paillier_pubkey for job jobId: ""201910030524353448356_hetero_lr_0""
""2019-10-03 05:22:00,268 - hetero_lr_arbiter.py[line:107] - INFO: remote public_key to guest""Please check the deployment configuration, focusing on the Federation service.",1,2019-10-03 05:49:59,2019-11-05 06:29:12,2019-11-05 06:29:12
https://github.com/FederatedAI/FATE/issues/638,[],grpc error occured running sync_encrypted_splitinfo_host  during SBT training,"grpc error occured running sync_encrypted_splitinfo_host  during SBT training**Describe the bug**

- guest
  -party_id:111
  -train_data_dimension: 1908*2
- host
  -party_id:166
  -train_data_dimension: 4236*3830 

error occured in host when guest get encrypted splitinfo  of depth 2, batch 0 from host:
```
""2019-09-30 10:51:41,101 - task_executor.py[line:120] - ERROR: <_Rendezvous of RPC that terminated with:
        status = StatusCode.CANCELLED
        details = ""Received RST_STREAM with error code 8""
        debug_error_string = ""{""created"":""@1569811901.083113452"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1017,""grpc_message"":""Received RST_STREAM with error code 8"",""grpc_status"":1}""
>""
Traceback (most recent call last):
  File ""/data/projects/fate/python/fate_flow/driver/task_executor.py"", line 109, in run_task
    run_object.run(parameters, task_run_args)
  File ""/data/projects/fate/python/federatedml/tree/hetero_secureboosting_tree_host.py"", line 232, in run
    self._run_data(args[""data""], stage)
  File ""/data/projects/fate/python/federatedml/model_base.py"", line 95, in _run_data
    self.cross_validation(train_data)
  File ""/data/projects/fate/python/federatedml/tree/boosting_tree.py"", line 137, in cross_validation
    kflod_obj.run(cv_param, data_instances, self)
  File ""/data/projects/fate/python/federatedml/model_selection/KFold.py"", line 109, in run
    model.fit(train_data)
  File ""/data/projects/fate/python/federatedml/tree/hetero_secureboosting_tree_host.py"", line 124, in fit
    tree_inst.fit()
  File ""/data/projects/fate/python/federatedml/tree/hetero_decision_tree_host.py"", line 335, in fit
    self.sync_encrypted_splitinfo_host(encrypted_splitinfo_host, dep, batch)
  File ""/data/projects/fate/python/federatedml/tree/hetero_decision_tree_host.py"", line 158, in sync_encrypted_splitinfo_host
    idx=-1)
  File ""/data/projects/fate/python/arch/api/federation.py"", line 73, in remote
    return RuntimeInstance.FEDERATION.remote(obj=obj, name=name, tag=tag, role=role, idx=idx)
  File ""/data/projects/fate/python/arch/api/cluster/federation.py"", line 171, in remote
    _table.put(_tagged_key, obj)
  File ""/data/projects/fate/python/arch/api/cluster/eggroll.py"", line 95, in put
    _EggRoll.get_instance().put(self, k, v, use_serialize=use_serialize)
  File ""/data/projects/fate/python/arch/api/cluster/eggroll.py"", line 350, in put
    raise e
  File ""/data/projects/fate/python/arch/api/cluster/eggroll.py"", line 347, in put
    self.kv_stub.put(kv_pb2.Operand(key=k, value=v), metadata=_get_meta(_table))
  File ""/data/projects/fate/venv/lib/python3.6/site-packages/grpc/_channel.py"", line 533, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/data/projects/fate/venv/lib/python3.6/site-packages/grpc/_channel.py"", line 467, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:
        status = StatusCode.CANCELLED
        details = ""Received RST_STREAM with error code 8""
        debug_error_string = ""{""created"":""@1569811901.083113452"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1017,""grpc_message"":""Received RST_STREAM with error code 8"",""grpc_status"":1}""
```
no other warning or error log in roll/egg/federation/storage-service-cxx

**Expected behavior**
- support big object(more than 32MB) transfer while training
- more specific error information


**Additional context**
I added some log and find that job failed when the size of encrypted_splitinfo_host object exceeded 32MB. 
![image](https://user-images.githubusercontent.com/52624066/65856746-fd8c3500-e394-11e9-8bda-dba7f8062a52.png)
![image](https://user-images.githubusercontent.com/52624066/65856789-1a286d00-e395-11e9-8cdb-3c5ce2cc611c.png)

In this version or earlier, SecureBoost regard the complete feature histogram as a single value, it maybe very big when bin_num * feature_shape * encrypted_size > 32M. We will use another strategy in later version to fix this issue. Thanks for your feedback.",1,2019-09-30 07:15:29,2021-12-17 03:24:57,2021-12-17 03:24:57
https://github.com/FederatedAI/FATE/issues/632,[],"run_toy_example.py——job running time exceed, please check federation or eggroll log——io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2Exception$HeaderListSizeException: Header size exceeded max allowed size (8192)","run_toy_example.py——job running time exceed, please check federation or eggroll log——io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2Exception$HeaderListSizeException: Header size exceeded max allowed size (8192)**Describe the bug**
job running time exceed
io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2Exception$HeaderListSizeException: Header size exceeded max allowed size (8192)

**To Reproduce**
### 1.when I run the:

**python run_toy_example.py 10001 10002 1**

I got this:
...
 job status is running
job status is running
Traceback (most recent call last):
  File ""run_toy_example.py"", line 197, in <module>
    exec_toy_example(runtime_config)
  File ""run_toy_example.py"", line 179, in exec_toy_example
    raise ValueError(""job running time exceed, please check federation or eggroll log"")
ValueError: job running time exceed, please check federation or eggroll log_

When I change the MAX_TIME from 10 to 30,the error is not changed.


### 2.then I check the egg and roll log
when I cat the **error.log on /data/projects/fate/roll/logs**

I got this:
Warning: Stream Error
io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2Exception$HeaderListSizeException: Header size exceeded max allowed size (8192)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2Exception.headerListSizeError(Http2Exception.java:171)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2CodecUtil.headerListSizeExceeded(Http2CodecUtil.java:228)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.HpackEncoder.encodeHeadersEnforceMaxHeaderListSize(HpackEncoder.java:122)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.HpackEncoder.encodeHeaders(HpackEncoder.java:106)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.DefaultHttp2HeadersEncoder.encodeHeaders(DefaultHttp2HeadersEncoder.java:68)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.DefaultHttp2FrameWriter.writeHeadersInternal(DefaultHttp2FrameWriter.java:499)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.DefaultHttp2FrameWriter.writeHeaders(DefaultHttp2FrameWriter.java:266)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2OutboundFrameLogger.writeHeaders(Http2OutboundFrameLogger.java:60)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.DecoratingHttp2FrameWriter.writeHeaders(DecoratingHttp2FrameWriter.java:53)
        at io.grpc.netty.shaded.io.grpc.netty.NettyServerHandler$WriteMonitoringFrameWriter.writeHeaders(NettyServerHandler.java:948)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:205)
        at io.grpc.netty.shaded.io.netty.handler.codec.http2.DefaultHttp2ConnectionEncoder.writeHeaders(DefaultHttp2ConnectionEncoder.java:146)
        at io.grpc.netty.shaded.io.grpc.netty.NettyServerHandler.sendResponseHeaders(NettyServerHandler.java:638)
        at io.grpc.netty.shaded.io.grpc.netty.NettyServerHandler.write(NettyServerHandler.java:575)
        at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)
        at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730)
        at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:816)
        at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:723)
        at io.grpc.netty.shaded.io.netty.channel.DefaultChannelPipeline.write(DefaultChannelPipeline.java:1061)
        at io.grpc.netty.shaded.io.netty.channel.AbstractChannel.write(AbstractChannel.java:295)
        at io.grpc.netty.shaded.io.grpc.netty.WriteQueue$AbstractQueuedCommand.run(WriteQueue.java:174)
        at io.grpc.netty.shaded.io.grpc.netty.WriteQueue.flush(WriteQueue.java:112)
        at io.grpc.netty.shaded.io.grpc.netty.WriteQueue.access$000(WriteQueue.java:32)
        at io.grpc.netty.shaded.io.grpc.netty.WriteQueue$1.run(WriteQueue.java:44)
        at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
        at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404)
        at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:462)
        at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
        at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)


### when I cat the **/data/projects/fate/roll/logs/console.log**
I get this :

java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.unaryProcessToStorageLocatorUnaryCall(EggProcessServiceClient.java:140)
        at com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.mapValues(EggProcessServiceClient.java:58)
        at com.webank.ai.fate.eggroll.roll.service.async.processor.MapValuesServiceProcessor.call(MapValuesServiceProcessor.java:35)
        at com.webank.ai.fate.eggroll.roll.service.async.processor.MapValuesServiceProcessor.call(MapValuesServiceProcessor.java:26)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
        at com.webank.ai.fate.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:155)
        at com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.unaryProcessToStorageLocatorUnaryCall(EggProcessServiceClient.java:138)
        ... 7 more
Caused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
        at io.grpc.Status.asRuntimeException(Status.java:526)
        at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
        at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
        at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
        at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
        at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
        at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
        at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
        at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
        at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
        at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
        at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
        at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
        at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        ... 3 more
Caused by: io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.16.1.123:50013
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at io.grpc.netty.shaded.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
        at io.grpc.netty.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
        at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
        at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
        at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
        at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
        at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
        at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        ... 1 more
Caused by: java.net.ConnectException: 拒绝连接
        ... 11 more
        at com.webank.ai.fate.core.error.exception.MultipleRuntimeThrowables.<init>(MultipleRuntimeThrowables.java:32)
        ... 12 more


How can I deal with this problemI restart all the servers and the problem not be repeated",1,2019-09-27 04:06:24,2019-09-28 01:06:04,2019-09-28 01:04:49
https://github.com/FederatedAI/FATE/issues/626,['bug'],Fix import error in eggroll api doc ,"Fix import error in eggroll api doc **Describe the bug**
There are some import path eggroll in eggroll_api doc, should be fixedalready pr, closed",1,2019-09-26 10:01:26,2019-09-26 10:07:07,2019-09-26 10:07:07
https://github.com/FederatedAI/FATE/issues/625,[],"when uploaded data with dimension 2.5 million * 150, it came up with ""_end_unary_response_blocking"" error","when uploaded data with dimension 2.5 million * 150, it came up with ""_end_unary_response_blocking"" error**Describe the bug**
Hi :
      when tried to upload data whose dimension is 2.5 million * 150, it came up with error logs in fate/python/jobs/*job_id*/std.log.

**logs**
 File ""/data/projects/fate/venv/lib/python3.6/site-packages/grpc/_channel.py"", line 625, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/data/projects/fate/venv/lib/python3.6/site-packages/grpc/_channel.py"", line 467, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:
        status = StatusCode.UNKNOWN
        details = ""Exception iterating requests!""
        debug_error_string = ""None""

This situation didn't appearance when upload small data set. I doubt whether it was caused by the data set is large. And what's the proper way to upload large data set.
![image](https://user-images.githubusercontent.com/46129889/65652763-645ec680-e002-11e9-920b-6322e36fd8df.png)
Please provide a log of the roll at the time, thank you.@zengjice sorry, i had delete the related logs. And after i change to another machine with bigger disk space to upload data, the issue was disappeared. I will attach the logs if encounter this issue in the future. thank you all the same.",2,2019-09-26 02:15:54,2019-11-05 06:32:34,2019-11-05 06:32:34
https://github.com/FederatedAI/FATE/issues/619,[],secboost example run failed,"secboost example run failedHello,

I followed the README files and started fate_flow and fateboard successfully, and also successfully ran the lr example in `examples/federatedml-1.0-examples/hetero_logistic_regression`, however, task failed when I tried secboost example in `examples/federatedml-1.0-examples/hetero_secureboost`.

Below is my config:

version: v1.0.2
fate_flow server: standanloe mode, use sqlite db.

In examples/federatedml-1.0-examples/quick_run.py:
DSL_PATH = 'hetero_secureboost/test_secureboost_train_dsl.json'
SUBMIT_CONF_PATH = 'hetero_secureboost/test_secureboost_train_binary_conf.json'

and then quick_run.py prints:
```
Upload data config json: {'file': 'examples/data/breast_b.csv', 'head': 1, 'partition': 10, 'work_mode': 0, 'table_name': 'breast_b', 'namespace': 'breast_b_guest'}
stdout:{
    ""data"": {
        ""namespace"": ""breast_b_guest"",
        ""pid"": 24848,
        ""table_name"": ""breast_b""
    },
    ""jobId"": ""201909251558299848924"",
    ""meta"": null,
    ""retcode"": 0,
    ""retmsg"": ""success""
}


Upload data config json: {'file': 'examples/data/breast_a.csv', 'head': 1, 'partition': 10, 'work_mode': 0, 'table_name': 'breast_a', 'namespace': 'breast_a_host'}
stdout:{
    ""data"": {
        ""namespace"": ""breast_a_host"",
        ""pid"": 24853,
        ""table_name"": ""breast_a""
    },
    ""jobId"": ""201909251558331064515"",
    ""meta"": null,
    ""retcode"": 0,
    ""retmsg"": ""success""
}


dsl_path: /home/yuwen/repo/github/FATE/examples/federatedml-1.0-examples/user_config/train_dsl.config_1569398316_1861, conf_path: /home/yuwen/repo/github/FATE/examples/federatedml-1.0-examples/user_config/train_conf.config_1569398316_2790
stdout:{
    ""data"": {
        ""board_url"": ""http://30.54.206.135:8080/index.html#/dashboard?job_id=201909251558362437256&role=guest&party_id=10000"",
        ""job_dsl_path"": ""/home/yuwen/repo/github/FATE/jobs/201909251558362437256/job_dsl.json"",
        ""job_runtime_conf_path"": ""/home/yuwen/repo/github/FATE/jobs/201909251558362437256/job_runtime_conf.json"",
        ""model_info"": {
            ""model_id"": ""arbiter-10000#guest-10000#host-10000#model"",
            ""model_version"": ""201909251558362437256""
        }
    },
    ""jobId"": ""201909251558362437256"",
    ""meta"": null,
    ""retcode"": 0,
    ""retmsg"": ""success""
}

Please check your task in fate-board, url is : http://30.54.206.135:8080/index.html#/dashboard?job_id=201909251558362437256&role=guest&party_id=10000
The log info is located in /home/yuwen/repo/github/FATE/examples/federatedml-1.0-examples/../../logs/201909251558362437256
Task is running, wait time: 10.356188535690308
Task Failed
```

The job failed in dataio_1 section:
![image](https://user-images.githubusercontent.com/3414971/65581800-b8d85680-dfae-11e9-8127-12ad260c2ed9.png)

logs:
```
""2019-09-25 15:58:42,495 - model_manager.py[line:74] - INFO: parse DataIOMeta proto object normal""
""2019-09-25 15:58:42,503 - model_manager.py[line:74] - INFO: parse DataIOParam proto object normal""
""2019-09-25 15:58:42,628 - api_utils.py[line:70] - INFO: local api request: http://30.54.206.135:9380/v1/schedule/201909251558362437256/dataio_1/201909251558362437256_dataio_1/host/10000/status""
""2019-09-25 15:58:42,736 - api_utils.py[line:73] - INFO: {""data"":null,""jobId"":null,""meta"":null,""retcode"":0,""retmsg"":""success""}
""
""2019-09-25 15:58:42,736 - api_utils.py[line:75] - INFO: local api response: /v1/schedule/201909251558362437256/dataio_1/201909251558362437256_dataio_1/host/10000/status {'data': None, 'jobId': None, 'meta': None, 'retcode': 0, 'retmsg': 'success'}""
""2019-09-25 15:58:42,736 - task_executor.py[line:109] - INFO: run 201909251558362437256 dataio_1 201909251558362437256_dataio_1 host 10000 task""
""2019-09-25 15:58:42,736 - task_executor.py[line:110] - INFO: {'DataIOParam': {'input_format': 'dense', 'delimitor': ',', 'data_type': 'float64', 'tag_with_value': False, 'tag_value_delimitor': ':', 'missing_fill': False, 'default_value': 0, 'missing_fill_method': None, 'missing_impute': None, 'outlier_replace': False, 'outlier_replace_method': None, 'outlier_impute': None, 'outlier_replace_value': 0, 'with_label': True, 'label_idx': 0, 'label_type': 'int', 'output_format': 'dense'}, 'initiator': {'role': 'guest', 'party_id': 10000}, 'job_parameters': {'work_mode': 0, 'model_id': 'arbiter-10000#guest-10000#host-10000#model', 'model_version': '201909251558362437256'}, 'role': {'guest': [10000], 'host': [10000], 'arbiter': [10000]}, 'config': '/home/yuwen/repo/github/FATE/examples/federatedml-1.0-examples/user_config/train_conf.config_1569398316_2790', 'dsl': '/home/yuwen/repo/github/FATE/examples/federatedml-1.0-examples/user_config/train_dsl.config_1569398316_1861', 'function': 'submit_job', 'local': {'role': 'host', 'party_id': 10000}, 'CodePath': 'federatedml/util/data_io.py/DataIO', 'module': 'DataIO'}""
""2019-09-25 15:58:42,736 - task_executor.py[line:111] - INFO: {'data': {'data': ['args.eval_data']}, 'model': ['dataio_0.dataio']}""
""2019-09-25 15:58:42,736 - data_io.py[line:108] - INFO: start to read dense data and change data to instance""
""2019-09-25 15:58:42,747 - task_executor.py[line:123] - ERROR: Count of data_instance is 0""
Traceback (most recent call last):
  File ""/home/yuwen/repo/github/FATE/fate_flow/driver/task_executor.py"", line 112, in run_task
    run_object.run(parameters, task_run_args)
  File ""/home/yuwen/repo/github/FATE/federatedml/model_base.py"", line 188, in run
    self._run_data(args[""data""], stage)
  File ""/home/yuwen/repo/github/FATE/federatedml/model_base.py"", line 161, in _run_data
    self.data_output = self.transform(data)
  File ""/home/yuwen/repo/github/FATE/federatedml/util/data_io.py"", line 741, in transform
    return self.reader.read_data(data_inst, ""transform"")
  File ""/home/yuwen/repo/github/FATE/federatedml/util/data_io.py"", line 110, in read_data
    abnormal_detection.empty_table_detection(input_data)
  File ""/home/yuwen/repo/github/FATE/federatedml/util/abnormal_detection.py"", line 25, in empty_table_detection
    raise ValueError(""Count of data_instance is 0"")
ValueError: Count of data_instance is 0
""2019-09-25 15:58:42,747 - api_utils.py[line:70] - INFO: local api request: http://30.54.206.135:9380/v1/schedule/201909251558362437256/dataio_1/201909251558362437256_dataio_1/host/10000/status""
""2019-09-25 15:58:42,917 - api_utils.py[line:73] - INFO: {""data"":null,""jobId"":null,""meta"":null,""retcode"":0,""retmsg"":""success""}
""
""2019-09-25 15:58:42,917 - api_utils.py[line:75] - INFO: local api response: /v1/schedule/201909251558362437256/dataio_1/201909251558362437256_dataio_1/host/10000/status {'data': None, 'jobId': None, 'meta': None, 'retcode': 0, 'retmsg': 'success'}""
""2019-09-25 15:58:42,917 - task_executor.py[line:137] - INFO: finish 201909251558362437256 dataio_1 201909251558362437256_dataio_1 host 10000 failed task""
```

It seems like input data is empty, but `dataio_0` do have output data successfully, so can you help me?hello! could you include your quick_run.py code? I suspect the problem lies in the ""submit_job"" function @usafchn Dataio_0  not passes data to dataio_1, it only passes model to dataio_1, so you should also upload dataio_1's data. @mgqa34 thanks, I solved this problemhello~ I have met the same problem, Could you tell me how to solve this problem.@4Details   example 中默认的 DSL 定义有问题，节点`dataio_1`只有模型输入，没有数据输入，改一下 DSL 配置就行了",5,2019-09-25 08:23:07,2019-11-25 08:16:48,2019-10-11 12:25:47
https://github.com/FederatedAI/FATE/issues/588,[],Support  validation data evaluation during training process. ,"Support  validation data evaluation during training process. **Is your feature request related to a problem? Please describe.**
Support validation data evaluate during training process, for examples, evaluates validation data after each epoch. Since FATE-1.1, hetero(homo)-lr\secureboost\hetero-linr\hetero-poisson-regression support evaluate data during training !",1,2019-09-18 02:29:26,2019-11-04 11:26:00,2019-11-04 11:26:00
https://github.com/FederatedAI/FATE/issues/584,[],An InterSect  Issue,"An InterSect  Issue**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
Some Editing:

run.sh guest fast host_table_name_1567432160_7370 host_table_namespace_1567432160_7370 

then something occured below,and there is no errors in logs; .... 

[Intersect] Start intersect job status checker, status counter: 56, jobid:201909022152082643742 [Intersect] cur job status:running, wait_time: 583.2567648887634 
[Intersect] Start intersect job status checker, status counter: 57, jobid:201909022152082643742 [Intersect] cur job status:running, wait_time: 593.5020263195038 
[Intersect] Start intersect job status checker, status counter: 58, jobid:201909022152082643742 [Intersect] cur job status:running, wait_time: 603.73965716362 
[Intersect] reach max intersect time:{}, intersect task may be failed, and exit now Traceback (most recent call last): File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 500, in  raise ValueError(""intersect task is failed"") ValueError: intersect task is failed

In FATE-Official-Technical-Exchange Wechat group, I saw that your problem have fixed. Do you have any other question ?  If not, this issue will be closed soon.",2,2019-09-16 02:06:48,2019-09-17 05:52:31,2019-09-17 05:52:31
https://github.com/FederatedAI/FATE/issues/583,[],How to change the default selected evaluation scores?,"How to change the default selected evaluation scores?I use the docker version of FATE over Standalone. And I run the experiment about the hetero_secure_boost algorithm with my data by quick_run.py.

In the evaluation_0 module of the FATE board,  I found the default selected evaluation scores are AUC and K-S. 

If I want to use the accuracy with thresholds and the auc, what should I do? Another question: what is the meaning of party id in quick_run.py? If I adopt a different dataset, should I modify its value?please check the figure of evaluation in board， it should has accuracy， roc .... if not, check the log of evaluation to find if something error or warning. the party id is the ID of role, for example, guest:9999, host:10000. It is nothing to do with what dataset you use![image](https://user-images.githubusercontent.com/17498125/65022169-0b889180-d963-11e9-9bf2-ed0d225ce662.png)
I mean in this figure, how to change the auc and ks to auc and acc?

> please check the figure of evaluation in board， it should has accuracy， roc .... if not, check the log of evaluation to find if something error or warning.

![image](https://user-images.githubusercontent.com/19305597/65025832-01b65c80-d96a-11e9-9e34-d8967d580c8f.png)
I could obtain these curves. But I want to display the above table with different items. In here,  they are auc and ks. But if I want to use auc and acc, what should I do?You can not do this only if you change the code of evaluation. FATE's evaluation now support fixed format of evaluate output.> You can not do this only if you change the code of evaluation. FATE's evaluation now support fixed format of evaluate output.

Fine, thanks your help.",8,2019-09-15 08:48:26,2019-09-17 09:16:46,2019-09-17 09:16:46
https://github.com/FederatedAI/FATE/issues/579,[],io_exception rpc,"io_exception rpchelp!
when i run  python run_toy_example.py 9999 10000 1,it get errors,
what cause this error,how can i slove it?

Traceback (most recent call last):
  File ""/data/projects/fate/python/fate_flow/utils/api_utils.py"", line 55, in remote_api
    _return = stub.unaryCall(_packet)
  File ""/home/app/anaconda3/lib/python3.6/site-packages/grpc/_channel.py"", line 533, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/home/app/anaconda3/lib/python3.6/site-packages/grpc/_channel.py"", line 467, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:
	status = StatusCode.UNAVAILABLE
	details = ""io exception""
	debug_error_string = ""{""created"":""@1568210336.533187742"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1017,""grpc_message"":""io exception"",""grpc_status"":14}""this is my ports desc
<img width=""368"" alt=""image"" src=""https://user-images.githubusercontent.com/26199641/64704519-0483fe00-d4e1-11e9-9edb-f2d7d91ac2ee.png"">
processor的服务没启动： 1、cd /data/projects/fate/egg sh service.sh stop 
2、cd /data/projects/fate/python sh service.sh start sh service.sh stop
 3、cd /data/projects/fate/egg sh service.sh start my processor is on, but the problem is still confuse me!
the error  occur again",3,2019-09-11 14:09:29,2019-09-17 05:52:59,2019-09-17 05:52:59
https://github.com/FederatedAI/FATE/issues/566,[],"Sometimes misreporting occurs, ValueError: job running time exceed, please check federation or eggroll log","Sometimes misreporting occurs, ValueError: job running time exceed, please check federation or eggroll logwhen I run :
`python run_toy_example.py 10000 10000 1`
I got this:
```
...
job status is running
job status is running
Traceback (most recent call last):
  File ""run_toy_example.py"", line 197, in <module>
    exec_toy_example(runtime_config)
  File ""run_toy_example.py"", line 179, in exec_toy_example
    raise ValueError(""job running time exceed, please check federation or eggroll log"")
ValueError: job running time exceed, please check federation or eggroll log
```

I checked run_toy_example.py and found `MAX_TIME = 10`
This value may be too small. It should be changed to 20.In ToyExample' default runtime conf , data size is 1000, and partition num is 10, there are only several kilobytes network transmission, so we think 10 seconds are enough.  By the way, after changed to 20s, do your job finish successfully ? If it is 10, it will fail for the first time. If it changes to 20, it will always succeed.We will take this issue into consideration. Thanks for your feedback!We  change MAX_TIME = 10 to MAX_TIME = 100 since FATE-1.1, hope this will help.",4,2019-09-09 07:21:37,2019-11-04 11:27:04,2019-11-04 11:27:03
https://github.com/FederatedAI/FATE/issues/549,[],No module named 'federatedml.param.param' when run 'run_ftl_ct_standalone.sh',"No module named 'federatedml.param.param' when run 'run_ftl_ct_standalone.sh'**Describe the bug**
When I run the script `run_ftl_ct_standalone.sh`, it raise ModuleNotFoundError. Like this:
```
Traceback (most recent call last):
  File ""run_arbiter.py"", line 19, in <module>
    from workflow.hetero_ftl_workflow.hetero_arbiter_workflow import FTLArbiterWorkFlow
  File ""/fate/workflow/hetero_ftl_workflow/hetero_arbiter_workflow.py"", line 19, in <module>
    from federatedml.ftl.hetero_ftl.hetero_ftl_arbiter import HeteroFTLArbiter
  File ""/fate/federatedml/ftl/hetero_ftl/hetero_ftl_arbiter.py"", line 24, in <module>
    from federatedml.param.param import FTLModelParam
ModuleNotFoundError: No module named 'federatedml.param.param'
```

**To Reproduce**
Steps to reproduce the behavior:
1. Go to folder `hetero_ftl`
2. Run the script `sh run_ftl_ct_standalone.sh 123`
3. See error on `arbiter.log`
hello,
Please provide the version of the FATE you deployed. And you can check the PYTHONPATH.",1,2019-09-05 08:50:04,2019-11-05 06:30:49,2019-11-05 06:30:49
https://github.com/FederatedAI/FATE/issues/540,[],KFold remote index bug,"KFold remote index bug**Describe the bug**
When synchronizing data in KFold, the index  is mismatched.  
Already Fixed",1,2019-09-04 09:49:39,2019-09-16 02:59:50,2019-09-16 02:59:49
https://github.com/FederatedAI/FATE/issues/529,[],Error when running the sh ./federatedml/test/run_test.sh after installing using manual,"Error when running the sh ./federatedml/test/run_test.sh after installing using manualI got errors when running the federatedml/test mainly because of ModuleNotFoundError: No module named 'xxxxx', xxxxx can be 'federatedml', 'arch', and so on. How to solve it?

start to run test /home/ysaputra/Data/urise/FATE/federatedml/test/../feature/feature_scale/test/min_max_scale_test.py
Traceback (most recent call last):
  File ""/home/ysaputra/Data/urise/FATE/federatedml/test/../feature/feature_scale/test/min_max_scale_test.py"", line 9, in <module>
    from arch.api import eggroll
ModuleNotFoundError: No module named 'arch'
start to run test /home/ysaputra/Data/urise/FATE/federatedml/test/../feature/feature_scale/test/standard_scale_test.py
Traceback (most recent call last):
  File ""/home/ysaputra/Data/urise/FATE/federatedml/test/../feature/feature_scale/test/standard_scale_test.py"", line 9, in <module>
    from arch.api import eggroll
ModuleNotFoundError: No module named 'arch'
start to run test /home/ysaputra/Data/urise/FATE/federatedml/test/../feature/test/bucket_binning_test.py
Traceback (most recent call last):
  File ""/home/ysaputra/Data/urise/FATE/federatedml/test/../feature/test/bucket_binning_test.py"", line 21, in <module>
    from arch.api import eggroll
ModuleNotFoundError: No module named 'arch'
start to run test /home/ysaputra/Data/urise/FATE/federatedml/test/../feature/test/feature_select_test.py
Traceback (most recent call last):
  File ""/home/ysaputra/Data/urise/FATE/federatedml/test/../feature/test/feature_select_test.py"", line 26, in <module>
    from federatedml.feature.feature_selection import UniqueValueFilter
ModuleNotFoundError: No module named 'federatedml'
........................
Please set PYTHONPATH, such like export PYTHONPATH=I have set PYTHONPATH based on where the bin file exists but the same problem occurs. When should I set the PYTHONPATH? After or before sh init.sh process?I have encountered the same problem.The solution is to execute first:
export PYTHONPATH=/home/ysaputra/Data/urise/FATENot yet. I still have the same problem. Please let me know if you can solve
this problem.

On Tue, Oct 29, 2019 at 4:43 PM Shao Lun <notifications@github.com> wrote:

> I have encountered the same problem. Have you solved it?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/FederatedAI/FATE/issues/529?email_source=notifications&email_token=AF4BMUJYG2J6472STQKBVFDQQ7EQJA5CNFSM4ITCUCBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECPJ2LI#issuecomment-547265837>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF4BMUKQNYDSJLEIZOM5MCDQQ7EQJANCNFSM4ITCUCBA>
> .
>


-- 
Best Regards,

Yuris Mulya Saputra
was this resolved? same error for my execution",5,2019-09-03 06:54:08,2019-11-09 23:15:30,2019-11-05 06:23:07
https://github.com/FederatedAI/FATE/issues/522,[],sh run.sh guest fast ${host_table} ${host_namespace} failed,"sh run.sh guest fast ${host_table} ${host_namespace} failedHello!
When I run:
sh run.sh guest fast host_table_name_1567432160_7370  host_table_namespace_1567432160_7370
then something occured below,and there is no errors in logs;
....
[Intersect] Start intersect job status checker, status counter: 56, jobid:201909022152082643742
[Intersect] cur job status:running, wait_time: 583.2567648887634
[Intersect] Start intersect job status checker, status counter: 57, jobid:201909022152082643742
[Intersect] cur job status:running, wait_time: 593.5020263195038
[Intersect] Start intersect job status checker, status counter: 58, jobid:201909022152082643742
[Intersect] cur job status:running, wait_time: 603.73965716362
[Intersect] reach max intersect time:{}, intersect task may be failed, and exit now
Traceback (most recent call last):
  File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 500, in <module>
    raise ValueError(""intersect task is failed"")
ValueError: intersect task is failed
*********************
*******finish!*******i meet this problem，can you tell me hwo do you slove it!i meet this problem too，can you tell me hwo do you slove it!",2,2019-09-02 14:08:08,2019-09-27 09:23:15,2019-09-12 01:14:39
https://github.com/FederatedAI/FATE/issues/518,['bug'],Fix bug: when run quick_run.py raise ImportError: cannot import name 'itemgetter',"Fix bug: when run quick_run.py raise ImportError: cannot import name 'itemgetter'**Describe the bug**
When run the script of quick_run.py, it will rasie ImportError like this:
```
stdout:Traceback (most recent call last):
  File ""/home/wy/FATE/examples/federatedml-1.0-examples/../../fate_flow/fate_flow_client.py"", line 17, in <module>
    import argparse
  File ""/home/wy/.pyenv/versions/3.6.0/lib/python3.6/argparse.py"", line 86, in <module>
    import collections as _collections
  File ""/home/wy/.pyenv/versions/3.6.0/lib/python3.6/collections/__init__.py"", line 26, in <module>
    from operator import itemgetter as _itemgetter, eq as _eq
ImportError: cannot import name 'itemgetter'
```

**To Reproduce**
Steps to reproduce the behavior:
1. Go to 'python quick_run.py'
2. See error
@shikanon this error is relative to python's import system behavior.  The module `collections` fails to  import the standards module `operator`  because there is a same file named `operator` under the same directory with fate_flow_client.  But this won't be a problem in python 3.6 unless fate_flow/  was manually added to  `PATHTHONPATH` environment.",1,2019-08-29 16:35:51,2019-09-05 12:23:13,2019-09-05 12:23:13
https://github.com/FederatedAI/FATE/issues/509,[],The PSI implemented is not fully secure.,"The PSI implemented is not fully secure.**Describe the bug**
The PSI implemented in FATE/federatedml/statistic/intersect/ is not fully secure.

**To Reproduce**
Suppose A and B are doing PSI.

Step 1: B  generates  all  possible  primes  p_1,p_2,...p_n  <  2^32    (about  200,000,000  x)
Step 2: B  sends  r_i^e * p_i,  together  with  other  ""legal"" data to  A    (The  extra n pieces of data is  reasonable  in  a  billions  vs  billions  PSI)
Step 3: Innocent  A  cast  (r_i^e * p_i)^d  (which is  r_i * p_i^d) on  all  of  the  data r_i^e * p_i sent  by  B.
Step 4: B  receives  r_i * p_i^d and computes p_i^d

Now  suppose  B  want  to  guess  whether  a  data  D  is  in  A's  data  set.    

Step  1:  Compute  SHA256(D),  obtain  256  bit  integer  s.
Step  2:  Factor  s.  There's  an  unnegligible  chance  that  s  could  be  factored  into  short  (<2^32)  primes,  which  is  already  pre-computed.  [http://www.crypto-uni.lu/jscoron/publications/padding.pdf]
//the  chance  is  above 2^-23  ≈  1  out  of  8,000,000  ，  which  means  several  hundreds  of  possible leakages  in  the  case  of  mobile  phone  number  /  device  id.
Step  3.  Compute  s^d  from  p_i^d,  compute  h  =  SHA256(s^d)
Step  4.  Check  if  h  is  in  the  data  set  from  A

**Additional context**

Suggested countermeasure 1: Use a PSI algorithm that is provable secure.
Suggested countermeasure 2 (If 1 cannot be done) : Change SHA256 to SHA512 to reduce the leakage.
Thanks for your feedback， we will checkout this @dylan-fan Not sure why this is closed. Talked with Tianjian Chen today, and it should be re-opened.> @dylan-fan Not sure why this is closed. Talked with Tianjian Chen today, and it should be re-opened.

Thank you for your advice.
First, we will change sha256 to sha512
Second, we will add some other psi algorithms.If convenient, could you please provide your email or wechat? We will continue our communication@vincehong  @dylan-fan @Tianjian 
A few points from my understanding:
1. The attack example given by @vincehong is an **adversarial** attack, while the blind RSA approach is applicable for **semi-honest** parties (see the original paper).  Thus, the attack example may not be fair for the blind RSA approach.
2. The first hash in the blind RSA approach shall be a full-domain hash (in theory, see the original paper).
3. There is some chance that Party A can **detect** the attack example given by @vincehong.  So, Party B better not carry out such an attack.",5,2019-08-27 07:06:08,2021-11-11 03:10:38,2021-11-11 03:10:38
https://github.com/FederatedAI/FATE/issues/506,[],Host should be able to store models with non-encrypted version in HomoLR,"Host should be able to store models with non-encrypted version in HomoLR**Is your feature request related to a problem? Please describe.**
Currently, there is no way for homo-lr host party to store models.
May I get your wechat please? I have some questions to ask you...I am a PHD student from Xi'an jiaotong university.",2,2019-08-26 02:52:30,2019-11-05 01:40:15,2019-11-05 01:40:15
https://github.com/FederatedAI/FATE/issues/502,['bug'],meet a bug when running logistic regression example,"meet a bug when running logistic regression exampleHi, I met a python bug when runing ""homo_logistic_regression"" example.It is shown blow:
```shell
sh-4.2$ python /data/projects/FATE/fate_flow/fate_flow_client.py -f submit_job -c /data/projects/FATE/examples/federatedml-1.0-examples/homo_logistic_regression/test_homolr_evaluate_job_conf.json -d /data/projects/FATE/examples/federatedml-1.0-examples/homo_logistic_regression/test_homolr_evaluate_job_dsl.json
Traceback (most recent call last):
  File ""/data/projects/FATE/fate_flow/fate_flow_client.py"", line 17, in <module>
    import argparse
  File ""/data/projects/common/miniconda3/lib/python3.7/argparse.py"", line 87, in <module>
    import re as _re
  File ""/data/projects/common/miniconda3/lib/python3.7/re.py"", line 125, in <module>
    import functools
  File ""/data/projects/common/miniconda3/lib/python3.7/functools.py"", line 21, in <module>
    from collections import namedtuple
  File ""/data/projects/common/miniconda3/lib/python3.7/collections/__init__.py"", line 21, in <module>
    from operator import itemgetter as _itemgetter, eq as _eq
ImportError: cannot import name 'itemgetter' from 'operator' (/data/projects/FATE/fate_flow/operator/__init__.py)
```

I dont know where is the missing lib from, FATE project or standard python libs？And how can I solve the problem?https://github.com/FederatedAI/FATE/issues/518ok, thank you",2,2019-08-23 06:50:56,2019-09-02 08:21:10,2019-09-02 08:21:10
https://github.com/FederatedAI/FATE/issues/459,['bug'],The Process running WorkFlow cannot stop,"The Process running WorkFlow cannot stop**Describe the bug**
When put Workflow subclasses such as IntersectHostWorkFlow, IntersectGuestWorkFlow in a Process and start it in ** STANDALONE ** mode, the process will not return from join()

**To Reproduce**
Run the following code under the root directory and it will not stop. The root cause is Executor in Standalone does not  call shutdown after submit futures.
```
import uuid

import sys
from multiprocessing import Process

from arch.api import eggroll, RuntimeInstance
from arch.api.storage import save_data
from examples.load_file.load_file import read_data
from workflow.intersect_workflow.intersect_guest_workflow import IntersectGuestWorkFlow
from workflow.intersect_workflow.intersect_host_workflow import IntersectHostWorkFlow

guest_partyid=10000
host_partyid=9999
work_mode=0
a_file = r'examples/data/breast_a.csv'
b_file = r'examples/data/breast_b.csv'
a_table = 'a_table'
b_table = 'b_table'
namespace = 'test_workflow'


def upload_file(file, table):
    data = read_data(a_file)
    save_data(data, table, namespace)

def start_guest(job_id):
    guest_conf = '''
    {
  ""local"": {
    ""role"": ""guest"",
    ""party_id"": 10000
  },
  ""role"": {
    ""host"": [
      9999
    ],
    ""guest"": [
      10000
    ]
  },
  ""DataIOParam"": {
    ""with_label"": false,
    ""partition"": 1
  },
  ""WorkFlowParam"": {
    ""method"": ""intersect"",
    ""data_input_table"": ""a_table"",
    ""data_input_namespace"": ""test_workflow"",
    ""intersect_data_output_table"": ""guest_output_table"",
    ""intersect_data_output_namespace"": ""output"",
    ""work_mode"": 0
  },
  ""IntersectParam"": {
    ""intersect_method"": ""rsa"",
    ""random_bit"": 128,
    ""is_send_intersect_ids"": true,
    ""is_get_intersect_ids"": true,
    ""join_role"": ""host"",
    ""with_encode"": true,
    ""only_output_key"": true
  },
  ""EncodeParam"": {
    ""encode_method"": ""sha256"",
    ""salt"": ""12345"",
    ""base64"": false
  }
}
'''
    with open('guest.conf', 'w') as f:
        f.write(guest_conf)
    sys.argv.extend(['-c', 'guest.conf', '-j', job_id])
    guest_wf = IntersectGuestWorkFlow()
    print('guest_wf start running')
    guest_wf.run()
    print('guest_wf stop running')


def start_host(job_id):
    host_conf = '''
    {
  ""local"": {
    ""role"": ""host"",
    ""party_id"": 9999
  },
  ""role"": {
    ""host"": [
      9999
    ],
    ""guest"": [
      10000
    ]
  },
  ""DataIOParam"": {
    ""with_label"": false,
    ""partition"": 1
  },
  ""WorkFlowParam"": {
    ""method"": ""intersect"",
    ""data_input_table"": ""b_table"",
    ""data_input_namespace"": ""test_workflow"",
    ""intersect_data_output_table"": ""host_output_table"",
    ""intersect_data_output_namespace"": ""output"",
    ""work_mode"": 0
  },
  ""IntersectParam"": {
    ""intersect_method"": ""rsa"",
    ""random_bit"": 128,
    ""is_send_intersect_ids"": true,
    ""is_get_intersect_ids"": true,
    ""join_role"": ""host"",
    ""with_encode"": true,
    ""only_output_key"": true
  },
  ""EncodeParam"": {
    ""encode_method"": ""sha256"",
    ""salt"": ""12345"",
    ""base64"": false
  }
}
'''
    with open('host.conf', 'w') as f:
        f.write(host_conf)
    sys.argv.extend(['-c', 'host.conf', '-j', job_id])
    host_wf = IntersectHostWorkFlow()
    print('host_wf start running')
    host_wf.run()
    print('host_wf stop running')


if __name__ == '__main__':
    job_id = str(uuid.uuid1().hex)
    eggroll.init(job_id=job_id, mode=0)
    ps = [Process(target=upload_file, args=(file, table))for file, table in ((a_file, a_table), (b_file, b_table))]
    for p in ps:
        p.start()
    for p in ps:
        p.join()
    job_id = 'intersection_' + str(uuid.uuid1())
    print('job_id:', job_id)
    ps = [Process(target=fun, args=(job_id,)) for fun in (start_guest, start_host)]
    for p in ps:
        p.start()
    for p in ps:
        p.join()
```


hello，can you give me your email， we can discuss detail about your pr requestHi, my email is cleaner.leon@outlook.com<mailto:cleaner.leon@outlook.com>. I am also in wechat group “FinTechathon  AI赛道十强群” with name “F4-卫巍”, and we may talk about the issue in wechat.

Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows 10

________________________________
发件人: dylan-fan <notifications@github.com>
发送时间: Wednesday, August 21, 2019 11:22:09 AM
收件人: WeBankFinTech/FATE <FATE@noreply.github.com>
抄送: cleanerleon <cleaner.leon@outlook.com>; Author <author@noreply.github.com>
主题: Re: [WeBankFinTech/FATE] The Process running WorkFlow cannot stop (#459)


hello，can you give me your email， we can discuss detail about your pr request

―
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/WeBankFinTech/FATE/issues/459?email_source=notifications&email_token=AE4E35ZCA5LOFLA2TVIPBWDQFSYGDA5CNFSM4IMLFX22YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4YJWHI#issuecomment-523279133>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AE4E355HUZEIN4QKXHIGSLLQFSYGDANCNFSM4IMLFX2Q>.",2,2019-08-16 17:07:57,2019-11-05 06:22:26,2019-11-05 06:22:26
https://github.com/FederatedAI/FATE/issues/453,[],Question about the usage of ftl,"Question about the usage of ftlHi, I've read the code and the paper about ftl. In my mind, the typical scene of using ftl is when there are not so many overlapped samples. But during the inference or serving, seems we still need to make sure that the sample to be predict appears in both sides. Does it mean we should use ftl in scenes when we have rare overlapped samples during training but have enough overlapped samples for predicting?I read the paper again and maybe I misunderstood the transfer algorithm. During the inference process, we need features from the host(B side) and caculated phi from guest(A side), so there is no need for overlapping, right?
",1,2019-08-16 07:52:42,2021-07-02 08:07:20,2021-07-02 08:07:20
https://github.com/FederatedAI/FATE/issues/392,[],the prediction can only predict part of the input data table,"the prediction can only predict part of the input data tableHello, when I run the ""predict"" mode in the cluster form, initially, the prediction work well for about 20000 samples when the num_trees = 2, while when the model is trained by the num_trees = 10, the prediction can only predict part of the input data table. 

And I found the problem can be associated with the processor launched by the ""egg"" module. However, there is no place that I can find the exception information in the egg nor in the processor logs file. And the port range from 50001 to 50008 which are all active after checking.

I am confused about the stability of the trained model, can anyone give me some suggestions?Can you check logs in roll ? Maybe help with error\console.log under roll's log directory",1,2019-08-08 08:52:12,2019-08-22 10:09:33,2019-08-22 10:09:33
https://github.com/FederatedAI/FATE/issues/390,[],Cluster deployment problem,"Cluster deployment problemHello, I meet a problem when I deploy the cluster version FATE in a linux server, the OS of the server is centos6.5. 
It failed in the step of running ""bash auto-deploy.sh"".  
<img width=""800"" alt=""image-20190808094139603"" src=""https://user-images.githubusercontent.com/9210893/62669614-c61b9080-b9c2-11e9-9ac4-dc8a79395fde.png"">
The result is shown like above. When I typed the password of user ""app"" which I masked in screenshot, it was blocked for a while and then disconnected from the server. 
BTW, I edited ""configurations.sh"" file following the issue https://github.com/WeBankFinTech/FATE/issues/278. And just modify the ip, I wonder if there is problem with my process, Thank you.The operating system requires centos7.2, and it is recommended to use 7.2.
In addition, the app user needs to be free from sudo to root and needs to be configured.
please try,thanks.> The operating system requires centos7.2, and it is recommended to use 7.2.
> In addition, the app user needs to be free from sudo to root and needs to be configured.
> please try,thanks.

Ok, I will try the deployment on Centos7.2.
I have given the app user root right, and you mentioned ""the app user needs to be configured"", I am not sure which step you mean, would you mind be more specific, thank you so much!",2,2019-08-08 01:55:27,2019-08-22 09:55:57,2019-08-22 09:55:57
https://github.com/FederatedAI/FATE/issues/385,[],"when i run logistic_regression,i come to ""fail to post status running"" problem","when i run logistic_regression,i come to ""fail to post status running"" problemconfig path is /opt/FATE/examples/hetero_logistic_regression/conf/arbiter_runtime_conf.json_hetero_logistic_regression_example_standalone_20190806234846
jobid is hetero_logistic_regression_example_standalone_20190806234846
fail to post status running
Finish encrypt parameter check!
Finish evaluation parameter check!
Finish predict parameter check!
Finish predict parameter check!
Finish evaluation parameter check!
Finish workerflow parameter check!
Finish init parameter check!
Finish encode parameter check!
Finish encode parameter check!
Finish intersect parameter check!
Finish init parameter check!
Finish encrypt parameter check!
Finish logistic parameter check!
Finish encrypt parameter check!
Finish scale parameter check!
Finish one_vs_rest parameter check!
Finish all parameter checkers
Get in base workflow initialize
Finish init parameter check!
Finish encrypt parameter check!
Finish logistic parameter check!
Finish predict parameter check!
Finish evaluation parameter check!
Finish workerflow parameter check!if the PYTHONPATH been set ?long time no reply, close this issue",2,2019-08-06 15:59:08,2019-08-20 14:08:04,2019-08-20 14:08:04
https://github.com/FederatedAI/FATE/issues/364,[],Deployment error happened after installing MySQL,"Deployment error happened after installing MySQLI am trying to deploying the cluster-verion FATE and exactly followed the instructions. However I stacked and got the error ""ERROR 2002 (HY000): Can't connect to local MySQL server through socket"" after excuting ``$/data/projects/common/mysql/mysql-8.0/bin/mysql` -uroot -p -S /data/projects/common/mysql/mysql-8.0/mysql.sock``.  Also I noticed MySQL service was not started. As a temptation to solve this issue, I installed mariadb-server, started MySQL service but still got the same error. Can anyone let me know why this happened and how to solve it?
1.Check  whether  mysql is started  。
2.if   mysql is started,check whether the file /data/projects/common/mysql/mysql-8.0/mysql.sock  exists .if not exists,  try to  check mysql's configration  to find out where is the mysql.sock",1,2019-08-03 09:09:48,2019-08-09 12:28:01,2019-08-09 12:28:01
https://github.com/FederatedAI/FATE/issues/350,[],"When building a securetree model, if the user in the Party A don't have the data in the Party B, what will the model inference?","When building a securetree model, if the user in the Party A don't have the data in the Party B, what will the model inference?Hi,I am confused about when we built a securetree model, if the user in the Party A don't have the data in the Party B, what will the model inference for him/her?
Will the model makes a auto-completion for him/her by some strategy? or just filter it and do nothing? Thanks for answering!Now Fate's secureboost does not support missing value now. It will support it in later version.long time no reply, close this issue",2,2019-08-01 11:37:07,2019-08-20 14:07:34,2019-08-20 14:07:34
https://github.com/FederatedAI/FATE/issues/313,[],Intermediate result visualizable.,"Intermediate result visualizable.**Is your feature request related to a problem? Please describe.**
When building model, intermediate results such as loss, evaluation results and so on are needed for users. 

Provide callback interfaces by fate-flow, so that these result can be transfer to fate-board. Then fate-board will show these results in UI.  
Already finished",1,2019-07-23 06:40:31,2019-08-29 14:20:42,2019-07-23 08:06:20
https://github.com/FederatedAI/FATE/issues/299,[],bug when test FATE,"bug when test FATEbug report:
start to run test /fate/federatedml/test/../feature/test/quantile_binning_test.py
.E
======================================================================
ERROR: test_quantile_binning (__main__.TestQuantileBinningSpeed)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/fate/federatedml/test/../feature/test/quantile_binning_test.py"", line 140, in test_quantile_binning
    split_points = quan_bin.fit_split_points(self.table)
  File ""/fate/federatedml/feature/binning/quantile_binning.py"", line 76, in fit_split_points
    summary_dict = data_instances.mapPartitions(f)
  File ""/fate/arch/api/standalone/eggroll.py"", line 703, in mapPartitions
    results = self._submit_to_pool(func, do_map_partitions)
  File ""/fate/arch/api/standalone/eggroll.py"", line 687, in _submit_to_pool
    results.append(Standalone.get_instance().pool.submit(_do_func, _p))
  File ""/usr/local/lib/python3.6/concurrent/futures/process.py"", line 466, in submit
    self._start_queue_management_thread()
  File ""/usr/local/lib/python3.6/concurrent/futures/process.py"", line 427, in _start_queue_management_thread
    self._adjust_process_count()
  File ""/usr/local/lib/python3.6/concurrent/futures/process.py"", line 446, in _adjust_process_count
    p.start()
  File ""/usr/local/lib/python3.6/multiprocessing/process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""/usr/local/lib/python3.6/multiprocessing/context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""/usr/local/lib/python3.6/multiprocessing/context.py"", line 277, in _Popen
    return Popen(process_obj)
  File ""/usr/local/lib/python3.6/multiprocessing/popen_fork.py"", line 19, in __init__
    self._launch(process_obj)
  File ""/usr/local/lib/python3.6/multiprocessing/popen_fork.py"", line 66, in _launch
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory
----------------------------------------------------------------------
Ran 2 tests in 14.053s

FAILED (errors=1)
start to run test /fate/federatedml/test/../feature/test/quantile_summaries_test.py
min_rank: 89800, found_rank: 89932, max_rank: 90200
.min_rank: 89800, found_rank: 90047, max_rank: 90200
min_rank: 89800, found_rank: 90045, max_rank: 90200
min_rank: 89800, found_rank: 90033, max_rank: 90200
min_rank: 89800, found_rank: 89916, max_rank: 90200
min_rank: 89800, found_rank: 89979, max_rank: 90200
question:
how pass the errorThis might cause by the data size in test case is too large. I will change it. Thanks for your report.",1,2019-07-19 09:58:34,2019-08-08 02:49:09,2019-08-08 02:49:09
https://github.com/FederatedAI/FATE/issues/296,[],bug when install at 7step,"bug when install at 7stepStep 7/9 : RUN apt-get update && apt-get install libgmp3-dev -y && apt-get install -y libmpfr-dev libmpfr-doc libmpfr4 libmpfr4-dbg && apt-get install libmpc-dev -y
 ---> Running in a908d36d4503
Get:1 http://deb.debian.org/debian buster InRelease [118 kB]
Get:2 http://security.debian.org/debian-security buster/updates InRelease [39.1 kB]
Get:3 http://security.debian.org/debian-security buster/updates/main amd64 Packages [42.5 kB]
Get:4 http://deb.debian.org/debian buster-updates InRelease [46.8 kB]
Get:5 http://deb.debian.org/debian buster/main amd64 Packages [7897 kB]
Fetched 8143 kB in 27s (307 kB/s)
Reading package lists...
Reading package lists...
Building dependency tree...
Reading state information...
The following NEW packages will be installed:
  libgmp3-dev
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 15.7 kB of archives.
After this operation, 22.5 kB of additional disk space will be used.
Get:1 http://deb.debian.org/debian buster/main amd64 libgmp3-dev amd64 2:6.1.2+dfsg-4 [15.7 kB]
debconf: delaying package configuration, since apt-utils is not installed
Fetched 15.7 kB in 2s (9017 B/s)
Selecting previously unselected package libgmp3-dev:amd64.
(Reading database ... 24544 files and directories currently installed.)
Preparing to unpack .../libgmp3-dev_2%3a6.1.2+dfsg-4_amd64.deb ...
Unpacking libgmp3-dev:amd64 (2:6.1.2+dfsg-4) ...
Setting up libgmp3-dev:amd64 (2:6.1.2+dfsg-4) ...
Reading package lists...
Building dependency tree...
Reading state information...
E: Unable to locate package libmpfr4
E: Unable to locate package libmpfr4-dbg
The command '/bin/sh -c apt-get update && apt-get install libgmp3-dev -y && apt-get install -y libmpfr-dev libmpfr-doc libmpfr4 libmpfr4-dbg && apt-get install libmpc-dev -y' returned a non-zero code: 100
+ rm docker/standalone/fate.tar
+ rm docker/standalone/requirements.txtso do iCould you tell me what your environment ?
I have used Ubuntu 16.04 LTS and Ubuntu 18.04, but i cannot resolve this problem at alljdk1.8
python3.6
virtualenv
mysql8.0
redis-5.0.4
docker client 18.09.8
docker server 18.09.8
CentOS7.2solution：
/docker/standalone/Dockerfile:

FROM python:3.6

WORKDIR /fate

COPY . /fate

RUN rm /bin/sh && ln -sf /bin/bash /bin/sh

RUN tar -xf fate.tar

RUN rm fate.tar
RUN apt-get update && apt-get -y install wget && apt-get -y install libc6 libgmp10 multiarch-support
RUN wget http://ftp.cn.debian.org/debian/pool/main/m/mpfr4/libmpfr4_3.1.2-2_amd64.deb
RUN wget http://ftp.cn.debian.org/debian/pool/main/m/mpfr4/libmpfr4-dbg_3.1.2-2_amd64.deb
RUN dpkg -i libmpfr4_3.1.2-2_amd64.deb
RUN dpkg -i libmpfr4-dbg_3.1.2-2_amd64.deb

RUN apt-get update && apt-get install libgmp3-dev -y && apt-get install -y libmpfr-dev libmpfr-doc libmpfr4 libmpfr4-dbg && apt-get install libmpc-dev -y

RUN pip install -r requirements.txt

ENV PYTHONPATH /fate
thanks a lot, I have changed the dockerfile like yours, and it worked out.> Step 7/9 : RUN apt-get update && apt-get install libgmp3-dev -y && apt-get install -y libmpfr-dev libmpfr-doc libmpfr4 libmpfr4-dbg && apt-get install libmpc-dev -y
> ---> Running in a908d36d4503
> Get:1 http://deb.debian.org/debian buster InRelease [118 kB]
> Get:2 http://security.debian.org/debian-security buster/updates InRelease [39.1 kB]
> Get:3 http://security.debian.org/debian-security buster/updates/main amd64 Packages [42.5 kB]
> Get:4 http://deb.debian.org/debian buster-updates InRelease [46.8 kB]
> Get:5 http://deb.debian.org/debian buster/main amd64 Packages [7897 kB]
> Fetched 8143 kB in 27s (307 kB/s)
> Reading package lists...
> Reading package lists...
> Building dependency tree...
> Reading state information...
> The following NEW packages will be installed:
> libgmp3-dev
> 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
> Need to get 15.7 kB of archives.
> After this operation, 22.5 kB of additional disk space will be used.
> Get:1 http://deb.debian.org/debian buster/main amd64 libgmp3-dev amd64 2:6.1.2+dfsg-4 [15.7 kB]
> debconf: delaying package configuration, since apt-utils is not installed
> Fetched 15.7 kB in 2s (9017 B/s)
> Selecting previously unselected package libgmp3-dev:amd64.
> (Reading database ... 24544 files and directories currently installed.)
> Preparing to unpack .../libgmp3-dev_2%3a6.1.2+dfsg-4_amd64.deb ...
> Unpacking libgmp3-dev:amd64 (2:6.1.2+dfsg-4) ...
> Setting up libgmp3-dev:amd64 (2:6.1.2+dfsg-4) ...
> Reading package lists...
> Building dependency tree...
> Reading state information...
> E: Unable to locate package libmpfr4
> E: Unable to locate package libmpfr4-dbg
> The command '/bin/sh -c apt-get update && apt-get install libgmp3-dev -y && apt-get install -y libmpfr-dev libmpfr-doc libmpfr4 libmpfr4-dbg && apt-get install libmpc-dev -y' returned a non-zero code: 100
> 
> * rm docker/standalone/fate.tar
> * rm docker/standalone/requirements.txt

@nickzhou123 
Thank for your pointing out.
After testing, Ubuntu does not support the automatic installation of libmpfr4 packages, instead of libmpfr6, which may require you to modify docker/standalone/Docker file: RUN apt-get update & apt-get install libgmp3-dev-y & apt-get-install-y libmpfr-dev libmpfr-doc libmpfr4-dbg & apt-get-install-bmpc-d Ev-y
Amendment to
RUN apt-get update & apt-get install libgmp3-dev-y & apt-get install-y libmpfr-dev libmpfr-doc libmpfr6 & apt-get install libmpc-dev-y
It's true that this docker guidance document is a bit old, and we'll update it as soon as possible.
Thank you.",6,2019-07-18 13:20:57,2019-08-22 09:52:38,2019-08-22 09:52:38
https://github.com/FederatedAI/FATE/issues/294,[],Error: No such container:docker,"Error: No such container:docker使用docker安装standalone版本的时候，执行第三步出现如题的错误，请问各位有没有遇到过啊？你们是怎么解决的呢？Can you see the build of docker images? Execute the command :docker images.If it exists, execute ps -a to see if the container exists or not?execute the command : docker images  return some anonymous repository  image.
and when i execute the command: docker run -t -d fate/standalone , the result is error : Unable to find the image ""fate/standalone:latest"" locally.
by the way, the first command sh ,there are some red line:
`debconf : delaying package configuration,since apt-utils is not installed `( but i have installed apt-utils)
```
E: Unable to locate package libmpfr4
E: Unable to locate package libmpfr4-dbg
```this is dockfile:
FROM python:3.6

WORKDIR /fate

COPY . /fate

RUN rm /bin/sh && ln -sf /bin/bash /bin/sh

RUN tar -xf fate.tar

RUN rm fate.tar

RUN apt-get update && apt-get install libgmp3-dev -y && apt-get install -y libmpfr-dev libmpfr-doc libmpfr4 libmpfr4-dbg && apt-get install libmpc-dev -y

RUN pip install -r requirements.txt

ENV PYTHONPATH /fate


Confirm whether the environment can access the external network? The docker host machine can execute:
RUN apt-get update && apt-get install libgmp3-dev -y && apt-get install -y libmpfr-dev libmpfr-doc libmpfr4 libmpfr4-dbg && apt-get install libmpc-dev -yShould be dependent package can not be installed, you need to check the network or yum source.i'am sure the network is fine ,and the dockerfile is just like yours，but cannot locate the package libmpfr4 and libmpfr4-dbg. like the ISSUE No.51After testing, ubuntu official website does not support the automatic installation of libmpfr4 package, instead of libmpfr6, you may need to modify the docker/standalone/Dockerfile: 

RUN apt-get update && apt-get install libgmp3-dev -y && apt-get Install -y libmpfr-dev libmpfr-doc libmpfr4 libmpfr4-dbg && apt-get install libmpc-dev -y

changed to

RUN apt-get update && apt-get install libgmp3-dev -y && apt-get install -y libmpfr-dev libmpfr-doc libmpfr6 && apt-get install libmpc-dev -y

We will modify the instructions in the follow-up, you can try it first, thank you.Thanks a lot, and I’ll try it later. 
By the way, I have changed the dockerfile mentioned in issue 51 (use wget command to install libmpfr4 and libmpfr4-dbg from Debian website) , it worked out.
I have another question about the results after running the logistic regression example, I cannot find the log directory as the README mentioned, could you tell me the other way I can get the log ,thank you.I came across this problem and solved it,may it help you.
(1) ensure you successfully download the tar.gz file;
(2) for line in the README.md
`CONTAINER_ID='docker ps -aqf ""name=fate_python""'`
use 
`docker ps -aqf ""name=fate_python""`
to get the Container ID by hand(for me it is ""d4a95de0b4dd""),and copy it and assign it to CONTAINER_ID,such as 
`CONTAINER_ID=""d4a95de0b4dd""`
(3)then  you can do as the tutorial,
`docker exec -t -i ${CONTAINER_ID} bash`

actually  I don't know why but it works for me.

Gook Luck!",9,2019-07-18 09:04:32,2020-04-14 08:03:22,2019-08-22 06:47:41
https://github.com/FederatedAI/FATE/issues/291,[],运行run_toy_example_cluster.sh guest一直卡住,"运行run_toy_example_cluster.sh guest一直卡住部署好host和guest两台机器之后，运行run_toy_example_cluster.sh 示例，但是guest端一直卡住了。但是查看各个模块的日志，也没有发现错误。Can you please give more information? like roll logsLong time no reply, close it, if needed, please reopen the issue",2,2019-07-16 14:51:48,2019-08-20 14:11:06,2019-08-20 14:11:06
https://github.com/FederatedAI/FATE/issues/286,['bug'],v0.3.1 first test task fix bugs,"v0.3.1 first test task fix bugsfix some bugs:
a. scale transform func not return header
b. scale module column selection parameter repeat problem
c. scale param_checker problem
d. fit iv_percentile bug when percentile = 1.0, the selected length would exceed total list.

other change:
1. add ""fast""  to workflow param_validation.json
2. remove scale module limit for the column what has same valueclosed with pr #285",1,2019-07-11 05:32:27,2019-08-29 14:20:41,2019-07-11 09:32:51
https://github.com/FederatedAI/FATE/issues/283,[],About the “Proxy” Module,"About the “Proxy” ModuleHi,

1.  If I can set only one ip address serving as the ""proxy"" node for two or even more parties? 

Thanks a lot!!!Add:

I know different proxy module in different party has different ip address. And the same port ""9370"" can have a conflict for serving. Can I change the port so that each party can have just one exchangeip address? @YangjieZhou 

> Hi,
> 
> 1. If I can set only one ip address serving as the ""proxy"" node for two or even more parties?
> 
> Thanks a lot!!!

The answer is yes.Each of module can set only one ip address by changing port.By the other hand, multiple ""proxy"" can use only one exchangeip address.@zzzcq 

In that case, what does the exchangeip address work for?Hi yangjie,
Proxy and exchange are similar to soft routes in order to build a federated network. If the two parties do not have network isolation, they can be simply deployed. For example, only one proxy can be used, and the routing information of the two parties is configured on the route config. The serving is also on-demand, has nothing to do with the proxy, and should not conflict. 
Thanks.",4,2019-07-10 09:56:07,2019-08-29 14:20:41,2019-08-22 06:44:27
https://github.com/FederatedAI/FATE/issues/279,[],启动egg服务时无法正常启动grpc服务,"启动egg服务时无法正常启动grpc服务**Describe the bug**
参考https://github.com/WeBankFinTech/FATE/tree/master/cluster-deploy 部署文档操作，执行
sh /data/projects/fate/services.sh 时发现grpc服务启动失败。

**Additional context**
修改/data/projects/fate/python/processor.sh脚本，在首行加入`source /data/projects/fate/venv/bin/activate`后，即可正常启动。@sookieqin Hello，how is your file ‘/data/projects/fate/services.sh’ generated smoothly? Thank you!> @sookieqin Hello，how is your file ‘/data/projects/fate/services.sh’ generated smoothly? Thank you!
just follow the introduction, but make a few modifications before execute ""sh /data/projects/fate/services.sh"":
1.add 'source /data/projects/fate/venv/bin/activate' in the /data/projects/fate/python/processor.sh file

Hope it works for u.> > @sookieqin您好，您的文件'/data/projects/fate/services.sh'如何顺利生成？谢谢！
> > 只需按照介绍，但在执行“sh /data/projects/fate/services.sh”之前进行一些修改：
> 
> 1. 使用
>    / data / projects / fate / python / processor中的'storage- serivice'2.add'source / data / projects / fate / venv / bin / activate' 替换services.sh文件中的'storage- serivice -cxx' .sh文件
> 
> 希望它对你有用。

Thank you for your last reply. There are still problems.
According to the introduction, modifying the configuration items corresponding to the configurations.sh file according to the configuration, before execute the auto-packaging.sh script.
It must be mentioned that,I only make cluster-deploy on one computer.How to modify the configurations.sh？Thank you for your time.
..........................................................................
Thank U! I get the services.sh file!> > @sookieqin Hello，how is your file ‘/data/projects/fate/services.sh’ generated smoothly? Thank you!
> > just follow the introduction, but make a few modifications before execute ""sh /data/projects/fate/services.sh"":
> 
> 1. replace 'storage-serivice-cxx' in the services.sh file with 'storage-serivice'
>    2.add 'source /data/projects/fate/venv/bin/activate' in the /data/projects/fate/python/processor.sh file
> 
> Hope it works for u.

don't advice to replace ""replace 'storage-serivice-cxx' in the services.sh file with 'storage-serivice'""Hi:
      I encountered some problems when deploying, I hope to discuss with you! The following is a description of my problem:

When I executed the ""sh services.sh all start"" command in the ""/data/projects/fate"" directory, I found the following problem:
[INFO] processing: storage-service-cxx
--------------
Service not running
So when I test the distributed version, I will show that my RPC call failed in the log.

How do you generate the ""processor"" folder, I can't find this folder in my ""python"" directory.
How can I successfully generate the ""processor"" folder?
thank you very much!> Hi:
>       I encountered some problems when deploying, I hope to discuss with you! The following is a description of my problem:
> 
> ## When I executed the ""sh services.sh all start"" command in the ""/data/projects/fate"" directory, I found the following problem:
> [INFO] processing: storage-service-cxx
> Service not running
> So when I test the distributed version, I will show that my RPC call failed in the log.
> 
> How do you generate the ""processor"" folder, I can't find this folder in my ""python"" directory.
> How can I successfully generate the ""processor"" folder?
> thank you very much!

1. processor.sh is a script in ""/data/projects/fate/python"" 
2. if storage-service-cxx failed to start, you can check log files under folder ""/data/projects/fate/storage-service-cxx/logs""  or you can try this: `cd /data/projects/fate/storage-service-cxx; make clean; make; sh service.sh start`",6,2019-07-08 16:12:44,2019-08-29 14:20:40,2019-08-22 06:50:08
https://github.com/FederatedAI/FATE/issues/278,[],About 'auto - deploy. sh' and 'configurations.sh',"About 'auto - deploy. sh' and 'configurations.sh'**Is your feature request related to a problem? Please describe.**
According to the introduction, modifying the configuration items corresponding to the configurations.sh file according to the configuration, before execute the auto-packaging.sh(or auto-deploy.sh) script,like this:
partyA.id | Indicates the partyid of partyA
A.MS-ip | Indicates the node where the meta-Service module of partyA is located.
A.F-ip | Indicates the ip of the node where the federation module of partyA is located.
A.P-ip | Indicates the ip of the node where partyA's Proxy module is located.
A.R-ip | Indicates the ip of the node where the party module's Roll module is located.
A.TM-ip | Indicates the server ipexchange where the partyA's Task-manager is located.
A.S-ip | Indicates the server ip of the partyA's Serving-server. If there are multiple, the number is incremented.
A.E-ip | Indicates the server ip of partyA's Egg. If there are more than one, add the number increment.
exchangeip | Exchange server ip

It must be mentioned that,I only make cluster-deploy on one computer.Here's how I modified it:
#!/bin/bash
user=app
dir=/data/projects/fate
mysqldir=/data/projects/common/mysql/mysql-8.0
javadir=/data/projects/common/jdk/jdk1.8
venvdir=/data/projects/fate/venv
redispass=fate1234
partylist=(10000)
JDBC0=(192.168.**.** A.dbname A.user A.password)
iplist=(192.168.**.** 192.168.**.** 192.168.**.** 192.168.**.** 192.168.**.** 192.168.**.**)
fedlist0=(192.168.**.**)
meta0=(192.168.**.**)
proxy0=(192.168.**.**)
roll0=(192.168.**.**)
egglist0=(192.168.**.**)
tmlist0=(192.168.**.**)
serving0=(192.168.**.**)
exchangeip=exchangeip

**Describe alternatives you've considered**
Execute ”auto-deploy.sh“, after the changes mentioned above, the following statement appears:

app$: sh auto-deploy.sh
copy is ok!
The authenticity of host '192.168.**.**(192.168.**.**)' can't be established.
ECDSA key fingerprint is SHA256:*****
ECDSA key fingerprint is MD5:******
Are you sure youwant to continue connecting (yes/no)? yes
Warning:Permanently added '192.168.**.**'(ECDSA) to the list of known hosts.
Authentication failed.
lost conntecting
app@192.168.**.**'s password:******
cd /data/projects/fate
tar -zxf fate.tar
rm -rf fate.tar
exit
Logout
Connection to 192.168.**.** closed.
......
So，Is there any problem with my modification？
Thank you for your time!

> **Is your feature request related to a problem? Please describe.**
> According to the introduction, modifying the configuration items corresponding to the configurations.sh file according to the configuration, before execute the auto-packaging.sh(or auto-deploy.sh) script,like this:
> partyA.id | Indicates the partyid of partyA
> A.MS-ip | Indicates the node where the meta-Service module of partyA is located.
> A.F-ip | Indicates the ip of the node where the federation module of partyA is located.
> A.P-ip | Indicates the ip of the node where partyA's Proxy module is located.
> A.R-ip | Indicates the ip of the node where the party module's Roll module is located.
> A.TM-ip | Indicates the server ipexchange where the partyA's Task-manager is located.
> A.S-ip | Indicates the server ip of the partyA's Serving-server. If there are multiple, the number is incremented.
> A.E-ip | Indicates the server ip of partyA's Egg. If there are more than one, add the number increment.
> exchangeip | Exchange server ip
> 
> It must be mentioned that,I only make cluster-deploy on one computer.Here's how I modified it:
> #!/bin/bash
> user=app
> dir=/data/projects/fate
> mysqldir=/data/projects/common/mysql/mysql-8.0
> javadir=/data/projects/common/jdk/jdk1.8
> venvdir=/data/projects/fate/venv
> redispass=fate1234
> partylist=(10000)
> JDBC0=(192.168.**.** A.dbname A.user A.password)
> iplist=(192.168.**.** 192.168.**.** 192.168.**.** 192.168.**.** 192.168.**.** 192.168.**.**)
> fedlist0=(192.168.**.**)
> meta0=(192.168.**.**)
> proxy0=(192.168.**.**)
> roll0=(192.168.**.**)
> egglist0=(192.168.**.**)
> tmlist0=(192.168.**.**)
> serving0=(192.168.**.**)
> exchangeip=exchangeip
> 
> **Describe alternatives you've considered**
> Execute ”auto-deploy.sh“, after the changes mentioned above, the following statement appears:
> 
> app$: sh auto-deploy.sh
> copy is ok!
> The authenticity of host '192.168.**.**(192.168.**.**)' can't be established.
> ECDSA key fingerprint is SHA256:*****
> ECDSA key fingerprint is MD5:******
> Are you sure youwant to continue connecting (yes/no)? yes
> Warning:Permanently added '192.168.**.**'(ECDSA) to the list of known hosts.
> Authentication failed.
> lost conntecting
> app@192.168.**.**'s password:******
> cd /data/projects/fate
> tar -zxf fate.tar
> rm -rf fate.tar
> exit
> Logout
> Connection to 192.168.**.** closed.
> ......
> So，Is there any problem with my modification？
> Thank you for your time!

@Yuki-99 
Hello, there is no problem with your modification.The problem is the server of execute the ”auto-deploy.sh“ should has the right to login the deploy server('192.168.**.**') without encryption,even they are the same server.Only in this way the ”auto-deploy.sh“ can be execute fluently.",1,2019-07-08 09:52:22,2019-08-29 14:20:40,2019-08-22 06:45:40
https://github.com/FederatedAI/FATE/issues/272,[],"About the new dependency ""redis""","About the new dependency ""redis""Hi,

Could you please give a bried introduction about what does the redis works for?

Does it word as a monitor for task queue? If it does, what kind of task is in the queue?

Thank you very much!!Now redis in only used in fate-serving.Hi yangjie,

In the current version redis is only used by fate-serving. It is mainly used to cache the inference results of remote parties  to reduce the elapsed of federated inference. However, future redis will be used for other system.
Thanks.",2,2019-07-05 09:52:28,2019-07-11 14:08:57,2019-07-11 14:08:57
https://github.com/FederatedAI/FATE/issues/270,[],Data question of Hetero_lr ,"Data question of Hetero_lr Hello!
In hetero_lr, the data of breast_a and breast_b both have x0~x9 labels, are these labels consistent?Actually, x* in breast_a and breast_b including x0~x9 are the features of data, the label of this data is in breast_b with title ""y""thanks,  but I have another question.
In the prediction of hetero_lr, guest has x0~x9 features, host has x0~x19 features.
So, if we have a sample with x0~x19 features, but guest don't have all features, how should we predict?If the size of train_data's features is different from predict data's, it will cause a error. So you should make they same, for example, set the missing feature to 0, but actually it is not recommended to do so.",3,2019-07-05 07:47:41,2019-07-09 03:28:09,2019-07-09 03:28:09
https://github.com/FederatedAI/FATE/issues/267,[],Minimization testing issue,"Minimization testing issueWhen I just run this command 'sh run.sh host fast ', there is a stuck like 

role is host
task is fast
Traceback (most recent call last):
  File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 455, in <module>
    data_file)
  File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 137, in import_id_library
    stdout = exec_task(json_info, ""upload"", role)
  File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 72, in exec_task
    stdout = json.loads(stdout)
  File ""/home/mi/anaconda3/envs/FATE/lib/python3.6/json/__init__.py"", line 354, in loads
    return _default_decoder.decode(s)
  File ""/home/mi/anaconda3/envs/FATE/lib/python3.6/json/decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/home/mi/anaconda3/envs/FATE/lib/python3.6/json/decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
finish upload intersect data
{'file': '/data/projects/fate/python/examples/min_test_task/../data/breast_a.csv', 'head': 1, 'partition': 10, 'gen_table_info': True, 'local': {'party_id': 10000, 'role': 'host'}, 'role': {'guest': [9999], 'host': [10000], 'arbiter': [10000]}, 'data_type': 'train_input'}
Traceback (most recent call last):
  File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 458, in <module>
    data_file, data_type)
  File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 164, in upload
    stdout = exec_task(json_info, ""upload"", role)
  File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 72, in exec_task
    stdout = json.loads(stdout)
  File ""/home/mi/anaconda3/envs/FATE/lib/python3.6/json/__init__.py"", line 354, in loads
    return _default_decoder.decode(s)
  File ""/home/mi/anaconda3/envs/FATE/lib/python3.6/json/decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/home/mi/anaconda3/envs/FATE/lib/python3.6/json/decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
finish upload train data
{'file': '/data/projects/fate/python/examples/min_test_task/../data/breast_a.csv', 'head': 1, 'partition': 10, 'gen_table_info': True, 'local': {'party_id': 10000, 'role': 'host'}, 'role': {'guest': [9999], 'host': [10000], 'arbiter': [10000]}, 'data_type': 'predict_input'}
Traceback (most recent call last):
  File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 458, in <module>
    data_file, data_type)
  File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 164, in upload
    stdout = exec_task(json_info, ""upload"", role)
  File ""/data/projects/fate/python/examples/min_test_task/run_task.py"", line 72, in exec_task
    stdout = json.loads(stdout)
  File ""/home/mi/anaconda3/envs/FATE/lib/python3.6/json/__init__.py"", line 354, in loads
    return _default_decoder.decode(s)
  File ""/home/mi/anaconda3/envs/FATE/lib/python3.6/json/decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/home/mi/anaconda3/envs/FATE/lib/python3.6/json/decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
finish upload predict data
*********************
*******finish!*******


actually I can run successful for the stand alone testing but the minimization testing has this problem. Why is that, thank you. please check this ""ps -ef | grep task_manager"" to find if the server of task_manager is runningHave you solved this problem? I came across the same issue. @wwwwww1020",2,2019-07-02 07:59:27,2019-08-22 06:49:25,2019-08-22 06:49:25
https://github.com/FederatedAI/FATE/issues/266,[],bugs when install FATE,"bugs when install FATEThere are three questions summaried when install FATE framework(master branch).
1,  install_py3.sh
     pip install -r ./pip-dependencies/requirements.txt -f ./pip-dependencies  --no-index
     errors occures at this line,it seems that it could not find the specific version,when we delete  --no-index,errors disappears.
2,  install_mysql.sh
     this script installs mysql into the path ""/data/projects/common/mysql/mysql-8.0/"",but in next step ,FATE want user to change password,  command could be :
      /data/projects/common/mysql/mysql-8.0.13/bin/mysql -uroot -p -S/data/projects/common/mysql/mysql-8.0.13/mysql.sock
      but the directory mysql-8.0.13  does  not exists.
3,  After installing the mysql, user should change the password,but the password is  difficult to find, it is shown in the  process of installing,such as 
     [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: ******
     l hope  the hint could be more friendly.
     Thanks.@lytofd Hello, thank you for your question.
The answers for your three questions are as follows:
1. When using pip install to download Python dependent packages, ""-no-index"" means that you specify the form of offline installation of downloaded packages. If you install online, you do not need to use the way in install_py3.sh. Of course, it is feasible to delete ""-no-index"" manually, but when installing offline, ""-no-index"" It is necessary.
2. The MySQL path is needed to match the actual installation path, such as ""/ data/projects/common/mysql/mysql-8.0"", so the command can be / data/projects/common/common/mysql/common/mysql-8.0/mysql-8.0/bin/mysql-uroot-p-s/data/projects/common/mysql/mysql-8.0/mysql.sock. There is a slight deviation in the path of the example.
3. Finding passwords in Mysql's installation log is a bit of a hassle, but we can't do much because changing passwords requires you to customize them, but we'll still try to make the installation more automated in the script that takes MySQL into account.
Thank you for your detailed questions.",1,2019-07-01 08:22:29,2019-08-22 09:52:18,2019-08-22 09:52:18
https://github.com/FederatedAI/FATE/issues/262,[],input_data.mapValues() returns null result in cluster deploy mode,"input_data.mapValues() returns null result in cluster deploy mode**Describe the bug**
In line 101 of file `federatedml/util/data_io.py`, the input_data.mapValues(lambda ...) returns an empty `_DTable`. This function is supposed to split the str value of `input_data` as it does in standalone mode.

This line seems to publish tasks in the MySQL server and wait for `egg`s to finished the tasks. But when I turn to MySQL, the `task` table is empty. The `job_info` and `job_queue` table in database `task_manager` is also empty.

No error logs found in egg's and roll's log file. The `status_tracer_decorator.log` reports NoneType error (because of the empty `_DTable` returned by `mapValues(...)`)

**To Reproduce**
Steps to reproduce the behavior:
1. Deploy all services as suggested in the cluster-deploy instructions.
2. Breakpoint debuging `hetero_host_workflow.py`, step into `gen_data_instance` then step into `reader.read_data`
3. After loading input data in line 79 of `federatedml/util/data_io.py`, you can find that the input data are already loaded in `input_data`.
4. After line 101 `input_data_features = input_data.mapValues(...)`, evaluate `list(input_data_features.collect())` and you can find an empty result, while evaluating this to the `input_data` instance you can find the `input_data` is filled with values.

**Expected behavior**
It is expected to show the splited values as it was in standalone deployment mode.

**Desktop (please complete the following information):**
 - OS: Ubuntu server 18.04
 - Python 3.6
 - grpc 1.19

I am wondering where can I locate the bug and how can I fix it. Thanks!

> status_tracer_decorator.log

```
""2019-06-28 18:24:36,877 - status_tracer_decorator.py[line:74] - INFO: Traceback (most recent call last):
  File ""/home/FATE/workflow/status_tracer_decorator.py"", line 69, in wrapper
    res = func(self, *args, **kwargs)
  File ""/home/FATE/workflow/workflow.py"", line 690, in run
    self.workflow_param.train_input_namespace)
  File ""/home/FATE/workflow/workflow.py"", line 611, in gen_data_instance
    mode=mode)
  File ""/home/FATE/federatedml/util/data_io.py"", line 104, in read_data
    data_instance = self.fit(input_data_features, input_data_labels, table_name, namespace)
  File ""/home/FATE/federatedml/util/data_io.py"", line 113, in fit
    input_data_features = self.fill_missing_value(input_data_features, ""fit"")
  File ""/home/FATE/federatedml/util/data_io.py"", line 136, in fill_missing_value
    replace_value=self.default_value)
  File ""/home/FATE/federatedml/feature/imputer.py"", line 320, in fit
    process_data, replace_value = self.__replace(data, replace_method, replace_value, output_format)
  File ""/home/FATE/federatedml/feature/imputer.py"", line 277, in __replace
    replace_value = [replace_value for _ in range(shape)]
TypeError: 'NoneType' object cannot be interpreted as an integer
""
```
Can you please give more information about roll logs ? The logs are in roll/logs/fate-roll.logThanks for replying, sure thing, the roll log is attached as below:

[fate-roll.log](https://github.com/WeBankFinTech/FATE/files/3339123/fate-roll.log)

Thanks, we need another log to locate the error,  roll/logs/error.log,  sorry to trouble you again.When a task is started, the error.log remains empty until it exits. And it remains empty for a while after the task exit. Some WARNINGs appear in error.log about 20 minites after the task exits. (Seems not caused by the task itself)

The error.log is as follow:

```
Jun 28, 2019 10:42:36 AM io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler$3 onGoAwayReceived
WARNING: Received GOAWAY with ENHANCE_YOUR_CALM. Debug data: {1}
Jun 28, 2019 10:42:36 AM io.grpc.internal.AtomicBackoff$State backoff
WARNING: Increased keepalive time nanos to 720,000,000,000
Jun 28, 2019 10:42:36 AM io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler$3 onGoAwayReceived
WARNING: Received GOAWAY with ENHANCE_YOUR_CALM. Debug data: {1}
Jun 28, 2019 10:42:36 AM io.grpc.internal.AtomicBackoff$State backoff
WARNING: Increased keepalive time nanos to 720,000,000,000
Jun 28, 2019 10:48:36 AM io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler$3 onGoAwayReceived
WARNING: Received GOAWAY with ENHANCE_YOUR_CALM. Debug data: {1}
Jun 28, 2019 10:48:36 AM io.grpc.internal.AtomicBackoff$State backoff
WARNING: Increased keepalive time nanos to 720,000,000,000
Jun 28, 2019 10:48:36 AM io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler$3 onGoAwayReceived
WARNING: Received GOAWAY with ENHANCE_YOUR_CALM. Debug data: {1}
Jun 28, 2019 10:48:36 AM io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler$3 onGoAwayReceived
WARNING: Received GOAWAY with ENHANCE_YOUR_CALM. Debug data: {1}
Jun 28, 2019 10:48:36 AM io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler$3 onGoAwayReceived
WARNING: Received GOAWAY with ENHANCE_YOUR_CALM. Debug data: {1}
Jun 28, 2019 10:48:36 AM io.grpc.internal.AtomicBackoff$State backoff
WARNING: Increased keepalive time nanos to 720,000,000,000
Jun 28, 2019 10:48:36 AM io.grpc.internal.AtomicBackoff$State backoff
WARNING: Increased keepalive time nanos to 720,000,000,000
Jun 28, 2019 10:48:36 AM io.grpc.internal.AtomicBackoff$State backoff
WARNING: Increased keepalive time nanos to 720,000,000,000
Jun 28, 2019 10:48:36 AM io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler$3 onGoAwayReceived
WARNING: Received GOAWAY with ENHANCE_YOUR_CALM. Debug data: {1}
Jun 28, 2019 10:48:36 AM io.grpc.internal.AtomicBackoff$State backoff
WARNING: Increased keepalive time nanos to 720,000,000,000

```Thanks for your replying again, could you please offer some details about what happens after `input_data.mapValues(...)` called in cluster mode? I think it would help me better understand FATE. Thanks in advanced!And some additional info for your reference, all services have been started and standby in the background, as shown in the following:

```
(FATE) user@ubuntu-18-04:~$ ps -aux | grep java
user   27091  0.1  2.5 4640052 211284 ?      Sl   18:02   0:09 java -cp conf/:lib/*:fate-federation.jar com.webank.ai.fate.driver.Federation -c conf/federation.properties
user   27120  0.2  3.5 5315800 287520 ?      Sl   18:02   0:18 java -cp conf/:lib/*:fate-meta-service.jar com.webank.ai.fate.eggroll.MetaService -c conf/meta-service.properties
user   27155  0.1  2.9 5572048 237900 ?      Sl   18:02   0:09 java -cp conf/:lib/*:fate-egg.jar com.webank.ai.fate.eggroll.Egg -c conf/egg.properties
user   27189  0.2  3.4 5789276 282948 ?      Sl   18:02   0:16 java -cp conf/:lib/*:fate-roll.jar com.webank.ai.fate.eggroll.Roll -c conf/roll.properties
user   27231  0.1  1.7 4833004 142716 ?      Sl   18:02   0:07 java -cp conf/:lib/*:fate-proxy.jar com.webank.ai.fate.networking.Proxy -c conf/proxy.properties
user   31061  0.0  0.0  13136  1008 pts/0    S+   20:14   0:00 grep --color=auto java
(FATE) user@ubuntu-18-04:~$ netstat -tupln | grep java
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp6       0      0 :::8011                 :::*                    LISTEN      27189/java
tcp6       0      0 :::8590                 :::*                    LISTEN      27120/java
tcp6       0      0 :::7888                 :::*                    LISTEN      27155/java
tcp6       0      0 :::9394                 :::*                    LISTEN      27091/java
(FATE) user@ubuntu-18-04:~$ netstat -tupln | grep python
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 0.0.0.0:9380            0.0.0.0:*               LISTEN      27452/python
tcp6       0      0 :::9360                 :::*                    LISTEN      27452/python
tcp6       0      0 :::50001                :::*                    LISTEN      11293/python
tcp6       0      0 :::50002                :::*                    LISTEN      11307/python
tcp6       0      0 :::50003                :::*                    LISTEN      11309/python
tcp6       0      0 :::50004                :::*                    LISTEN      11311/python
```Can you provide the followings:
1. Running `ps aux | grep processor.py | grep -v grep` in terminal.
2. Open a new terminal and run the following script to test if `mapValues` is ok:

``` python
from arch.api import eggroll
eggroll.init('a', 1)
a = eggroll.parallelize(range(10), partition=2)
b = a.mapValues(lambda v:v+1)
list(b.collect())
```

3. Also, please check the following:

`lsof -i:7778`1. Running `ps aux ...`:

```
(FATE) user@ubuntu-18-04:~$ ps aux | grep processor.py | grep -v grep
user   11293  0.0  0.9 1028116 74436 ?       Sl   Jun27   0:21 python /home/user/FATE/python/arch/processor/processor.py -p 50001 -d /home/user/FATE/data-dir
user   11307  0.0  0.9 1028116 74444 ?       Sl   Jun27   0:20 python /home/user/FATE/python/arch/processor/processor.py -p 50002 -d /home/user/FATE/data-dir
user   11309  0.0  0.9 1028116 78016 ?       Sl   Jun27   0:20 python /home/user/FATE/python/arch/processor/processor.py -p 50003 -d /home/user/FATE/data-dir
user   11311  0.0  0.9 1028116 80748 ?       Sl   Jun27   0:21 python /home/user/FATE/python/arch/processor/processor.py -p 50004 -d /home/user/FATE/data-dir
```

2. Executing the python code returns an empty list [].

3. `lsof -i:7778`:

```
(FATE) user@ubuntu-18-04:~/FATE$ lsof -i:7778
COMMAND     PID   USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
java      27189 user  133u  IPv6 988801      0t0  TCP node1:41118->node1:7778 (ESTABLISHED)
storage-s 27267 user    5u  IPv6 948124      0t0  TCP *:7778 (LISTEN)
storage-s 27267 user    9u  IPv6 989793      0t0  TCP node1:7778->node1:41118 (ESTABLISHED)
```

And the `roll/logs/error.log` have nothing new, new logs in `roll/logs/fate-roll.log` are as follow:

```
[INFO ] 2019-06-28T12:22:43,979 [grpcServiceExecutor-19] [RollKvServiceImpl:113] - Kv.createIfAbsent request received. request: {""storageLocator"":{""type"":""LMDB"",""namespace"":""a"",""name"":""__federation__"",""fragment"":0},""fragmentCount"":10}
[INFO ] 2019-06-28T12:22:43,981 [grpcServiceExecutor-19] [BaseCrudClient:95] - [COMMON] invalid channel. status: SHUTDOWN
[INFO ] 2019-06-28T12:22:43,981 [pool-3-thread-15] [GrpcChannelFactory:79] - [COMMON][CHANNEL][REMOVE] removed for ip: node1, port: 7778, hostname: . reason: EXPIRED
[INFO ] 2019-06-28T12:22:43,982 [pool-3-thread-15] [GrpcChannelFactory:85] - [COMMON][CHANNEL][REMOVE] removed channel state: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:22:43,983 [pool-3-thread-15] [GrpcChannelFactory:108] - [COMMON][CHANNEL][CREATE] creating insecure channel for endpoint: ip: node1, port: 8590, hostname: 
[INFO ] 2019-06-28T12:22:43,983 [pool-3-thread-15] [GrpcChannelFactory:200] - [COMMON][CHANNEL][CREATE] creating channel to {""ip"":""node1"",""port"":8590,""hostname"":""""}, isSecure: false
[INFO ] 2019-06-28T12:22:43,984 [pool-3-thread-15] [GrpcChannelFactory:230] - [COMMON][CHANNEL][CREATE] created channel to {""ip"":""node1"",""port"":8590,""hostname"":""""}, isSecure: false
[INFO ] 2019-06-28T12:22:43,985 [grpcServiceExecutor-19] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:22:52,574 [grpcServiceExecutor-19] [RollKvServiceImpl:113] - Kv.createIfAbsent request received. request: {""storageLocator"":{""type"":""IN_MEMORY"",""namespace"":""a"",""name"":""735b7222-999f-11e9-b797-005056bbda3d"",""fragment"":0},""fragmentCount"":2}
[WARN ] 2019-06-28T12:22:52,600 [grpcServiceExecutor-19] [StoreInfo:45] - no fragment in store info
[INFO ] 2019-06-28T12:22:52,601 [grpcServiceExecutor-19] [RollKvServiceImpl:178] - Kv.putAll request received: [""com.webank.ai.fate.core.io.StoreInfo"",{""type"":""IN_MEMORY"",""nameSpace"":""a"",""tableName"":""735b7222-999f-11e9-b797-005056bbda3d""}]
[INFO ] 2019-06-28T12:22:52,601 [grpcServiceExecutor-19] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:22:52,602 [grpcServiceExecutor-19] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:22:52,602 [grpcServiceExecutor-19] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:22:52,625 [asyncThreadPool-49] [StorageServiceClient:120] - [ROLL][PUTALL][SUBTASK] putAll subTask request received: [""com.webank.ai.fate.core.io.StoreInfo"",{""type"":""IN_MEMORY"",""nameSpace"":""a"",""tableName"":""735b7222-999f-11e9-b797-005056bbda3d"",""fragment"":1}]
[INFO ] 2019-06-28T12:22:52,628 [pool-3-thread-15] [GrpcChannelFactory:108] - [COMMON][CHANNEL][CREATE] creating insecure channel for endpoint: ip: node1, port: 7778, hostname: 
[INFO ] 2019-06-28T12:22:52,629 [pool-3-thread-15] [GrpcChannelFactory:200] - [COMMON][CHANNEL][CREATE] creating channel to {""ip"":""node1"",""port"":7778,""hostname"":""""}, isSecure: false
[INFO ] 2019-06-28T12:22:52,629 [pool-3-thread-15] [GrpcChannelFactory:230] - [COMMON][CHANNEL][CREATE] created channel to {""ip"":""node1"",""port"":7778,""hostname"":""""}, isSecure: false
[INFO ] 2019-06-28T12:22:52,630 [asyncThreadPool-49] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:22:52,631 [asyncThreadPool-50] [StorageServiceClient:120] - [ROLL][PUTALL][SUBTASK] putAll subTask request received: [""com.webank.ai.fate.core.io.StoreInfo"",{""type"":""IN_MEMORY"",""nameSpace"":""a"",""tableName"":""735b7222-999f-11e9-b797-005056bbda3d"",""fragment"":0}]
[INFO ] 2019-06-28T12:22:52,634 [asyncThreadPool-50] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:22:52,637 [asyncThreadPool-50] [StorageKvPutAllRequestStreamProcessor:65] - [PUTALL][SUBTASK] actual completes putAll sub task. remaining: 0, entryCount: 5
[ERROR] 2019-06-28T12:22:52,646 [asyncThreadPool-50] [PutAllProcessorListenableFutureCallback:79] - [ROLL][KV][PUTALL][ONSUCCESS] put all success. storeInfo: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=735b7222-999f-11e9-b797-005056bbda3d, fragment=0), latch count: 2, address: node1:7778
[INFO ] 2019-06-28T12:22:52,682 [asyncThreadPool-49] [StorageKvPutAllRequestStreamProcessor:65] - [PUTALL][SUBTASK] actual completes putAll sub task. remaining: 0, entryCount: 5
[ERROR] 2019-06-28T12:22:52,691 [asyncThreadPool-49] [PutAllProcessorListenableFutureCallback:79] - [ROLL][KV][PUTALL][ONSUCCESS] put all success. storeInfo: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=735b7222-999f-11e9-b797-005056bbda3d, fragment=1), latch count: 1, address: node1:7778
[INFO ] 2019-06-28T12:22:52,692 [grpcServiceExecutor-19] [RollKvPutAllServerRequestStreamObserver:212] - [ROLL][KV][PUTALL] waiting put all to finish. storeInfo: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=735b7222-999f-11e9-b797-005056bbda3d, fragment=null), current latch count: 0, finished count: 2
[INFO ] 2019-06-28T12:22:52,692 [grpcServiceExecutor-19] [RollKvPutAllServerRequestStreamObserver:232] - [ROLL][PROCESS][PUTALL] put all completed. storeInfo: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=735b7222-999f-11e9-b797-005056bbda3d, fragment=null), totalCount: 10
[INFO ] 2019-06-28T12:23:00,506 [grpcServiceExecutor-19] [RollProcessServiceImpl:113] - [ROLL][PROCESS][MapValues] request received: {""info"":{""task_id"":""a"",""function_id"":""7815d028-999f-11e9-b797-005056bbda3d"",""function_bytes"":""gASVEgEAAAAAAACMGmFyY2guYXBpLnV0aWxzLmNsb3VkcGlja2xllIwOX2ZpbGxfZnVuY3Rpb26Uk5QoaACMD19tYWtlX3NrZWxfZnVuY5STlGgAjA1fYnVpbHRpbl90eXBllJOUjAhDb2RlVHlwZZSFlFKUKEsBSwBLAUsCS0NDCHwAZAEXAFMAlE5LAYaUKYwBdpSFlIwHPHN0ZGluPpSMCDxsYW1iZGE+lEsBQwCUKSl0lFKUSv////99lIeUUpR9lCiMB2dsb2JhbHOUfZSMCGRlZmF1bHRzlE6MBGRpY3SUfZSMBm1vZHVsZZSMCF9fbWFpbl9flIwOY2xvc3VyZV92YWx1ZXOUTowIcXVhbG5hbWWUaA91dFIu""},""operand"":{""type"":""IN_MEMORY"",""namespace"":""a"",""name"":""735b7222-999f-11e9-b797-005056bbda3d"",""fragment"":0}}
[INFO ] 2019-06-28T12:23:00,507 [grpcServiceExecutor-19] [RollProcessServiceImpl:238] - [ROLL][PROCESS][ProcessServiceTemplate] requestStorageLocator: {""type"":""IN_MEMORY"",""namespace"":""a"",""name"":""735b7222-999f-11e9-b797-005056bbda3d"",""fragment"":0}
[INFO ] 2019-06-28T12:23:00,507 [grpcServiceExecutor-19] [BaseCrudClient:95] - [COMMON] invalid channel. status: SHUTDOWN
[INFO ] 2019-06-28T12:23:00,508 [grpcServiceExecutor-19] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:00,513 [grpcServiceExecutor-19] [BaseCrudClient:95] - [COMMON] invalid channel. status: SHUTDOWN
[INFO ] 2019-06-28T12:23:00,513 [grpcServiceExecutor-19] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:00,523 [grpcServiceExecutor-19] [RollProcessServiceImpl:268] - [ROLL][PROCESS][ProcessServiceTemplate] storeInfoWithFragment: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=735b7222-999f-11e9-b797-005056bbda3d, fragment=0), fragmentNodeId: 4, storageNode: Node [Hash = -1659922994, nodeId=4, host=null, ip=node1, port=7778, type=STORAGE, status=HEALTHY, lastHeartbeatAt=null, createdAt=Fri Jun 28 16:37:21 UTC 2019, updatedAt=Fri Jun 28 16:37:21 UTC 2019], target: node1
[INFO ] 2019-06-28T12:23:00,527 [pool-3-thread-15] [GrpcChannelFactory:108] - [COMMON][CHANNEL][CREATE] creating insecure channel for endpoint: ip: node1, port: 7888, hostname: 
[INFO ] 2019-06-28T12:23:00,527 [pool-3-thread-15] [GrpcChannelFactory:200] - [COMMON][CHANNEL][CREATE] creating channel to {""ip"":""node1"",""port"":7888,""hostname"":""""}, isSecure: false
[INFO ] 2019-06-28T12:23:00,528 [pool-3-thread-15] [GrpcChannelFactory:230] - [COMMON][CHANNEL][CREATE] created channel to {""ip"":""node1"",""port"":7888,""hostname"":""""}, isSecure: false
[INFO ] 2019-06-28T12:23:00,530 [grpcServiceExecutor-19] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:00,542 [grpcServiceExecutor-19] [RollProcessServiceImpl:268] - [ROLL][PROCESS][ProcessServiceTemplate] storeInfoWithFragment: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=735b7222-999f-11e9-b797-005056bbda3d, fragment=1), fragmentNodeId: 4, storageNode: Node [Hash = -1659922994, nodeId=4, host=null, ip=node1, port=7778, type=STORAGE, status=HEALTHY, lastHeartbeatAt=null, createdAt=Fri Jun 28 16:37:21 UTC 2019, updatedAt=Fri Jun 28 16:37:21 UTC 2019], target: node1
[INFO ] 2019-06-28T12:23:00,543 [pool-3-thread-15] [GrpcChannelFactory:79] - [COMMON][CHANNEL][REMOVE] removed for ip: 10.44.12.49, port: 50004, hostname: . reason: EXPIRED
[INFO ] 2019-06-28T12:23:00,543 [pool-3-thread-15] [GrpcChannelFactory:85] - [COMMON][CHANNEL][REMOVE] removed channel state: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:00,543 [pool-3-thread-15] [GrpcChannelFactory:79] - [COMMON][CHANNEL][REMOVE] removed for ip: 10.44.12.49, port: 50001, hostname: . reason: EXPIRED
[INFO ] 2019-06-28T12:23:00,543 [grpcServiceExecutor-19] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:00,543 [pool-3-thread-15] [GrpcChannelFactory:85] - [COMMON][CHANNEL][REMOVE] removed channel state: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:00,543 [pool-3-thread-15] [GrpcChannelFactory:79] - [COMMON][CHANNEL][REMOVE] removed for ip: 10.44.12.49, port: 50003, hostname: . reason: EXPIRED
[INFO ] 2019-06-28T12:23:00,543 [pool-3-thread-15] [GrpcChannelFactory:85] - [COMMON][CHANNEL][REMOVE] removed channel state: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:00,544 [pool-3-thread-15] [GrpcChannelFactory:108] - [COMMON][CHANNEL][CREATE] creating insecure channel for endpoint: ip: 10.44.12.49, port: 50004, hostname: 
[INFO ] 2019-06-28T12:23:00,544 [pool-3-thread-15] [GrpcChannelFactory:200] - [COMMON][CHANNEL][CREATE] creating channel to {""ip"":""10.44.12.49"",""port"":50004,""hostname"":""""}, isSecure: false
[INFO ] 2019-06-28T12:23:00,544 [pool-3-thread-15] [GrpcChannelFactory:230] - [COMMON][CHANNEL][CREATE] created channel to {""ip"":""10.44.12.49"",""port"":50004,""hostname"":""""}, isSecure: false
[INFO ] 2019-06-28T12:23:00,545 [asyncThreadPool-49] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:00,554 [pool-3-thread-15] [GrpcChannelFactory:108] - [COMMON][CHANNEL][CREATE] creating insecure channel for endpoint: ip: 10.44.12.49, port: 50001, hostname: 
[INFO ] 2019-06-28T12:23:00,555 [pool-3-thread-15] [GrpcChannelFactory:200] - [COMMON][CHANNEL][CREATE] creating channel to {""ip"":""10.44.12.49"",""port"":50001,""hostname"":""""}, isSecure: false
[INFO ] 2019-06-28T12:23:00,555 [pool-3-thread-15] [GrpcChannelFactory:230] - [COMMON][CHANNEL][CREATE] created channel to {""ip"":""10.44.12.49"",""port"":50001,""hostname"":""""}, isSecure: false
[INFO ] 2019-06-28T12:23:00,557 [asyncThreadPool-49] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:00,562 [grpcServiceExecutor-19] [RollProcessServiceImpl:312] - [ROLL][PROCESS][ProcessServiceTemplate] valid result. ready to return storeInfo: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=735b7222-999f-11e9-b797-005056bbda3d, fragment=0), processType: MapValuesServiceProcessor
[INFO ] 2019-06-28T12:23:00,562 [grpcServiceExecutor-19] [ProcessServiceStorageLocatorResultHandler:41] - [HANDLE][RESULT] {""type"":""IN_MEMORY"",""namespace"":""a"",""name"":""7815d028-999f-11e9-b797-005056bbda3d"",""fragment"":0}, unprocessedResults.size: 2
[INFO ] 2019-06-28T12:23:00,564 [grpcServiceExecutor-19] [RollKvServiceImpl:113] - Kv.createIfAbsent request received. request: {""storageLocator"":{""type"":""IN_MEMORY"",""namespace"":""a"",""name"":""7815d028-999f-11e9-b797-005056bbda3d"",""fragment"":0},""fragmentCount"":2}
[WARN ] 2019-06-28T12:23:11,178 [grpcServiceExecutor-19] [StoreInfo:45] - no fragment in store info
[INFO ] 2019-06-28T12:23:11,179 [grpcServiceExecutor-19] [RollKvServiceImpl:230] - Kv.iterate request received: [""com.webank.ai.fate.core.io.StoreInfo"",{""type"":""IN_MEMORY"",""nameSpace"":""a"",""tableName"":""7815d028-999f-11e9-b797-005056bbda3d""}]
[INFO ] 2019-06-28T12:23:11,180 [grpcServiceExecutor-19] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:11,180 [grpcServiceExecutor-19] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:11,192 [asyncThreadPool-49] [IterateProcessor:123] - [ROLL][ITERATOR][PROCESSOR] final minChunkSize: 4194304
[INFO ] 2019-06-28T12:23:11,193 [asyncThreadPool-49] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:11,195 [asyncThreadPool-49] [IterateProcessor:270] - [ROLL][KV][ITERATE][PROCESSOR] waiting latch for: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=7815d028-999f-11e9-b797-005056bbda3d, fragment=0)
[INFO ] 2019-06-28T12:23:11,195 [asyncThreadPool-49] [IterateProcessor:275] - [ROLL][KV][ITERATE][PROCESSOR] broker: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=7815d028-999f-11e9-b797-005056bbda3d, fragment=0), closable: true, ready: false
[INFO ] 2019-06-28T12:23:11,196 [asyncThreadPool-49] [IterateProcessor:284] - [ROLL][KV][ITERATE][PROCESSOR] data arrived. size: 0, fragment order: 0, node address: node1:7778, range start: , range end: 
[INFO ] 2019-06-28T12:23:11,196 [asyncThreadPool-49] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-28T12:23:11,197 [asyncThreadPool-49] [IterateProcessor:270] - [ROLL][KV][ITERATE][PROCESSOR] waiting latch for: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=7815d028-999f-11e9-b797-005056bbda3d, fragment=1)
[INFO ] 2019-06-28T12:23:11,198 [asyncThreadPool-49] [IterateProcessor:275] - [ROLL][KV][ITERATE][PROCESSOR] broker: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=7815d028-999f-11e9-b797-005056bbda3d, fragment=1), closable: true, ready: false
[INFO ] 2019-06-28T12:23:11,198 [asyncThreadPool-49] [IterateProcessor:284] - [ROLL][KV][ITERATE][PROCESSOR] data arrived. size: 0, fragment order: 1, node address: node1:7778, range start: , range end: 
[INFO ] 2019-06-28T12:23:11,198 [asyncThreadPool-49] [RollKvServiceImpl:249] - [ROLL][KV][ITERATE] finished without error. storeInfo: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=7815d028-999f-11e9-b797-005056bbda3d, fragment=null), request: {""start"":"""",""end"":"""",""minChunkSize"":""0""}
[INFO ] 2019-06-28T12:23:11,198 [grpcServiceExecutor-19] [RollKvServiceImpl:283] - [ROLL][KV][ITERATE] roll iterate successfully. totalIterated: 0, storeInfo: StoreInfo(type=IN_MEMORY, nameSpace=a, tableName=7815d028-999f-11e9-b797-005056bbda3d, fragment=null), request: {""start"":"""",""end"":"""",""minChunkSize"":""0""}

```Duplicate of #276",9,2019-06-28 10:54:27,2019-07-08 03:43:12,2019-07-08 03:43:12
https://github.com/FederatedAI/FATE/issues/257,[],errors occures when  we use FATE,"errors occures when  we use FATEwe  installs crn on a  ubuntu server,to fasten  trainning speed on FATE,but error occures in grpc 
      Failed to set TCP_NODELAY
https://github.com/grpc/grpc/blob/3891e03ece195810a521ec379a512bdc2e4262cd/src/core/lib/iomgr/socket_utils_common_posix.cc#L214
this link is the source code of grpc  ,we trace this error on line 221, to prove our idea, l write a simple client-server model, after create a socket, l insert a piece of code which is :
      int low_lengcy = 1;
      int val = (low_lengcy!=0);
      int newval;
      socklen_t intlen = sizeof(newval);
      if (0 != setsockopt(sockfd, IPPROTO_TCP, TCP_NODELAY, &val, sizeof(val)))
    {
      printf(""设置失败"");
      exit(1);
    }
     if (0 != getsockopt(sockfd, IPPROTO_TCP, TCP_NODELAY, &newval, &intlen))
    {
      printf(""获取属性失败"");
      exit(1);
    }
    if ((newval != 0) != val)
    {
     printf(""tcp设置失败"");
    }
  when we installs crn, error occures, after remove crn, error disappears.
 
Would you please provide more details about crn? Thanks.",1,2019-06-24 12:57:36,2019-06-26 02:01:25,2019-06-26 02:01:25
https://github.com/FederatedAI/FATE/issues/256,[],General cross validation module,"General cross validation module**Is your feature request related to a problem? Please describe.**
Make a general cross validation module so that every other modules can use it to process cv. Already finished",1,2019-06-24 09:18:59,2019-07-23 08:03:48,2019-07-23 08:03:48
https://github.com/FederatedAI/FATE/issues/255,[],Connection failed between parties after rebooting the server,"Connection failed between parties after rebooting the serverWe deployed FATE in our servers on Alibaba Cloud. While there are some connection problems after system rebooting. The bug can not be reproduced stably. And the error information is shown as follow: 

debug_error_string = ""{""created"":""@1560843215.697088846"",""description"":""Failed to create subchannel"",""file"":""src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":2721,""referenced_errors"":[{""created"":""@1560843215.697086805"",""description"":""Pick Cancelled"",""file"":""src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":241,""referenced_errors"":[{""created"":""@1560843215.697078506"",""description"":""Connect Failed"",""file"":""src/core/ext/filters/client_channel/subchannel.cc"",""file_line"":689,""grpc_status"":14,""referenced_errors"":[{""created"":""@1560843215.697076865"",""description"":""OS Error"",""errno"":97,""file"":""src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":373,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::1]:8011""}]}]}]}""@lytofd Did you manage to solve this issue? I'm facing the same problem.",1,2019-06-24 08:34:28,2020-06-10 13:34:13,2019-11-05 06:13:14
https://github.com/FederatedAI/FATE/issues/254,[],[cluster version]toy example failed,"[cluster version]toy example failed**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. in host party:
    cd /data/projects/fate/python/examples/toy_example;
    bash run_toy_example_cluster.sh host test_0624_1 11 9;
2. in guest party:
    cd /data/projects/fate/python/examples/toy_example;
    bash run_toy_example_cluster.sh guest test_0624_1 11 9;
3. error occured: 
    jobid is test_0624_1
Running...
Finish, time cost is 0.0453
Secure Add Example Task Is FAIL!!!

**Screenshots**
task log in guest:
![image](https://user-images.githubusercontent.com/30313245/59999252-18fbeb80-9694-11e9-82da-c3a2c9ddd03b.png)
task log in host:
![image](https://user-images.githubusercontent.com/30313245/59999296-34ff8d00-9694-11e9-8694-fc5c6b6101ec.png)


**error log in host**
""2019-06-24 15:07:55,561 - status_tracer_decorator.py[line:73] - INFO: job status is failed""
""2019-06-24 15:07:55,561 - status_tracer_decorator.py[line:74] - INFO: Traceback (most recent call last):
  File ""/data/projects/fate/python/workflow/status_tracer_decorator.py"", line 69, in wrapper
    res = func(self, *args, **kwargs)
  File ""../../workflow/toy_example_workflow/secure_add_host_workflow.py"", line 66, in run
    self.secure_add_host_inst.run()
  File ""/data/projects/fate/python/federatedml/toy_example/secure_add_host.py"", line 68, in run
    self.secure()
  File ""/data/projects/fate/python/federatedml/toy_example/secure_add_host.py"", line 39, in secure
    y_shares = self.y.mapValues(self.share)
  File ""/data/projects/fate/python/arch/api/cluster/eggroll.py"", line 109, in mapValues
    return _EggRoll.get_instance().map_values(self, func)
  File ""/data/projects/fate/python/arch/api/cluster/eggroll.py"", line 302, in map_values
    resp = self.proc_stub.mapValues(unary_p)
  File ""/data/projects/common/miniconda3/lib/python3.6/site-packages/grpc/_channel.py"", line 533, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/data/projects/common/miniconda3/lib/python3.6/site-packages/grpc/_channel.py"", line 467, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:
	status = StatusCode.INTERNAL
	details = ""172.16.0.9:8011: com.webank.ai.fate.core.error.exception.MultipleRuntimeThrowables: error occured in sub tasks. processor type: MapValuesServiceProcessor
	at com.webank.ai.fate.eggroll.roll.api.grpc.server.RollProcessServiceImpl$ProcessServiceTemplate.run(RollProcessServiceImpl.java:324)
	at com.webank.ai.fate.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:51)
	at com.webank.ai.fate.eggroll.roll.api.grpc.server.RollProcessServiceImpl.mapValues(RollProcessServiceImpl.java:115)
	at com.webank.ai.fate.api.eggroll.processor.ProcessServiceGrpc$MethodHandlers.invoke(ProcessServiceGrpc.java:626)
	at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
	at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
	at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Throwable: idx: 0
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.unaryProcessToStorageLocatorUnaryCall(EggProcessServiceClient.java:146)
	at com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.mapValues(EggProcessServiceClient.java:57)
	at com.webank.ai.fate.eggroll.roll.service.async.processor.MapValuesServiceProcessor.call(MapValuesServiceProcessor.java:35)
	at com.webank.ai.fate.eggroll.roll.service.async.processor.MapValuesServiceProcessor.call(MapValuesServiceProcessor.java:26)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.reflect.InvocationTargetException
	at com.webank.ai.fate.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:155)
	at com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.unaryProcessToStorageLocatorUnaryCall(EggProcessServiceClient.java:144)
	... 7 more
Caused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
	at io.grpc.Status.asRuntimeException(Status.java:526)
	at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
	at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
	at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
	at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
	at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	... 3 more
Caused by: io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: /172.16.0.9:50001
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.grpc.netty.shaded.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
	at io.grpc.netty.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
	at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
	at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
	at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
	at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
	at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
	at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
Caused by: java.net.ConnectException: 拒绝连接
	... 11 more



	at com.webank.ai.fate.core.error.exception.MultipleRuntimeThrowables.<init>(MultipleRuntimeThrowables.java:32)
	... 12 more
""
	debug_error_string = ""{""created"":""@1561360075.560867908"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1017,""grpc_message"":""172.16.0.9:8011: com.webank.ai.fate.core.error.exception.MultipleRuntimeThrowables: error occured in sub tasks. processor type: MapValuesServiceProcessor\n\tat com.webank.ai.fate.eggroll.roll.api.grpc.server.RollProcessServiceImpl$ProcessServiceTemplate.run(RollProcessServiceImpl.java:324)\n\tat com.webank.ai.fate.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:51)\n\tat com.webank.ai.fate.eggroll.roll.api.grpc.server.RollProcessServiceImpl.mapValues(RollProcessServiceImpl.java:115)\n\tat com.webank.ai.fate.api.eggroll.processor.ProcessServiceGrpc$MethodHandlers.invoke(ProcessServiceGrpc.java:626)\n\tat io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)\n\tat io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)\n\tat io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)\n\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.Throwable: idx: 0\njava.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n\tat com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.unaryProcessToStorageLocatorUnaryCall(EggProcessServiceClient.java:146)\n\tat com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.mapValues(EggProcessServiceClient.java:57)\n\tat com.webank.ai.fate.eggroll.roll.service.async.processor.MapValuesServiceProcessor.call(MapValuesServiceProcessor.java:35)\n\tat com.webank.ai.fate.eggroll.roll.service.async.processor.MapValuesServiceProcessor.call(MapValuesServiceProcessor.java:26)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat com.webank.ai.fate.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:155)\n\tat com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.unaryProcessToStorageLocatorUnaryCall(EggProcessServiceClient.java:144)\n\t... 7 more\nCaused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\n\tat io.grpc.Status.asRuntimeException(Status.java:526)\n\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)\n\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)\n\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)\n\tat io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)\n\tat io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)\n\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)\n\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)\n\tat io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)\n\tat io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)\n\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)\n\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)\n\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\n\t... 3 more\nCaused by: io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AnnotatedConnectException: \u00e6\u008b\u0092\u00e7\u00bb\u009d\u00e8\u00bf\u009e\u00e6\u008e\u00a5: /172.16.0.9:50001\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.grpc.netty.shaded.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)\n\tat io.grpc.netty.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)\n\tat io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)\n\tat io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)\n\tat io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)\n\tat io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)\n\tat io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)\n\tat io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\t... 1 more\nCaused by: java.net.ConnectException: \u00e6\u008b\u0092\u00e7\u00bb\u009d\u00e8\u00bf\u009e\u00e6\u008e\u00a5\n\t... 11 more\n\n\n\n\tat com.webank.ai.fate.core.error.exception.MultipleRuntimeThrowables.<init>(MultipleRuntimeThrowables.java:32)\n\t... 12 more\n"",""grpc_status"":13}""
>
""
Please check the 50001 port on the 172.16.0.9 and check if the processor is successfully started.
You can refer to processor.sh in the cluster-deploy/example-dir-tree/python directory.
Thanks.grpc server failed to start because of python env. thx> grpc server failed to start because of python env. thx

I also have the same error, how to run grpc server correctly?",3,2019-06-24 07:28:37,2019-07-15 14:04:12,2019-06-25 06:23:50
https://github.com/FederatedAI/FATE/issues/253,[],Cluster version cannot load data,"Cluster version cannot load dataWhen I use this command 'python $FATE_install_path/arch/task_manager/task_manager_client.py -f upload -c conf/load_file_tm_guest.json', it always have exception like failed to establish a new connection: [Errno 111] Connection refused. How can I fix it. Thank you.



**Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/urllib3/connection.py"", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File ""/usr/local/lib/python3.6/site-packages/urllib3/util/connection.py"", line 80, in create_connection
    raise err
  File ""/usr/local/lib/python3.6/site-packages/urllib3/util/connection.py"", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen
    chunked=chunked)
  File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/local/lib/python3.6/http/client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/usr/local/lib/python3.6/http/client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/usr/local/lib/python3.6/http/client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/usr/local/lib/python3.6/http/client.py"", line 1026, in _send_output
    self.send(msg)
  File ""/usr/local/lib/python3.6/http/client.py"", line 964, in send
    self.connect()
  File ""/usr/local/lib/python3.6/site-packages/urllib3/connection.py"", line 181, in connect
    conn = self._new_conn()
  File ""/usr/local/lib/python3.6/site-packages/urllib3/connection.py"", line 168, in _new_conn
    self, ""Failed to establish a new connection: %s"" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fdfbd4d2b38>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send
    timeout=timeout
  File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 399, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=9380): Max retries exceeded with url: /data/upload (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fdfbd4d2b38>: Failed to establish a new connection: [Errno 111] Connection refused',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/fate/arch/task_manager/task_manager_client.py"", line 102, in <module>
    response = call_fun(args.function, config_data)
  File ""/fate/arch/task_manager/task_manager_client.py"", line 56, in call_fun
    response = requests.post(""/"".join([LOCAL_URL, ""data"", func]), json=config_data)
  File ""/usr/local/lib/python3.6/site-packages/requests/api.py"", line 116, in post
    return request('post', url, data=data, json=json, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/requests/api.py"", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=9380): Max retries exceeded with url: /data/upload (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fdfbd4d2b38>: Failed to establish a new connection: [Errno 111] Connection refused',))**
Hi, 
This seems to be that the task manager server is not started. 
Are you deploying with the automated deployment script provided by FATE? 
Please check if the task manager server is deployed completely. You can refer to the service.sh startup script which in arch/task_manager/. And also check that this configuration file is configured correctly.
Thanks.There is no communication for a long time, closed it.
Thank you.",2,2019-06-24 02:48:18,2019-07-14 11:58:29,2019-07-14 11:58:29
https://github.com/FederatedAI/FATE/issues/245,[],Stop when runing the homo LR,"Stop when runing the homo LRHi,

After successfully deploy 2 machine as 2 party. The homoLR example stop, the host is succeed to fetch the weight by federation.get(). However, when the ""host"" use federation.get() to fetch the ""converge_flag"" by 

###
converge_flag = federation.get(name=self.transfer_variable.converge_flag.name,
                                           tag=converge_flag_id,
                                           idx=0)
###

It is stuck

while as for the ""guest"", it stops to fetch the weight by

###
w = federation.get(name=self.transfer_variable.final_model.name,
                               tag=model_transfer_id,
                               idx=0)
###

Each ""fate-*.log"" file seems to be normal in the logs directory. Could you please help me that where can I check the probable error appear?

Thanks a lot~I find a exceptiong in the ""fate-federation.log"" which shows that

-----------------------------------------------

[INFO ] 2019-06-18T15:28:46,862 [transferJobSchedulerExecutor-17] [GrpcStubFactory:41] - [CORE] create stub. channel status: isShutdown: false, isTerminated: false
[INFO ] 2019-06-18T15:28:46,866 [grpcClientExecutor-9] [UnaryCallServerRequestStreamObserver:50] - UnaryCallServerRequestStreamObserver source streaming error: io.grpc.StatusRuntimeException: INTERNAL: HTTP/2 error code: INTERNAL_ERROR
Received Rst Stream
        at io.grpc.Status.asRuntimeException(Status.java:526)
        at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
        at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
        at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
        at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
        at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
        at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
        at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
        at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
        at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
        at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
        at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
        at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
        at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
        at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

[ERROR] 2019-06-18T15:28:46,866 [transferJobSchedulerExecutor-17] [SendProcessor:94] - java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at com.webank.ai.fate.driver.federation.transfer.api.grpc.client.ProxyClient.unaryCall(ProxyClient.java:130)
        at com.webank.ai.fate.driver.federation.transfer.api.grpc.client.ProxyClient.requestSendStart(ProxyClient.java:141)
        at com.webank.ai.fate.driver.federation.transfer.communication.processor.SendProcessor.run(SendProcessor.java:74)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
        at com.webank.ai.fate.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:155)
        at com.webank.ai.fate.driver.federation.transfer.api.grpc.client.ProxyClient.unaryCall(ProxyClient.java:128)
        ... 7 more
Caused by: io.grpc.StatusRuntimeException: INTERNAL: HTTP/2 error code: INTERNAL_ERROR
Received Rst Stream
-------------------------------

Could you please help me that which stage and reason can cause this exception, thank you very much",1,2019-06-18 05:48:16,2019-11-05 06:12:59,2019-11-05 06:12:59
https://github.com/FederatedAI/FATE/issues/238,"['bug', 'enhancement']",fix federatedml serving bug and add key hit rate statistics,"fix federatedml serving bug and add key hit rate statistics1. fix federatedml scale class and hetero-lr class double type convert
2. add key hit rate and input data utilization rateadd pr #240",1,2019-06-18 02:39:37,2019-06-18 09:57:40,2019-06-18 09:57:40
https://github.com/FederatedAI/FATE/issues/230,['bug'],arch/api/utils/donwload.py fails to deserialize custom objects using pickle,"arch/api/utils/donwload.py fails to deserialize custom objects using pickle**Describe the bug**
1. failed to download a dtable which had custom object by using arch/api/utils/donwload.py
2. copy download.py to another directory can work
3. the download script in arch/api/utils will use the pickle.py in the same directory instead of the native picklearch/api/utils/pickle.py is not used, which is said by eden . It can be deleted in next version",1,2019-06-16 10:23:19,2019-08-22 09:48:09,2019-08-22 09:48:09
https://github.com/FederatedAI/FATE/issues/229,[],Stoping when the eggroll.table is used,"Stoping when the eggroll.table is usedHi, 

When I am running the ""run_logistics_homo_cluster.sh"" as a host role.

The program stuck at the eggroll.init() when the RuntimeInstance.EGGROLL.table(""__federation__"", job_id, partition=10) which is exectly when the eggroll.table() is runing the self._create_table() which invoke the grpc client funcition self.kv_stub.createIfAbsent(create_table_info)

I check the ""self.kv_stub"" object creation that the self.channel has the correct port and host.

It seems like the request is sent and keep waiting for the response. And I didn't find the file which set up the KVServiceServicer.

Could you please tell me that what can lead to this problem and is the grpc server start up by some other files?

And if you can provide the module Readme file link for the module including the eggroll, federation and so on. It will really helpful.

Thank you very much!!I have check the instalment of grpc by running the helloworld.proto to create a simple server and client.i have the same problem, need help!!!!It seems like the server file cannot be found.Could you share your server_conf.json?
Normally you can find it at arch/conf/server_conf.json.The server_conf.json is something like that

{
""party_id"": ""1"",
""servers"": {
""manager"": {
""host"": ""192.172.0.199"",
""grpc.port"": 9360,
""http.port"": 9380
},
""servings"": [
""192.172.0.182:8001"",
"":8001""
],
""proxy"": {
""host"": ""192.172.0.199"",
""port"": 9370
},
""roll"": {
""host"": ""192.172.0.199"",
""port"": 8011
},
""federation"": {
""host"": ""192.172.0.199"",
""port"": 9394
}
}
}

And I find this conf json in the other server have all the same ip values, while in this server, there is a different value in the ""servings"". Does it affect?

Thank you!This looks like a configuration issue so we have to check on logs to see what goes wrong.

Please check `fate-roll.log` on 192.172.0.199. You can `tail -f` it in one terminal, and run `eggroll.init(job_id, 1)` in another terminal to see if there is any error message in `fate-roll.log`, or what are the last messages before getting stuck. Thanks.

Oh, almost forgot. Serving module does not affect eggroll.",6,2019-06-14 08:06:00,2019-06-18 07:15:17,2019-06-18 07:15:17
https://github.com/FederatedAI/FATE/issues/227,[],Quantile point query in statistic module,"Quantile point query in statistic module**Is your feature request related to a problem? Please describe.**
Add quantile point query for statistic module.


**Describe the solution you'd like**
Add an api for that
Already finished",1,2019-06-13 08:24:54,2019-07-23 08:04:13,2019-07-23 08:04:13
https://github.com/FederatedAI/FATE/issues/226,[],"The post request in the ""workflow/status_tracer_decorator.py""","The post request in the ""workflow/status_tracer_decorator.py""Hello,

When running the ""run_toy_example_cluster.sh"" which will invoke the ""workflow/toy_example_workflow/secure_add_host_workflow.py "", the run() function inside the latter file is decorated by the status_tracer_decorator.status_trace which will invoke the call_back() function.

And this call_back() function will send a post request to a url ""http://192.168.0.100:9380/job/jobStatus/test0/host/0"" where ""test0"" is a jobid and the last ""0"" is the Partyid.

I am confused that which file is act to start this server service? As for the task-manager service, it is not associated with the jobid.the code is quite confusing...... I also encounter this problem!The url is used to post-back job status to task-manager and the task-manager use jobid to identify the unique job. What's more,  we use task-manager to start and control the jobs in real scenario, so jobs need to callback job status to task-manager to life cycle management of the job, status_tracer_decorator is also for this.
run_toy_example_cluster.sh or other run_$algorithm_cluster.sh scripts are just help us to be quickly familiar with Fate and for self-debugging too. If you want to know how to start task by task-manager,  have a look at this link: https://github.com/WeBankFinTech/FATE/tree/master/examples/task_manager_examples",2,2019-06-13 03:33:03,2019-06-17 12:41:41,2019-06-17 12:41:41
https://github.com/FederatedAI/FATE/issues/225,[],what's the difference between online and offline module,"what's the difference between online and offline moduleHello,

In the cluster-deploy directory README.md. Under the 4.3. Modify Configuration File there is a note says that 

*Note: tmipList and serving0, serving1 need to be configured only when online deployment is required, and configuration is not required only for offline deployment.*

Is the ""online"" means for the  cluster providing the online service? online represent online inference serving",1,2019-06-13 03:10:38,2019-06-17 12:41:55,2019-06-17 12:41:55
https://github.com/FederatedAI/FATE/issues/224,[],About the storage-service-cxx service.sh start,"About the storage-service-cxx service.sh startHello,

After running successfully the ""auto-deploy.sh"", we continue to start the service. And when running the ""service.sh"" in the /data/projects/fate as ""bash service.sh all start"" the storage-service-cxx is fail because of the 

""service.sh: line 34: ./storage-service: No such file or directory""

how should I change the command or Can I just start the service in the storage-service ""/data/projects/fate/storage-service/service.sh"" for **running the cluster deploy example**?

Thank you very much!Hi, I think the auto-deploy.sh have a certain fail to compile the “storage-service.o” and ""storage-service.cc"" in /data/projects/fate/storage-service-cxx/ to the executable file ""storage-service-0.2"".

And I didn't find any command in the auto-deploy.sh or auto-packaging.sh to make this file. Which command can generate the ""storage-service-0.2"" in the /data/projects/fate/storage-service-cxx/?

Thank you very much!What are the error messages when you cd into storage-service-cxx and make?@maxwong 

When I deploy 2 parties.

The latter party always can't directly compile the ""grpc"" casuing a lask of the executable file ""/data/projects/fate/storage-service-cxx/third_party/bin/grpc_cpp_plugin"".

As a result, in the ""/data/projects/fate/storage-service-cxx"" when execute ""make"" to compile to get the ""storage-service-0.2"" it will always raise up the error

##########
 DEPENDENCY ERROR

You don't have the grpc c++ protobuf plugin installed in your path.
Please install grpc. You can find it here:

   https://github.com/grpc/grpc

Here is what I get when trying to detect if you have the plugin:

which /data/projects/fate/storage-service-cxx/third_party/bin/grpc_cpp_plugin
Makefile:107: recipe for target 'system-check' failed
make: [system-check] Error 1 (ignored)
############

While the machine of partyA is deployed and compiled successfully with the ""/data/projects/fate/storage-service-cxx/third_party/bin/grpc_cpp_plugin""


Could you please tell me what can lead to the same deploy code have different compile result?


Could the difference between the version of Ubuntu in 2 machine cause this situation?@maxwong What if I couldn't successful install the storage-service-cxx, can It be replaced by the jave version ""service.sh"" inside the ""storage-service""? 

should both party use the ""storage-service""? Or can one of them start the storage-service-cxx""?

Really Thanks!!!!!!!!@maxwong Hi

If I can just cp the successfully installed ""/data/projects/fate/storage-service-cxx"" into the other party?

I think all it need is the executable files which can be used independently.

`grpc_cpp_plugin` is used in Makefile to generate grpc classes from .proto files. If 2 parties are using the same version of operating system (which has same versions of glibc and so on), it is possible that you can copy one binary file into another and use. Worth a shot.
`grpc_cpp_plugin` can be installed or be found when you have successfully made grpc (which is a third_party dependency of storage-service-cxx). You may want to use `find` command to locate it.

As for `storage-service` and `storage-service-cxx`, the former implementation is based on Java (using JNI). In concurrency situation it may crash. So we recommend users using the c++ implementation whenever possible.",7,2019-06-13 02:43:22,2019-08-22 10:06:04,2019-08-22 10:06:04
https://github.com/FederatedAI/FATE/issues/222,[],Question about the cluster Logistics Regression Example,"Question about the cluster Logistics Regression ExampleHi,

When running the homo LR, it will execute ""from arch.api import eggroll"" and inside the eggroll there is a syntax error in line 26.

def init(job_id=None, mode: WorkMode = WorkMode.STANDALONE):

does this definition want to set the mode default value to WorkMode.STANDALONE ?I am not sure if the "":"" is used to defined the data type of the ""mode""? 

But even it has this meaning, the python compiler will raise error when the "":"" inside the def sentence as ""SyntaxError: invalid syntax""we advise your python version is >= 3.6.
And you can search ""Type Hints"" about python on google. 
""https://sikasjc.github.io/2018/07/14/type-hint-in-python/ "" may be helpful for you @dylan-fan

I get the problem

When I source the venv/bin/activate, I find that when run ""python""  command, it will attend the python2.7 environment. I think it is the establishment in certain file causing this problem.

Can I newly create a venv pointing to python3? and pip install the requirement.txt in https://github.com/WeBankFinTech/FATE/blob/master/requirements.txt

Thanks a lot",3,2019-06-12 05:46:19,2019-06-14 13:05:27,2019-06-14 13:05:27
https://github.com/FederatedAI/FATE/issues/219,[],About the auto-deploy.sh -- grpc instalment,"About the auto-deploy.sh -- grpc instalment**Is your feature request related to a problem? Please describe.**

When the auto-deploy.sh comes to compile the grpc, it can attend the ""third_party/protobuf"", but there are no files inside and the following commands will get error;

'''
cd third_party/protobuf
./autogen.sh
./configure --prefix=$dir/third_party
'''

the printout of these three lines command is shown below;

[INSTALL] Installing grpc protoc plugins
[INSTALL] Installing root certificates
third_party/protobufdata/projects/fate/storage-service-cxx/third_party/grpc# cd  
d_party/protobuf# **./autogen.sh**cts/fate/storage-service-cxx/third_party/grpc/third
bash: ./autogen.sh: No such file or directory
d_party/protobuf# ./configure --prefix=/data/projects/fate/third_party/grpc/third
bash: ./configure: No such file or directory
d_party/protobuf# maketa/projects/fate/storage-service-cxx/third_party/grpc/third
make: *** No targets specified and no makefile found.  Stop.
d_party/protobuf# make checkjects/fate/storage-service-cxx/third_party/grpc/third
make: *** No rule to make target 'check'.  Stop.
d_party/protobuf# make installcts/fate/storage-service-cxx/third_party/grpc/third
make: *** No rule to make target 'install'.  Stop.
d_party/protobuf# cd /data/projects/fate/storage-service-cxx/ird_party/grpc/third
cts/fate/third_party/* ./third_party/e/storage-service-cxx# rsync -a /data/proje 
(base) root@lxy-PA:/data/projects/fate/storage-service-cxx# 



**Describe the solution you'd like**

Are there some matter with the auto-deploy.sh file? ThanksDid you execute ""git submodule update --init --recursive""?Chek if the folder ""third_party/protobuf"" is empty first.Hi, the instruction said that this command can be replaced by download 

""the list of glog-0.4.0、grpc-1.19.1、boost-1.68.0、lmdb-0.9.22、protobuf-3.6.1 yourself if conditions permit.""

By downloading this git respositories, the protobuf directory also have many files.
@paulbaogang
Hi, the instruction said that this command can be replaced by download

""the list of glog-0.4.0、grpc-1.19.1、boost-1.68.0、lmdb-0.9.22、protobuf-3.6.1 yourself if conditions permit.""

By downloading this git respositories, the protobuf directory also have many files.

Hello,I hava know your problem,when you download these modules,they also have many sub-modules.So the solution is execute ""git submodule update --init --recursive"" angin.You can try it.Ok, I get it. Thank you very much!!

@paulbaogang
And I have observed that the ""auto-deploy.sh"" will deploy both the <PartyA> and <PartyB>. In that case, is that correct that I can just run the ""auto-deploy.sh"" just unilaterally?

What's the exact difference between the unilateral deployment and bilateral deployment?

Hi @paulbaogang

When I am running to install the git submodule in ""grpc"" the submodule
'third_party/libFuzzer'
is failed due to the timeout. And I find that it is due to the ""Proxy"" as shown below

""fatal: unable to access 'https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer/': Failed to connect to chromium.googlesource.com port 443: Connection timed out""

I am not sure if I can download the 'https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer/' and move it to the ""third_party/libFuzzer"". Do you guys also meet this problem before?

Thank you very much> Ok, I get it. Thank you very much!!
> 
> @paulbaogang
> And I have observed that the ""auto-deploy.sh"" will deploy both the and . In that case, is that correct that I can just run the ""auto-deploy.sh"" just unilaterally?
> 
> What's the exact difference between the unilateral deployment and bilateral deployment?

Hello@YangjieZhou
Thank you for your question.
You can think of both sides of FATE as servers and clients. When you need to cooperate with servers, you can use one-sided deployment on both sides respectively. When you can deploy both servers and clients at the same time, you can use bilateral deployment.> Hi @paulbaogang
> 
> When I am running to install the git submodule in ""grpc"" the submodule
> 'third_party/libFuzzer'
> is failed due to the timeout. And I find that it is due to the ""Proxy"" as shown below
> 
> ""fatal: unable to access 'https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer/': Failed to connect to chromium.googlesource.com port 443: Connection timed out""
> 
> I am not sure if I can download the 'https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer/' and move it to the ""third_party/libFuzzer"". Do you guys also meet this problem before?
> 
> Thank you very much
I think there may be some problems with your download source. I suggest that you download it and execute ""git submodule update -- init -- recursive"" directly under the FATE directory.
> Hi @paulbaogang
> 
> When I am running to install the git submodule in ""grpc"" the submodule
> 'third_party/libFuzzer'
> is failed due to the timeout. And I find that it is due to the ""Proxy"" as shown below
> 
> ""fatal: unable to access 'https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer/': Failed to connect to chromium.googlesource.com port 443: Connection timed out""
> 
> I am not sure if I can download the 'https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer/' and move it to the ""third_party/libFuzzer"". Do you guys also meet this problem before?
> 
> Thank you very much

Hi @YangjieZhou 
This download source may have problems. You can try to download and put it in the corresponding directory by yourself, or I have downloaded packages that can be sent to you.@paulbaogang

Thank you for the detailed answer. As for the one-sided deployment on both sides respectively. 

* Should the configuration file in /data/projects/FATE/cluster-deploy/scripts/configurations.sh be the same in both sides?
* And If it shouldn't, this file in two sides should be reversed and which ""IP address"" should be regarded as the PartyA?
* should each sides both run the ""auto-deploy.sh"" for the unilateral deployment?

It is really kind of you thanks.
 > @paulbaogang
> 
> Thank you for the detailed answer. As for the one-sided deployment on both sides respectively.
> 
> * Should the configuration file in /data/projects/FATE/cluster-deploy/scripts/configurations.sh be the same in both sides?
> * And If it shouldn't, this file in two sides should be reversed and which ""IP address"" should be regarded as the PartyA?
> * should each sides both run the ""auto-deploy.sh"" for the unilateral deployment?
> 0
> It is really kind of you thanks.

Hello @YangjieZhou 
If you deploy only one side, the configuration file  in /data/projects/FATE/cluster-deploy/scripts/configurations.sh should have only one configuration, not two, just like Federation 0, without Federation 1. Similarly, if you need to deploy two parties, there should be Federation 0, Federation 1;if you need to deploy three parties, there should be Federation 0, Federation 1 and Federation 2. Like this.
Thank for you question.@YangjieZhou 

您好，我和你一样在研究，微众的在线学习，怎么运行，
测试了下
sh run_logistic_regression_cluster.sh guest $jobid $guestpartyid $hostpartyid $arbiterpartyid 方式
也是失败了的，失败原因也是数据交互的服务没运行起来，我仔细的查看了代码，也没有找到运行服务的程序。
看了下您问的问题，感觉您应该也在测同样的流程，想问问您，您成功了吗？
测试版的没成功的话，后来测试的auto-deploy 成功了吗？

非常期待您的回复！",12,2019-06-11 04:14:56,2019-08-22 09:56:26,2019-08-22 09:56:26
https://github.com/FederatedAI/FATE/issues/211,[],if not passed at ./federatedml/test/run_test.sh :34,"if not passed at ./federatedml/test/run_test.sh :34**Describe the bug**
In the file  ./federatedml/test/run_test.sh  
the 34 line should be like this: 
` 34         elif [ $file != _test.py$ ] && [ $1 != /test$ ] ; then
`
not as : 
    elif [[ $file =~ _test.py$ ]] && [[ $1 =~ /test$ ]]; then

**To Reproduce**
Steps to reproduce the behavior:
1.   sh ./federatedml/test/run_test.sh
Can the tell me more infomation like OS enviroment and other error message?  Here =~ operator is use to  regexp matching.Just use the ubuntu 16.04changed to use ubuntu 18.04",3,2019-06-07 11:02:12,2019-06-21 09:49:36,2019-06-21 09:49:36
https://github.com/FederatedAI/FATE/issues/202,[],"debconf: delaying package configuration, since apt-utils is not installed","debconf: delaying package configuration, since apt-utils is not installed**Describe the bug**
When I input 'sh build_standalone_docker.sh', few seconds later it will print that 'debconf: delaying package configuration, since apt-utils is not installed' .However I have installed apt-utils. Furthermore when I finish the install , I tried to run the example. There is an exception 'OSError: loading json file config from '/fate/arch/conf/server_conf.json' failed' . I check the file and find that  the conf doesn't exist. May I ask how to fix it. Thank you 
                                                                   


**Desktop (please complete the following information):**
 - OS: [Ubuntu]
 - Browser [chrome,]
Thank you very much for your feedback. This is an omission. We have submitted a PR to fix it. If it is more urgent, we can do this:

1.Delete the generated docker container and image.
2.pull the code.
3.Modify the build_standalone_docker.sh code and change ""git archive -o ./docker/standalone/fate.tar $(git rev-parse HEAD) arch/api federatedml workflow examples"" to ""git archive -o ./docker/standalone /fate.tar $(git rev-parse HEAD) arch federatedml workflow examples"".
4.Rebuild a docker.
Try again, thank youI try again and follow above steps, there is a new exception OSError: loading json file config from '/fate/host_runtime_conf' failed! what should I do now 
Thank you.",2,2019-06-04 06:27:47,2019-08-22 09:28:04,2019-08-22 09:28:03
https://github.com/FederatedAI/FATE/issues/193,[],Provide data transform in feature binning,"Provide data transform in feature binning**Is your feature request related to a problem? Please describe.**
After find out split points of a column, provide a data transform function to convert the binned column into its bin index。 In other words, provide a discretization function for data.

**Describe the solution you'd like**
Set this function in params and transfer base on the param setting. 
Already finished",1,2019-05-30 03:28:28,2019-07-23 08:04:50,2019-07-23 08:04:49
https://github.com/FederatedAI/FATE/issues/191,[],Lack of log infomation for feature selection.,"Lack of log infomation for feature selection.**Describe the bug**
Add some log information for feature selection for testing. Already finished ",1,2019-05-29 11:01:01,2019-06-11 01:47:18,2019-06-11 01:47:18
https://github.com/FederatedAI/FATE/issues/188,[],"seems 'remote' attribute is missing in the definition of the class RuntimeInstance in the file FATE/arch/api/__init__.py"" ","seems 'remote' attribute is missing in the definition of the class RuntimeInstance in the file FATE/arch/api/__init__.py""   File "".../PycharmProjects/FATE/arch/api/federation.py"", line 73, in remote
    return RuntimeInstance.FEDERATION.remote(obj=obj, name=name, tag=tag, role=role, idx=idx)
AttributeError: 'NoneType' object has no attribute 'remote'
Could you provide your initialization steps?

I suppose it should be something like this:
```python
from arch.api import eggroll
from arch.api import federation

eggroll.init(job_id, WORK_MODE)
federation.init(job_id, runtime_conf)
```

where job_id is a string and runtime_conf is a json or dict.I ran the sh ./examples/hetero_logistic_regression/run_logistic_regression_standalone.sh 
and got the error @maxwong 

Plus, we cannot see the log 
/Users/xiangni/PycharmProjects/FATE/examples/hetero_logistic_regression/../../logs/hetero_logistic_regression_example_standalone_20190531112937/workflow.log: No such file or directory

could you tell me where can we find the log?Hi @zazd could you provide some information on this issue?@cacoderquan is something wrong in nohup under the path:  ./examples/hetero_logistic_regression/",4,2019-05-27 12:52:27,2019-08-22 06:51:22,2019-08-22 06:51:22
https://github.com/FederatedAI/FATE/issues/184,[],The details of HomoLogisticRegression are different from `this paper`,"The details of HomoLogisticRegression are different from `this paper`**Describe the bug**
In this [paragraph](https://github.com/WeBankFinTech/FATE/tree/master/federatedml/logistic_regression#1-homogeneous-lr), it is described ""More details is available in this paper"". However, the implementation of HomoLR doesn't involve SecureAggregation, which is the major contribution of the paper this paragraph refers to. 

Is there something wrong?
Yes, there is something wrong in the attached paper. The paper showed before is a paper for Hetero-LR. This one[https://acmccs.github.io/papers/p1175-bonawitzA.pdf](url) is the correct one. However, we haven't realize all the mechanism metioned. The aggregation methods used can be treated as a simple version of the paper described. More secure aggregation methods will be available in the future. 

Thanks for your indication and sorry for your inconvenience. Thanks. If you've fixed it, we can close this issue.",2,2019-05-27 02:09:36,2019-06-14 13:18:56,2019-06-14 13:18:56
https://github.com/FederatedAI/FATE/issues/183,[],error in install requirement.txt,"error in install requirement.txt**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
when I install -r requirement.txt，it shows the following ERROR.


OS: [CentOS]
Browser [chrome]
![FedAIinstallerror1](https://user-images.githubusercontent.com/51027709/58372061-ce7b3800-7f4a-11e9-8a4a-568c258f89cb.PNG)
I am using CentOS and python 2.7, and I got the following error.
""ERROR: Could not find a version that satisfies the requirement ipython==6.5.0""FATE are supported by python 3.6 only now which has been shown in our deploy doc here: https://github.com/WeBankFinTech/FATE/blob/master/cluster-deploy/README.md#32-software-version-requirements.

Hope that is helpful for you.",3,2019-05-25 16:07:13,2019-05-27 04:36:25,2019-05-27 04:36:25
https://github.com/FederatedAI/FATE/issues/182,[],The Information of Deployment Environment is needed,"The Information of Deployment Environment is needed**Describe the bug**
the version of FATE and Deployment environment should be more clearly，where can I get these information？
![mmexport1558789476977](https://user-images.githubusercontent.com/50725045/58369747-5d7b5680-7f31-11e9-82b8-50c1cb39981b.jpg)
Thanks for asking. What is the version of your pip is using? For higher than version 18.1, this problem is supposed to disapear. 

However, pip version is also needed for deploy doc. Thanks for your suggestion.",1,2019-05-25 13:14:13,2019-05-27 04:36:40,2019-05-27 04:36:40
https://github.com/FederatedAI/FATE/issues/170,[],"While predict  one_vs_rest, the predict label type not match the input_data label type","While predict  one_vs_rest, the predict label type not match the input_data label typeIf do predict of one_vs_rest after load_model from protobuf, the labels(classifier) type will be string, 
but input_data type may not be string(int or others sometimes). This will result to the evaluation results to be abnormal.fix this problem with #171",1,2019-05-22 08:46:33,2019-05-22 08:52:19,2019-05-22 08:52:19
https://github.com/FederatedAI/FATE/issues/137,[],detailed supported documents wanted,"detailed supported documents wanted**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
where are the more detailed supported documents about how it works and what features it supports and how to use it.If you want to know the features which FATE support, you can read the RELEASE.md file.
In the example module,  we give some examples and documents which told how to use.
In the federatedml modules，we also add some READEME.md file in some algorithms sub-modules.
If you have any other confusions, please tell us.
thanksok, thanks.",3,2019-05-06 08:58:06,2019-05-10 07:47:46,2019-05-10 07:47:46
https://github.com/FederatedAI/FATE/issues/135,[],try to deploy cluster mode ,"try to deploy cluster mode **Describe the bug**
```
==> python/logs/processor-50002.log <==
Exception calling application: [Errno 2] No such file or directory
Traceback (most recent call last):
  File ""/data/projects/fate/python/arch/processor/processor.py"", line 57, in get_function
    return cloudpickle.loads(function_bytes)
  File ""/data/projects/fate/python/federatedml/toy_example/secure_add_host.py"", line 21, in <module>
    from federatedml.util.transfer_variable import SecureAddExampleTransferVariable
  File ""/data/projects/fate/python/federatedml/util/__init__.py"", line 21, in <module>
    from federatedml.util.param_checker import DataIOParamChecker
  File ""/data/projects/fate/python/federatedml/util/param_checker.py"", line 28, in <module>
    LOGGER = log_utils.getLogger()
  File ""/data/projects/fate/python/arch/api/utils/log_utils.py"", line 94, in getLogger
    frame = inspect.stack()[1]
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 1494, in stack
    return getouterframes(sys._getframe(1), context)
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 1471, in getouterframes
    frameinfo = (frame,) + getframeinfo(frame, context)
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 1441, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 725, in getmodule
    file = getabsfile(object, _filename)
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 709, in getabsfile
    return os.path.normcase(os.path.abspath(_filename))
  File ""/data/projects/fate/venv/lib/python3.6/posixpath.py"", line 376, in abspath
    cwd = os.getcwd()
FileNotFoundError: [Errno 2] No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/projects/fate/venv/lib/python3.6/site-packages/grpc/_server.py"", line 389, in _call_behavior
    return behavior(argument, context), True
  File ""/data/projects/fate/python/arch/processor/processor.py"", line 111, in mapValues
    _mapper, _serdes = self.get_function_and_serdes(task_info)
  File ""/data/projects/fate/python/arch/processor/processor.py"", line 233, in get_function_and_serdes
    return self.get_function(_function_bytes), self._serdes
  File ""/data/projects/fate/venv/lib/python3.6/site-packages/cachetools/__init__.py"", line 46, in wrapper
    v = func(*args, **kwargs)
  File ""/data/projects/fate/python/arch/processor/processor.py"", line 60, in get_function
    return pickle._loads(function_bytes)
  File ""/data/projects/common/miniconda3/lib/python3.6/pickle.py"", line 1558, in _loads
    encoding=encoding, errors=errors).load()
  File ""/data/projects/common/miniconda3/lib/python3.6/pickle.py"", line 1050, in load
    dispatch[key[0]](self)
  File ""/data/projects/common/miniconda3/lib/python3.6/pickle.py"", line 1347, in load_stack_global
    self.append(self.find_class(module, name))
  File ""/data/projects/common/miniconda3/lib/python3.6/pickle.py"", line 1388, in find_class
    __import__(module, level=0)
  File ""/data/projects/fate/python/federatedml/toy_example/secure_add_host.py"", line 21, in <module>
    from federatedml.util.transfer_variable import SecureAddExampleTransferVariable
  File ""/data/projects/fate/python/federatedml/util/__init__.py"", line 21, in <module>
    from federatedml.util.param_checker import DataIOParamChecker
  File ""/data/projects/fate/python/federatedml/util/param_checker.py"", line 28, in <module>
    LOGGER = log_utils.getLogger()
  File ""/data/projects/fate/python/arch/api/utils/log_utils.py"", line 94, in getLogger
    frame = inspect.stack()[1]
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 1494, in stack
    return getouterframes(sys._getframe(1), context)
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 1471, in getouterframes
    frameinfo = (frame,) + getframeinfo(frame, context)
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 1441, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 725, in getmodule
    file = getabsfile(object, _filename)
  File ""/data/projects/common/miniconda3/lib/python3.6/inspect.py"", line 709, in getabsfile
    return os.path.normcase(os.path.abspath(_filename))
  File ""/data/projects/fate/venv/lib/python3.6/posixpath.py"", line 376, in abspath
    cwd = os.getcwd()
FileNotFoundError: [Errno 2] No such file or directory

```Another excpetion is:

Tutorial: https://github.com/WeBankFinTech/FATE/tree/master/cluster-deploy
when I run 
```
export PYTHONPATH=/data/projects/fate/python 
source /data/projects/fate/venv/bin/activate
cd /data/projects/fate/python/examples/toy_example/
sh run_toy_examples_standalone.sh
``` 
 It shows success.

However I run
```
export PYTHONPATH=/data/projects/fate/python 
source /data/projects/fate/venv/bin/activate
cd /data/projects/fate/python/examples/toy_example/
 sh run_toy_example_cluster.sh host job_1 1000 999
sh run_toy_example_cluster.sh guest job_1 1000 999
``` 

```
(venv) [app@e1ff17e6162d fate]$ cd python/logs/
(venv) [app@e1ff17e6162d logs]$ ls
123  toy_example_20190429125615
(venv) [app@e1ff17e6162d logs]$ cat 
123/                        toy_example_20190429125615/ 
(venv) [app@e1ff17e6162d logs]$ cat 123/
eggroll.log                  status_tracer_decorator.log  
(venv) [app@e1ff17e6162d logs]$ cat 123/eggroll.log 
""2019-04-29 12:56:46,012 - eggroll.py[line:160] - DEBUG: created table: type:LMDB namespace:123 name:__federation__ partitions:10""
""2019-04-29 12:56:48,491 - eggroll.py[line:177] - DEBUG: created table: type:IN_MEMORY namespace:123 name:3e99101c-6a7e-11e9-b2e0-0242ac110002 partitions:1""
""2019-04-29 12:56:51,870 - eggroll.py[line:160] - DEBUG: created table: type:LMDB namespace:123 name:__federation__ partitions:10""
""2019-04-29 12:56:52,520 - eggroll.py[line:177] - DEBUG: created table: type:IN_MEMORY namespace:123 name:4216fbdc-6a7e-11e9-bb5a-0242ac110002 partitions:1""
(venv) [app@e1ff17e6162d logs]$ cat 123/status_tracer_decorator.log 
""2019-04-29 12:56:49,725 - status_tracer_decorator.py[line:73] - INFO: job status is failed""
""2019-04-29 12:56:49,726 - status_tracer_decorator.py[line:74] - INFO: Traceback (most recent call last):
  File ""/data/projects/fate/python/workflow/status_tracer_decorator.py"", line 69, in wrapper
    res = func(self, *args, **kwargs)
  File ""../../workflow/toy_example_workflow/secure_add_host_workflow.py"", line 66, in run
    self.secure_add_host_inst.run()
  File ""/data/projects/fate/python/federatedml/toy_example/secure_add_host.py"", line 68, in run
    self.secure()
  File ""/data/projects/fate/python/federatedml/toy_example/secure_add_host.py"", line 39, in secure
    y_shares = self.y.mapValues(self.share)
  File ""/data/projects/fate/python/arch/api/cluster/eggroll.py"", line 109, in mapValues
    return _EggRoll.get_instance().map_values(self, func)
  File ""/data/projects/fate/python/arch/api/cluster/eggroll.py"", line 302, in map_values
    resp = self.proc_stub.mapValues(unary_p)
  File ""/data/projects/fate/venv/lib/python3.6/site-packages/grpc/_channel.py"", line 533, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/data/projects/fate/venv/lib/python3.6/site-packages/grpc/_channel.py"", line 467, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:
	status = StatusCode.INTERNAL
	details = ""172.17.0.2:8011: com.webank.ai.fate.core.error.exception.MultipleRuntimeThrowables: error occured in sub tasks. processor type: MapValuesServiceProcessor
	at com.webank.ai.fate.eggroll.roll.api.grpc.server.RollProcessServiceImpl$ProcessServiceTemplate.run(RollProcessServiceImpl.java:324)
	at com.webank.ai.fate.core.api.grpc.server.GrpcServerWrapper.wrapGrpcServerRunnable(GrpcServerWrapper.java:51)
	at com.webank.ai.fate.eggroll.roll.api.grpc.server.RollProcessServiceImpl.mapValues(RollProcessServiceImpl.java:115)
	at com.webank.ai.fate.api.eggroll.processor.ProcessServiceGrpc$MethodHandlers.invoke(ProcessServiceGrpc.java:626)
	at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
	at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
	at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.Throwable: idx: 0
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.unaryProcessToStorageLocatorUnaryCall(EggProcessServiceClient.java:146)
	at com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.mapValues(EggProcessServiceClient.java:57)
	at com.webank.ai.fate.eggroll.roll.service.async.processor.MapValuesServiceProcessor.call(MapValuesServiceProcessor.java:35)
	at com.webank.ai.fate.eggroll.roll.service.async.processor.MapValuesServiceProcessor.call(MapValuesServiceProcessor.java:26)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at com.webank.ai.fate.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:153)
	at com.webank.ai.fate.eggroll.roll.api.grpc.client.EggProcessServiceClient.unaryProcessToStorageLocatorUnaryCall(EggProcessServiceClient.java:144)
	... 7 more
Caused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
	at io.grpc.Status.asRuntimeException(Status.java:526)
	at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
	at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
	at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
	at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
	at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	... 3 more
Caused by: io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.17.0.2:50001
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.grpc.netty.shaded.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
	at io.grpc.netty.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
	at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
	at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
	at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
	at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
	at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
	at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
Caused by: java.net.ConnectException: Connection refused
	... 11 more
```




See https://github.com/grpc/grpc/issues/12116",2,2019-04-29 09:49:50,2019-08-29 14:20:39,2019-04-29 14:48:20
https://github.com/FederatedAI/FATE/issues/107,[],Add doc in examples and main features. ,"Add doc in examples and main features. **Is your feature request related to a problem? Please describe.**
Add README doc in each examples and some feature directory so that users can use them easier. 

**Describe the solution you'd like**
Add doc
Has provided in version 0.2",1,2019-04-18 09:42:14,2019-04-19 06:17:06,2019-04-19 06:17:06
https://github.com/FederatedAI/FATE/issues/101,['bug'],hetero-lr minibatch bug,"hetero-lr minibatch bugThe hetero-lr loss converge depend on iter loss， but now in hetero-lr， it depend on batch loss. This means finishing one batch training, it will decide whether converge or not.close by Feature 0.2 fix bugs #100",1,2019-04-13 09:06:25,2019-04-13 11:51:27,2019-04-13 11:51:27
https://github.com/FederatedAI/FATE/issues/93,['bug'],accurate average gradient and loss,"accurate average gradient and loss**Is your feature request related to a problem? Please describe.**
current average algorithm is E = (s1/cnt1 + s2/cnt2 + ...)/partitions， not a really  mean

**Describe the solution you'd like**
E = (s1 + s2 + ...)/(cn1 + cnt2 + ...)


In fact, we evaluator two methods in some datasets. The results show that the two methods are very close. 
But your solution seems to be better mathematically, so we merged your pr.",1,2019-04-12 02:51:15,2019-08-29 14:20:38,2019-04-13 05:07:32
https://github.com/FederatedAI/FATE/issues/85,['bug'],Use clean up In status_tracer_decorator cause bug,"Use clean up In status_tracer_decorator cause bug**Describe the bug**
use clean up api in status tracer_decorator will cause bugs. Because in many situations, mostly standalone version, host or guest finish first will clean all job's data.


bug fix, delete cleanup api use in status_tracer_decorator",1,2019-04-08 08:58:30,2019-04-08 09:12:46,2019-04-08 09:11:42
https://github.com/FederatedAI/FATE/issues/79,[],SecureBoost online inference,"SecureBoost online inference**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
support secureboost online inference in fate-serving",1,2019-04-04 12:10:43,2019-11-05 06:11:39,2019-11-05 06:11:39
https://github.com/FederatedAI/FATE/issues/77,['bug'],Input data abnormal,"Input data abnormal**Describe the bug**
If input DTABLE of any component （lr， secureboost，feature eng ，etc） is empty（not any keys，or have keys but values are None） ， what happen？


Added two abnormal checking methods. It's avaible to check if a table has no value or count is 0. These situation will be treat as abnormal so that raise a ValueError.",1,2019-04-04 11:45:50,2019-08-29 14:20:38,2019-04-13 11:54:35
https://github.com/FederatedAI/FATE/issues/56,[],How to improve the efficiency of paillier or other homomorphic encryption algorithm?,"How to improve the efficiency of paillier or other homomorphic encryption algorithm?I evaluated the computational efficiency of paillier in Fate(denoted as paillier_fate) and paillier implemented in python(https://github.com/n1analytics/python-paillier , denoted as paillier_python), the  calculation efficiency of paillier_fate is more than six times higher than paillier_python. 

The implementations of the two algorithms are similar. So, i want to know how to improve the efficiency of paillier.  Are there any other homomorphic encryption libraries or implementation tips that can improve efficiency.we test our fate-paillier is same as paillier_python.   
paillier_python： generate_paillier_keypair(n_length=1024) 
 fate-paillier：   PaillierKeypair.generate_keypair(n_length=1024)  

Maybe n_length is not same in your testcaseThanks for your reply! @dylan-fan

1. It is indeed due to the n_length. Moreover, if n_length is set too samll, the decoding will fail. So, what is the smallest value of n_length in safe condition.

2. How much is the efficiency difference between an encrypted LR and an unencrypted LR?

3. Have you tried other homomorphic encryption librarys? such as HELib、SEAL、Pyfhel、FV-NFLib...?


> 
> 
> we test our fate-paillier is same as paillier_python.
> paillier_python： generate_paillier_keypair(n_length=1024)
> fate-paillier： PaillierKeypair.generate_keypair(n_length=1024)
> 
> Maybe n_length is not same in your testcase

Thanks for your reply! @dylan-fan

1. It is indeed due to the n_length. Moreover, if n_length is set too samll, the decoding will fail. So, what is the smallest value of n_length in safe condition.

2. How much is the efficiency difference between an encrypted LR and an unencrypted LR?

3. Have you tried other homomorphic encryption librarys? such as HELib、SEAL、Pyfhel、FV-NFLib...?
1.  n_length>=1024  maybe better
2. encrypted LR  is slower than unencrypted LR，but accuracy is similar. Efficiency  problem in encrypted LR can be resolved in distributed computing. You can run our lr example and see the result.
3. phe , google homomorphic encryption lib.Deep learning is very computationally intensive, and the use of homomorphic encryption will make the efficiency of training very low.  Could you tell me what homomorphic encryption libraries you used to implement the deep learning model, or what optimization techniques did you use on the paillier library ?",4,2019-03-28 11:10:19,2019-04-03 08:36:10,2019-04-03 08:36:10
https://github.com/FederatedAI/FATE/issues/55,"['bug', 'arch']",One-second latency after each transfer event,"One-second latency after each transfer eventNote that the Proxy.Packet (in arch/networking/proxy/src/main/java/com/webank/ai/fate/networking/proxy/grpc/client/DataTransferPipedClient.java line 96)do one extra read from the pipe which is already drained, and the read function set a timeout of 1 second to ensure the pipe has no packets. This may cause an one-second latency at the end of each transfer event. 

This can be solved by adding a ""isDrain"" check before polling packets from the pipe. 

I have tried to add a line of code ""if (isDrained()) return result;"" between line 66 and line 67 of PacketQueuePipe.java(in arch/networking/proxy/src/main/java/com/webank/ai/fate/networking/proxy/infra/impl/) , the training examples works properly and the latency is removed. 

Thanks for your feedback. We are looking into it.thanks，welcome to FATE contributormerge",3,2019-03-28 08:16:18,2019-03-29 03:18:41,2019-03-29 03:18:41
https://github.com/FederatedAI/FATE/issues/43,"['bug', 'arch']",ERROR in SendProcessor with limited number of CPUs,"ERROR in SendProcessor with limited number of CPUsWe rent multiple machines on google cloud for running the FATE project. In the beginning, we used machines with 2 vCPUs. While running the cluster code for hetero_logistic_regression, we get an Error from arbiter when sending public-keys to guest and host(console.log in federation):

[ERROR] 2019-03-18T07:11:15,361 [transferJobSchedulerExecutor-2] [SendProcessor:94] - java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
        at java.util.ArrayList.rangeCheck(ArrayList.java:657)
        at java.util.ArrayList.get(ArrayList.java:433)
        at com.webank.ai.fate.driver.federation.transfer.service.impl.DefaultProxySelectionService.select(DefaultProxySelectionService.java:81)
        at com.webank.ai.fate.driver.federation.transfer.communication.processor.SendProcessor.run(SendProcessor.java:70)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

[ERROR] 2019-03-18T07:11:24,396 [transferJobSchedulerExecutor-2] [GrpcChannelFactory:119] - [COMMON][CHANNEL][ERROR] Error getting ManagedChannel after retries
[ERROR] 2019-03-18T07:11:24,397 [transferJobSchedulerExecutor-2] [TransferJobScheduler:127] - [FEDERATION][SCHEDULER] processor failed: transferMetaId: cxz-HeteroLRTransferVariable.paillier_pubkey-HeteroLRTransferVariable.paillier_pubkey.0-2-arbiter-1-guest, exception: java.lang.RuntimeException: should never get here
        at com.webank.ai.fate.core.factory.GrpcStubFactory.createGrpcStub(GrpcStubFactory.java:47)
        at com.webank.ai.fate.core.factory.GrpcStubFactory.createGrpcStub(GrpcStubFactory.java:56)
        at com.webank.ai.fate.core.api.grpc.client.GrpcAsyncClientContext.createStub(GrpcAsyncClientContext.java:207)
        at com.webank.ai.fate.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpc(GrpcStreamingClientTemplate.java:106)
        at com.webank.ai.fate.core.api.grpc.client.GrpcStreamingClientTemplate.calleeStreamingRpcWithImmediateDelayedResult(GrpcStreamingClientTemplate.java:149)
        at com.webank.ai.fate.driver.federation.transfer.api.grpc.client.ProxyClient.unaryCall(ProxyClient.java:98)
        at com.webank.ai.fate.driver.federation.transfer.api.grpc.client.ProxyClient.requestSendEnd(ProxyClient.java:121)
        at com.webank.ai.fate.driver.federation.transfer.communication.processor.SendProcessor.run(SendProcessor.java:98)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

and we tried the same code using machines with 8 vCPUs, the ERROR cannot be reproduced. 

Here are the configurations of the two groups of machines: 
The former ones: Google Cloud n1-standard-2 machines with 2 vCPUs and 7.5GB RAM

The latter ones: Google Cloud n1-standard-8 machines with 8 vCPUs and 30GB RAM



Thanks for your feedback. We are looking into it.",1,2019-03-22 03:54:10,2019-11-05 06:12:29,2019-11-05 06:12:29
https://github.com/FederatedAI/FATE/issues/32,[],Discussions,"DiscussionsThis issue is specially opened for discussion. Here, you can feedback some problem such as install problem when you install FATE or new feature which you think is important for you. 
Finally， hope to use English to ask your questions.
Thanks.
dylanfanWhen you installed FATE ,  What was the worst experience?For some cooperative partners, docker for cluster is necessary, because they have to ask for more physical computers. This may cost more time and resources for them but just experience FATE they want.@dylan-fan 
    hi,can you give a sketch to explain the relationship between the various components in the cluster mode? I don't quite understand how to deploy a cluster.After installed the packages, I tried to run  ""sh run_logistic_regression_standalone.sh"". But it is extremely slow. it keeps printing something like 
cat: /fate/examples/homo_logistic_regression/../../logs/logistic_regression_example_standalone_20190528090656/workflow.log: No such file or directory
please wait or check more info in /fate/examples/homo_logistic_regression/../../logs/logistic_regression_example_standalone_20190528090656/workflow.log

In general, how lone does it takes to run the program? Is there any way to accelerate the process? Thanks. 
@cacoderquan It should be completed in one minute. If not, could you please check logs in this path: /fate/examples/homo_logistic_regression/../../logs/logistic_regression_example_standalone_20190528090656/   and see if there is a status_tracer_decorator.log file exist and see if there is error in it? Or, is there any error info in nohup file in your current path?@tanmc123 Thanks. But could you tell me how to go to /fate/examples/homo_logistic_regression/../../logs/logistic_regression_example_standalone_20190528090656/

I type cd examples/homo_logistic_regression/../../logs/logistic_regression_example_standalone_20190528090656/  but got errors like   

root@f94b7a1fdfe7:/fate# cd examples/homo_logistic_regression/../../logs/logistic_regression_example_standalone_20190528090656/
bash: cd: examples/homo_logistic_regression/../../logs/logistic_regression_example_standalone_20190528090656/: No such file or directory
r@cacoderquan well, in your current path, you can just type cd ../../logs/logistic_regression_example_standalone_20190528090656/Thanks. But I type cd ../../logs/hetero_logistic_regression_example_standalone_20190529012020/
it does not work(I use another container)

root@45c7dd1d05a1:/fate# cd ../../logs/hetero_logistic_regression_example_standalone_20190529012020/
bash: cd: ../../logs/hetero_logistic_regression_example_standalone_20190529012020/: No such file or directory


find there is a new logs directory, but could not find any results under it. 
root@45c7dd1d05a1:/fate# ls
Dockerfile  arch  data  examples  federatedml  logs  requirements.txt  workflow
root@45c7dd1d05a1:/fate# cd logs
root@45c7dd1d05a1:/fate/logs# ls
root@45c7dd1d05a1:/fate/logs# 
@tanmc123 You should be able to see three nohup files in your example/homo_logistic_regression folder. Please see if there is any error info in one of them.config path is /fate/examples/hetero_logistic_regression/conf/guest_runtime_conf.json_hetero_logistic_regression_example_standalone_20190529012020 
jobid is hetero_logistic_regression_example_standalone_20190529012020 
Traceback (most recent call last): 
  File ""/fate/arch/api/utils/file_utils.py"", line 40, in load_json_conf 
    with open(json_conf_path) as f: 
FileNotFoundError: [Errno 2] No such file or directory: '/fate/arch/conf/server_conf.json' 


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""../../workflow/hetero_lr_workflow/hetero_guest_workflow.py"", line 22, in <module>
    from workflow.workflow import WorkFlow
  File ""/fate/workflow/workflow.py"", line 52, in <module>
    from workflow import status_tracer_decorator
  File ""/fate/workflow/status_tracer_decorator.py"", line 30, in <module>
    server_conf = file_utils.load_json_conf(""arch/conf/server_conf.json"")
  File ""/usr/local/lib/python3.6/site-packages/cachetools/__init__.py"", line 46, in wrapper
    v = func(*args, **kwargs)
  File ""/fate/arch/api/utils/file_utils.py"", line 43, in load_json_conf
    raise EnvironmentError(""loading json file config from '{}' failed!"".format(json_conf_path))
OSError: loading json file config from '/fate/arch/conf/server_conf.json' failed!
seems there is some error on loading json file config@tanmc123  this confuses me because I look at server_conf.json which has no problem (as below)

{
	""servers"": {
		""roll"": {
			""host"": ""localhost"",
			""port"": 8011
		},
		""federation"": {
			""host"": ""localhost"",
			""port"": 9394
		},
		""manager"": {
			""host"": ""localhost"",
			""http.port"": 9380,
			""grpc.port"": 9360
		},
		""servings"": [
			""127.0.0.1:8000""
		],
		""proxy"": {
			""host"": ""localhost"",
			""port"": 9370
		}
	}
}@tanmc123 I tried to debug it, but could not find any solutions. Could you give me some hint/suggestions ? Thanks a lot!@cacoderquan Have you add /fate into you PYTHONPATH enviroment? 
Like 

export $PYTHONPATH=/fateI have added /fate into my PYTHONPATH enviroment but still get the same error

root@d28bbf5328ac:/fate/examples/hetero_logistic_regression#  echo $PYTHONPATH
/fate
root@d28bbf5328ac:/fate/examples/hetero_logistic_regression# ls
conf  kill.sh  run_arbiter.sh  run_guest.sh  run_host.sh  run_logistic_regression.sh  run_logistic_regression_cluster.sh  run_logistic_regression_standalone.sh
root@d28bbf5328ac:/fate/examples/hetero_logistic_regression# sh run_logistic_regression_standalone.sh
@tanmc123 Hello, I am running the test with 'sh ./federatedml/test/run_test.sh', the error is as follows：
File ""project/FATE/arch/api/standalone/eggroll.py"", line 104, in _open_env
    return lmdb.open(path, create=True, max_dbs=1, max_readers=1024, lock=write, sync=True, map_size=10_737_418_240)
SyntaxError: invalid syntax

When I changed the code at FATE/arch/api/standalone/eggroll.py"", line 104 to ‘    return lmdb.open(path, create=True, max_dbs=1, max_readers=1024, lock=write, sync=True, map_size=(10,737,418,240))’，The error has become ‘ File ""/project/FATE/arch/api/standalone/eggroll.py"", line 104, in _open_env
    return lmdb.open(path, create=True, max_dbs=1, max_readers=1024, lock=write, sync=True, map_size=(10,737,418,240))
TypeError: unorderable types: tuple() >= int()’

So I would like to ask you what is the reason, thank you!> Hello, I am running the test with 'sh ./federatedml/test/run_test.sh', the error is as follows：
> File ""project/FATE/arch/api/standalone/eggroll.py"", line 104, in _open_env
> return lmdb.open(path, create=True, max_dbs=1, max_readers=1024, lock=write, sync=True, map_size=10_737_418_240)
> SyntaxError: invalid syntax
> 
> When I changed the code at FATE/arch/api/standalone/eggroll.py"", line 104 to ‘ return lmdb.open(path, create=True, max_dbs=1, max_readers=1024, lock=write, sync=True, map_size=(10,737,418,240))’，The error has become ‘ File ""/project/FATE/arch/api/standalone/eggroll.py"", line 104, in _open_env
> return lmdb.open(path, create=True, max_dbs=1, max_readers=1024, lock=write, sync=True, map_size=(10,737,418,240))
> TypeError: unorderable types: tuple() >= int()’
> 
> So I would like to ask you what is the reason, thank you!

@wkzsgxxn Can you tell me the python version?  It seems that python env is a litter old, which does not support  ""map_size=10_737_418_240"". To support this syntax it should be update to python 3.6 or higher. > config path is /fate/examples/hetero_logistic_regression/conf/guest_runtime_conf.json_hetero_logistic_regression_example_standalone_20190529012020
> jobid is hetero_logistic_regression_example_standalone_20190529012020
> Traceback (most recent call last):
> File ""/fate/arch/api/utils/file_utils.py"", line 40, in load_json_conf
> with open(json_conf_path) as f:
> FileNotFoundError: [Errno 2] No such file or directory: '/fate/arch/conf/server_conf.json'
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
> File ""../../workflow/hetero_lr_workflow/hetero_guest_workflow.py"", line 22, in 
> from workflow.workflow import WorkFlow
> File ""/fate/workflow/workflow.py"", line 52, in 
> from workflow import status_tracer_decorator
> File ""/fate/workflow/status_tracer_decorator.py"", line 30, in 
> server_conf = file_utils.load_json_conf(""arch/conf/server_conf.json"")
> File ""/usr/local/lib/python3.6/site-packages/cachetools/**init**.py"", line 46, in wrapper
> v = func(*args, **kwargs)
> File ""/fate/arch/api/utils/file_utils.py"", line 43, in load_json_conf
> raise EnvironmentError(""loading json file config from '{}' failed!"".format(json_conf_path))
> OSError: loading json file config from '/fate/arch/conf/server_conf.json' failed!

Do you fix that? I got same problem with you.@wwwwww1020
Thank you very much for your feedback. This is an omission. We have submitted a PR to fix it. If it is more urgent, we can do this:
1. Delete the generated docker container and image.
2. pull the code.
3. Modify the build_standalone_docker.sh code and change ""git archive -o ./docker/standalone/fate.tar $(git rev-parse HEAD) arch/api federatedml workflow examples"" to ""git archive -o ./docker/standalone /fate.tar $(git rev-parse HEAD) arch federatedml workflow examples"".
4. Rebuild a docker.
Try again, thank you",23,2019-03-18 03:05:17,2019-08-22 09:56:46,2019-08-22 09:56:46
https://github.com/FederatedAI/FATE/issues/27,['bug'],Training validation in workflow need to reset flowid,"Training validation in workflow need to reset flowid**Describe the bug**
In workflow, if not reset the flowid in validation stage, guest will receive old federation object, may raise a bug.

**To Reproduce**
Steps to reproduce the behavior:
set train and predict data totally different id sets, and run examples.

**Additional context**
If reset flowid in validation stage, it works perfectly.
fix！！",1,2019-03-12 09:18:38,2019-08-29 14:20:37,2019-04-03 04:07:19
https://github.com/FederatedAI/FATE/issues/18,[],Lacking of Basic Dashboard Support,"Lacking of Basic Dashboard Support**Is your feature request related to a problem? Please describe.**
It's very hard to observe a training progress, when there is no dashboard or visualization of the whole process

**Describe the solution you'd like**
Tensorboard like or at  least something that is on the  same level with the spark dashboard

**Describe alternatives you've considered**
Nope
Thanks for your advice. This is a very important feature. We plan to implement the feature (view task list, tracking task progress, visualization, etc) in the next version.",1,2019-03-04 07:31:33,2019-08-29 14:20:36,2019-04-04 12:13:46
https://github.com/FederatedAI/FATE/issues/6,[],Docker support,"Docker support**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
Hi ljch2018, please provide more details of your request. Your request seems like a request template only.

BTW, Docker is supported in standalone deployment.  You may want to have a look at `FATE/docker/standalone/`. Thanks.HI !  please provide a cluster-deploy for docker by docker-compose，so we can use it straight!,Parameters preferably support one-click configuration. your packaging.sh exits error ""targets=`find ""$base_dir"" -type d -name ""target"" -mindepth 2`""Hi lijiacaigit, thanks for your advice. Docker images for cluster deploy has been planned and will be available in one of future releases, and we are working on it. For now I'm afraid you have to deploy them manually.

As for the error you reported, packaging.sh has been tested in our env before releasing. So could you provide us the followings:
a. The error message, either copying it or uploading a screenshot;
b. Your environment info, e.g. operating system and its version, bash version etc.

Thanks.Hi.
1.when will the docker version of cluster deployment be available? 
2.packaging.sh has `find ""$base_dir"" -type dir -name ""target"" -mindepth 2`"",you can execute the command in your terminal , and you will get error like ""find: Arguments to -type should contain only one letter""


> Hi.
> 1.when will the docker version of cluster deployment be available?
> 2.packaging.sh has `find ""$base_dir"" -type dir -name ""target"" -mindepth 2`"",you can execute the command in your terminal , and you will get error like ""find: Arguments to -type should contain only one letter""

1. We are planning to contain cluster deployment on docker in version 0.3. Hopefully in late April or early May this year.
2. Checked manual and `-type d` works better indeed. Will change this soon. 

Thank you for the feedback.Ok！
Mysql service required for cluster deployment?
When I deployed, I found that some environment dependencies were ambiguous，your documentation is more detailed, and it will be easier to deploy, preferably with one-click configuration> Ok！
> Mysql service required for cluster deployment?
> When I deployed, I found that some environment dependencies were ambiguous，your documentation is more detailed, and it will be easier to deploy, preferably with one-click configuration

Yes, mysql service is required for cluster deployment.
Thank you for your advice and we will add dependency description in documents. 
And we will try to automate deployment, though it could be quite a process.Closing this issue as has been resolved. User may reopen it if there is further question on this issue.",8,2019-02-21 03:57:35,2019-03-04 08:16:47,2019-02-28 09:06:50