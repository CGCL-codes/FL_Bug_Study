url	label	title	all_text	comments	created_time	updated_time	closed_time
https://github.com/adap/flower/issues/1023	[]	Secure gRPC: Logging still reporing insecure connection	"Secure gRPC: Logging still reporing insecure connection### Discussed in https://github.com/adap/flower/discussions/1020

<div type='discussions-op-text'>

<sup>Originally posted by **NathanSchot** January 21, 2022</sup>
Hey,

My team and I have enjoyed working with flower for a few months now. We are very excited with the recent implementation of secure gRPC connections, as security is one of our main focus points.

I realise I may be a bit early (and potentially nitpicky) with this question as secure gRPC is still under construction, but we noticed that the logging of the FL process does not take the existence of root certificates in account, therefore printing incorrect information. The two lines where this happens are:

https://github.com/adap/flower/blob/cb66854dcbf2e0c5bf14ee48149053a84be8a3be/src/py/flwr/client/app.py#L89
and 
https://github.com/adap/flower/blob/cb66854dcbf2e0c5bf14ee48149053a84be8a3be/src/py/flwr/client/grpc_client/connection.py#L114

Thanks in advance.</div>Thanks for the report @NathanSchot, appreciated! CC @tanertopal @NathanSchot much appreciated! We will update that asap!"	2	2022-01-24 18:20:24	2022-01-25 16:15:50	2022-01-25 15:00:15
https://github.com/adap/flower/issues/955	[]	FL starting but no progress!	"FL starting but no progress!Hi, I am running pyTorch CIFAR-10 Example. I make the environment and change the server and client file with localhost:8080. It start but went to some infinite loop. No error message to find the mistake. 

**Server side messages:**
INFO flower 2021-12-22 15:06:15,217 | app.py:77 | Flower server running (insecure, 1 rounds)
INFO flower 2021-12-22 15:06:15,218 | server.py:118 | Initializing global parameters
INFO flower 2021-12-22 15:06:15,219 | server.py:304 | Requesting initial parameters from one random client
INFO flower 2021-12-22 15:06:24,682 | server.py:307 | Received initial parameters from one random client
INFO flower 2021-12-22 15:06:24,683 | server.py:120 | Evaluating initial parameters
INFO flower 2021-12-22 15:06:24,684 | server.py:133 | FL starting

**Client side messages:**
DEBUG flower 2021-12-22 15:06:24,674 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2021-12-22 15:06:24,676 | connection.py:36 | ChannelConnectivity.CONNECTING
INFO flower 2021-12-22 15:06:24,676 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-12-22 15:06:24,676 | connection.py:36 | ChannelConnectivity.READYHi again! I just find if you initiate 2 clients then it works fine and 0 failure. I think some issue of indexing if we initiate just one client."	1	2021-12-22 15:26:07	2021-12-22 16:13:35	2021-12-22 16:12:48
https://github.com/adap/flower/issues/910	[]	Loading a saved model in PyTorch - Confusion regarding the Weights and Parameters classes	"Loading a saved model in PyTorch - Confusion regarding the Weights and Parameters classesI'm saving a model according to the guidelines here: https://flower.dev/docs/saving-progress.html

This saves the ""aggregated_weights"" into an "".npz"" file. However, loading this file and its contents into a PyTorch model afterwards for inference is not clear to me. I'm having a bit of a hard time understanding the ""aggregated_weights"" variable as well as it appears to be a flwr.commons.Parameters class that is not printable (my code hangs whenever I try to print)

So my question is: How can I instead make these ""aggregated_weights"" into the standard PyTorch state_dict format and save it as "".pt""?

I tried treating the ""aggregated_weights"" variable like the ""parameters variable is treated in this example: https://github.com/adap/flower/blob/main/examples/quickstart_pytorch/client.py#L105-L108

But that didn't work for me. Perhaps this stems from my confusion regarding the flwr.commons.Parameters and Weights classesHi @Linardos, thanks for reaching out. This is a good question that many users might have, so I'll try to answer it with a lot of detail.

`flwr.client.NumPyClient` is a convenience class built on top of `flwr.client.Client`. The difference is that `Client` sends/receives lower-level values that are close to the actual gRPC messages sent over the wire, which in turn means that you have to write a bit more boilerplate code when implementing it. `NumPyClient` is more opinionated (which also means less flexible), but it does a lot of this boilerplate work for you in return.

One example for this boilerplate work is the conversion of model parameters. On the gRPC level, Flower communicates model parameters as byte arrays. To be more precise, Flower uses a gRPC/ProtoBuf message called [`Parameters`](https://github.com/adap/flower/blob/4d95d2627afb27153bfe66d5edff78a905f3f3e8/src/proto/flwr/proto/transport.proto#L24) to represent your model parameters in the following way:

```
message Parameters {
  repeated bytes tensors = 1;
  string tensor_type = 2;
}
```

`repeated` is ProtoBuf-syntax for a list/array-ish data structure. So `Parameters` is a list of byte arrays and a string `tensor_type` that tells someone receiving a `Parameters` object how those byte arrays should be interpreted (after all, the byte arrays could contain anything). The Flower code examples usually serialize a single neural network layer into a byte arrays, so in that case, the `Parameters` object should have a list `tensors` with as many entries as the model has layers.

Coming back to your question, what's the relationship between PyTorch, `NumPyClient`, and `aggregated_weights`?

Since the transport layer uses byte arrays, we have to convert *whatever* model parameters we have into byte arrays. `NumPyClient` is the convenience class that does it for you by asking you to use a list of NumPy `ndarray` objects. So when you return from `fit`, you take the PyTorch `state_dict` and turn it into a list of NumPy `ndarray`s. `NumPyClient` then does a second conversion from this list into a list of byte arrays.

On the server side, the reverse happens: we receive the `Parameters` object with the list of byte arrays, but then we have de-serialize it into a list of NumPy `ndarray`s before we can do any computation on it. This is done inside the `aggregate_fit` method: it converts `Parameters` to `ndarray`s, averages them, and converts them back to `Parameters` ([implementation](https://github.com/adap/flower/blob/4d95d2627afb27153bfe66d5edff78a905f3f3e8/src/py/flwr/server/strategy/fedavg.py#L254)).

Now, to finally answer the question, yes, one can absolutely save the aggregated parameters as a PyTorch `state_dict`. `aggregate_fit` returns a `Parameters` object that has to be transformed into a list of NumPy `ndarray`s, and those have to be transformed into the PyTorch `state_dict` (just the way it's done on the client side). Here's the code:

```python
class SaveModelStrategy(fl.server.strategy.FedAvg):
    def aggregate_fit(
        self,
        rnd: int,
        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],
        failures: List[BaseException],
    ) -> Optional[fl.common.Weights]:
        aggregated_parameters = super().aggregate_fit(rnd, results, failures)
        if aggregated_parameters is not None:
            aggregated_parameters = fl.common.para(aggregated_parameters)
            
            # Convert `Parameters` to `List[np.ndarray]`
            aggregated_weights: List[np.ndarray] = fl.common.parameters_to_weights(aggregated_parameters)
            
            # Load PyTorch model
            net = Net().to(DEVICE)

            # Convert `List[np.ndarray]` to PyTorch`state_dict`
            params_dict = zip(net.state_dict().keys(), aggregated_weights)
            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})
            net.load_state_dict(state_dict, strict=True)

            # TODO Save PyTorch `state_dict` as `.pt`

        return aggregated_parameters
```

The reason we chose to migrate (almost) all of our API's to use `Parameters` objects is that we want Flower to be serialization-agnostic. In the future, we will make the serialization and deserialization pluggable (i.e., user-configurable through a plugin system).

I also realized that some of the namings in the documentation weren't particularly helpful, so we'll update those to make them easier to follow. When we started the migration to `Parameters`, we tried to use `Weights` as a synonym for `List[np.ndarray]` and `Parameters` for the serialized version thereof, but there are still some historical artifacts where we're not yet following that convention."	1	2021-11-24 22:51:25	2021-11-25 14:38:35	2021-11-25 14:38:35
https://github.com/adap/flower/issues/888	[]	tensorflow fit_generator does not work	"tensorflow fit_generator does not workHello, 

I am trying to run train a segmentation model using fit_generator(), nothing seems to happen. I do not receive an error message but also the models are not training. Does flower support fit_generator? 

Thanks in advance, The issue is not FLOWer but TensorFlow version, upgrading it solved the issue. Happy to hear :)"	2	2021-10-21 11:53:13	2021-10-22 14:11:22	2021-10-22 09:03:00
https://github.com/adap/flower/issues/831	[]	Advanced Tensorflow example using GPU	"Advanced Tensorflow example using GPULike the title suggest, i would like to try using GPU instead of CPU for the training session of the Advanced_tensorflow project inside the example of flower.

The problem is that i get this error doing so:

```
$ bash run.sh
2021-09-04 01:25:26.609146: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-04 01:25:26.973252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5499 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:07:00.0, compute capability: 8.6
Starting client 0
Starting client 1
Starting client 2
Starting client 3
Starting client 4
Starting client 5
Starting client 6
Starting client 7
Starting client 8
Starting client 9
INFO flower 2021-09-04 01:25:28,560 | app.py:73 | Flower server running (insecure, 4 rounds)
INFO flower 2021-09-04 01:25:28,560 | server.py:118 | Getting initial parameters
INFO flower 2021-09-04 01:25:28,560 | server.py:300 | Received initial parameters from strategy
INFO flower 2021-09-04 01:25:28,560 | server.py:120 | Evaluating initial parameters
2021-09-04 01:25:28.790720: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
DEBUG flower 2021-09-04 01:25:33,445 | connection.py:36 | ChannelConnectivity.IDLE
INFO flower 2021-09-04 01:25:33,446 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-09-04 01:25:33,446 | connection.py:36 | ChannelConnectivity.CONNECTING
DEBUG flower 2021-09-04 01:25:33,477 | connection.py:36 | ChannelConnectivity.READY
DEBUG flower 2021-09-04 01:25:33,759 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2021-09-04 01:25:33,760 | connection.py:36 | ChannelConnectivity.CONNECTING
DEBUG flower 2021-09-04 01:25:33,760 | connection.py:36 | ChannelConnectivity.READY
INFO flower 2021-09-04 01:25:33,760 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-09-04 01:25:34,272 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2021-09-04 01:25:34,272 | connection.py:36 | ChannelConnectivity.CONNECTING
DEBUG flower 2021-09-04 01:25:34,275 | connection.py:36 | ChannelConnectivity.READY
INFO flower 2021-09-04 01:25:34,291 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-09-04 01:25:34,335 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2021-09-04 01:25:34,355 | connection.py:36 | ChannelConnectivity.CONNECTING
DEBUG flower 2021-09-04 01:25:34,355 | connection.py:36 | ChannelConnectivity.READY
INFO flower 2021-09-04 01:25:34,371 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-09-04 01:25:34,611 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2021-09-04 01:25:34,611 | connection.py:36 | ChannelConnectivity.CONNECTING
DEBUG flower 2021-09-04 01:25:34,626 | connection.py:36 | ChannelConnectivity.READY
INFO flower 2021-09-04 01:25:34,627 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-09-04 01:25:35,553 | connection.py:36 | ChannelConnectivity.IDLE
INFO flower 2021-09-04 01:25:35,554 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-09-04 01:25:35,554 | connection.py:36 | ChannelConnectivity.CONNECTING
DEBUG flower 2021-09-04 01:25:35,554 | connection.py:36 | ChannelConnectivity.READY
DEBUG flower 2021-09-04 01:25:36,011 | connection.py:36 | ChannelConnectivity.IDLE
INFO flower 2021-09-04 01:25:36,012 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-09-04 01:25:36,012 | connection.py:36 | ChannelConnectivity.CONNECTING
DEBUG flower 2021-09-04 01:25:36,013 | connection.py:36 | ChannelConnectivity.READY
2021-09-04 01:25:36.024932: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101
DEBUG flower 2021-09-04 01:25:36,091 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2021-09-04 01:25:36,091 | connection.py:36 | ChannelConnectivity.READY
INFO flower 2021-09-04 01:25:36,091 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-09-04 01:25:36,123 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2021-09-04 01:25:36,123 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2021-09-04 01:25:36,124 | connection.py:36 | ChannelConnectivity.CONNECTING
DEBUG flower 2021-09-04 01:25:36,124 | connection.py:36 | ChannelConnectivity.CONNECTING
INFO flower 2021-09-04 01:25:36,124 | app.py:61 | Opened (insecure) gRPC connection
INFO flower 2021-09-04 01:25:36,124 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-09-04 01:25:36,331 | connection.py:36 | ChannelConnectivity.READY
DEBUG flower 2021-09-04 01:25:36,331 | connection.py:36 | ChannelConnectivity.READY
2021-09-04 01:25:43.368500: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 512.00M (536870912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.368750: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 460.80M (483183872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.368922: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 414.72M (434865664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.369093: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 373.25M (391379200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.369112: W tensorflow/core/common_runtime/bfc_allocator.cc:338] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.
2021-09-04 01:25:43.403354: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.403377: W tensorflow/core/common_runtime/bfc_allocator.cc:338] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.
2021-09-04 01:25:43.415854: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.415888: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 516.34MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-04 01:25:43.424359: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.424378: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 516.34MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-04 01:25:43.432990: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.433007: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 50.27MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-04 01:25:43.441596: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.441613: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 50.27MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-04 01:25:43.450590: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.450608: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.35MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-04 01:25:43.459132: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.459149: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.21MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-04 01:25:43.469236: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.469257: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.14MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-04 01:25:43.477976: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.477995: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.14MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-04 01:25:43.486435: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.486452: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 218.71MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-04 01:25:43.494806: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.494823: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 218.71MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-09-04 01:25:43.503244: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.511735: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.522888: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.531444: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.539976: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.548434: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.556858: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.565334: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.578277: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.586650: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.595462: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.604018: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.612514: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.621010: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.631608: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.640018: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.649903: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.658313: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.666985: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.675339: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.683823: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.692229: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.703361: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.712197: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.722405: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.730863: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.739273: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.747633: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.756120: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.764533: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:43.770860: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.

  1/157 [..............................] - ETA: 39:22 - loss: 2.3026 - accuracy: 0.0938
  3/157 [..............................] - ETA: 4s - loss: 2.3026 - accuracy: 0.1458
  7/157 [>.............................] - ETA: 3s - loss: 2.3026 - accuracy: 0.1071
 11/157 [=>............................] - ETA: 2s - loss: 2.3026 - accuracy: 0.0994
 15/157 [=>............................] - ETA: 2s - loss: 2.3026 - accuracy: 0.1021
 19/157 [==>...........................] - ETA: 2s - loss: 2.3026 - accuracy: 0.1020
 23/157 [===>..........................] - ETA: 2s - loss: 2.3026 - accuracy: 0.1005
 27/157 [====>.........................] - ETA: 2s - loss: 2.3026 - accuracy: 0.1053
 31/157 [====>.........................] - ETA: 2s - loss: 2.3026 - accuracy: 0.1028
 35/157 [=====>........................] - ETA: 2s - loss: 2.3026 - accuracy: 0.1054
 39/157 [======>.......................] - ETA: 1s - loss: 2.3026 - accuracy: 0.1018
 43/157 [=======>......................] - ETA: 1s - loss: 2.3026 - accuracy: 0.1017
 47/157 [=======>......................] - ETA: 1s - loss: 2.3026 - accuracy: 0.1004
 51/157 [========>.....................] - ETA: 1s - loss: 2.3026 - accuracy: 0.0993
 55/157 [=========>....................] - ETA: 1s - loss: 2.3026 - accuracy: 0.1006
 59/157 [==========>...................] - ETA: 1s - loss: 2.3026 - accuracy: 0.1001
 63/157 [===========>..................] - ETA: 1s - loss: 2.3026 - accuracy: 0.0967
 67/157 [===========>..................] - ETA: 1s - loss: 2.3026 - accuracy: 0.0970
 71/157 [============>.................] - ETA: 1s - loss: 2.3026 - accuracy: 0.0977
 75/157 [=============>................] - ETA: 1s - loss: 2.3026 - accuracy: 0.0975
 79/157 [==============>...............] - ETA: 1s - loss: 2.3026 - accuracy: 0.0977
 83/157 [==============>...............] - ETA: 1s - loss: 2.3026 - accuracy: 0.0968
 87/157 [===============>..............] - ETA: 1s - loss: 2.3026 - accuracy: 0.0970
 91/157 [================>.............] - ETA: 1s - loss: 2.3026 - accuracy: 0.0968
 95/157 [=================>............] - ETA: 0s - loss: 2.3026 - accuracy: 0.0947
 99/157 [=================>............] - ETA: 0s - loss: 2.3026 - accuracy: 0.0947
103/157 [==================>...........] - ETA: 0s - loss: 2.3026 - accuracy: 0.0934
107/157 [===================>..........] - ETA: 0s - loss: 2.3026 - accuracy: 0.0935
110/157 [====================>.........] - ETA: 0s - loss: 2.3026 - accuracy: 0.0929
114/157 [====================>.........] - ETA: 0s - loss: 2.3026 - accuracy: 0.0932
118/157 [=====================>........] - ETA: 0s - loss: 2.3026 - accuracy: 0.0940
122/157 [======================>.......] - ETA: 0s - loss: 2.3026 - accuracy: 0.0950
126/157 [=======================>......] - ETA: 0s - loss: 2.3026 - accuracy: 0.0945
130/157 [=======================>......] - ETA: 0s - loss: 2.3026 - accuracy: 0.0962
134/157 [========================>.....] - ETA: 0s - loss: 2.3026 - accuracy: 0.0977
138/157 [=========================>....] - ETA: 0s - loss: 2.3026 - accuracy: 0.0978
142/157 [==========================>...] - ETA: 0s - loss: 2.3026 - accuracy: 0.0973
146/157 [==========================>...] - ETA: 0s - loss: 2.3026 - accuracy: 0.0972
150/157 [===========================>..] - ETA: 0s - loss: 2.3026 - accuracy: 0.0973
154/157 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.09762021-09-04 01:25:46.403678: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.412048: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.420519: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.428947: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.441299: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.449572: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.458047: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.466429: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.480143: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.488524: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.496894: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.505300: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.516316: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.524798: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.533258: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.541761: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.550196: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.558585: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.569473: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.577827: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.586288: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.594697: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.603360: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.611753: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.622922: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.631593: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.640124: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.648523: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.657015: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.665501: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.677421: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.685830: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.694237: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.702647: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.716326: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.724800: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.733344: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.741704: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.750124: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.758502: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.769664: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.778118: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.786817: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.795288: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.803762: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.812151: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.823225: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.831705: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.840101: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.848504: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.856928: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.865328: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.877301: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.885744: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.894332: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.902700: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.913653: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.922065: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.930461: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.938845: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.947231: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.955630: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.966927: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.975315: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.983774: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:46.992076: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.000529: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.008961: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.019962: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.028513: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.037212: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.045603: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.054030: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.062444: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.071203: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.079624: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.089277: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.097949: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.106512: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.115045: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.123515: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.132026: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.144583: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.152915: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.161395: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.170667: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.181616: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.190010: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.198422: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.206955: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.215327: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.223699: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.232798: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.241180: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.250854: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.259339: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.267774: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.276263: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.284669: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.293052: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.305320: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.313683: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.322118: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.330480: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.339027: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.347443: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.356710: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.365212: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.375453: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.383867: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.392281: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.400691: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.409075: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.417483: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.429751: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.438111: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.446610: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.455041: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.463712: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.472129: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.481916: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.490338: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.498752: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.509328: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.517751: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.526181: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.538108: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.546784: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.555341: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.563725: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.572152: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.580760: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.591749: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.600176: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.608662: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.617036: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.625591: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.634089: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.643170: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.651674: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.661574: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.669983: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.678405: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.686863: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.695306: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.703697: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.716165: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.724529: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.732940: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.741289: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.752324: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.760734: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.769210: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.777688: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.786118: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.794591: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.804284: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.812641: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.822393: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.830856: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.839277: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.847676: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.856144: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.864584: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.873294: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.881686: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.891349: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.899746: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.908175: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.916585: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.925089: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.933452: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.942423: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.951131: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.961182: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.969593: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.977987: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.986402: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:47.994802: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.003400: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.014395: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.022801: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.031270: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.039742: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.048232: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.056591: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.065311: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.073703: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.083531: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.092000: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.100477: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.110734: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.119155: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.127532: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.137522: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.145922: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.155735: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.164465: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.173118: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.181667: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.190012: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.198367: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.207182: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.215548: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.225227: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.233959: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.242362: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.250756: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.259255: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-09-04 01:25:48.267676: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory


```

How can i fix it?

I have a radeon 3060 ti, tensorflow can see the GPU and use it with no problem. The code is like the example, i just added those lines:

```
physical_devices = tf.config.experimental.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
```

Inside client.py and server.py to see if i could fix the memory allocation error, but didn't work.It seems you are running out of GPU memory.

Did you try a lower batch_size? Alternatively, try to run fewer clients and see if that fixes your issue.Hello @fanto88 , how did you solve this?"	2	2021-09-03 23:29:26	2022-02-16 01:43:33	2021-10-25 10:11:55
https://github.com/adap/flower/issues/817	[]	Error: 'Sequential' object has no attribute 'set_weight'	"Error: 'Sequential' object has no attribute 'set_weight'So I am trying out Federated Learning on structured data and following this [dataset of German credit data](https://www.kaggle.com/kabure/german-credit-data-with-risk). I have a simple tensorflow based model and code. 

Here is my client class- 

```
class GCR_Client(fl.client.NumPyClient):
    def __init__(self, model, x_train, y_train, client_name):
        self.model = model
        self.x_train = x_train
        self.y_train = y_train
        self.client_name = client_name
    
    def get_parameters(self):
        return self.model.get_weights()
    
    def fit(self, parameters, config):
        self.model.set_weights(parameters)
        self.model.fit(self.x_train.values, self.y_train.values, epochs=15, batch_size=64)
        return self.model.get_weights(), len(self.x_train), {""Fit Data from"":self.client_name}
    
    def evaluate(self, parameters, config):
        self.model.set_weight(parameters)
        return 0,0,{""Clinet Evaluation"" : ""Undefined"", ""Client Name"":self.client_name}
```


In order to remove all the external factors I removed the code where I calculate the evaluation. 

Whenever I run the code with 1 server and multiple clients, I get error in the `def evaluate(self, parameters, config)` function. 
Error is `'Sequential' object has no attribute 'set_weight'`

`def fit(self, parameters, config):` method contains the same line but the error is only being generated from `evaluate` function only. I am not sure what's wrong with the code. @Samvid95 thanks for reaching out. Could it be the missing `s`?

- `self.model.set_weights(parameters)` in `fit`
- `self.model.set_weight(parameters)` in `evaluate`Hahaha I completely missed it. Thank you 

Now let me remove this question from the face of the internet. /s"	2	2021-08-17 04:58:41	2021-08-18 08:26:47	2021-08-18 08:26:47
https://github.com/adap/flower/issues/813	[]	FL results are not available to the user as raw data at the end of each round. 	"FL results are not available to the user as raw data at the end of each round. ## Problem

Results from a FL experiment are available via terminal logs at the end of the complete experiment. It would be nice to have access to metrics not only at the end of the complete FL training, but also at the end of each round for (1) partial logging using different interfaces such as `Tensorboard` and `Weight and Biases` and (2) decision making (reduce LR, etc...).

Right now `server.server.fit(self, num_rounds: int) -> History:` returns a History, which is not forwarded by [server.app._fl](https://github.com/adap/flower/blob/de2394ed9f65a1f9563a3a4ebd7103460fdae0dc/src/py/flwr/server/app.py#L114).

## Suggested Solutions:
The current implementation is user-friendly and should probably be kept. 
I suggest we provide a *complete example* that basically opens up `app.py` and allows users to insert function calls at the end of each round. This could be tutorial on how to use Weight and Biases. 
What if we just store the `EvaluateRes` and `FitRes` objects given `server.server.app`? How is the information that is relayed to us / what we receive different from the configured server and client logger i.e. ` flwr.common.logger.configure(f""client_{args.client_partition_idx}"", host=args.log_host)` and internal functions such as `_fl ` and `server.fit` that can be re-written to store the dictionary objects? Our end goal here would be to store the average client-side accuracy given each round. We only want to store the metrics information for clients who fall under `fraction_fit` in this case. What would be the best way to do this? 
Hi @ferasbg , for selected clients in each round,  `metrics` are now returned inside a `History` object as in https://github.com/adap/flower/blob/cc4dbb243fa268e29317680a41b0714d2aa5d23e/src/py/flwr/server/app.py#L120. 
For all other users, you could use either have a centralized approach (research-only) where at the end of each round you call a centralized evaluation on the entire dateset (collection of all clients' datasets)."	2	2021-08-04 17:34:49	2022-01-19 09:28:50	2022-01-19 09:20:24
https://github.com/adap/flower/issues/807	[]	set_parameters wrong	"set_parameters wrongHello, when using the efficientnetb0 pre-training model, the joint training will stop at the position shown in the figure below, and the following error will be reported.
![image](https://user-images.githubusercontent.com/59415080/127867393-9591b26e-e0df-49f3-8f7d-869cd3fbe248.png)
![image](https://user-images.githubusercontent.com/59415080/127867469-9b6f244a-35a0-43af-a842-e3fd7af22767.png)
Hello,I have the same problem. Have you solved it？I've faced the same problem too.I’m watching the fedBN example of flower official website.because your model has the 'bn' layer, the model parameters to numpy parameters will wrong
![image](https://user-images.githubusercontent.com/59415080/130625040-9c9e315a-3a63-4386-93b1-5033128812b9.png)
This official code gives the correct idea.server and client add like the next picture describe.
![image](https://user-images.githubusercontent.com/59415080/130626479-f20a1900-b7b9-4aaf-b0fc-078b72c70da4.png)"	3	2021-08-02 13:12:55	2021-08-24 13:39:54	2021-08-24 13:39:31
https://github.com/adap/flower/issues/772	[]	How to set server and client address?	"How to set server and client address?Hi, all,

I tried the [pytorch_from_centralized_to_federated](https://github.com/adap/flower/tree/main/examples/pytorch_from_centralized_to_federated) on my Mac, which worked OK. However, when I tried it on the remote server with addresses setting as [::]:8080 and [::]:8080, it showed 'failed to connect to all addresses' (please see the attached screenshot). The same error came out when I set addresses as localhost:8080 and localhost:8080 as well as X.X.X.X:8080 and X.X.X.X:8080. Could anyone have any idea which could solve my problem? Many thanks!

<img width=""959"" alt=""截屏2021-07-16 下午9 04 37"" src=""https://user-images.githubusercontent.com/18133362/125952324-a212bd0d-17a9-4d1c-bb5f-7063318a6139.png"">

<img width=""1421"" alt=""截屏2021-07-16 下午9 03 27"" src=""https://user-images.githubusercontent.com/18133362/125952147-094cac7a-b5cd-4c5f-911c-8f1dc9f45d61.png"">
Hi, 

you can parse the arguments or write the IP address of the server hard inside: `src/py/flwr/server/app.py` in line: DEFAULT_SERVER_ADDRESS = ""[::]:8080""

Here an example of mine using argument parser on both sides server and client.
Server (`src/py/flwr_example/quickstart_pytorch/server.py`):

```
    # Instantiate the parser
    parser = argparse.ArgumentParser(description='Optional app description')

    ### Add Arguments
    parser.add_argument('--ip', type=str, required=True, help='Enter Server IP')
    parser.add_argument('--port', type=str, default='8080', help='Enter Server Port, default: 8080')

    # Parse arguments
    args = parser.parse_args()

    fl.server.start_server(
	server_address=f""{args.ip}:{args.port}"",
    	config={""num_rounds"": 3}
    	)
```

For the client, the argument parser is already implemented, see `src/py/flwr_example/quickstart_pytorch/sclient.py`.

Best regardsThanks! I solved the problem by setting IP as 127.0.0.1:8080 at both server and client.@Sylarair Thanks for your comment. I was having the same issue with Tensorflow and your solution also worked for me. Thanks :)

Cheers,
Shehroz"	3	2021-07-16 12:37:50	2022-01-19 09:19:10	2022-01-19 09:19:10
https://github.com/adap/flower/issues/765	[]	Using Fed+ of FedProx	"Using Fed+ of FedProxHi everyone, I have started using Flower and I would like to know if it possible to use the fusion algorithms such as Fed+ or FedProx instead of  FedAvg.
Thanks in advance.Hello @Enrique-Marmol you should be able to adapt the Flower version of FedAvg to FedProx by setting FedAvg to accept failures in the initial parameters and then adding the proximal term to the loss of your network during training on the clients."	1	2021-07-02 10:49:40	2022-01-19 09:20:50	2022-01-19 09:20:50
https://github.com/adap/flower/issues/756	[]	Client not starting	"Client not startingAm getting the following error when I try to start client after server while trying to run the below example.

[pytorch_from_centralized_to_federated](https://github.com/adap/flower/tree/main/examples/pytorch_from_centralized_to_federated)

`Files already downloaded and verified
Files already downloaded and verified
DEBUG flower 2021-06-10 15:54:23,140 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2021-06-10 15:54:23,140 | connection.py:36 | ChannelConnectivity.CONNECTING
DEBUG flower 2021-06-10 15:54:23,141 | connection.py:36 | ChannelConnectivity.TRANSIENT_FAILURE
INFO flower 2021-06-10 15:54:23,141 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2021-06-10 15:54:23,343 | connection.py:68 | Insecure gRPC channel closed
Traceback (most recent call last):
  File ""client.py"", line 98, in <module>
    main()
  File ""client.py"", line 94, in main
    fl.client.start_numpy_client(""[::]:8080"", client)
  File ""C:\Users\HP\anaconda3\lib\site-packages\flwr\client\app.py"", line 112, in start_numpy_client
    start_client(
  File ""C:\Users\HP\anaconda3\lib\site-packages\flwr\client\app.py"", line 64, in start_client
    server_message = receive()
  File ""C:\Users\HP\anaconda3\lib\site-packages\flwr\client\grpc_client\connection.py"", line 60, in <lambda>
    receive: Callable[[], ServerMessage] = lambda: next(server_message_iterator)
  File ""C:\Users\HP\anaconda3\lib\site-packages\grpc\_channel.py"", line 416, in __next__
    return self._next()
  File ""C:\Users\HP\anaconda3\lib\site-packages\grpc\_channel.py"", line 689, in _next
    raise self
grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
        status = StatusCode.UNAVAILABLE
        details = ""failed to connect to all addresses""
        debug_error_string = ""{""created"":""@1623320663.140000000"",""description"":""Failed to pick subchannel"",""file"":""src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":4134,""referenced_errors"":[{""created"":""@1623320663.140000000"",""description"":""failed to connect to all addresses"",""file"":""src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":398,""grpc_status"":14}]}""
>`
![image](https://user-images.githubusercontent.com/16716026/121510537-5edb2700-ca05-11eb-9d22-90c7fed7c863.png)

Python Version: 3.8.5
OS: Windows 10
RAM: 8 GB
Using conda base environment to run this.I had the same problem on Windows. Just try to insert ""localhost"" instead of ""[::]"" in the address provided in the client and server code. That is:
[client.py] fl.client.start_numpy_client(""localhost:8080"", client=client)
[server.py] fl.server.start_server(""localhost:8080"", config={""num_rounds"": HERE_YOUR_NUM_OF_ROUNDS}, strategy=strategy)"	1	2021-06-10 10:33:55	2021-06-11 07:31:17	2021-06-11 07:31:16
https://github.com/adap/flower/issues/751	[]	grpc client server communication is not working when client and server is started from different terminals	"grpc client server communication is not working when client and server is started from different terminalsHi flower developers,

I tried the examples provided with flower and they work if I start the server and clients with the run.sh script, but communication stalls if client and server are started from different terminals.  The log from the client:

DEBUG flower 2021-06-07 17:37:16,844 | connection.py:36 | ChannelConnectivity.READY

This is where it waits indefinitely. At a keyboard interrupt, the log is the following:

File ""client.py"", line 34, in <module>
    fl.client.start_numpy_client(""[::]:8080"", client=CifarClient())
  File ""/home/sallogy/anaconda3/envs/flower/lib/python3.7/site-packages/flwr/client/app.py"", line 115, in start_numpy_client
    grpc_max_message_length=grpc_max_message_length,
  File ""/home/sallogy/anaconda3/envs/flower/lib/python3.7/site-packages/flwr/client/app.py"", line 64, in start_client
    server_message = receive()
  File ""/home/sallogy/anaconda3/envs/flower/lib/python3.7/site-packages/flwr/client/grpc_client/connection.py"", line 60, in <lambda>
    receive: Callable[[], ServerMessage] = lambda: next(server_message_iterator)
  File ""/home/sallogy/anaconda3/envs/flower/lib/python3.7/site-packages/grpc/_channel.py"", line 426, in __next__
    return self._next()
  File ""/home/sallogy/anaconda3/envs/flower/lib/python3.7/site-packages/grpc/_channel.py"", line 817, in _next
    _common.wait(self._state.condition.wait, _response_ready)
  File ""/home/sallogy/anaconda3/envs/flower/lib/python3.7/site-packages/grpc/_common.py"", line 141, in wait
    _wait_once(wait_fn, MAXIMUM_WAIT_TIMEOUT, spin_cb)
  File ""/home/sallogy/anaconda3/envs/flower/lib/python3.7/site-packages/grpc/_common.py"", line 106, in _wait_once
    wait_fn(timeout=timeout)
  File ""/home/sallogy/anaconda3/envs/flower/lib/python3.7/threading.py"", line 300, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt

The log from the server:

INFO flower 2021-06-07 17:36:33,616 | app.py:76 | Flower server running (insecure, 3 rounds)
INFO flower 2021-06-07 17:36:33,617 | server.py:118 | Getting initial parameters
INFO flower 2021-06-07 17:37:16,913 | server.py:306 | Received initial parameters from one random client
INFO flower 2021-06-07 17:37:16,913 | server.py:120 | Evaluating initial parameters
INFO flower 2021-06-07 17:37:16,913 | server.py:133 | FL starting

It seems that it blocks in python threading. I tried also with different versions of python, the same result. What can be the problem?@sallogy can you share a bit more information about your runtime environment?

* Which OS (incl. version) e.g. Ubuntu 18.04
* Which CPU/GPU
* How much RAM

Also what code did you run? A specific Flower example? Did you bootstrap your own project without any example? Some context is very much appreciated.
Sorry, my mistake. When I dug into the source code, I realized that the default FedAvg strategy expects minimum 2 clients in order to start the training, and I tried with 1 client (actually I dockerized the client and server, and I thought that it is some issue related to this, but not). . Anyway, some informative message about the minimum number of clients required to start the training would be helpful.   @sallogy Happy to hear you could resolve the problem. Would you like to propose it as an idea here: https://github.com/adap/flower/discussions/categories/ideas and explain a little what you would consider a good solution? I would close this ticket for now as it's resolved."	3	2021-06-07 16:14:50	2021-06-11 07:35:00	2021-06-11 07:35:00
https://github.com/adap/flower/issues/712	[]	Using results returned from client's fit	"Using results returned from client's fitI want to save the results returned to the server by the client's fit method and save them. 
I am not sure how I am supposed to access the results in the server script.

The example in https://flower.dev/docs/saving-progress.html#aggregate-custom-evaluation-results only covers results from the clients evaluate function but does not show results from the client's fit function.

In the [advanced_tensorflow](https://github.com/adap/flower/blob/main/examples/advanced_tensorflow/client.py) example, the client's `fit` method is returning some results to the server, but the `server.py` is not using the these results anywhere. 

I could also do this if there was a way to access all the returned accuracies and results from the clients after the fl is finished.

is there an api reference of the returned result object ?Hi @cozek , thanks for the questions. What you want to do isn't properly documented yet (and there will be built-in improvements in future releases), but you could do it in the same way as it's done in the docs you linked above. The only difference is that you'd override `aggregate_fit` instead of `aggregate_evaluate`:

```python
class SaveCustomFitMetricStrategy(fl.server.strategy.FedAvg):
    def aggregate_fit(
        self,
        rnd: int,
        results: List[Tuple[ClientProxy, FitRes]],
        failures: List[BaseException],
    ) -> Optional[Weights]:
        """"""Aggregate evaluation losses using weighted average.""""""
        if not results:
            return None

        # Save results
        for _, fit_res in results:
            fit_metrics = fit_res.metrics
            print(""TODO: save individual results here"", fit_metrics)

        # Call aggregate_fit from base class (FedAvg)
        return super().aggregate_fit(rnd, results, failures)
```

The `results` object is a list of tuples (pairs of `(ClientProxy, FitRes)`), the code example above shows how to get the dictionary returned by `Client.fit` from `FitRes`. The `flwr.common.typing` module contains many type definitions (for future reference):

https://github.com/adap/flower/blob/main/src/py/flwr/common/typing.py@danieljanes Thanks for the code example and quick reply. This solves my use case for now. "	2	2021-05-06 12:37:05	2021-05-07 19:40:25	2021-05-07 19:40:25
https://github.com/adap/flower/issues/710	[]	Connection issue tf-quickstart	"Connection issue tf-quickstartHi there, 

I am trying to implement flower and going through the tensorflow quickstart tutorial. And I am encountering the following error: 

```
 status = StatusCode.UNAVAILABLE
 details = ""failed to connect to all addresses""
```

My issue is the same one as #648 & #537  

I tried changing the port value to IPv6 as well as tried multiple ports but nothing worked for me.  Is there anything else I can do?

My setup: 
Windows 10 with Anaconda virtual environment.

Best,
SamvidHi @Samvid95 , thanks for reaching out.

We had Windows users who had issues with their firewall, could you try turning that off?

Other than that you could try to use WSL2 on Windows 10, that should work like a charm (some of the Flower core devs use it on a daily basis w/ Ubuntu 18.04 or 20.04). Awesome. It works.@Samvid95 great, happy to hear!"	3	2021-05-05 14:45:36	2021-05-11 15:59:17	2021-05-11 13:24:18
https://github.com/adap/flower/issues/659	[]	QFedAvg Loss Unitialized Error	"QFedAvg Loss Unitialized ErrorHello,
I am trying to use the qffedavg strategy and getting ""NameError: free variable 'loss' referenced before assignment in enclosing scope."" I think the problem is that I did not specify an evaluation function as a parameter so the loss variable is not initialized before it is called later in the program. If I understand right, then if the loss really is necessary for q-FedAvg the evaluation function parameter should not be optional. Otherwise, the for loop after the line where loss is supposed to be initialized (line 185) should first check that loss is not None.

Also, from the paper, I think the correct name of the algorithm is qFedAvg not qFFedAvg as the strategy is named here. 

Is there an example for the evaluation function?


Server code:
```
import flwr as fl
from flwr.server import Server, SimpleClientManager

def main():
    strategy = fl.server.strategy.QffedAvg(min_available_clients=2)

    myServer = Server(client_manager = SimpleClientManager(), strategy=strategy)

    fl.server.start_server(server_address= ""localhost:8080"", server = myServer, config={""num_rounds"": 5})

if __name__ == ""__main__"":
    main()
```

Error message produced:
```
Traceback (most recent call last):
  File ""C:\Users\wishi\OneDrive - University of Kentucky\FLCode\Organized_Pytorch\DataDistributions\centralizedDist\trimmedServer.py"", line 12, in <module>
    main()
  File ""C:\Users\wishi\OneDrive - University of Kentucky\FLCode\Organized_Pytorch\DataDistributions\centralizedDist\trimmedServer.py"", line 9, in main
    fl.server.start_server(server_address= ""localhost:8080"", server = myServer, config={""num_rounds"": 5})
  File ""C:\Users\wishi\AppData\Local\Programs\Python\Python37\lib\site-packages\flwr\server\app.py"", line 79, in start_server
    _fl(server=initialized_server, config=initialized_config)
  File ""C:\Users\wishi\AppData\Local\Programs\Python\Python37\lib\site-packages\flwr\server\app.py"", line 108, in _fl
    hist = server.fit(num_rounds=config[""num_rounds""])
  File ""C:\Users\wishi\AppData\Local\Programs\Python\Python37\lib\site-packages\flwr\server\server.py"", line 92, in fit
    weights_prime = self.fit_round(rnd=current_round)
  File ""C:\Users\wishi\AppData\Local\Programs\Python\Python37\lib\site-packages\flwr\server\server.py"", line 181, in fit_round
    return self.strategy.aggregate_fit(rnd, results, failures)
  File ""C:\Users\wishi\AppData\Local\Programs\Python\Python37\lib\site-packages\flwr\server\strategy\qffedavg.py"", line 193, in aggregate_fit
    [np.float_power(loss + 1e-10, self.q_param) * grad for grad in grads]
  File ""C:\Users\wishi\AppData\Local\Programs\Python\Python37\lib\site-packages\flwr\server\strategy\qffedavg.py"", line 193, in <listcomp>
    [np.float_power(loss + 1e-10, self.q_param) * grad for grad in grads]
NameError: free variable 'loss' referenced before assignment in enclosing scope


```
Hi @smoser82 , thanks a lot for the detailed report!

We do have an example for the evaluation function in the Advanced TensorFlow Example (https://github.com/adap/flower/tree/main/examples/advanced_tensorflow, and a few more in deprecated parts of the codebase under `src/py/flwr_example` and `src/py/flwr_experimental`, but those will be removed eventually).

The gist is to hand a function to the strategy that takes model parameters and return the evaluation result:

```python
def main() -> None:
    # Create strategy
    strategy = fl.server.strategy.FedAvg(
        fraction_fit=0.3,
        fraction_eval=0.2,
        min_fit_clients=3,
        min_eval_clients=2,
        min_available_clients=10,
        eval_fn=get_eval_fn(),
        on_fit_config_fn=fit_config,
        on_evaluate_config_fn=evaluate_config,
    )
    # Start Flower server for four rounds of federated learning
    fl.server.start_server(""[::]:8080"", config={""num_rounds"": 4}, strategy=strategy)


def get_eval_fn():
    """"""Return an evaluation function for server-side evaluation.""""""

    # Load data and model here to avoid the overhead of doing it in `evaluate` itself
    (x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()

    # Use the last 5k training examples as a validation set
    x_val, y_val = x_train[45000:50000], y_train[45000:50000]

    # Load and compile model
    model = tf.keras.applications.EfficientNetB0(
        input_shape=(32, 32, 3), weights=None, classes=10
    )
    model.compile(""adam"", ""sparse_categorical_crossentropy"", metrics=[""accuracy""])

    # The `evaluate` function will be called after every round
    def evaluate(weights: fl.common.Weights) -> Optional[Tuple[float, float]]:
        model.set_weights(weights)  # Update model with the latest parameters
        loss, accuracy = model.evaluate(x_val, y_val)
        return loss, accuracy

    return evaluate
```

Does this solve your issue?

I'll take a look at the paper and talk to the original author of this strategy to learn more about the naming and the issue with the optional nature of the evaluation function.That's a very helpful example, thank you. I think it's working now!The renaming just got merged into `main` and will become available in tomorrow's Flower nightly release (and the full 0.17 a little later) - thanks again for pointing this out @smoser82 !"	3	2021-03-03 16:20:12	2021-08-15 12:03:42	2021-08-15 12:01:34
https://github.com/adap/flower/issues/648	[]	Trying to run quickstart_tensorflow	"Trying to run quickstart_tensorflowHello,
I am trying to run your quickstart project but I got an error. When I tried to run server.py with port 8080 it failed, so I just changed the port to 5040 in both files (+ I checked if it is a free port ). The server runs ok, but then when I tried to run client.py I could not connect to the server. Both files client and server are running from the same PC with anaconda environment. Any clue what I did wrong? 

![flower quickstart2](https://user-images.githubusercontent.com/72346204/108837436-8af7a700-75d2-11eb-8ff5-c4604ee30efa.png)
![flower quickstart3](https://user-images.githubusercontent.com/72346204/108838494-fdb55200-75d3-11eb-8e3a-7b80d7fd2e95.png)

Hi @Martin-Stevlik , can you try to use IPv6 (so `[::]:8080`) for both server and client? We've recently changed the example code to use this because IPv4 produced an error in some cases.Thank you for your quick response. I have tried it and it works just fine. Is there a way for it to work with IPv4 in future? Because I want to run the server.py in AZURE VM and most of VM does not support IPv6 ...Great to hear it works @Martin-Stevlik - IPv4 is generally supported, but we're seeing the error you reported every now and then, however, it's not quite clear where it's coming from. Which platform are you on? We've tested the examples with both IPv4 and IPv6 and some people experienced issues while others didn't.

In the case of Docker, it's actually recommended to use IPv4 over IPv6 (IPv6 requires additional configuration in Docker).I am using Windows 10 and as a virtual environment Anaconda. My next step is to try your embedded devices quickstart on my Raspberry PI's. I will see how it goes there...@Martin-Stevlik were you able to resolve your issue? Let us know if we can help you further. With regards to Flower on Raspberry this might be useful for you: https://github.com/adap/flower/tree/main/examples/embedded_devices


Hi, I followed your instructions in the embedded devices tutorial and came across the same connecting issue. I have a PC witch runs server.py and 2 raspberries that runs client.py. Both raspberries and PC are on the same network and I am using your latest release 0.14.0. I don't know if you should put the port number in the client's code, but I have tried it but it would not resolve an issue.

![flower_raspberry3 1](https://user-images.githubusercontent.com/72346204/110376751-da6ab800-8053-11eb-9e40-daa9e0df2869.png)
![flower_raspberry](https://user-images.githubusercontent.com/72346204/110376764-dfc80280-8053-11eb-9071-c961a3a2211e.png)
![flower_raspberry2](https://user-images.githubusercontent.com/72346204/110376776-e22a5c80-8053-11eb-861a-e58bb8456085.png)
@Martin-Stevlik I think the issue is that the clients also need the port of the server. Can you try with `192.168.1.32:5000` when calling the `run_pi.sh`?

If that's the issue I will adjust the https://github.com/adap/flower/blob/main/examples/embedded_devices/README.md file to clarify it further.@tanertopal as you can see on the right side of the image. I have already tried it and it did not work ... I will try using IPv6 and give you an update on how it goes. @Martin-Stevlik my bad. Missed that. Am I guessing right that the server is running on a Windows machine? I think we did not test the setup with a Windows OS. I'll try it out in the next couple days on Windows 10 if you can confirm my guess.

In general can you tell me a little bit about your overall setup?

- Which exact OS and version
- Version of the tools involved e.g. DockerI am using 64 bit OS, Windows 10 PRO version 2004. For the environment on PC, I am using an anaconda with python=3.8.3, flwr=0.14.0, tensorflow-cpu=2.4.1 

As for raspberries I have raspberry 3 model B, OS is 64bit Ubuntu Server 20.04.2 LTS with docker version 20.10.4@tanertopal so I have tried it with IPv6 and the same issue comes up. I have put ""[::]:5040"" in server.py and my IPv6 address in client.py, if I do this in quickstart_tensorflow project it works ...@Martin-Stevlik so I assume you have run the `quickstart_tensorflow` example without docker as otherwise, you would have to activate [IPv6 support in Docker](https://docs.docker.com/config/daemon/ipv6/)? I am going to set up a new Raspberry Pi and try to replicate your issue. Need to check if I have a Raspberry 3 B somewhere lying around (I think so 😄  ).

As another idea. Are you already part of our [Slack community](https://flower.dev/join-slack)? Maybe we can solve your issue faster there.

A few more ideas/questions:
1. Is the IP of the server you are using visible when you run `Get-NetIPConfiguration` in Powershell on the Windows server?
2. Can you modify the `run_pi.sh` and `run_jetson.sh` to include `--net=host`
    * `docker run --rm flower_client ${@}` => `docker run --net=host --rm flower_client ${@}` 
3. What happens when you run `telnet SERVER_ADDRESS SERVER_PORT` on the Raspberry Pi? Does it connect? You can cancel the connection if so using CTRL+D. This will give us a hint if its a Docker issue.Okay, so the key advice for running this ""embedded_devices"" project on windows 10 is to fully disable the firewall ;)"	13	2021-02-23 11:39:53	2021-03-18 19:49:45	2021-03-18 19:49:45
https://github.com/adap/flower/issues/552	[]	Improve docs (misc)	"Improve docs (misc)# Description
Improve the docs by removing obsolete parts and making it overall more readable (also on mobile).

# Tasks
- [x] Remove obsolete parts of the docs aka. How To AWS
- [x] Fix formatting
- [x] Use new Theme
Related PR's
* #547 
* #548 
* #551 Also relates to #482 and #472"	2	2021-01-05 17:16:41	2021-01-19 11:16:41	2021-01-19 11:16:29
https://github.com/adap/flower/issues/542	[]	transport_pb2.pyi file	"transport_pb2.pyi fileHow to generate the `transport_pb2.pyi` file? I cannot find any information about this on gRPC's official documentation. https://github.com/dropbox/mypy-protobuf
Problem solved. Thanks!"	1	2020-12-27 09:17:36	2020-12-27 09:56:52	2020-12-27 09:56:52
https://github.com/adap/flower/issues/540	['bug']	Evaluation of the final model on empty client set	"Evaluation of the final model on empty client setHello,
I'm trying to run the basic [Tensorflow Example](https://github.com/adap/flower/tree/main/examples/quickstart_tensorflow) but when the [final model evaluation is executed](https://github.com/adap/flower/blob/42ef63b6a8097c3b571b65e11b5471477253a48d/src/py/flwr/server/app.py#L97) the client returns an empty set, thus the evaluation is not executed.
This should be due to the fact that the server [closes the connections](https://github.com/adap/flower/blob/42ef63b6a8097c3b571b65e11b5471477253a48d/src/py/flwr/server/server.py#L117) with the clients after the fit.

Thank youThanks for the report @NicholasRasi ! What you're describing is right, good catch. We'll fix this asap.This is fixed by #553 and will be available in Flower 0.13.0."	2	2020-12-23 08:45:41	2021-01-06 09:37:37	2021-01-06 09:37:37
https://github.com/adap/flower/issues/537	[]	grpc error: details: failed to connect to all addresses	"grpc error: details: failed to connect to all addressesHi, I was just trying the examples on [https://flower.dev/docs/example_walkthrough_pytorch_mnist.html](url). After runing  `bash ./run-clients.sh`, I got the following errors. Can someone provides any help? 
`$ DEBUG flower 2020-12-17 14:45:45,394 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2020-12-17 14:45:45,394 | connection.py:36 | ChannelConnectivity.TRANSIENT_FAILURE
INFO flower 2020-12-17 14:45:45,394 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2020-12-17 14:45:45,410 | connection.py:36 | ChannelConnectivity.IDLE
DEBUG flower 2020-12-17 14:45:45,410 | connection.py:36 | ChannelConnectivity.CONNECTING
DEBUG flower 2020-12-17 14:45:45,410 | connection.py:36 | ChannelConnectivity.TRANSIENT_FAILURE
INFO flower 2020-12-17 14:45:45,410 | app.py:61 | Opened (insecure) gRPC connection
DEBUG flower 2020-12-17 14:45:45,597 | connection.py:68 | Insecure gRPC channel closed 
`
`Traceback (most recent call last):  
  File ""E:\Anaconda3\envs\ptq\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""E:\Anaconda3\envs\ptq\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""E:\Anaconda3\envs\ptq\lib\site-packages\flwr_example\quickstart_pytorch\client.py"", line 96, in <module>
    fl.client.start_client(args.server_address, client)
  File ""E:\Anaconda3\envs\ptq\lib\site-packages\flwr\client\app.py"", line 64, in start_client
    server_message = receive()
  File ""E:\Anaconda3\envs\ptq\lib\site-packages\flwr\client\grpc_client\connection.py"", line 60, in <lambda>
    receive: Callable[[], ServerMessage] = lambda: next(server_message_iterator)
  File ""E:\Anaconda3\envs\ptq\lib\site-packages\grpc\_channel.py"", line 416, in __next__
    return self._next()
  File ""E:\Anaconda3\envs\ptq\lib\site-packages\grpc\_channel.py"", line 786, in _next
    raise self
grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
        status = StatusCode.UNAVAILABLE
        details = ""failed to connect to all addresses""
        debug_error_string = ""{""created"":""@1608187545.406000000"",""description"":""Failed to pick subchannel"",""file"":""src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":4143,""referenced_errors"":[{""created"":""@1608187545.406000000"",""description"":""failed to connect to all addresses"",""file"":""src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":398,""grpc_status"":14}]}""`issue solved by changing the ipv4 port.Hey I am having the same problem. I tried changing the port. But it still having the same issue. Can you tell me what did you exactly change?I have the same issue but after I changed port 5040 for both server and client, the FL worked fine.@lfzhagn  For me changing the [::] to localhost for both client and server side as follow worked:

`fl.client.start_numpy_client(""[::]:8080"", client=CifarClient())`
to
`fl.client.start_numpy_client(""localhost:8080"", client=CifarClient())`"	4	2020-12-17 07:07:00	2022-01-10 23:38:05	2020-12-17 09:12:56
https://github.com/adap/flower/issues/530	['bug']	`Server.fit` method crash with `TypeError: 'int' object is not iterable` error	"`Server.fit` method crash with `TypeError: 'int' object is not iterable` errorHello,
since release `0.11.0` I'm not able to launch code that worked fine with release `0.10.0`

I have the same problem with release `0.12.0` 

Here is the Error Stack : 
```
File ""/srv/code/utils/ml_server_flower.py"", line 70, in run
    hist = server.fit(num_rounds=federated_config['ROUND_BEFORE_AGGREGATION'])
  File ""/usr/local/lib/python3.7/dist-packages/flwr/server/server.py"", line 110, in fit
    res_fed = self.evaluate(rnd=current_round)
  File ""/usr/local/lib/python3.7/dist-packages/flwr/server/server.py"", line 133, in evaluate
    rnd=rnd, weights=self.weights, client_manager=self._client_manager
  File ""/usr/local/lib/python3.7/dist-packages/flwr/server/strategy/fedavg.py"", line 121, in configure_evaluate
    parameters = weights_to_parameters(weights)
  File ""/usr/local/lib/python3.7/dist-packages/flwr/common/parameter.py"", line 28, in weights_to_parameters
    tensors = [ndarray_to_bytes(ndarray) for ndarray in weights]
TypeError: 'int' object is not iterable
```
The context where the issue appears

```python
client_manager = SimpleClientManager()
strategy = FedAvg()
server = Server(client_manager=client_manager, strategy=strategy)

model = KerasModel(data[0].shape[1], data[0].shape[2], data[1].shape[1])

grpc_server = start_insecure_grpc_server(
	client_manager=server.client_manager(), server_address=params['grpc_host']
)
hist = server.fit(num_rounds=federated_config['ROUND_BEFORE_AGGREGATION'])
...
```

the model i use :
```python
class KerasModel:
    def __init__(self, n_timesteps, n_features, n_outputs):
        """"""
        with : n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]
        """"""
        #Personalization of the optimizer
        sgd = optimizers.SGD(lr=float(keras_config['LEARNING_RATE']), momentum=0.0, decay=0.0,
                             nesterov=False)
        adam = optimizers.Adam(lr=float(keras_config['LEARNING_RATE']))
        rmsProp = optimizers.RMSprop(lr=float(keras_config['LEARNING_RATE']), rho=0.9, epsilon=None, decay=0.0)
        adagrad = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)
        adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)
        adamax = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)

	    #The network
        self.model = Sequential()
        self.model.add(Conv1D(filters=int(keras_config['FILTERS_CONV1D']),
                         kernel_size=int(keras_config['KERNEL_CONV1D']), activation=keras_config['ACTIVATION_CONV1D'],
                         input_shape=(n_timesteps,n_features)))
        #self.model.add(Conv1D(filters=8, kernel_size=5, activation='relu'))
        self.model.add(Dropout(float(keras_config['DROPOUT_RATIO'])))
        self.model.add(MaxPooling1D(pool_size=int(keras_config['POOL_SIZE_MAXPOOL'])))

        self.model.add(Conv1D(filters = int(keras_config['FILTERS_CONV1D']),
                              kernel_size=int(keras_config['KERNEL_CONV1D']), activation = keras_config['ACTIVATION_CONV1D']))
        self.model.add(Dropout(float(keras_config['DROPOUT_RATIO'])))
        self.model.add(MaxPooling1D(pool_size=int(keras_config['POOL_SIZE_MAXPOOL'])))
 
        self.model.add(Flatten())
		self.model.add(Dense(int(keras_config['DENSE1_NEURONS']), activation=keras_config['ACTIVATION_DENSE1']))
        self.model.add(Dense(n_outputs, activation=keras_config['ACTIVATION_DENSE2']))
        self.model.compile(loss=keras_config['LOSS'], optimizer=keras_config['OPTIMIZER'], metrics=['accuracy'])
 
    def set_weights(self, weights):
        self.model.set_weights(weights)
 
    def fit(self, x_train, y_train, epochs=None):
        self.model.fit(x_train, y_train, epochs=epochs, batch_size=common_config['BATCH_SIZE'])
 
    def get_weights(self):
        return self.model.get_weights()
 
    def evaluate(self, x_test, y_test):
        return self.model.evaluate(x_test, y_test)
```

configuration dictionary : 

```python
federated_config = {
    'ROUND_BEFORE_AGGREGATION': 3,
    'EPOCHS_PER_ROUND': 2,
}

common_config = {
    'TEST_BATCH_SIZE': 1000,
    'BATCH_SIZE': 64,
    'EPOCHS': 5,
}

keras_config = {
    'LEARNING_RATE': 0.0004,
    'FILTERS_CONV1D': 8,
    'KERNEL_CONV1D': 3,
    'ACTIVATION_CONV1D': 'relu',
    'DROPOUT_RATIO': 0.5,
    'POOL_SIZE_MAXPOOL': 3,
    'L2_REGU': False,
    'DENSE1_NEURONS': 20,
    'ACTIVATION_DENSE1': 'relu',
    'VALUE_L2_REGU': 0.001,
    'ACTIVATION_DENSE2': 'softmax',
    'LOSS': 'mean_squared_error',
    'OPTIMIZER': 'adam',
}
Thanks for the report @altor ! We went through your code and everything looks fine. The issues seems to be related to parameter serialization for distributed evaluation (i.e., on the clients). Does the initial round of training work before Flower tries to do the evaluation?
IIRC there were no changes related to that part of the codebase during the 0.11.0 and 0.12.0 release. Would it be possible to open source your code (or alternatively invite @tanertopal  and @danieljanes  to a private repo) so that we can try to run it?Closing this now because we couldn't reproduce. Feel free to reopen if this is still an issue for you @altor ."	2	2020-12-09 18:39:19	2021-02-05 10:10:08	2021-02-05 10:10:07
https://github.com/adap/flower/issues/408	['bug']	execution of client example crash after training is terminated	"execution of client example crash after training is terminatedHello !

I'm trying to use the example you provide ([quickstart](https://github.com/adap/flower/tree/main/src/py/flwr_example/quickstart) and [tensorflow](https://github.com/adap/flower/tree/main/src/py/flwr_example/tensorflow)). I achieve to train models but clients can't achieve to stop themselves without crashing.

I just run `run_server.sh` and `run_clients.sh` in separate terminals, see the clients downloading data and train their models. After training, the server evaluate the model ant stop itself properly.
At this moments, clients crash by rising an exception with this message : 

```
Traceback (most recent call last):
  File ""client.py"", line 114, in <module>
    main()
  File ""client.py"", line 110, in main
    fl.client.start_keras_client(args.server_address, client)
  File ""/usr/local/lib/python3.7/dist-packages/flwr/client/app.py"", line 47, in start_keras_client
    start_client(server_address, flower_client)
  File ""/usr/local/lib/python3.7/dist-packages/flwr/client/app.py"", line 35, in start_client
    server_message = receive()
  File ""/usr/local/lib/python3.7/dist-packages/flwr/client/grpc_client/connection.py"", line 59, in <lambda>
    receive: Callable[[], ServerMessage] = lambda: next(server_message_iterator)
  File ""/usr/local/lib/python3.7/dist-packages/grpc/_channel.py"", line 416, in __next__
    return self._next()
  File ""/usr/local/lib/python3.7/dist-packages/grpc/_channel.py"", line 706, in _next
    raise self
grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
        status = StatusCode.UNAVAILABLE
        details = ""Socket closed""
        debug_error_string = ""{""created"":""@1601383903.613729459"",""description"":""Error received from peer ipv6:[::]:8080"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1055,""grpc_message"":""Socket closed"",""grpc_status"":14}""
```
 Hi @altor - thanks for creating the issue! We are aware of this, the reason for this exception is that the current `Client`/`KerasClient` implementations do not implement a *graceful shutdown* when the server is done training. We'll soon release an update that fixes the issue :) until we do, if the server shows the final accuracy/loss logs, your workload ran successfully.Ok, For the moment, i'll just catch the exception and close the client properly
Thanks for your answer !We'll update this issue once the change has landed.Thanks !
PR is ready: #449 PR is merged, graceful shutdown is available on branch main. New release will follow shortly.thanks !@altor the new release 0.10.0 is now available on PyPI"	8	2020-09-29 13:11:20	2020-11-09 11:28:22	2020-11-09 11:28:22
