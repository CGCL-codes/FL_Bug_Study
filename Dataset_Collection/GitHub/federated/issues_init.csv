url,label,title,all_text,comments,created_time,updated_time,closed_time
https://github.com/tensorflow/federated/issues/3049,['bug'],Getting a TypeError ('Adam' object is not callable) with build_weighted_fed_avg_with_optimizer_schedule,"Getting a TypeError ('Adam' object is not callable) with build_weighted_fed_avg_with_optimizer_scheduleCan someone please tell me how to set the `client_learning_rate_fn` and `client_optimizer_fn` in `build_weighted_fed_avg_with_optimizer_schedule`?

Part of my code is as follows:

```
iterative_process = build_weighted_fed_avg_with_optimizer_schedule(
    model_fn,
    client_learning_rate_fn = lambda x: 0.001, 
    client_optimizer_fn=tf.keras.optimizers.Adam(learning_rate= client_lr, beta_1 = 0.9, beta_2 = 0.999,epsilon = 1e-07),
    server_optimizer_fn=optimizers.SGD)
```
This throw the error:

```

Traceback (most recent call last):
  File ""/home/Desktop/FL/fed.py"", line 352, in <module>
    iterative_process = build_weighted_fed_avg_with_optimizer_schedule(
  File ""/home/anaconda3/envs/fl/lib/python3.9/site-packages/tensorflow_federated/python/learning/algorithms/fed_avg_with_optimizer_schedule.py"", line 276, in build_weighted_fed_avg_with_optimizer_schedule
    client_work = build_scheduled_client_work(model_fn, client_learning_rate_fn,
  File ""/home/anaconda3/envs/fl/lib/python3.9/site-packages/tensorflow_federated/python/learning/algorithms/fed_avg_with_optimizer_schedule.py"", line 98, in build_scheduled_client_work
    whimsy_optimizer = optimizer_fn(1.0)
TypeError: 'Adam' object is not callable
```


As noted in the documentation (https://www.tensorflow.org/federated/api_docs/python/tff/learning/algorithms/build_weighted_fed_avg_with_optimizer_schedule), `build_weighted_fed_avg_with_optimizer_schedule` requires the `client_optimizer_fn` to be a function that takes as input the learning rate and returns an optimizer.

In your snippet above, you can do something like
```
def client_optimizer_fn(learning_rate):
    return tf.keras.optimizers.Adam(learning_rate=learning_rate, ...)
```For context: This is for two reasons. First, we need to be able to take the learning rate for a new round, and get a new optimizer. Second, we have to avoid constructing the keras optimizer outside of the graph context in which it will be used.Hi @zcharles8 ,

Bases on your reply, I tried:

```
def client_optimizer_fn(learning_rate):
    return tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1 = 0.9, beta_2 = 0.999,epsilon = 1e-07)

iterative_process = build_weighted_fed_avg_with_optimizer_schedule(
    model_fn,
    client_learning_rate_fn = lambda x: 0.001, 
    client_optimizer_fn=client_optimizer_fn(client_lr),
    server_optimizer_fn=optimizers.SGD)
```
But I am still getting the same error.

@shifdz Can you post the stack trace? It looks like the same problem is in your `server_optimizer_fn` which should also be callable (but a no-arg one) (though I'm not sure what `optimizers` refers to there)Hi @zcharles8 ,

My `server_optimizer_fn=tf.keras.optimizers.SGD` (no-arg) and I know the default is SGD with learning_rate 1.0. 

I even tried the following but still getting the same error.

```
def server_optimizer_fn():
    return tf.keras.optimizers.SGD
server_optimizer_fn= server_optimizer_fn()
```

This is the full error traceback:

```
Traceback (most recent call last):
  File ""/home/Desktop/FL/fed.py"", line 352, in <module>
    iterative_process = build_weighted_fed_avg_with_optimizer_schedule(
  File ""/home/anaconda3/envs/fl/lib/python3.9/site-packages/tensorflow_federated/python/learning/algorithms/fed_avg_with_optimizer_schedule.py"", line 276, in build_weighted_fed_avg_with_optimizer_schedule
    client_work = build_scheduled_client_work(model_fn, client_learning_rate_fn,
  File ""/home/anaconda3/envs/fl/lib/python3.9/site-packages/tensorflow_federated/python/learning/algorithms/fed_avg_with_optimizer_schedule.py"", line 98, in build_scheduled_client_work
    whimsy_optimizer = optimizer_fn(1.0)
TypeError: 'Adam' object is not callable
```
It looks like  `client_optimizer_fn` (Adam) is not accepting any arguments, but then how do I set `client_lr` and what is the default?Ah, I see the issue. In your snippet above you did

```
def client_optimizer_fn(learning_rate):
    return tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1 = 0.9, beta_2 = 0.999,epsilon = 1e-07)

iterative_process = build_weighted_fed_avg_with_optimizer_schedule(
    model_fn,
    client_learning_rate_fn = lambda x: 0.001, 
    client_optimizer_fn=client_optimizer_fn(client_lr),
    server_optimizer_fn=optimizers.SGD)
```
whereas you should not actually instantiate the `client_optimizer_fn`, ie. you should do something like
```
iterative_process = build_weighted_fed_avg_with_optimizer_schedule(
    model_fn,
    client_learning_rate_fn = lambda x: 0.001, 
    client_optimizer_fn=client_optimizer_fn,
    ...)
```Did you mean something like this:
```
def client_optimizer(learning_rate):
    return tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1 = 0.9, beta_2 = 0.999,epsilon = 1e-07)

client_optimizer_fn = client_optimizer(client_lr)

iterative_process = build_weighted_fed_avg_with_optimizer_schedule(
    model_fn,
    client_learning_rate_fn = lambda x: 0.001, 
    client_optimizer_fn=client_optimizer_fn,
    ...)
```

It still doesn't work. Tried this as well, 

```
client_optimizer_fn = tf.keras.optimizers.Adam(learning_rate=0.001)

iterative_process = build_weighted_fed_avg_with_optimizer_schedule(
    model_fn,
    client_learning_rate_fn = lambda x: 0.001, 
    client_optimizer_fn=client_optimizer_fn,
    ...)
```
But still getting the same error.

No, I meant the snippet that I posted. The core of it is that the `client_optimizer_fn` must be of type `Callable[[float], tf.keras.optimizers.Optimizer]`. In both of the examples you post, the `client_optimizer_fn` you pass is actually an optimizer.

So for example, one could do:
```
def client_optimizer_fn(learning_rate):
    return tf.keras.optimizers.SGD(learning_rate)

tff.learning.algorithms.build_weighted_fed_avg_with_optimizer_schedule(
    model_fn,
    client_learning_rate_fn = lambda x: 0.001,
    client_optimizer_fn=client_optimizer_fn)
````Maybe at an intuitive level: The fact that the argument is named `client_optimizer_fn` is important: It is a function that constructs optimizers, it is not an optimizer itself.Thank you so much @zcharles8 for the detailed explanation. It worked. So the `learning_rate` in `client_optimizer_fn` is assigned through `client_learning_rate_fn `, right?Exactly! It's worth noting that this complexity is primarily due to having to construct keras optimizers within the graph context in which they're going to be used. There's a simpler pathway for this kind of thing through `tff.learning.optimizers`, but we don't have a dedicated API symbol for FedAvg with client learning rate scheduling just for those optimizers (yet?). We're looking into making this kind of thing easier, but we have no concrete plans yet.Hi @zcharles8 , however, when trained with the `build_weighted_fed_avg_with_optimizer_schedule` algorithm training accuracy stay constant at around 0.5. I tried with several different learning rates ranging from 0.01 to 0.001. This behavior is not observed with other algorithms like fedprox and fedavg. Could you please tell me what could be the issue?",12,2022-07-12 18:57:24,2022-07-14 05:36:13,2022-07-13 19:55:31
https://github.com/tensorflow/federated/issues/3035,['bug'],AttributeError: 'LearningAlgorithmState' object has no attribute 'model',"AttributeError: 'LearningAlgorithmState' object has no attribute 'model'Hello, 

I am using tensorflow_federated version 0.28.

When I do `state.model.assign_weights_to(eval_model)` I get the following error:

`AttributeError: 'LearningAlgorithmState' object has no attribute 'model'`

Any help would be appreciated. 

**Additional context**
Add any other context about the problem here.
Hi @shifdz. Can you post the code snippet you used to get this?

It looks like you're using something like
```
fed_avg = tff.learning.algorithms.build_weighted_fed_avg(...)`
```
If so, the state of `fed_avg` (eg. as returned by `fed_avg.initialize()`) does not have a `.model` attribute in its state. You can get the model weights by instead calling `fed_avg.get_model_weights(state)`.Hi @zcharles8 ,

Thank you. That worked.",2,2022-07-05 19:32:24,2022-07-08 19:59:36,2022-07-08 19:59:36
https://github.com/tensorflow/federated/issues/2888,['bug'],tf.print not outputting anything in federated computation in multi-machine setup,"tf.print not outputting anything in federated computation in multi-machine setupHi, is there a way to output to std.out or a file from a client directly when running in a multi-machine setup?

For example:
```
@computations.federated_computation(FederatedType(tf.string, placements.CLIENTS))
def fed_print(x):
  tf.print(x)

federated_map(fed_print, client_messages)
```

Doesn't printout anything on the head or worker nodes.Doing something like this *is possible*, but the `tf.print` op must be captured in a TF computation. That is, something like the following should work:

```python
@tff.tf_computation(tf.string)
@tf.function
def print_and_return(x):
  # we get program order semantics here due to the tf.function,
  # which is why we use that as a second-decorator. Otherwise
  # we'd need to control-dep.
  tf.print(x)
  return x

@tff.federated_computation(tff.type_at_clients(tf.string))
def map_and_print(x):
  return tff.federated_map(print_and_return, x)
```

I'd share a colab, but I think they're all broken at the moment due to version mismatch (unless Colab has updated to Python 3.9). In general, TFF does not support the 'mixing' of TensorFlow and federated code; all TensorFlow code should be captured in TFF's `tf_computation` decorator. This was an early design choice in TFF, in order to help separate concerns--`federated_computation` decorators should usually contain logic which 'moves' tensors around--broadcasting, aggregating, etc.--or transforms them 'in groups'--mapping functions across the client or the server, for instance. Local logic should usually be expressed in one of TFF's local computation decorators, like `tf_computation` or the recently introduced `jax_computation`.",1,2022-05-25 16:01:50,2022-07-16 18:47:03,2022-07-16 18:47:02
https://github.com/tensorflow/federated/issues/2770,['bug'],Cannot import library tensorflow_federated,"Cannot import library tensorflow_federatedI run Tensorflow's notebook tutorial for Federated Learning for Image Classification (https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) and it fails to import the library tensorflow_federated in the third code cell of the notebook despite the fact that the library is properly installed on the first cell.

 It returns error:

TypeError                                 Traceback (most recent call last)
[<ipython-input-2-a23308ec3f7c>](https://localhost:8080/#) in <module>()
      3 import numpy as np
      4 import tensorflow as tf
----> 5 import tensorflow_federated as tff
      6 
      7 np.random.seed(0)

I dont understand why this is happening. I was able to run successfully this notebook until last week.

It's 2 days now I'm facing the same issue, I read elsewhere and they said we should update the python version. But still it is not working. If using Colab, this likely occurs because Colab is running Python 3.7 but TFF requires Python 3.9.

The following can version the local Python version.

```python
import sys
print(sys.version)
```

If using Colab, for now follow the instructions in https://github.com/tensorflow/federated/issues/2748#issuecomment-1107437271Thanks, 
But I tried to upgrade the python to the 3.9.12 version. But still, Please what should I do concretely?
Should I just install the previous release TF-F? 
Thanks for your comment If using https://colab.research.google.com/, installing the `0.20.0` version of TFF following the instructions at https://github.com/tensorflow/federated/issues/2748#issuecomment-1107437271 is recommended.

Otherwise including the following details would help:

- What is the output of `print(sys.version)` when run in a cell of the notebook?
- What is the output of `pip freeze`?
- Please copy-paste the entire stacktrace included with the error.Thanks for the support> Here are the results 

import sys
print(sys.version)

3.7.13 (default, Apr 24 2022, 01:04:09) 
[GCC 7.5.0]

pip freeze

output : 

Traceback (most recent call last):
  File ""/usr/local/bin/pip3"", line 5, in <module>
    from pip._internal.cli.main import main
  File ""/usr/lib/python3/dist-packages/pip/__init__.py"", line 29, in <module>
    from pip.utils import get_installed_distributions, get_prog
  File ""/usr/lib/python3/dist-packages/pip/utils/__init__.py"", line 23, in <module>
    from pip.locations import (
  File ""/usr/lib/python3/dist-packages/pip/locations.py"", line 9, in <module>
    from distutils import sysconfig
ImportError: cannot import name 'sysconfig' from 'distutils' (/usr/lib/python3.9/distutils/__init__.py)Here is the type error I have while trying to import 
import tensorflow_federated as tff

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
[<ipython-input-32-dff7d3b60363>](https://localhost:8080/#) in <module>()
      1 import tensorflow as tf
----> 2 import tensorflow_federated as tff

7 frames
[/usr/local/lib/python3.7/dist-packages/tensorflow_federated/python/common_libs/structure.py](https://localhost:8080/#) in <module>()
    263 
    264 def to_odict(struct: Struct,
--> 265              recursive: bool = False) -> collections.OrderedDict[str, Any]:
    266   """"""Returns `struct` as an `OrderedDict`, if possible.
    267 

TypeError: 'type' object is not subscriptableI finally got the solution: 
Try this out: 

!pip install --quiet  tensorflow-federated==0.20.0   # The latest version of tensorflow-federated is not working with the colab python version
!pip install --quiet --upgrade nest-asyncioDuplicate of #2748",8,2022-05-02 09:01:40,2022-05-03 14:55:08,2022-05-03 14:55:08
https://github.com/tensorflow/federated/issues/2768,['bug'],Accuracy getting stucked at around 0.1 in some models,"Accuracy getting stucked at around 0.1 in some modelsI'm trying train a federated model for the mnist dataset. I am using the code avaible at https://www.tensorflow.org/federated/tutorials/simulations for the setup. 
The dataset version being used is the the one from keras (not the federated version from leaf that is used in tff). I'm making a partition of it, saving it on a dictionary and implementing my ClientData instance with `tff.simulation.datasets.TestClientData`.  
Applying this change works just fine. However, if I change the model from the simulation, every round gives me a ~0.1 accuracy.  

The model in the tutorial is as simple as it can get, an input layer of 28*28=784 neurons stacked over an output layer of dim 10 with Softmax activation: 

    model = tf.keras.models.Sequential([
      tf.keras.layers.InputLayer(input_shape=(784,)), 
      tf.keras.layers.Dense(units=10, kernel_initializer='zeros'),
      tf.keras.layers.Softmax(),
    ])

And the new model is a cnn:

```
 model = tf.keras.Sequential(
        [
            tf.keras.layers.Conv2D(
                16,
                8,
                strides=2,
                padding=""same"",
                activation=""relu"",
                input_shape=(28, 28, 1),
            ),
            tf.keras.layers.MaxPool2D(2, 1),
            tf.keras.layers.Conv2D(
                32, 4, strides=2, padding=""valid"", activation=""relu""
            ),
            tf.keras.layers.MaxPool2D(2, 1),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(32, activation=""relu""),
            tf.keras.layers.Dense(10),
        ]
    )
```

Accuracy changed from round to round on the first case, increasing, reaching 0.94 quite fast. 
On the second case I ran it for about 240 rounds with 3 fixed clients, 20k elements each, 10 epochs, batch size 32. Still couldn't get out of the ~0.1 accuracy and loss of ~2.3 

The model works fine for this dataset. I already tested it on a centrilized version and a federated version using Flower framework reaching 0.99 accuracy. But for some reason I can't make it work on tff.

Environment:
MacOs BigSur
tensorflow==2.8.0
tensorflow-federated==0.22.0

I expect the metrics and loss to change more. Could it be that there is a problem with using other Models?

Full code:

```
from tensorflow.keras.datasets import cifar10, mnist
import numpy as np

EPOCHS = 10
BATCH_SIZE = 32

# ROUND_CLIENTS <= NUM_CLIENTS
ROUND_CLIENTS = 3
NUM_CLIENTS = 3

NUM_ROUNDS = 400

    
def make_client(num_clients,X, y):
    total_image_count = len(X)
    image_per_set = int(np.floor(total_image_count/num_clients))

    client_train_dataset = collections.OrderedDict()
    for i in range(1, num_clients+1):
        client_name = i-1
        start = image_per_set * (i-1)
        end = image_per_set * i

        print(f""Adding data from {start} to {end} for client : {client_name}"")
        data = collections.OrderedDict((('label', y[start:end]), ('pixels', X[start:end])))
        client_train_dataset[client_name] = data
    
    train_dataset = tff.simulation.datasets.TestClientData(client_train_dataset)
    
    return train_dataset


def preprocess(X: np.ndarray, y: np.ndarray):
    """"""Basic preprocessing for MNIST dataset.""""""
    X = np.array(X, dtype=np.float32) / 255
    X = X.reshape((X.shape[0], 28, 28, 1))

    y = np.array(y, dtype=np.int32)
    y = tf.keras.utils.to_categorical(y, num_classes=10)

    return X, y


(X_train, y_train), (X_test, y_test) = mnist.load_data()
(X_train, y_train) = preprocess(X_train, y_train)
(X_test, y_test) = preprocess(X_test, y_test)

mnistFedTrain = make_client(NUM_CLIENTS,X_train,y_train)

def map_fn(example):
    return collections.OrderedDict(
      x=example['pixels'], 
        y=example['label'])


def client_data(client_id):
    ds = mnistFedTrain.create_tf_dataset_for_client(mnistFedTrain.client_ids[client_id])
    return ds.repeat(EPOCHS).shuffle(500).batch(BATCH_SIZE).map(map_fn)


train_data = [client_data(n) for n in range(ROUND_CLIENTS)]
element_spec = train_data[0].element_spec

def create_cnn_model() -> tf.keras.Model:
    """"""Returns a sequential keras CNN Model.""""""
    return tf.keras.Sequential(
        [
            tf.keras.layers.Conv2D(
                16,
                8,
                strides=2,
                padding=""same"",
                activation=""relu"",
                input_shape=(28, 28, 1),
            ),
            tf.keras.layers.MaxPool2D(2, 1),
            tf.keras.layers.Conv2D(
                32, 4, strides=2, padding=""valid"", activation=""relu""
            ),
            tf.keras.layers.MaxPool2D(2, 1),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(32, activation=""relu""),
            tf.keras.layers.Dense(10),
        ]
    )

def model_fn():
    model = create_cnn_model()
    return tff.learning.from_keras_model(
      model,
      input_spec=element_spec,
      loss=tf.keras.losses.CategoricalCrossentropy(
                from_logits=True, reduction=tf.losses.Reduction.NONE
            ),
      metrics=[tf.keras.metrics.CategoricalAccuracy()]
    )


trainer = tff.learning.build_federated_averaging_process(
    model_fn, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.02))


def evaluate(num_rounds=NUM_ROUNDS):
    state = trainer.initialize()
    for i in range(num_rounds):
        t1 = time.time()
        state, metrics = trainer.next(state, train_data)
        t2 = time.time()
        print('\n Round {r}: metrics {m}, round time {t:.2f} seconds'.format(
            m=metrics['train'], r=i, t=t2 - t1))

t1 = time.time()
evaluate(NUM_ROUNDS)
t2 = time.time()

print('Seconds:',t2 - t1,' = Minutes:', (t2 - t1)/60)
```


I've had a similar problem with other models as well, e.g.  MobileNetV2 implemented in tf for cifar10:
`model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)Were you able to find the solution?I tried to test the same models in a centralized setup and I had similar results. The problem was that the loss, output type and accuracy have to match. What i mean is:

When using one-hot encoding for the output the loss has to be of the type SparseCategorical, the accuracy as well.
So I changed 
```
loss=tf.keras.losses.CategoricalCrossentropy(
          from_logits=True, reduction=tf.losses.Reduction.NONE
      ),
metrics=[tf.keras.metrics.CategoricalAccuracy()]
)
```

into 

```
  loss=tf.keras.losses.SparseCategoricalCrossentropy(),
  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
```

also dropping the parameters in the loss, `from_logits=True` makes the loss believe you are NOT gonna be outputing probabilities, which is what my models where doing.

Also careful with `kernel_initializer='zeros'`. Its ok in the output layer  but using it on a hidden layer makes the gradient vanish, see [this](https://datascience.stackexchange.com/questions/37378/what-are-kernel-initializers-and-what-is-their-significance), which was another reason why my models where learning bananas. 

The problem wasn't in the TFF side of things. If there is anything to improve on their side is to just make it simple with the models in the tutorials. Like leave as much default parameters in the keras models as you can.",2,2022-04-29 03:02:22,2022-06-03 03:47:55,2022-06-03 03:47:55
https://github.com/tensorflow/federated/issues/2706,['bug'],How to fix this issue? AttributeError：'EnhancedModel'object has no attribute 'report_local_unfinalized_metrics',"How to fix this issue? AttributeError：'EnhancedModel'object has no attribute 'report_local_unfinalized_metrics'# Hello, I am learning your code, but there will be such a bug when running your code([federated]federated/tensorflow_federated/examples/simple_fedavg), may I ask what is going on?

- <img width=""815"" alt=""162010135-21680480-d38e-40b9-b9fc-0ac6c1860de3"" src=""https://user-images.githubusercontent.com/52623788/162102045-97571d42-d168-40f9-a58c-00fa9d1f3b1b.png"">

- <img width=""818"" alt=""162009169-5b14e97c-b95d-4184-86cf-ae965be74753"" src=""https://user-images.githubusercontent.com/52623788/162102053-81851819-9444-4238-9e3f-b9cd2c198443.png"">
I already know why. 
![捕获](https://user-images.githubusercontent.com/52623788/162115439-be156c75-269c-4458-be50-0073eeae1170.PNG)
Reference:https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model#report_local_outputs
Could you tell clearly? I see no problem with the picture.Thanks!Version problem, never mind.do you mean tensorflow_federated version? yes! In the new version, some functions have replaced.

> I already know why. ![捕获](https://user-images.githubusercontent.com/52623788/162115439-be156c75-269c-4458-be50-0073eeae1170.PNG) Reference:https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model#report_local_outputs

The picture tells us to use report_local_unfinalized_metrics and the code sample does exactly that.My tensorflow_federated version is 0.19.0 .Could you remember how you modify the code?pip install tensorflow_federatedI have already install tensorflow_federated=0.19.0 ,you mean I should lower the version?
I have solved the problem. Thank you for your answer",9,2022-04-07 01:38:02,2022-04-22 09:58:13,2022-04-07 03:39:19
https://github.com/tensorflow/federated/issues/2685,['bug'],Multiple output and multiple losses with tensorflow_federated,"Multiple output and multiple losses with tensorflow_federatedI have the following model architecture:

    def create_keras_model():
        input = keras.Input(shape=(24, ), name=""original_inpt"")
        x = tf.keras.layers.Dense(512,activation='relu')(input)
        x = tf.keras.layers.Dense(256,activation='relu')(x)
        x = tf.keras.layers.Dense(64,activation='relu')(x)

        out1 = keras.layers.Dense(1,name = 'loss1')(x)
        out2 = keras.layers.Dense(1,name = 'loss2')(x)


        encoder = keras.Model(inputs = input, outputs = [out1, out2], name=""encoder"")
        return encoder

How do I pass the model_fn function:

    def model_fn():
        keras_model = create_keras_model()
        return tff.learning.from_keras_model(
          keras_model,
          input_spec=preprocessed_sample_dataset.element_spec,
           loss = [tf.keras.losses.MeanSquaredError(),tf.keras.losses.AbsoluteError()],,
          metrics = {
               ""loss1"": [tf.keras.metrics.MeanSquaredError()],
               ""loss2"": [tf.keras.metrics.MeanSquaredError()]
           })

It gives the following error:
InvalidArgumentError: Graph execution error:

Incompatible shapes: [500,1] vs. [2,500]
	 [[{{node SquaredDifference}}]]
	 [[StatefulPartitionedCall_1/StatefulPartitionedCall/ReduceDataset]] [Op:__inference_pruned_97396]

Does `keras.Input(shape=(24, ), name=""original_inpt"")` match the shape of your input dataset? Also, metrics should be a list, not a dictionary.@wennanzhu Thank you for your response. Yes. Also, I was able to fix the error by changing code to this:

  def create_keras_model():

      input = keras.Input(shape=(24, 1), name=""original_inpt"")
      x = tf.keras.layers.Conv1D(filters=4,kernel_size=4,padding = 'same',input_shape=(24,1))(input)
      x = tf.keras.layers.MaxPool1D(pool_size=2)(x)
      x = tf.keras.layers.Conv1D(filters=8,kernel_size=4,padding = 'same')(x)
      x = tf.keras.layers.MaxPool1D(pool_size=2)(x)
      x = tf.keras.layers.LSTM(16, return_sequences=False)(x)
      
      
      x1_2 = tf.keras.layers.Dense(4,activation='relu')(x)
      x2_2 = tf.keras.layers.Dense(4,activation='relu')(x)

      out1 = keras.layers.Dense(1,name = 'hypo_loss')(x1_2)
      out2 = keras.layers.Dense(1,name = 'hyper_loss')(x2_2)

       encoder = keras.Model(inputs = input, outputs = [out1, out2], name=""encoder"")
       return encoder

  def model_fn():

      keras_model = create_keras_model()
      return tff.learning.from_keras_model(
        keras_model,
        input_spec=preprocessed_sample_dataset.element_spec,
        loss = [custom_loss_hyper(),custom_loss_hypo()],
        metrics = [tf.keras.metrics.MeanSquaredError(),tf.keras.metrics.MeanSquaredError()])


But my model is performing terribly compared to a central model. The training-loss in a central model reduces to about 600-700. In the FL setup, it reduces until 6000 but starts to increase after that.
I checked all my local models individually to see if there are outlier issues but they all seem to work fine. In addition, I got access to a manual code that averages weights across local models and seems to work as expected giving loss in the range near to the central model. Not sure, why it is not performing the same with 'tensorflow_federated'. Can you please help@darpitdavetamu It's unfortunately difficult to debug this without the full code. That being said, loss diverging is often symptomatic of a learning rate (either client or server) being too large. I'd recommend trying a couple. Note that the learning rate needed may be slightly different than the ""manual code"" you refer to depending on how certain normalizations occur when taking averages.@zcharles8 Thank you for your reply. That's an interesting point. I did try to play with a few but not that rigorously. Give me a day or two to rigorously go through multiple combinations and check if there is change in model behavior. I will get back here for the same. 

In the manual code and tensorflow_federated:
Client optimizer: Adam and lr = 0.001
Server optimizer: SGD and lr = 1.0

However, I suspect that in the manual codes weights (\theta) are directly being averaged as opposed to differences between \theta_t and \theta_(t-1) as in 'tensorflow_federated'. Any suggestions on the changes I should focus on to match the performance?

Also, here is the link to the code I am using to implement federated learning: https://github.com/darpitdavetamu/Federated-Learning/blob/main/FL_MultiOutput(Manual_Code).ipynb",4,2022-03-28 23:26:33,2022-04-29 17:38:04,2022-04-29 17:38:04
https://github.com/tensorflow/federated/issues/2659,['bug'],dataset_computation not implemented error,"dataset_computation not implemented errorWhile trying to implement the python program working_with_client_data.ipynb provided in githhub, I am getting the below error as he function is not implemented. Can anyone help me when this will be available for public use? I was trying to check the program if it helps in the model training on the client device data.

def dataset_computation(self):
    raise NotImplementedError(""b/162106885"")




Looking in our code, I think this implementation should be finished (that bug is [no longer referenced in TFF](https://github.com/tensorflow/federated/search?q=b%2F162106885)). This jibes with my recollection as well.

It might be that you're running an old version of TFF? Can you check you're on the 0.20.0 release, as I *believe* this should contain the appropriate implementations?Apologies!! Updated the version to 0.20.0 and it works fine!!",2,2022-03-18 14:49:09,2022-04-04 15:13:34,2022-04-04 15:13:33
https://github.com/tensorflow/federated/issues/2593,[],Running a tff computation inside an uvloop event loop,"Running a tff computation inside an uvloop event loopHello, I am trying to run tff computations inside an async server that is using an [uvloop](https://uvloop.readthedocs.io/) event loop. I cannot use a different type of event loop as this would imply modifications in the used framework.

Here is an simplified example from the websockets [docs](https://websockets.readthedocs.io/en/9.0.1/intro.html) that uses an uvloop event loop and also tries to run a basic tff computation. 

```
import asyncio
import uvloop

import websockets
import tensorflow_federated as tff
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())  

async def hello(websocket, path):
    name = await websocket.recv()
    print(f""< {name}"")

    greeting = f""Hello {name}!""
    tff.federated_computation(lambda: 'Hello World')()

    await websocket.send(greeting)
    print(f""> {greeting}"")

start_server = websockets.serve(hello, ""localhost"", 8765)

asyncio.get_event_loop().run_until_complete(start_server)
asyncio.get_event_loop().run_forever()
```

When I send a request to the server I get the following error.

> RuntimeError: Cannot run the event loop while another loop is running

I have tried to run nest_asyncio as [recommended](https://github.com/tensorflow/federated/blob/3a2e85c71e1d2fefdad57c62e5fc6d3dd7e9f99d/docs/tutorials/README.md) in the case of Jupyter notebooks, but I get 

> ValueError: Can't patch loop of type <class 'uvloop.Loop'>

because at the moment nest_asyncio does not support other types of event loops apart from asyncio as seen [here](https://github.com/erdewit/nest_asyncio). I understand that generally this is not a wanted behaviour in [python](https://bugs.python.org/issue22239) but do you know a workaround for this issue? Or maybe, is there a way to run computations without an event loop?I think what you want is just to *avoid* TFF's synchronous Python interface.

That is, TFF implements (but AFAIK does not expose via a public namespace) an [asynchronous execution context](https://github.com/tensorflow/federated/blob/main/tensorflow_federated/python/core/impl/execution_contexts/async_execution_context.py), where the computation invocation in your code above (the ""Hello world"" bit) would actually *be* a coroutine function.

Supposing we exported our asynchronous execution context in some manner like `tff.backends.native.set_local_python_execution_context(async=True)`, I think your code snippet would naturally read:

```
import asyncio
import uvloop

import websockets
import tensorflow_federated as tff
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())  

async def hello(websocket, path):
    name = await websocket.recv()
    print(f""< {name}"")

    greeting = f""Hello {name}!""
    string = await tff.federated_computation(lambda: 'Hello World')()
    print(string)

    await websocket.send(greeting)
    print(f""> {greeting}"")

# this line sets some global state, and will make the federated computation invocation above return an awaitable.
tff.backends.native.set_local_python_execution_context(async=True)
start_server = websockets.serve(hello, ""localhost"", 8765)

asyncio.get_event_loop().run_until_complete(start_server)
asyncio.get_event_loop().run_forever()
```

and I *believe* this would 'just work', since TFF would not be trying to run its own event loop, wrapping asynchronous code as synchronous. FWIW this limitation/issue seems to be [sufficiently well-known](https://bugs.python.org/issue22239) that Python folks are perhaps reconsidering the possibility of nested event loops.

We (TFF) would need to introduce the appropriate parameterization of our top-level convenience functions for constructing and installing execution contexts, but I don't think this would be too difficult or controversial. This would allow you to build TFF from source and write code like the above, or wait for the next pip package release. We're often slow to make releases, but I think that is in part because there isn't often a compelling use-case to make one. If this is something that would make your life much easier, we could probably push the timeline a little quicker.Thank you for your help! Indeed this issue seems solved, both in the example I showed earlier and in my actual codebase.",2,2022-03-03 20:22:50,2022-03-16 16:55:03,2022-03-16 16:55:03
https://github.com/tensorflow/federated/issues/2561,['bug'],Docs site: Tutorials link straight to ipynb files instead of colab,"Docs site: Tutorials link straight to ipynb files instead of colab**Describe the bug**
Links from [this tutorials page](https://www.tensorflow.org/federated/tutorials/tutorials_overview) are just pointing to downloadable iPython notebooks. I would've expected them to open in colab, since the tutorial page specifically calls the tutorials ""colab-based"".

**Environment (please complete the following information):**
Brave Browser (don't think this matters though)

**Expected behavior**
Tutorials should either open up in colab or link to the GitHub-hosted version of the rendered ipynb. Alternatively, the tutorials page should just say that the tutorials are ipynb instead of ""colab-based""@jvmncs Thanks for reporting this. I saw that this was indeed happening yesterday, but can't seem to reproduce it today. Can you confirm whether or not this is still an issue for you? A nightly TF doc change might have fixed it.Yes, looks like the tutorials are redirecting to the tensorflow.org docs site on my end again. Thanks for prompt response 👍 ",2,2022-02-22 16:31:58,2022-02-24 22:23:09,2022-02-24 22:23:09
https://github.com/tensorflow/federated/issues/2455,[],Custom loss function not working,"Custom loss function not working### My custom-loss function
 def custom_loss(keras.losses.loss):
      return K.mean(K.square((self.y_pred - self.y_true)/self.y_true))

### Model parameters

def model_fn():    
    keras_model = create_keras_model()
    return tff.learning.from_keras_model(
      keras_model,
      input_spec=preprocessed_sample_dataset.element_spec,
      loss = custom_loss(),
      metrics=[tf.keras.metrics.MeanSquaredError()])

TypeError: Expected keras.losses.Loss, found type.

I tried looking at Custom Federated Algorithims Part 2: URL: https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2#defining_a_loss_function

But I am unable to replicate the same in my code?
If you want to use the `loss` argument in `tff.learning.from_keras_model`, you should assign `loss` to a `tf.keras.losses.Loss` object. See https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss. 

The https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2 tutorial shows how to build a customized federated computation without using the `tff.learning.from_keras_model` API.@wennanzhu you mean something like this:

class custom_loss(Loss):

  def call(self, y_true, y_pred):
    y_pred = tf.convert_to_tensor_v2(y_pred)
    y_true = tf.cast(y_true, y_pred.dtype)
    return tf.reduce_mean(math_ops.square((y_pred - y_true)/y_true), axis=-1)Yes, something like that.@wennanzhu unfortunately I have tried that. But it is not working. Keep getting the same error.Created custom loss exactly as above and used as here:

def model_fn():
    keras_model = create_keras_model()
    return tff.learning.from_keras_model(
      keras_model,
      input_spec=preprocessed_sample_dataset.element_spec,
      loss = custom_loss,
      metrics=[tf.keras.metrics.MeanSquaredError()])@darpitdavetamu I think you need an object of `tf.keras.losses.Loss`, could you try`loss = custom_loss()`?i am getting the following error:



TypeError: in user code:

    File ""/Users/darpitdave/opt/anaconda3/envs/Fl_Env/lib/python3.9/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py"", line 353, in _compute_local_training_and_client_delta  *
        client_output = client_delta_fn(dataset, initial_model_weights)
    File ""/Users/darpitdave/opt/anaconda3/envs/Fl_Env/lib/python3.9/site-packages/tensorflow_federated/python/learning/federated_averaging.py"", line 100, in reduce_fn  *
        output = model.forward_pass(batch, training=True)
    File ""/Users/darpitdave/opt/anaconda3/envs/Fl_Env/lib/python3.9/site-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py"", line 28, in _dataset_reduce_fn  *
        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)
    File ""/Users/darpitdave/opt/anaconda3/envs/Fl_Env/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py"", line 412, in forward_pass  *
        return self._forward_pass(batch_input, training=training)
    File ""/Users/darpitdave/opt/anaconda3/envs/Fl_Env/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py"", line 379, in _forward_pass  *
        batch_loss = tf.add_n([loss_fn(y_true=y_true, y_pred=predictions)] +
    File ""/var/folders/dl/qpbnclts6xz8qb73n20zb8q80000gn/T/ipykernel_65622/3531200346.py"", line 3, in call  *
        return K.mean(K.square((y_pred - y_true)/y_true))

    TypeError: Input 'y' of 'Sub' Op has type float64 that does not match type float32 of argument 'x'.It seems the custom loss function is working, but you are getting a type mismatch error in `tf.reduce_mean(math_ops.square((y_pred - y_true)/y_true), axis=-1)`. Have you tried to cast values to `tf.float32`?@wennanzhu Thank you for the help. Yes, i tweaked things using the cast function to match them. it's working all fine.",9,2022-02-04 22:48:03,2022-02-17 21:14:30,2022-02-17 21:14:30
https://github.com/tensorflow/federated/issues/2309,['bug'],"CIFAR100, Train Images Not Distorted?","CIFAR100, Train Images Not Distorted?**Describe the bug**
I noticed that the training images for the CIFAR100 dataset are cropped centrally (instead of distorted). Is this intentional? Thanks in advance for the help!

https://github.com/tensorflow/federated/blob/f3df9197881a93e93db6b2e7790738e829b62232/tensorflow_federated/python/simulation/baselines/cifar100/image_classification_tasks.py#L117-L120

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
* Python package versions (e.g., TensorFlow Federated, TensorFlow):
* Python version:
* Bazel version (if building from source):
* CUDA/cuDNN version:
* What TensorFlow Federated execution stack are you using?

Note: You can collect the Python package information by running `pip3 freeze`
from the command line and most of the other information can be collected using
TensorFlows environment capture
[script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).

**Expected behavior**
A clear and concise description of what you expected to happen.

**Additional context**
Add any other context about the problem here.
Hi @HanGuo97. While my vague recollection is that it may have been intentional due to the behavior of the random distortion within a TFF computation, I'm not 100% certain. Thanks for pointing this out. I'll investigate when I get a chance.

One thing I will note: The baselines are intended only as a kind of guideline on how to evaluate federated learning methods. We strongly encourage users to use whatever works best for them, including their own custom tasks.Got it, thanks for the quick response!Hi, I just want to follow up on this question. Notably, I noticed that in the `google-research/federated` library, the preprocessing is done differently ""in the past"" (notably `google-research/federated/optimization/`).

More precisely, before `google-research/federated` was standardized to use `tensorflow/federated` for preprocessing, the CIFAR100 dataset is preprocessed differently that did have random cropping. It uses TFF to load the data but uses its own preprocessing, which has a flag for random cropping that is set to True by default. Later it starts to rely on this repo. Since this repo does not use random cropping, both libraries stop adding randomization (presumably).

I understand this is likely a minor detail, but I'm curious if this is expected or desired behavior (i.e., removing randomization is better)?Hi @HanGuo97. This should be resolved by https://github.com/tensorflow/federated/commit/52eb9bdeadf1e82729e5fd572108bcd666aee8b4, which adds a toggle for whether to distort train images or not in the task. Thanks for pushing on this.Great to hear and thanks a ton!",5,2021-12-28 22:12:51,2022-04-05 17:04:09,2022-04-05 16:46:10
https://github.com/tensorflow/federated/issues/2307,['bug'],tensorflow-federated 0.19.0 doesn' work with tensorflow-metal on Intel MBP anymore,"tensorflow-federated 0.19.0 doesn' work with tensorflow-metal on Intel MBP anymore **Describe the bug**
I am able to use tff with tensorflow-metal 0.2.0 on an Intel based Macbook Pro in the venv with following packages
```
SYSTEM_VERSION_COMPAT=0 pip install tensorflow-macos==2.5.0 tensorflow-federated==0.19.0 tensorflow-metal==0.2.0
```
to run the example tff code at https://www.tensorflow.org/federated

But as I switched to tensorflow-metal 0.3.0
```
SYSTEM_VERSION_COMPAT=0 pip install tensorflow-macos==2.5.0 tensorflow-federated==0.19.0 tensorflow-metal==0.3.0
```
I got the following error for the same example code:
```
/usr/local/Cellar/python@3.8/3.8.12_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
```

Does the tensorflow-metal 0.3.0 breaks the tensorflow-federated 0.19.0 now?

**Environment (please complete the following information):**
* OS Platform and Distribution: 
macOS Monterey 12.1, 6 Core Intel Core i7, Radeo Pro 555X 4GB
* Python package versions (e.g., TensorFlow Federated, TensorFlow): 
tensorflow-macos==2.5.0 tensorflow-federated==0.19.0 tensorflow-metal==0.3.0
* Python version: 
python@3.8/3.8.12_1
* Bazel version (if building from source): 
no
* CUDA/cuDNN version: 
no
* What TensorFlow Federated execution stack are you using? 
simulation on single node with tff.learning.build_federated_averaging_process


**Expected behavior**
The example code from https://www.tensorflow.org/federated for single node simulation with tff and tensorflow-macos 2.5.0 and tensorflow-metal 0.3.0 runs. 

**Additional context**
It works with tensorflow-metal 0.2.0 but not with tensorflow-metal 0.3.0 any more.
An updated with the following MacOSX env and tf-federated seems to work again.
I am closing this issue.

```console
SYSTEM_VERSION_COMPAT=0 pip install tensorflow-macos==2.8.0 tensorflow-metal==0.4.0;
SYSTEM_VERSION_COMPAT=0 pip install tensorflow-federated==0.20.0
```

**Updated Environment** (please complete the following information):
* OS Platform and Distribution:
macOS Monterey 12.3, 6 Core Intel Core i7, Radeo Pro 555X 4GB
* Python package versions (e.g., TensorFlow Federated, TensorFlow):
tensorflow-macos==2.8.0 tensorflow-federated==0.20.0 tensorflow-metal==0.4.0
* Python version:
python@3.8/3.8.12_1",1,2021-12-25 11:26:40,2022-03-20 12:52:21,2022-03-20 12:52:20
https://github.com/tensorflow/federated/issues/2239,['bug'],Anyway to install tff with TensorFlow-rocm,"Anyway to install tff with TensorFlow-rocmI have a AMD GPU, so I can't use TensorFlow-gpu for GPU training. The only way for using gpu computing on TensorFlow is by tensorflow-rocm library. 
However I can not installing a tff version on tensorflow-rocm
When I am trying to install tensorflow_federated using pip install tensorflow_federated==0.18.0, it would automatically install another cpu version of tensorflow and replace tensorflow-rocm with tensorflow2.4.0cpu. Anyway to have a gpu version of tff with AMD gpu?Hi @yshen22. I'm not an expert in this kind of thing, but could you try something like the following:

1. `pip install tensorflow_federated`
2. `pip uninstall tensorflow`
3. `pip install tensorflow-rocm`

There may be mismatched dependencies that make this not work, but it could lead to what you want.@zcharles8  Thank you for your help. I have tried your steps. It is working now the on the surface as follows. However I am not sure whether it would be robust when I try more advanced functions. But I will come to your help when mismatched dependencies come. 
>>> import tensorflow_federated as tff
>>> print(tff.federated_computation(lambda: 'Hello World')())
WARNING:tensorflow:From /home/xx/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:59: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
WARNING:tensorflow:From /home/xx/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:59: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
b'Hello World'

It sounds like this is tentatively working for now. If something strange happens, feel free to reopen.",3,2021-12-07 16:03:47,2021-12-15 16:20:31,2021-12-15 16:20:30
https://github.com/tensorflow/federated/issues/2211,[],What is the role of 'clipped_count_stddev' in differentially private gaussian_adaptive aggregator?,"What is the role of 'clipped_count_stddev' in differentially private gaussian_adaptive aggregator?I was going through the documentation of [gaussian_adaptive aggregator](https://www.tensorflow.org/federated/api_docs/python/tff/aggregators/DifferentiallyPrivateFactory?version=nightly#gaussian_adaptive), and one of the function arguments is `clipped_count_stddev`. Description says that it is 
```
The stddev of the noise added to the clipped counts in the adaptive clipping algorithm
```
But I thought the standard deviation of Gaussian noise added is calculated as **'clipping_threshold * noise_multiplier'**, where clipping threshold is found adaptively and the noise multiplier is fixed. 

What am I missing? Thank you!`clipped_count_stddev` is the standard deviation added to the count of clipped updates for private estimation of the quantile for adaptive clipping. It corresponds to \sigma_b in the paper https://arxiv.org/pdf/1905.03871.pdf. If you do not set this argument in `gaussian_adaptive`, it will default to the recommended settings from the paper as implemented in `adaptive_clip_noise_params`. This should work very well for almost all users.Thank you @galenmandrew! So if I understand it right, `sigma_b` is the standard deviation of noise used in estimating the _clipping threshold_ and noise multiplier `z_delta` is used to add the noise to the updates themselves. 

I have another question then. If `clipped_count_stddev` is by default = _**0.05 * clients_per_round**_, then won't this noise be higher for more clients_per_round? Unless it is later normalized with respect to number of clients. 

I actually wanted to see the effect of number of clients participating in a round on training.Your first point is almost correct. The noise multiplier that you specify is the *effective noise multiplier* for DP accounting (z in the paper). Due to the need to account for privacy when estimating the clipping threshold, the actual amount of noise added to the updates (z_\Delta in the paper) will be very slightly (even negligibly) higher.

On the second point: yes the noise added to the sum of clipped counts will be higher for more clients per round, but note that it will be normalized by the `clients_per_round` so the noise on the average remains the same: 0.05. We find that 0.05 gives a reasonable amount of error in the estimation process while using a negligible fraction of the privacy budget.Thank you for clarifying! I have understood it now.",4,2021-12-02 16:52:45,2021-12-04 02:08:03,2021-12-04 02:08:03
https://github.com/tensorflow/federated/issues/2190,[],Using CIFAR-100 datased with VGG19 model in simple_fedavg example,"Using CIFAR-100 datased with VGG19 model in simple_fedavg exampleI'm actually trying to change the dataset and the model, but i can't get any positive feedback, the accuracy is always at 1%.

```
# Copyright 2020, The TensorFlow Federated Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Simple FedAvg to train EMNIST.

This is intended to be a minimal stand-alone experiment script built on top of
core TFF.
""""""

import collections
import functools
from absl import app
from absl import flags
import numpy as np
import tensorflow as tf
import tensorflow_federated as tff
import random

import simple_fedavg_tf
import simple_fedavg_tff
import os

np.set_printoptions(precision=None, suppress=None)

# Training hyperparameters
flags.DEFINE_integer('total_rounds', 50, 'Number of total training rounds.')
flags.DEFINE_integer('rounds_per_eval', 1, 'How often to evaluate')
flags.DEFINE_integer('train_clients_per_round', 1,
                     'How many clients to sample per round.')
flags.DEFINE_integer('client_epochs_per_round', 1,
                     'Number of epochs in the client to take per round.')
flags.DEFINE_integer('batch_size', 16, 'Batch size used on the client.')
flags.DEFINE_integer('test_batch_size', 128, 'Minibatch size of test data.')

# Optimizer configuration (this defines one or more flags per optimizer).
flags.DEFINE_float('server_learning_rate', 0.0005, 'Server learning rate.')
flags.DEFINE_float('client_learning_rate', 0.0005, 'Client learning rate.')

FLAGS = flags.FLAGS


def create_vgg19_model():
    model = tf.keras.applications.VGG19(include_top=True,
                                        weights=None,
                                        input_shape=(32, 32, 3),
                                        classes=100)
    return model


def get_cifar100_dataset():
    cifar100_train, cifar100_test = tff.simulation.datasets.cifar100.load_data()

    def element_fn(element):
        return collections.OrderedDict(
            x=tf.expand_dims(element['image'], -1), y=element['label'])

    def preprocess_train_dataset(dataset):
        # Use buffer_size same as the maximum client dataset size,
        # 418 for Federated EMNIST
        return dataset.map(element_fn).shuffle(buffer_size=418).repeat(
            count=FLAGS.client_epochs_per_round)  # .batch(
        # FLAGS.batch_size, drop_remainder=False)

    def preprocess_test_dataset(dataset):
        return dataset.map(element_fn).batch(
            FLAGS.test_batch_size, drop_remainder=False)

    cifar100_train = cifar100_train.preprocess(preprocess_train_dataset)
    cifar100_test = preprocess_test_dataset(
        cifar100_test.create_tf_dataset_from_all_clients())
    return cifar100_train, cifar100_test

def server_optimizer_fn():
    return tf.keras.optimizers.SGD(learning_rate=FLAGS.server_learning_rate)

def client_optimizer_fn():
    return tf.keras.optimizers.Adam(learning_rate=FLAGS.client_learning_rate)


def main(argv):
    if len(argv) > 1:
        raise app.UsageError('Too many command-line arguments.')
    client_devices = tf.config.list_logical_devices('GPU')
    print(client_devices)
    server_device = tf.config.list_logical_devices('CPU')[0]
    tff.backends.native.set_local_execution_context(
        server_tf_device=server_device, client_tf_devices=client_devices)

    train_data, test_data = get_cifar100_dataset()

    def tff_model_fn():
        """"""Constructs a fully initialized model for use in federated averaging.""""""
        # keras_model = create_original_fedavg_cnn_model(only_digits=False)
        keras_model = create_vgg19_model()
        # keras_model.summary()
        loss = tf.keras.losses.SparseCategoricalCrossentropy()
        return simple_fedavg_tf.KerasModelWrapper(keras_model,
                                                  test_data.element_spec, loss)

    iterative_process = simple_fedavg_tff.build_federated_averaging_process(
        tff_model_fn, tff_model_fn, tff_model_fn, tff_model_fn, server_optimizer_fn, client_optimizer_fn)
    server_state = iterative_process.initialize()

    metric = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')
    model = tff_model_fn()

    for round_num in range(FLAGS.total_rounds):
        sampled_clients = np.random.choice( train_data.client_ids,  size=FLAGS.train_clients_per_round,   replace=False)
        sampled_train_data = [ train_data.create_tf_dataset_for_client(client).batch( FLAGS.batch_size, drop_remainder=False)
            for client in sampled_clients
        ]

        server_state, train_metrics = iterative_process.next(
            server_state, sampled_train_data)

        print(f'Round {round_num} training loss: {train_metrics}')
        if round_num % FLAGS.rounds_per_eval == 0:
            model.from_weights(server_state.model_weights)
            accuracy = simple_fedavg_tf.keras_evaluate(model.keras_model, test_data,
                                                       metric)
            print(f'Round {round_num} validation accuracy: {accuracy * 100.0}')
if __name__ == '__main__':
    app.run(main)

```

This is the code, i just changed the model part and the dataset from the simple_fedavg example. Any idea? I tried with different optimizers, but still no luck.Hi @fanto88. Just to clarify, the code above runs without error, you just aren't getting good results on it?

This is generally out of scope for the issues tab here, I'd encourage you to instead post on Stack Overflow. That being said, one thing I'd encourage you to do is to first train a centralized model on the dataset to make sure that you can select a good optimizer, and that the model works.Closing this issue due to a lack of activity.",2,2021-11-29 19:57:15,2022-02-23 23:13:04,2022-02-23 23:13:03
https://github.com/tensorflow/federated/issues/2129,[],Running into some errors when running the TFF for Image Classification and Text Generation co-lab notebook,"Running into some errors when running the TFF for Image Classification and Text Generation co-lab notebookHi,

I am going through the material in the colab notebooks in [Federated Learning Workshop using TensorFlow Federated](https://events.withgoogle.com/demostutorials-workshop-on-federated-learning-and-analytics-2020/).

There are 3 errors in the notebook that am trying to understand why they are happening. This is the link to a copy of the colab I created [TFF for Image Classification and Text Generation](https://colab.research.google.com/drive/18Nn3SQVfc4QiZoNab3xr46g-T-kfAX97#scrollTo=NTUig7QmXavy&uniqifier=1).

There is the screen shots of the three errors: 

- ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.28.1 which is incompatible.
pymc3 3.11.4 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.
panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.28.1 which is incompatible.
fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.28.1 which is incompatible.
datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.

- AttributeError: module 'tensorflow_federated.python.simulation' has no attribute 'ClientData'

![image](https://user-images.githubusercontent.com/91956839/141482982-6576db88-169c-4853-9b6a-83d1ce46d167.png)

- AttributeError: module 'tensorflow_federated.python.learning' has no attribute 'assign_weights_to_keras_model'

![image](https://user-images.githubusercontent.com/91956839/141483097-be4b0e5c-e291-4bf7-abb5-e1f64d88802e.png)

Any help in understanding or resolving these errors would be greatly appreciated. 

Many thanks,
Solmaz
for this issue 
> AttributeError: module 'tensorflow_federated.python.learning' has no attribute 'assign_weights_to_keras_model'

I guess this function is depreciated in the current version. I replaced it with 
`state.model.assign_weights_to(keras_model)`
in my own experiment and it could work.
[https://tensorflow.google.cn/federated/api_docs/python/tff/learning/ModelWeights](url)
Hi all. Since those tutorials were written (> 1 year ago), there have been a number of updates to TFF, including:

*   `tff.simulation.ClientData` has been moved to `tff.simulation.datasets.ClientData`
*   `tff.learning.assign_weights_to_keras_model` has been deprecated in favor of `tff.learning.Model.assign_weights_to`.

We keep the official [TFF tutorials](https://www.tensorflow.org/federated/tutorials/tutorials_overview) up to date as the API changes. Please see the [image classification](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) and [text generation](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation) tutorials for the full details.Thanks you so much @NeoDH and @zcharles8 for sharing the information above. Will try out your suggestions.",3,2021-11-12 14:35:01,2021-12-15 14:09:18,2021-12-15 14:09:18
https://github.com/tensorflow/federated/issues/1958,['bug'],TFF simulations on GCP,"TFF simulations on GCPHello, Community:

I am trying to replicate the GCP setup steps.

https://www.tensorflow.org/federated/gcp_setup

But it returns an error that I don't understand:

`I1006 10:02:36.246272 139901428516608 remote_executor.py:84] Received retryable gRPC error: <_InactiveRpcError of RPC that terminated with:
        status = StatusCode.UNKNOWN
        details = ""Exception calling application: The executor service has not yet been configured with cardinalities and cannot execute any concrete requests.""
        debug_error_string = ""{""created"":""@1633514556.245906275"",""description"":""Error received from peer ipv4:XX.XXX.XX.X:8000"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1062,""grpc_message"":""Exception calling application: The executor service has not yet been configured with cardinalities and cannot execute any concrete requests."",""grpc_status"":2}""`

From client docker, I can connect from docker's client to the server instance without any problem.

`root@322de86816b2:/simulation# telnet XX.XXX.XX.X 8000
Trying XX.XXX.XX.X...
Connected to XX.XXX.XX.X.
Escape character is '^]'.`

I have seen that by default Docker's server deploys the container using IPv6:
![image](https://user-images.githubusercontent.com/54352971/136185868-aa9f148a-d69a-41d7-a304-2ecb76c13346.png)


Can anybody help me, please?
Thanks in advance.
See you!

**Environment**:
- Use Tensorflow Federated master version.
- Client and Server are two Compute Engine ""e2-micro"" into two different VPC.
- Allow HTTP traffic and Allow HTTPS traffic in Firewall.
- In both VPC networks, allow firewall rules for port 8000
![image](https://user-images.githubusercontent.com/54352971/136185464-b372a067-176b-464a-8b64-58dc6fc0f66d.png)


Glancing at the example, it looks like it was written long ago and hasn't been updated to reflect changes in the TFF runtime underneath.

In particular, the TFF runtime now attempts to infer the number of clients that need to be used to execute a given computation. Previously this number was hardcoded or flag-parameterized in various places; in this example I believe it was set by a flag at the worker.

However, this value cannot be inferred from the arguments to all computations--in particular, computations which do not accept CLIENTS-placed data. This computation is one such example. For this purpose, the higher-level executor stack constructors TFF exposes usually accept a `default_num_clients` arguments, which will be used in the case that this value cannot be inferred.

Opening a PR now to use our higher-level libraries and thus use this codepath. However, looking at the example indicates that this may *not* be such a great test of the GCP TFF runtime. In particular, since no logic is running at the `CLIENTS` placement, the TFF runtime reserves the right to (and in this case, in fact will) run all this logic on your local (client) machine. (Yes, the overloading of clients/server is painful in this context).

If you want to to more 'fully' exercise this setup, executing something like this should do it:
```python
@tff.federated_computation(tff.type_at_clients(tf.int32))
def sum_client_values(clients_int):
  return tff.federated_sum(clients_int)

assert sum_client_values(list(range(100)) == sum(range(100))
```
If this is a change you'd be interested in making to the `test.py` binary, I think we'd be happy to accept it.


@dvdgnzlz-maths with the commit above, this should be fixed. Can you verify?Hello @jkr26, 

Thanks for your help! I'm going to check and I will back with any comment. 
Best,Hello again, 

it works! Great!

> root@ae7b51a83b90:/simulation# python3 test.py --host 10.132.0.4
2021-10-18 13:40:08.799928: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
I1018 13:40:12.203198 139735513319168 executor_stacks.py:943] 1 TFF workers available out of a total of 1.
2021-10-18 13:40:12.214330: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2021-10-18 13:40:12.214455: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
b'Hello World'

Thanks @jkr26",4,2021-10-06 10:27:07,2021-10-18 13:45:17,2021-10-18 13:45:17
https://github.com/tensorflow/federated/issues/1811,['bug'],Minimal Stand-Alone Implementation of Federated Averaging doesnt work after the last commit,"Minimal Stand-Alone Implementation of Federated Averaging doesnt work after the last commitwith the last commit, ID'd 44d012f690005ecf9217e3be970a4f8a356e88ed, KerasModelWrapper is removed from [simple fedavg
](https://github.com/tensorflow/federated/tree/main/tensorflow_federated/python/examples/simple_fedavg) yet some parts arent updated properly. these are 

1) tff_model_fn still returns simple_fedavg_tf.KerasModelWrapper which no longer exists. should have returned tff.learning.from_keras_model according to my understanding of commit notes.

2) while loading server_weights for validation we use model.from_weights(server_state.model_weights) which no longer exists. I have used tf.nest.map_structure(lambda v,t: v.assign(t), model.weights, server_state.model_weights) instead.

3) for validation we use simple_fedavg_tf.keras_evaluate which exists but needs to be updated since it uses preds = model(batch['x'], training=False) and its input is model.keras_model we got errors. I guess this can be handled in two ways:

3.a) instead of creating model = tff_model_fn() we can create keras_model. use       
tf.nest.map_structure(lambda v,t: v.assign(t), model.trainable_weights, server_state.model_weights.trainable)
tf.nest.map_structure(lambda v,t: v.assign(t), model.non_trainable_weights, server_state.model_weights.non_trainable)
to load server weights and get accuracy with
accuracy = simple_fedavg_tf.keras_evaluate(model, test_data, metric)
when done like that we dont need to update keras_evaluate

3.b) keep using model = tff_model_fn() and use this model as the input of keras_evaluate. but this means we cant use 
preds = model(batch['x], training=False) in keras evaluate. I have used 
preds = model.forward_pass(batch, training=False).predictions
here things get a bit tricky since metric.update_state(y_true=batch['y'], y_pred=preds) returns 
AttributeError: 'NoneType' object has no attribute '_ unconditional update' for some reason. so if the metric of keras_evaluate and the metric we defined in the tff.learning.from_keras_model are same then I guess we can multiply the loss returned by forward_pass for each batch with the batch size and calculate loss_sum and total_size for all the batches and then divide it at the endI have used tff 0.17 with tf 2.3 on windows. the only code changes I have made except the ones listed above are had to change the return ServerUpdate in tf file to

  return ServerState(
      model_weights=model_weights,
      optimizer_state=server_optimizer.variables(),
      round_num=server_state.round_num + 1)

since the original uses tff.structure.update_struct which is not available in tff 0.17. and i also commented out 

client_devices = tf.config.list_logical_devices('GPU')
server_device = tf.config.list_logical_devices('CPU')[0]
tff.backends.native.set_local_python_execution_context(
server_tf_device=server_device, client_tf_devices=client_devices)It looks like the problem is solved with commit 8b769d83a57602507f622c4b973f07271f1d4ba6 so I am closing the issue.",2,2021-09-13 13:41:28,2021-09-14 18:38:09,2021-09-14 18:38:09
https://github.com/tensorflow/federated/issues/1784,[],How to deal with disconnection issues.,"How to deal with disconnection issues.How does TFF deal with disconnection issues. For example, during a certain aggregation process, most of the users suddenly dropped.  Thanks a lot.@veryhannibal Can you post more detail about this? In particular, what code you are attempting to run, what output (or error) you encountered. We have an issues template that you can follow that would be helpful for debugging this.@zcharles8    Thanks for your feedback. I just began to learn TFF.  My question is about how TFF can deal with straggle. For instance, I have one server to run FedAvg and several mobile devices to train model locally, but if some of the mobile devices dropped, will this task be failed? This sounds like a question about 1) the algorithm being employed and 2) the TFF executor being used. I don't think there's a one-size-fits-all answer. 

If you'd like to start a more general conversation about straggler tolerance in the context of federated learning and TFF, I would encourage you to a question on stack overflow using the `tensorflow-federated` tag ([link](https://stackoverflow.com/questions/tagged/tensorflow-federated)). However, given the more open-ended nature of this question I am closing this Github issue (which is primarily reserved for things such as bugs and feature requests).",3,2021-09-07 07:29:02,2021-09-08 17:18:17,2021-09-08 17:18:17
https://github.com/tensorflow/federated/issues/1739,['bug'],TensorFlow Federated (TFF) TypeError in tff.templates.IterativeProcess.next() when clients_per_round exceed 99,"TensorFlow Federated (TFF) TypeError in tff.templates.IterativeProcess.next() when clients_per_round exceed 99**Describe the bug**
I implemented a custom federated learning GAN training loop with TFF similar to [this code by Google Research](https://github.com/google-research/federated/blob/master/gans/tff_gans.py).

The client data for a particular training round is found using the following code snippet:

``` python
def client_dataset_fn():
    # Sample clients and data
    sampled_clients = np.random.choice(train_data.client_ids, size=cfg.clients_per_round, replace=False)
    datasets = [(next(client_gen_inputs_iterator),
                 train_data.create_tf_dataset_for_client(client_id).take(cfg.n_critic))
                for client_id in sampled_clients]
    return datasets

client_noise_inputs, client_real_data = zip(*client_dataset_fn())
```

This works perfectly up until `cfg.clients_per_round` is set to 99. When it is set to 100 or a larger value (with the total number of clients being larger of course), I receive the following error:

```
Traceback (most recent call last):
  File ""main.py"", line 109, in main
    metrics = run_single_trial(train_data, test_data, cfg)
  File ""/mnt/workspace/tff/GAN/federated/fedgan_main.py"", line 73, in run_single_trial
    metrics = train_loop(iterative_process, server_dataset_fn, client_dataset_fn, model, eval_hook_fn, cfg)
  File ""/mnt/workspace/tff/GAN/federated/fedgan_main.py"", line 124, in train_loop
    client_real_data)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/computation/function_utils.py"", line 525, in __call__
    return context.invoke(self, arg)
  File ""/usr/local/lib/python3.6/dist-packages/retrying.py"", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
  File ""/usr/local/lib/python3.6/dist-packages/retrying.py"", line 206, in call
    return attempt.get(self._wrap_exception)
  File ""/usr/local/lib/python3.6/dist-packages/retrying.py"", line 247, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 703, in reraise
    raise value
  File ""/usr/local/lib/python3.6/dist-packages/retrying.py"", line 200, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 226, in invoke
    _ingest(executor, unwrapped_arg, arg.type_signature)))
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete
    return future.result()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/tracing.py"", line 396, in _wrapped
    return await coro
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 111, in _ingest
    ingested = await asyncio.gather(*ingested)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 116, in _ingest
    return await executor.create_value(val, type_spec)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 294, in create_value
    value, type_spec))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 111, in create_value
    self._target_executor.create_value(value, type_spec))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 105, in _delegate
    result_value = await _delegate_with_trace_ctx(coro, self._event_loop)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/tracing.py"", line 396, in _wrapped
    return await coro
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/federating_executor.py"", line 394, in create_value
    return await self._strategy.compute_federated_value(value, type_spec)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/federated_composing_strategy.py"", line 279, in compute_federated_value
    py_typecheck.check_type(value, list)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/py_typecheck.py"", line 41, in check_type
    type_string(type_spec), type_string(type(target))))
TypeError: Expected list, found tuple.
```

During debugging, I looked at the `target` variable in the final line of the traceback and found it to be the abovementioned `client_real_data` and `client_noise_inputs`. Their types are in fact tuples, not lists, however, this does not change with different numbers of cfg.clients_per_round. The only usage of `cfg.clients_per_round` is shown above in the random choice. I really cannot explain why this is happening, maybe somebody out there has experienced something similar and can help me out.

As a workaround I now manually change the data type of `client_noise_inputs` and `client_real_data` using `list(tuple_var)`, but I am still curious as to why the list is required somehow.

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GNU/Linux 5.4.0-74-generic x86_64
* Python package versions (e.g., TensorFlow Federated, TensorFlow): TFF 0.19.0, TF 2.5.1
* Python version: 3.6.9
* CUDA/cuDNN version: CUDA 11.0, cuDNN 8
This seems to me to be an implementation distinction between the [`federated_composing_strategy`](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/impl/executors/federated_composing_strategy.py) and the [`federated_resolving_strategy`](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/impl/executors/federated_resolving_strategy.py). IIRC, by default we don't inject a composing executor into your stack until you hit 100 clients--which would be the source of this exciting mystery.

In particular, the composing strategy is programmed against the assumption that the incoming clients-placed value is represented [as a list](https://github.com/tensorflow/federated/blob/153f90562bdf0d9c7ee9240c9c1b1b1d8469737a/tensorflow_federated/python/core/impl/executors/federated_composing_strategy.py#L279), whereas the resolving strategy codes against a [much more flexible set](https://github.com/tensorflow/federated/blob/153f90562bdf0d9c7ee9240c9c1b1b1d8469737a/tensorflow_federated/python/core/impl/executors/federated_resolving_strategy.py#L255) of containers.

It's not wild to coerce your clients-placed value to a list--we also could extend the permitted representation of clients-placed values in the composing executor to match that in the resolving one, possibly pulling the appropriate logic to a shared place [like here](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/impl/executors/executor_utils.py). I think its a contribution wed be very happy to accept if youre up for it!Thanks a lot for your quick reply and for clearing up this issue. 

I am happy to put the contribution on my to-do list and will hopefully get to it soon.",2,2021-08-23 10:46:06,2021-08-25 08:50:10,2021-08-25 08:50:09
https://github.com/tensorflow/federated/issues/1593,['bug'],Multiple processes cannot simultaneously read StackOverflow word counts,"Multiple processes cannot simultaneously read StackOverflow word countsHi, 

Whenever I have multiple jobs trying to read the word counts from the StackOverflow dataset on TFF on a compute cluster, some of them end up crashing with this error:

```
File ""path_to_conda_env/lib/python3.8/site-packages/tensorflow_federated/python/simulation/datasets/stackoverflow.py"", line 167, in load_word_counts
    word, count = line.split()
ValueError: not enough values to unpack (expected 2, got 1)
```
The error does not occur every single time but it occurs quite frequently.

Here is a minimal working example which can reproduce the same error pretty reliably (you might have to run this a few times):
```
import tensorflow_federated as tff
import time
from multiprocessing import Pool

data_dir = ""/path/to/downloaded/data""

def my_func(t):
     time.sleep(t)  # optional
     vocab = tff.simulation.datasets.stackoverflow.load_word_counts(cache_dir=data_dir)
     return len(vocab) + t

with Pool(3) as p:
    print(p.map(my_func, [1, 5, 10]))

```

**Environment**
* OS Platform and Distribution: Ubuntu 20.04
* Python package versions: tff: 0.19.0 and tf: 2.5.0
* Python version: 3.8
* Bazel version (if building from source): N/A (downloaded via pip)
* CUDA/cuDNN version: CUDA:11.1 and cuDNN: 8. The error also occurs with CUDA_VISIBLE_DEVICES=""""
* What TensorFlow Federated execution stack are you using? Simulation


**Expected behavior**
I expect multiple jobs to be able to read this file simultaneously (given that it is not being edited by any of them). Fixing this issue will make large-scale experimentation much easier for me. Thank you!
@krishnap25 Just to get some more context, can you explain why you're trying to read the word counts in each process? On first examination, I would imagine that reading in one process, and then doing the post-reading work in parallel would be the easiest (and fastest) way forward.Hey @zcharles8, thanks for the prompt response! 

I'm trying to run a number of jobs in parallel on a slurm cluster. Each job loads the word counts and does some further processing. I have found that the jobs crash when they are start running at around the same time. The example I gave above with multiprocessing tried to simulate that in a self-contained manner. 

Since each job runs on a different machine and can potentially be scheduled to run at a different time, I cannot read the word counts once per batch of jobs. However, I realize that the simplest workaround for me is to read the word counts and store the relevant parts in a separate file, and have my jobs load this file directly. 

However, there is still a bug which prevents different processes/jobs from loading the word counts in parallel and this post can serve the purpose of bringing your attention to it.As far as I know, it's difficult to do multiple reads on the same file in parallel (eg. see https://stackoverflow.com/questions/18104481/read-large-file-in-parallel). Under the hood, `stackoverflow.load_word_counts()` simply downloads a file and then reads it in python. I suspect this will not behave well with parallel jobs.

One thing you may try is to have each process download the file to a separate location, to avoid parallel reads (which can be done by specifying the `cache_dir`). A more sophisticated solution may use queues to supply the dictionary of word counts to the multiple processes (eg. see https://stackoverflow.com/questions/11196367/processing-single-file-from-multiple-processes).

Disclaimer: I am not an expert in python multiprocessing.@krishnap25 Any updates on your end? As far as I understand, this is not really a bug on TFF, this is a manifestation of the fact that doing parallel reads in python on a single file can be slow. Is there some aspect to this that makes this addressable from TFF?@zcharles8 Sorry for the late response! 

I can have up to 32 jobs running in parallel, so keeping 32 copies of the stackoverflow dataset is infeasible, given its large size. My workaround is to process the word counts into a separate small file so that I do not encounter this error. 

I do not actually use Python multiprocessing in my use-case; I have multiple jobs running in parallel on a *slurm cluster* (the multiprocessing example is a minimal working example to reproduce that error without slurm). Therefore, I do not know how to implement a queue which can feed each job.

I'm closing this issue because I have a workaround. Thanks for your help!",5,2021-07-09 22:29:18,2021-07-18 23:53:37,2021-07-18 23:53:37
https://github.com/tensorflow/federated/issues/1570,['bug'],"Got ""RuntimeError: Event loop is closed"" when change the optimizer from SGD to DPGradientDescentGaussianOptimizer in emnist_fedavg tensorflow federated","Got ""RuntimeError: Event loop is closed"" when change the optimizer from SGD to DPGradientDescentGaussianOptimizer in emnist_fedavg tensorflow federatedI want to train the client model with dp-sgd, so I changed the client optimizer from tf.compat.v1.train.GradientDescentOptimizer to dp_optimizer.DPGradientDescentGaussianOptimizer
```
def client_update(model, dataset, server_weights, client_optimizer):
  client_weights = model.trainable_variables
  tf.nest.map_structure(lambda x, y: x.assign(y), client_weights, server_weights)
  for batch in dataset:
    outputs = model.forward_pass(batch)
    grads_and_vars = client_optimizer.compute_gradients(outputs.loss, client_weights)
    client_optimizer.apply_gradients(grads_and_vars)
  return client_weights
```

```
@tff.tf_computation(tf_dataset_type, model_weights_type)
def client_update_fn(tf_dataset, server_weights):
  model = model_fn()
  # client_optimizer=tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)
  client_optimizer = dp_optimizer.DPGradientDescentGaussianOptimizer(
          l2_norm_clip=1.0,
          noise_multiplier=1.0,
          num_microbatches=BATCH_SIZE,
          learning_rate=0.01)
  return client_update(model, tf_dataset, server_weights, client_optimizer)
```
But I got an error ""RuntimeError: Event loop is closed"". when I run
`server_state = federated_algorithm.next(server_state, federated_train_data)`

However, If I set the batchsize of tf_dataset as 1, the code runs well.

How could I solve this problem?


**Environment (please complete the following information):**
*  Linux Ubuntu 16.04
* TensorFlow Federated 0.19.0, TensorFlow 2.5.0, Tensorflow Privacy 0.5.2

Hi @wangdan269. Can you post the full stack trace? It's tough to say what's going on in your example, as it involves things such as `federated_train_data` that you haven't specified here.> Hi @wangdan269. Can you post the full stack trace? It's tough to say what's going on in your example, as it involves things such as `federated_train_data` that you haven't specified here.

Thanks for your reply! @zcharles8 .

Below is my example code.  TensorFlow Federated 0.19.0, TensorFlow 2.5.0, Tensorflow Privacy 0.5.2
It runs when the Batch_size =1, but has errors when Batch_size >1.   


ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c326ba8 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c3d53c8 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c3266a0 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c3265f8 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb0405d320 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c326cf8 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c3264a8 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c3269e8 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb0406df28 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed

Process finished with exit code 1



```
import collections
import attr
import functools
import numpy as np
import tensorflow as tf
import tensorflow_federated as tff
from tensorflow_privacy.privacy.optimizers import dp_optimizer

# nest_asyncio.apply()

emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()

NUM_CLIENTS = 10
BATCH_SIZE = 1
#if batch size >1, the code cannot work properly

def preprocess(dataset):

  def batch_format_fn(element):
    """"""Flatten a batch of EMNIST data and return a (features, label) tuple.""""""
    return (tf.reshape(element['pixels'], [-1, 784]),
            tf.reshape(element['label'], [-1, 1]))

  return dataset.batch(BATCH_SIZE).map(batch_format_fn)

""""""sample a small number of clients, and apply the preprocessing above to their datasets.""""""
client_ids = np.random.choice(emnist_train.client_ids, size=NUM_CLIENTS, replace=False)
federated_train_data = [preprocess(emnist_train.create_tf_dataset_for_client(x))
  for x in client_ids]

def create_keras_model():
  return tf.keras.models.Sequential([
      tf.keras.layers.Input(shape=(784,)),
      tf.keras.layers.Dense(10, kernel_initializer='zeros'),
      tf.keras.layers.Softmax(),
  ])


def model_fn():
  keras_model = create_keras_model()
  return tff.learning.from_keras_model(
      keras_model,
      input_spec=federated_train_data[0].element_spec,
      # loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

def initialize_fn():
  model = model_fn()
  return model.trainable_variables


@tf.function
def client_update(model, dataset, server_weights, client_optimizer):
  """"""Performs training (using the server model weights) on the client's dataset.""""""
  # Initialize the client model with the current server weights.
  client_weights = model.trainable_variables
  # Assign the server weights to the client model.
  tf.nest.map_structure(lambda x, y: x.assign(y), client_weights, server_weights)
  # Use the client_optimizer to update the local model.
  for batch in dataset:
    with tf.GradientTape() as tape:
      # Compute a forward pass on the batch of data
      outputs = model.forward_pass(batch)
    # Compute the corresponding gradient
    # grads = tape.gradient(outputs.loss, client_weights)
    # grads_and_vars = zip(grads, client_weights)
    grads_and_vars = client_optimizer.compute_gradients(outputs.loss, client_weights)
    # Apply the gradient using a client optimizer.
    client_optimizer.apply_gradients(grads_and_vars)
  return client_weights


@tf.function
def server_update(model, mean_client_weights):
  """"""Updates the server model weights as the average of the client model weights.""""""
  model_weights = model.trainable_variables
  # Assign the mean client weights to the server model.
  tf.nest.map_structure(lambda x, y: x.assign(y),
                        model_weights, mean_client_weights)
  return model_weights

@tff.tf_computation
def server_init():
  model = model_fn()
  print(""server init"")
  return model.trainable_variables

@tff.federated_computation
def initialize_fn():
  return tff.federated_value(server_init(), tff.SERVER)

whimsy_model = model_fn()
tf_dataset_type = tff.SequenceType(whimsy_model.input_spec)
model_weights_type = server_init.type_signature.result

@tff.tf_computation(tf_dataset_type, model_weights_type)
def client_update_fn(tf_dataset, server_weights):
  model = model_fn()
  # client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
  client_optimizer = dp_optimizer.DPGradientDescentGaussianOptimizer(
          l2_norm_clip=1.0,
          noise_multiplier=1.0,
          num_microbatches=BATCH_SIZE,
          learning_rate=0.01)
  return client_update(model, tf_dataset, server_weights, client_optimizer)

@tff.tf_computation(model_weights_type)
def server_update_fn(mean_client_weights):
  model = model_fn()
  return server_update(model, mean_client_weights)

federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)
federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)


@tff.federated_computation(federated_server_type, federated_dataset_type)
def next_fn(server_weights, federated_dataset):
    # Broadcast the server weights to the clients.
    server_weights_at_client = tff.federated_broadcast(server_weights)
    # Each client computes their updated weights.
    client_weights = tff.federated_map(
        client_update_fn, (federated_dataset, server_weights_at_client))
    # The server averages these updates.
    mean_client_weights = tff.federated_mean(client_weights)
    # The server updates its model.
    server_weights = tff.federated_map(server_update_fn, mean_client_weights)
    return server_weights

federated_algorithm = tff.templates.IterativeProcess(
    initialize_fn=initialize_fn,
    next_fn=next_fn
)
print(""Instantiation"")

central_emnist_test = emnist_test.create_tf_dataset_from_all_clients().take(1000)
central_emnist_test = preprocess(central_emnist_test)


def evaluate(server_state):
  keras_model = create_keras_model()
  keras_model.compile(
      loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
  )
  keras_model.set_weights(server_state)
  keras_model.evaluate(central_emnist_test)

server_state = federated_algorithm.initialize()
evaluate(server_state)



for round in range(100):
  print(""Ronud:"", round)
  server_state = federated_algorithm.next(server_state, federated_train_data)

evaluate(server_state)

```Are you running this in a Jupyter notebook? Regardless, the errors in the stack trace seem to be caused by `asyncio`, not by `tensorflow_federated`. This [stack overflow post](https://stackoverflow.com/questions/60966874/running-the-event-loop-in-tf-federated/60975831#60975831) may be relevant.> Are you running this in a Jupyter notebook? Regardless, the errors in the stack trace seem to be caused by `asyncio`, not by `tensorflow_federated`. This [stack overflow post](https://stackoverflow.com/questions/60966874/running-the-event-loop-in-tf-federated/60975831#60975831) may be relevant.

I run it in Linux Ubuntu 16.04. I have tried this method,
```
import nest_asyncio
nest_asyncio.apply()
```
but it doesn't work.Does the training code work with a `tf.keras.optimizer`? Additionally, is there more of the stack trace that we could look at? I don't see any reference to which line of the code you posted that it's failing at.> Does the training code work with a `tf.keras.optimizer`? Additionally, is there more of the stack trace that we could look at? I don't see any reference to which line of the code you posted that it's failing at.

It works with `tf.keras.optimizer`, but has errors after I change the optimizer into `dp_optimizer.DPGradientDescentGaussianOptimizer`. The full stack trace is shown below:

Traceback (most recent call last):
  File ""/root/userfolder/PPFL/test/MNIST-LDP-FL.py"", line 157, in <module>
    server_state = federated_algorithm.next(server_state, federated_train_data)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/computation/function_utils.py"", line 525, in __call__
    return context.invoke(self, arg)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/retrying.py"", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/retrying.py"", line 206, in call
    return attempt.get(self._wrap_exception)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/retrying.py"", line 247, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/six.py"", line 703, in reraise
    raise value
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/retrying.py"", line 200, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 230, in invoke
    _invoke(executor, comp, arg, result_type)))
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete
    return future.result()
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 396, in _wrapped
    return await coro
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 135, in _invoke
    result = await executor.create_call(comp, arg)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 341, in create_call
    return await comp_repr.invoke(self, arg)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 161, in invoke
    return await executor._evaluate(comp_lambda.result, new_scope)  # pylint: disable=protected-access
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 511, in _evaluate
    return await self._evaluate_block(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 475, in _evaluate_block
    return await self._evaluate(comp.block.result, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 503, in _evaluate
    return await self._evaluate_reference(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 427, in _evaluate_reference
    return await scope.resolve_reference(comp.reference.name)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 110, in resolve_reference
    return await value
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 505, in _evaluate
    return await self._evaluate_call(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 443, in _evaluate_call
    func, arg = await asyncio.gather(func, get_arg())
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 439, in get_arg
    return await self._evaluate(comp.call.argument, scope=scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 509, in _evaluate
    return await self._evaluate_struct(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 463, in _evaluate_struct
    values = await asyncio.gather(*values)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 503, in _evaluate
    return await self._evaluate_reference(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 427, in _evaluate_reference
    return await scope.resolve_reference(comp.reference.name)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 110, in resolve_reference
    return await value
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 505, in _evaluate
    return await self._evaluate_call(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 443, in _evaluate_call
    func, arg = await asyncio.gather(func, get_arg())
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 439, in get_arg
    return await self._evaluate(comp.call.argument, scope=scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 503, in _evaluate
    return await self._evaluate_reference(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 427, in _evaluate_reference
    return await scope.resolve_reference(comp.reference.name)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 110, in resolve_reference
    return await value
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 505, in _evaluate
    return await self._evaluate_call(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 444, in _evaluate_call
    return await self.create_call(func, arg=arg)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 339, in create_call
    comp_repr, delegated_arg))
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 120, in create_call
    return await self._delegate(self._target_executor.create_call(comp, arg))
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 105, in _delegate
    result_value = await _delegate_with_trace_ctx(coro, self._event_loop)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 396, in _wrapped
    return await coro
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/federating_executor.py"", line 460, in create_call
    comp.internal_representation.uri, arg)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/federating_executor.py"", line 138, in compute_federated_intrinsic
    return await fn(arg)  # pylint: disable=not-callable
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/federated_resolving_strategy.py"", line 465, in compute_federated_map
    return await self._map(arg, all_equal=False)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/federated_resolving_strategy.py"", line 342, in _map
    for (value, child) in zip(val, children)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/federated_resolving_strategy.py"", line 338, in _map_child
    return await child.create_call(fn_at_child, value)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 120, in create_call
    return await self._delegate(self._target_executor.create_call(comp, arg))
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 105, in _delegate
    result_value = await _delegate_with_trace_ctx(coro, self._event_loop)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 396, in _wrapped
    return await coro
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 683, in create_call
    comp.internal_representation(arg.internal_representation),
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 365, in <lambda>
    return lambda arg: fn_to_return(arg)  # pylint: disable=unnecessary-lambda
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 346, in fn_to_return
    destroy_after_invocation=destroy_after)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 203, in _call_embedded_tf
    result_parts = wrapped_fn(*param_elements)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1711, in __call__
    return self._call_impl(args, kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py"", line 247, in _call_impl
    args, kwargs, cancellation_manager)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1729, in _call_impl
    return self._call_with_flat_signature(args, kwargs, cancellation_manager)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1778, in _call_with_flat_signature
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1961, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 596, in call
    ctx=ctx)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input to reshape is a tensor with 1 values, but the requested shape requires a multiple of 2
	 [[{{node Reshape}}]]
	 [[StatefulPartitionedCall_1/ReduceDataset]] [Op:__inference_pruned_3604]

Function call stack:
pruned

ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc25fe48 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc5072b0 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc277940 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc25fd68 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc25f160 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc25fda0 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed

Process finished with exit code 1

This looks like the important part to me:

```
File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 1 values, but the requested shape requires a multiple of 2
[[{{node Reshape}}]]
[[StatefulPartitionedCall_1/ReduceDataset]] [Op:__inference_pruned_3604]
```Hi @wangdan269. Thanks to @cramertj's comment, I realized where the issue lies. You call at some point:

```
client_optimizer = dp_optimizer.DPGradientDescentGaussianOptimizer(
        l2_norm_clip=1.0,
        noise_multiplier=1.0,
        num_microbatches=BATCH_SIZE,
        learning_rate=0.01)
```

If you instead set `num_microbatches = None`, everything succeeds. I believe this is due to having batches at the end of a dataset that are smaller than `BATCH_SIZE`. An alternative would be to call `.batch(..., drop_remainder=True)` in your data pipeline.While this is not technically a bug on TFF, it is an unfortunate stack trace that you found. One suggestion I might have in the future is to try simply training on a single client before wrapping everything into an iterative process. I believe this will catch the error above, in a way that will have many fewer layers between you and the error.Thanks! @zcharles8 and @cramertj This problem is solved!!",10,2021-07-08 02:04:12,2021-07-22 01:14:13,2021-07-22 01:14:13
https://github.com/tensorflow/federated/issues/1471,['bug'],How do I modify this Python code?,"How do I modify this Python code?Hi,
I am trying to build a sequential model in Keras, but when I train the model, I am getting this error, How may I fix it?

I am following this code and trying to run the file federated_learning.ipynb

https://github.com/mikemikezhu/federated-learning-facial-expression-recognition


# Build federated average process
trainer = tff.learning.build_federated_averaging_process(create_federated_model)

# Create initial state
train_state = trainer.initialize()
ypeError: from_keras_model() missing 1 required positional argument: 'input_spe
TypeError: build_federated_averaging_process() missing 1 required positional argument: 'client_optimizer_fn'
Hi @amir11877. The GitHub issues for TFF is more of a forum for discussing bugs, feature requests, and other issues with the code/software. The `tensorflow-federated` tag on StackOverflow is a better place for asking questions about how to use TFF. Would you mind asking your question there instead?

Additionally, since this question seems to be about using `tff.learning.from_keras_model`, I encourage you to check out our tutorial on [Federated Learning for Image Classification](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification).Hi,
Unfortunately, I have trouble executing this code, but the developer
of this code is not responsible
Of course, policies need to change in these areas, such as declining reputation
https://github.com/mikemikezhu/federated-learning-facial-expression-recognition
I asked the question in the stack, but no answer has been given yet
https://stackoverflow.com/questions/68018178/federated-learning-for-image-classification-in-colab
https://stackoverflow.com/questions/67992927/how-can-i-modify-this-federated-learning-facial-expression-recognition-codecol
I read your link but my problem was not solved
I want to train facial expression recognition with Fedavg
Unfortunately, it seems that there are few or no experts in this field
or it is limited to dataset mnist
Thanks

[image: image.png]

<https://play.google.com/store/apps/details?id=com.google.android.apps.translate&hl=fa&gl=US>




On Fri, Jun 18, 2021 at 9:00 PM Zachary Charles ***@***.***>
wrote:

> Hi @amir11877 <https://github.com/amir11877>. The GitHub issues for TFF
> is more of a forum for discussing bugs, feature requests, and other issues
> with the code/software. The tensorflow-federated tag on StackOverflow is
> a better place for asking questions about how to use TFF. Would you mind
> asking your question there instead?
>
> Additionally, since this question seems to be about using
> tff.learning.from_keras_model, I encourage you to check out our tutorial
> on Federated Learning for Image Classification
> <https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification>
> .
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorflow/federated/issues/1471#issuecomment-864153031>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/APD3PIN2YTCJOEG4EHKSTMTTTNYCTANCNFSM46X5OTTA>
> .
>",2,2021-06-15 19:42:12,2021-06-19 15:52:33,2021-06-18 16:30:19
https://github.com/tensorflow/federated/issues/1470,['bug'],How do I modify this Python code?,"How do I modify this Python code?Hi,
I am trying to build a sequential model in Keras, but when I train the model, I am getting this error, How may I fix it?

I am following this code and trying to run the file federated_learning.ipynb

https://github.com/mikemikezhu/federated-learning-facial-expression-recognition


# Build federated average process
trainer = tff.learning.build_federated_averaging_process(create_federated_model)

# Create initial state
train_state = trainer.initialize()
ypeError: from_keras_model() missing 1 required positional argument: 'input_spe
TypeError: build_federated_averaging_process() missing 1 required positional argument: 'client_optimizer_fn'
Closed, as it is a duplicate of #1471.",1,2021-06-15 19:41:56,2021-06-18 16:30:43,2021-06-18 16:30:43
https://github.com/tensorflow/federated/issues/1458,['bug'],How do I modify this code?,"How do I modify this code?tf.keras.layers(body = concatenate([upbody1, downbody1],axis=-1)),
tf.keras.layers(body1 = LSTM(8, activation='tanh'))(body),

tf.keras.laayers(out = Dense(60, activation='softmax'))(body1),

tf.keras.layers(model = Model([bdbd, rhrh, lhlh, rfrf, lflf]), out),
])

File ""<ipython-input-367-2708f0ec1ea2>"", line 105
      ])
      ^
  SyntaxError: invalid syntaxClosed, as it is a duplicate of #1471.",1,2021-06-10 05:17:44,2021-06-18 16:30:53,2021-06-18 16:30:53
https://github.com/tensorflow/federated/issues/1434,['bug'],Jax serialization of tff.types.StructType,"Jax serialization of tff.types.StructTypeI am trying to implement the client update step in JAX while making use of the `jax.experimental.stax` module for the definition of my model. Initialized model weights are given as Jax TracedArray objects. However, serializing a function that takes model weights as a struct of jax TracedArray objects gives the error

`AttributeError: The `Struct` of length 2 does not have named field ""tensor_index"". Fields (up to first 10): []`

A short reconstruction of the error can be found [here](https://colab.research.google.com/drive/1NAsV7yfVQZv-izRTer2vBstoaEZsUh8H?usp=sharing)

It seems that `get_model_weights.type_signature.result` returns a nested `tff.StructType` object. The arrays at the bottom of this nested structure are correctly identified as `tff.TensorType`. However, the error I get keeps throwing `AttributeError: The `Struct` of length 2 does not have named field ""tensor_index"". Fields (up to first 10): []`. 

After looking into `tensorflow_federated/python//core/impl/jax_context/jax_serialization.py`, I found that the problem lies in line 147-148:

```
flattened_obj, _ = jax.tree_util.tree_flatten((args, kwargs))  
tensor_indexes = list(np.argsort([x.tensor_index for x in flattened_obj]))
```

`flattened_obj` does not seem to be flattened, there is still a `StructType` object nested in the list, which causes `x.tensor_index` to fail in the next line. 

Any suggestions on how this can be solved?Looking at this code, I think there is a bug here with nested structures. Writing a failing test now, will tag this issueShould be resolved by [9e1edc7](https://github.com/tensorflow/federated/commit/9e1edc7fc86b014642a1bfde263d89f035f94ba3); when TFF-Nightly is released overnight the fix should be usable.Problem still seems to be there after the update @jkr26. [See the same colab file again](https://colab.research.google.com/drive/1NAsV7yfVQZv-izRTer2vBstoaEZsUh8H?usp=sharing). Tff-nightly is up to date as can be seen.

`flattened_obj, _ = jax.tree_util.tree_flatten((args, kwargs))` still returns a nested Struct just as before. It seems that the tensor values at the leaf nodes are correctly serialized into `_XlaSerializerTensorArg` but the nested `Struct`objects fail to become `_XlaSerializerStructArg`

Flattening the `StructType` with `structure.flatten()` **before** feeding it into the jax serialization is a workaround but requires properly nesting it again within the jax function.



assigning @zcharles8 to this, he is taking a look at fleshing out TFF Jax relationship in Python, I think in particular what is failing at the moment.",4,2021-06-01 09:39:29,2022-06-30 19:31:28,2021-06-02 17:07:26
https://github.com/tensorflow/federated/issues/1381,['bug'],DPKerasSGDOptimizer throws AssertionError,"DPKerasSGDOptimizer throws AssertionError**Describe the bug**
[colab](https://colab.research.google.com/drive/1A1xaXL8-DPuVPIGG2Z2LX8aKzWqrTpbo?usp=sharing) notebook reproducing the error.

I changed the client optimizer to `DPKerasSGDOptimizer` from `tensorflow_privacy.privacy.optimizers.dp_optimizer_keras` in the [mnist tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) to play around. It throws the following AssertionError.  I used the [mnist_dpsgd_tutorial_keras.py])https://github.com/tensorflow/privacy/blob/master/tutorials/mnist_dpsgd_tutorial_keras.py) to set up the params, loss func, etc. I am not sure but, this is perhaps because TFF does not use the optimizer to calculate gradients and only uses the apply_gradient method from the optimizer. How can I fix/workaround this ?
```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-55-74bf67640887> in <module>()
      2     model_fn,
      3     client_optimizer_fn= get_optimizer,
----> 4     server_optimizer_fn= lambda: tf.keras.optimizers.SGD(learning_rate=1.0),
      5 )

13 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    984           except Exception as e:  # pylint:disable=broad-except
    985             if hasattr(e, ""ag_error_metadata""):
--> 986               raise e.ag_error_metadata.to_exception(e)
    987             else:
    988               raise

AssertionError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py:395 _compute_local_training_and_client_delta  *
        client_output = client_delta_fn(dataset, initial_model_weights)
    /usr/local/lib/python3.7/dist-packages/tensorflow_federated/python/learning/federated_averaging.py:96 reduce_fn  *
        optimizer.apply_gradients(zip(gradients, model.weights.trainable))
    /usr/local/lib/python3.7/dist-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py:28 _dataset_reduce_fn  *
        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:2178 reduce  **
        add_to_graph=False)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3898 __init__
        self._function = fn_factory()
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3144 get_concrete_function
        *args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3109 _get_concrete_function_garbage_collected
        graph_function, _ = self._maybe_define_function(args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3456 _maybe_define_function
        graph_function = self._create_graph_function(args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3301 _create_graph_function
        capture_by_value=self._capture_by_value),
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:999 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3873 wrapped_fn
        ret = wrapper_helper(*args)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3803 wrapper_helper
        ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
    /tmp/tmp3y46nwy4.py:23 reduce_fn
        ag__.converted_call(ag__.ld(optimizer).apply_gradients, (ag__.converted_call(ag__.ld(zip), (ag__.ld(gradients), ag__.ld(model).weights.trainable), None, fscope_1),), None, fscope_1)
    /usr/local/lib/python3.7/dist-packages/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras_vectorized.py:190 apply_gradients  **
        'Neither _compute_gradients() or get_gradients() on the '

    AssertionError: Neither _compute_gradients() or get_gradients() on the differentially private optimizer was called. This means the training is not differentially private. It may be the case that you need to upgrade to TF 2.4 or higher to use this particular optimizer.
```

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 'Ubuntu', '18.04', 'bionic' at Google Colab
* Python package versions (e.g., TensorFlow Federated, TensorFlow): TFF, TFP
* Python version: 3.7.10
* What TensorFlow Federated execution stack are you using? TFF-nightly

Output of  [script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).
```
numpy                         1.19.5             
protobuf                      3.12.4             
tensorflow-datasets           4.0.1              
tensorflow-estimator          2.4.0              
tensorflow-federated-nightly  0.18.0.dev20210501 
tensorflow-gcs-config         2.4.0              
tensorflow-hub                0.12.0             
tensorflow-metadata           0.29.0             
tensorflow-model-optimization 0.5.0              
tensorflow-privacy            0.5.2              
tensorflow-probability        0.12.1  
```

**Expected behavior**
The training should proceed as usual.


**Additional Context**
Using a DPSequential model from TFP with the default SGD optimizer works. I trust it is doing the same thing?
Is using dp_aggregator as follows equivalent to using dpsgd optimizer?
```
iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),
    model_update_aggregation_factory = tff.learning.dp_aggregator(0.1, 10)
)
```This error is from TFP https://github.com/tensorflow/privacy/blob/755ed26671f5567ba1519a4e80078eded7a6299b/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py#L180. 

TFP has some known compatibility issue with TF2 Keras. Please consider following https://github.com/tensorflow/privacy/issues/134. 

One workaround is to explicitly call `compute_gradients` of TFP optimizers. The default of TFF used keras `tape.gradient` at https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/learning/federated_averaging.py#L95. You probably want to write your customized TFF iterative process by following https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/examples/simple_fedavgHello @clippedgrad did you solve the problem? ThanksI have the same problem as you. Did you solve the problem? Thanks! @clippedgrad",3,2021-05-04 10:41:35,2021-07-06 05:20:47,2021-05-05 00:51:47
https://github.com/tensorflow/federated/issues/1358,['bug'],experimental_simulation_loop=True throws in multi-gpu computers,"experimental_simulation_loop=True throws in multi-gpu computers**Describe the bug**
when running basic `tff.learning.build_federated_averaging_process(..., use_experimental_simulation_loop=True)` with a simple model (emnist cnn) and running on a computer with multiple GPUs, I'm getting the following:

```
ValueError: Detected dataset reduce op in multi-GPU TFF simulation: `use_experimental_simulation_loop=True` for `tff.learning`; or use `for ... in iter(dataset)` for your own dataset iteration.Reduce op will be functional after b/159180073.
```

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux
* Python package versions (e.g., TensorFlow Federated, TensorFlow): nightly (24/4/21)
* Python version: 3.8
* Bazel version (if building from source):
* CUDA/cuDNN version: cuda 11 cuddnn 8.1
* What TensorFlow Federated execution stack are you using? not sure

**Expected behavior**
this should not fail (even if not using GPUs for some reason, this exception is not supposed to happen as `use_experimental_simulation_loop` should make sure the dataset is iterated properly)

**Additional context**
see also https://github.com/google-research/federated/issues/32#issuecomment-826058000
Sorry, this was caused by an internal file synchronization issue. Closing this.",1,2021-04-24 08:51:47,2021-04-24 12:03:34,2021-04-24 12:03:34
https://github.com/tensorflow/federated/issues/1336,['bug'],Missing imports and inconsistent documentation for tff.simulation for tensorflow-federated-nightly,"Missing imports and inconsistent documentation for tff.simulation for tensorflow-federated-nightly**Describe the bug**
I was trying to create my own federated dataset using `FromTensorSlicesClientData` and `ClientData`. But they are missing
from the nightly build which is used in many of the tutorials. 
```python
AttributeError                            Traceback (most recent call last)
<ipython-input-6-027cdf2aff21> in <module>()
----> 1 tff.simulation.ClientData

AttributeError: module 'tensorflow_federated.python.simulation' has no attribute 'ClientData'
```

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-7-3241544b8014> in <module>()
----> 1 tff.simulation.FromTensorSlicesClientData

AttributeError: module 'tensorflow_federated.python.simulation' has no attribute 'FromTensorSlicesClientData'
```


But the they seem to be present in the git source code which is weird. On further inspection many imports seem to be differ. They may have been moved which is also weird since the [documentation](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/FromTensorSlicesClientData) states it should be under tff.simulation.



This is also the case for
`tff.simulation.ClientData` which is what it says in the docs, but it is actually  `tff.simulation.datasets.ClientData`. This is consistent with the documentation. 

I might be missing something since I am new. If so please point me to the right documentation. 

[Colab](https://colab.research.google.com/drive/1LFgBiu9xUa-k92IW24fiSX_kVp7lb0SB#scrollTo=vmSsQ9koEAR_) Notebook reproducing the issue.

`simulation.__init__.py`  in the nightly build:
```python
from tensorflow_federated.python.simulation import datasets
from tensorflow_federated.python.simulation import models
from tensorflow_federated.python.simulation.checkpoint_manager import FileCheckpointManager
from tensorflow_federated.python.simulation.csv_manager import CSVMetricsManager
from tensorflow_federated.python.simulation.iterative_process_compositions import compose_dataset_computation_with_computation
from tensorflow_federated.python.simulation.iterative_process_compositions import compose_dataset_computation_with_iterative_process
from tensorflow_federated.python.simulation.metrics_manager import MetricsManager
from tensorflow_federated.python.simulation.sampling_utils import build_uniform_client_sampling_fn
from tensorflow_federated.python.simulation.sampling_utils import build_uniform_sampling_fn
from tensorflow_federated.python.simulation.server_utils import run_server
from tensorflow_federated.python.simulation.server_utils import server_context
from tensorflow_federated.python.simulation.tensorboard_manager import TensorBoardManager
from tensorflow_federated.python.simulation.training_loop import run_simulation
from tensorflow_federated.python.simulation.training_loop import run_simulation_with_callbacks
from tensorflow_federated.python.simulation.training_loop import TRAIN_STEP_TIME_KEY
from tensorflow_federated.python.simulation.training_loop import TRAIN_STEPS_PER_HOUR_KEY
from tensorflow_federated.python.simulation.training_loop import VALIDATION_METRICS_PREFIX
from tensorflow_federated.python.simulation.training_loop import VALIDATION_TIME_KEY
```

`simulation.__init__.py` in the source
```python
from tensorflow_federated.python.simulation import datasets
from tensorflow_federated.python.simulation import models
from tensorflow_federated.python.simulation.checkpoint_manager import FileCheckpointManager
from tensorflow_federated.python.simulation.client_data import ClientData
from tensorflow_federated.python.simulation.csv_manager import CSVMetricsManager
from tensorflow_federated.python.simulation.file_per_user_client_data import FilePerUserClientData
from tensorflow_federated.python.simulation.from_tensor_slices_client_data import FromTensorSlicesClientData
from tensorflow_federated.python.simulation.hdf5_client_data import HDF5ClientData
from tensorflow_federated.python.simulation.iterative_process_compositions import compose_dataset_computation_with_computation
from tensorflow_federated.python.simulation.iterative_process_compositions import compose_dataset_computation_with_iterative_process
from tensorflow_federated.python.simulation.metrics_manager import MetricsManager
from tensorflow_federated.python.simulation.sampling_utils import build_uniform_client_sampling_fn
from tensorflow_federated.python.simulation.server_utils import run_server
from tensorflow_federated.python.simulation.server_utils import server_context
from tensorflow_federated.python.simulation.tensorboard_manager import TensorBoardManager
from tensorflow_federated.python.simulation.transforming_client_data import TransformingClientData
```
**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
* Python package versions (e.g., TensorFlow Federated, TensorFlow): TFF Nightly


**Expected behavior**
Should be able to directly use
`tff.simulation.FromTensorSlicesClientData`  and `tff.simulation.ClientData`

seems to be solved in:
https://stackoverflow.com/questions/67147951/tensorflow-federated-python-simulation-has-no-attribute-fromtensorslicesclien/67150029#67150029",1,2021-04-20 07:12:40,2021-04-20 10:49:47,2021-04-20 10:49:47
https://github.com/tensorflow/federated/issues/1270,['bug'],TFF fails to load Stackoverflow dataset,"TFF fails to load Stackoverflow dataset**Description**
The package cannot finish loading Stackoverflow dataset. 
```
import tensorflow_federated as tff 
train, ho, test = tff.simulation.datasets.stackoverflow.load_data()
```

Standard output stops at stage '9076670464/9076663578'. 

For more details, see https://colab.research.google.com/drive/102j7aC0KcrQhiVLMGFYIVfiISS2S1VX5?usp=sharing 

**Environment:**
Standard Colab environment with pip installed tensorflow-federated

**Expected behavior**
The download is expected to finish at '9076663578/9076663578'.
After a while download finishes. Please delete the issue",1,2021-03-29 14:25:53,2021-03-29 14:48:43,2021-03-29 14:48:43
https://github.com/tensorflow/federated/issues/1265,['bug'],DataLossError upon loading dataset using tf.data.experimental.load,"DataLossError upon loading dataset using tf.data.experimental.loadI would like to train two independent ```TFF``` models using ```emnist dataset```. Each model should train on a ```1000``` distinct participants randomly drawn from the dataset.

Code below
```python
emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()

participants_ids = np.random.choice(a=emnist_train.client_ids, 
                                    size=1000,
                                    replace=False)

federated_dataset = 
        [data_train.create_tf_dataset_for_client(i) for i in participants_ids]

nested_dataset = tf.data.Dataset.from_tensor_slices(federated_dataset)
```
Trying to save the dataset using the function ```tf.data.experimental.save```

```python
tf.data.experimental.save(nested_dataset, 'model_dataset')
```
the warning below is generated. However, the save is completed.

```python
E tensorflow/core/framework/dataset.cc:89] The Encode() method is not implemented for DatasetVariantWrapper objects.
```

**The problem occurs upon loading the dataset and trying to inspect its contents**
```python
dataset = tf.data.experimental.load('model_dataset', 
                      element_spec= 
                      DatasetSpec(collections.OrderedDict([
                         ('label', TensorSpec(shape=(), dtype=tf.int32)),
                         ('pixels', TensorSpec(shape=(28, 28), dtype=tf.float32))]), 
                      TensorShape([])

# verifying elements
for example in dataset:
        print(example)
```
Error below
```python
tensorflow.python.framework.errors_impl.DataLossError: Unable to parse tensor from stored proto.
```
Trying other methods such as ```pickle.dump``` and ``` np.save```, all resulted in error below even after converting the dataset into ```numpy.ndarray``` using ```np.asarray()```
```python
pickle.dump(np.asarray(nested_dataset), open('model_dataset', 'wb'))

tensorflow.python.framework.errors_impl.InternalError: Tensorflow type 21 not convertible to numpy dtype.
```

Env details
```python
python -V 3.8.5
tensorflow==2.4.1
tensorflow-datasets==4.2.0
tensorflow-federated==0.18.0
Keras==2.4.3
numpy==1.19.2
```
Is this a ```bug```, or is there any good way to slice the dataset into clients and save the newly created datasets ?Hi @AHabes. Generally, `tf.data.experimental.save` is not suitable for saving federated datasets, since there is no way to disambiguate which examples are associated to each client.

If you're interested in saving federated datasets, you can use `tff.simulation.FilePerUserClientData`, see https://www.tensorflow.org/federated/api_docs/python/tff/simulation/FilePerUserClientData. Unless I'm mistaken, this seems to meet your exact use case.

Note: If you're using the tensorflow-federated-nightly, then you would instead use `tff.simulation.datasets.FilePerUserClientDatea`.Also note that `FilePerUserClientData` would be used to load the federated datasets, once you've saved the datasets to disk. How you want to do this is up to you, though typically a `tf.io.TFRecordWriter` is a good way to proceed in such cases (see https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/simulation/datasets/file_per_user_client_data_test.py for an example of how to use this).@zcharles8,Thanks for the follow up, and for the useful info indeed.

Actually, the main idea is to use this ```federated dataset``` to train a ```federated model```,  then transform this same dataset into a ```flatten``` one and save it in order to process and use it later to train a traditional ```centralized model```. 

The final intension is to establish a comparison between the ```federated model``` and the ```centralized model``` when trained on the same dataset. 

That is reason behind the attempt to convert the federated dataset into a flatten format usable by a traditional centralized model and save it.@AHabes is there a good reason to save the centralized dataset? It sounds like `ClientData.create_tf_dataset_from_all_clients` is exactly the functionality you want (amalgamating a dataset across clients).

From the code snippet above, it looks like you're trying to use the EMNIST dataset, but with only 1000 of the clients. Can you elaborate on this? For training purposes, you only need to sample a small number per round to get decent convergence on most models, and for testing, I don't see the benefit of throwing away client data. @zcharles8, Thanks for the follow up, much appreciated

Actually, I would run several experiments, each with different expected number of participants (ranging from ~10 to ~1000). 

In general, the process goes like this:

1. Experiment with various sampling techniques (Poisson, uniform, Binomial etc ..) to sample an expected number of participants per round (say 1000).
2. Train a federated model for x rounds based on the clients sampled at each round.
3. Collect the ids of clients participated in the federated model
4. Prepare a centralized dataset from the participants' datasets (and ideally save it for future usage and referencing).
4. Use the centralized dataset to train a traditional centralized model for x epochs.
5. Compare the performance of federated vs traditional models trained on the same dataset (to investigate the effects of sampling techniques and number of participants).

Using ```ClientData.create_tf_dataset_from_all_clients``` I would assume I need to first slice the ```ClientData``` to contain only the data of the sampled participants (step 1 above) before calling ```create_tf_dataset_from_all_clients```. Otherwise, it will create the centralized dataset out of all 3400 participants ?@AHabes I believe that https://www.tensorflow.org/federated/api_docs/python/tff/simulation/ClientData#from_clients_and_fn will do exactly what you want. Basically, you can use this to create a new `ClientData` that only uses the smaller number of clients, in which case `create_tf_dataset_from_all_clients` would give exactly the desired outcome.

Note that to make this more performant, you may want to pass in `emnist_train.dataset_computation` instead, so you would end up doing something like

```
emnist_train, _ = tff.simulation.datasets.emnist.load_data()
subsampled_clients = emnist_train.client_ids[:10]
subsampled_train = tff.simulation.datasets.ClientData.from_clients_and_fn(
  subsampled_clients, emnist_train.dataset_computation)
```
Hi @AHabes. I'm closing this issue due to a lack of activity. If the suggestion above does not work, feel free to re-open this.",7,2021-03-24 19:25:15,2021-06-30 20:06:42,2021-06-30 20:06:42
https://github.com/tensorflow/federated/issues/1245,['bug'],Memory explodes,"Memory explodesWhile running the following command:
bazel run main:federated_trainer -- --task=cifar100 --total_rounds=1500 --client_optimizer=sgd --client_learning_rate=0.1 --client_batch_size=20 --server_optimizer=sgd --server_learning_rate=1 --clients_per_round=10 --client_epochs_per_round=1 --experiment_name=cifar100_fedavg
the VRAM usage explodes.
It quickly reaches 8.9 GB in the first round. It remains stable, until around the 125. round, when it doubles to 16.9 GB. This phenomenon repeats itself until the training crashes.Given that this library functionally wraps each tensorflow native function for the conjugate training components, note that there may be a problem with tensorflow federated and the GPU memory allocation during training, which is especially something to keep in mind of if you are training on a single GPU with multiple instances of the networks e.g. the client models training in federated environment. This could be incorrect, but I would recommend looking into your system metadata to narrow down your problem. Hi @raghavchugh21. It seems like this might be related to https://github.com/google-research/federated/issues/29. Additionally, this issue seems to be specifically related to the `federated_research` repository, so it might be better to post your issue there.@raghavchugh21 I believe this issue is fixed by de97c52ca835b5557c0c1c9dbd768f742a2739f5. If possible, can you confirm this is the case by running the same command using the latest nightly version of TFF?Based on feedback from google-research/federated#29, I believe that this has been resolved. Feel free to re-open it if the issue persists.",4,2021-03-17 16:12:28,2021-04-09 17:25:36,2021-04-09 17:25:36
https://github.com/tensorflow/federated/issues/1232,[],TFF GCP Setup Tutorial/Guide syntax error,"TFF GCP Setup Tutorial/Guide syntax error**Issue**
Hi while I was going through the GCP Setup tutorial for TFF, I found that after running step
>6. Run the client container interactively.
> The string ""Hello World"" should print to the terminal.
`$ docker run \
    --interactive \
    --tty \
    --name=tff-client \
    --volume ~:/simulation \
    --workdir /simulation \
    <registry>/tff-client \
    bash`

of [4. Run a simulation on a client container.](https://www.tensorflow.org/federated/gcp_setup#4_run_a_simulation_on_a_client_container)
I got the following error regarding the `--volume` flag: 
![MicrosoftTeams-image](https://user-images.githubusercontent.com/33724568/111151235-17b8d380-8587-11eb-94b0-98aac1995b40.png)
**Potential Solution**
It looks like `--volume ~/:/simulation` solves the problem, but I'm still not getting ""Hello World"", could you pls fix that?Thank you for the report!

Would you be up for submitting a PR to update the guide to the correct docker `--volume` flag? The source can be found in this markdown file https://github.com/tensorflow/federated/blob/master/docs/gcp_setup.md

Its hard to tell without more information why `""Hello World!""` still isn't being printed. Any chance of attaching the logs of what happens after the `--volume` flag is changed?> **Issue**
> Hi while I was going through the GCP Setup tutorial for TFF, I found that after running step
> 
> > 1. Run the client container interactively.
> >    The string ""Hello World"" should print to the terminal.
> >    `$ docker run \     --interactive \     --tty \     --name=tff-client \     --volume ~:/simulation \     --workdir /simulation \     <registry>/tff-client \     bash`
> 
> of [4. Run a simulation on a client container.](https://www.tensorflow.org/federated/gcp_setup#4_run_a_simulation_on_a_client_container)
> I got the following error regarding the `--volume` flag:
> ![MicrosoftTeams-image](https://user-images.githubusercontent.com/33724568/111151235-17b8d380-8587-11eb-94b0-98aac1995b40.png)
> **Potential Solution**
> It looks like `--volume ~/:/simulation` solves the problem, but I'm still not getting ""Hello World"", could you pls fix that?",2,2021-03-15 12:11:03,2021-03-16 01:50:29,2021-03-16 01:50:29
https://github.com/tensorflow/federated/issues/1215,['bug'],Exploding memory consumption when training FL model with varying number of participants per round,"Exploding memory consumption when training FL model with varying number of participants per roundI'm running FL algorithm following the [image classification][1] tutorial. The number of participants vary at each round according to a predefined list of participants number.

```python
number_of_participants_each_round = 
[108, 113, 93, 92, 114, 101, 94, 93, 107, 99, 118, 101, 114, 111, 88, 
101, 86, 96, 110, 80, 118, 84, 91, 120, 110, 109, 113, 96, 112, 107, 
119, 91, 97, 99, 97, 104, 103, 120, 89, 100, 104, 104, 103, 88, 108]

```

The federated data is preprocessed and batched before starting the training.
```python

NUM_EPOCHS = 5
BATCH_SIZE = 20
SHUFFLE_BUFFER = 418
PREFETCH_BUFFER = 10

def preprocess(dataset):
    def batch_format_fn(element):
        return collections.OrderedDict(
            x=tf.reshape(element['pixels'], [-1, 784]),
            y=tf.reshape(element['label'], [-1, 1]))

    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(
        BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)


def make_federated_data(client_data, client_ids):
    return [preprocess(client_data.create_tf_dataset_for_client(x)) for x in client_ids]

federated_train_data = make_federated_data(data_train, data_train.client_ids)

```

Participants are randomly sampled from ```federated_train_data[0:expected_total_clients]``` at each round according to the ```number_of_participants_each_round```, then the ```iterative_process``` is executed for ```45 rounds```.

```python
expected_total_clients = 500
round_nums = 45

for round_num in range(0, round_nums):
    sampled_clients = np.random.choice(a=federated_train_data[0:expected_total_clients], 
                                      size=number_of_participants_each_round[round_num], 
                                      replace=False)
    
    state, metrics = iterative_process.next(state, list(sampled_clients))
    print('round {:2d}, metrics={}'.format(round_num + 1, metrics))

```

The problem is that the ```VRAM``` usage quickly explodes after few rounds, it reaches ```5.5 GB``` at round ```6~7```, and keeps increasing with an approx rate of ```0.8 GB/round``` until the training eventually crashes at round ```25~26``` where the VRAM reaches ```17 GB``` with ```+4000``` python threads created.

Error message below
```python
F tensorflow/core/platform/default/env.cc:72] Check failed: ret == 0 (35 vs. 0)Thread creation via pthread_create() failed.
```

**### Troubleshooting ###**

Reducing the ```number_of_participants_each_round``` to ```20``` allows the training to complete, but the memory consumption was still huge and growing.
 

Running the same code with fixed number of participants per round, memory consumption was fine with total of approx ```1.5 ~ 2.0 GB``` VRAM throughout the entire training.


```python
expected_total_clients = 500
clients_per_round = 100
sampled_clients = np.random.choice(a=federated_train_data[0:expected_total_clients],
                                   size=clients_per_round,
                                   replace=False)
    
state, metrics = iterative_process.next(state, list(sampled_clients))
print('round {:2d}, metrics={}'.format(round_num + 1, metrics))
```

Extra details:
```
OS: MacOS Mojave, 10.14.6
python -V: Python 3.8.5 then downgraded to Python 3.7.9
TF version: 2.4.1
TFF version: 0.18.0
Keras version: 2.4.3
```

Is this a normal memory behaviour or a ```bug```? Are there any refactoring/hints to optimize memory consumption ?


  [1]: https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classificationI think you've got an interesting call pattern here, and you're hitting an edge we did not anticipate.

The TFF runtime will inspect the arguments it receives for their implied cardinalities, and [construct an executor stack](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/impl/executors/execution_context.py#L201) based on these numbers. As currently implemented, a given executor stack can only run a *fixed* number of clients--thus the need for constructing them on demand.

This construction process involves several potentially heavyweight operations, and the constructed executors can start caching various static artifacts (e.g. a map from TF GraphDef -> eager TF callable, as this conversion is expensive and Python-heavy) and speed up your rounds only if they stay in memory. For this reason we cache the constructed executors for reuse when the same cardinalities is seen again.

This usually works great--as most of our users simply run computations with the same number of clients each round. However, this fails quite catastrophically in a use case like yours. Since the constructed executors are cached, there is no way for them to release their associated resources--threads being one example, as the TFF local runtime essentially uses a thread to represent each client. Eyeballing the numbers you have above, it looks to me like 4000 threads is about right.

There are a few options here. We can take a bug on ourselves to swap the naive dicts we use to store these executors today to something like LRUCaches capped at 5 or 10 elements. To unblock yourself, you should be able to release the resources associated to the executor factory you are using by an explicit call like:

```
tff.framework.get_context_stack().current.executor_factory.clean_up_executors()
```

I'll write a quick change; if you verify that this works then we can be fairly confident of the underlying issue.",1,2021-03-10 03:11:51,2021-03-11 18:01:26,2021-03-11 18:01:26
https://github.com/tensorflow/federated/issues/1212,[],"Problem of ""no attribute 'from_compiled_keras_model'"", any alternative?","Problem of ""no attribute 'from_compiled_keras_model'"", any alternative?I have tensorflow version 2.2.0 and tensorflow_federated version 0.14.0. In the code, I have the following line:

`return tff.learning.from_compiled_keras_model(keras_model, sample_batch)`

and it generates the following error:

`AttributeError: module 'tensorflow_federated.python.learning' has no attribute 'from_compiled_keras_model'`

I don't know what can I use as an alternative/equivalent to it (because `from_compiled_keras_model` is removed from the current tensorflow_federated). 

Thank you. Hi @sumeyyegsu 

This should be addressable simply by using `tff.learning.from_keras_model` instead, and just not compiling the Keras model before passing to TFF.",1,2021-03-07 20:33:08,2021-03-10 05:31:19,2021-03-10 05:31:19
https://github.com/tensorflow/federated/issues/1029,['bug'],Fed Avg on single machine multi GPU -- only ONE GPU is actually used ,"Fed Avg on single machine multi GPU -- only ONE GPU is actually used **Describe the bug**
Simulation is not using all GPUs. According to monitoring, it's using only 1 of 8.

**Environment (please complete the following information):**
* OS Platform and Distribution: Linux Ubuntu 16.04
* Python package versions: tff nightly
* Python version: 3.8.0
* CUDA/cuDNN version: cuDNN 8000
* What TensorFlow Federated execution stack are you using? `build_federated_averaging_process`

Note: You can collect the Python package information by running `pip3 freeze`
from the command line and most of the other information can be collected using
TensorFlows environment capture
[script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).

**Expected behavior**
Each client optimizer should run on a different GPU in parallel, improving the overall speed.

**Additional context**
According to https://www.tensorflow.org/federated/tutorials/simulations, I wonder if parallelization across GPUs is currently supported. 
Could the issue be updated with more details about how the simulation is being run?

If using the `tff.learning` package, could you try using the `use_experimental_simulation_loop=True` argument? https://www.tensorflow.org/federated/api_docs/python/tff/learning/build_federated_averaging_process?version=nightly#args

@nightldj may have tips on running with multiple GPUsLike in TF and other frameworks, multi-GPU is not on by default. Could you config the TFF executor like this https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/examples/simple_fedavg/emnist_fedavg_main.py#L140? And use `use_experimental_simulation_loop=True` as Zach suggested. 

A more detailed instruction is on my TODO list, but let me know if these works. @ZacharyGarrett Thanks so much for the tip! 

We are using `tff` for research on a custom compression algorithm. In detail, it's very much alike the tutorial at https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification. The main difference is that we created our own aggregation factory where we simulate our compression algorithm and passed that into `build_fed_avg_process`. 

@nightldj Thanks for the help! 

After setting `use_experimental_simulation_loop=True` and setting up devices like the following immediately at the beginning of the `main` function

```python
    client_devices = tf.config.list_logical_devices('GPU')
    server_device = tf.config.list_logical_devices('CPU')[0]
    tff.backends.native.set_local_execution_context(
        server_tf_device=server_device, client_tf_devices=client_devices
    )
```

According to `gpustat` still only one of the eight GPU's is being used. @zehao-sean-huang What is the output of `tf.config.list_logical_devices('GPU')`? Did you see speedup moving from one GPU to multi-GPU? I would double check if the application is IO bounded. We do not use `gpustat`. Would you mind trying TF profiler?@nightldj The output for `tf.config.list_logical_devices('GPU')` is the following. Some logs are also attached. If it's no use, the actual list of devices is at the bottom of the block.

```
2021-01-12 17:09:19.542496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-12 17:09:20.042851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.054410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.071153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 2 with properties: 
pciBusID: 0000:06:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.082475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 3 with properties: 
pciBusID: 0000:07:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.085875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 4 with properties: 
pciBusID: 0000:08:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.089990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 5 with properties: 
pciBusID: 0000:0b:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.095699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 6 with properties: 
pciBusID: 0000:0c:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.115366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 7 with properties: 
pciBusID: 0000:0d:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.115478: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-12 17:09:20.120351: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-01-12 17:09:20.120437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-01-12 17:09:20.123474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-12 17:09:20.123925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-12 17:09:20.127652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-01-12 17:09:20.128875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-01-12 17:09:20.129122: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-01-12 17:09:20.344744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-01-12 17:09:20.345421: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-12 17:09:29.743279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.747043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.756720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 2 with properties: 
pciBusID: 0000:06:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.760139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 3 with properties: 
pciBusID: 0000:07:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.763145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 4 with properties: 
pciBusID: 0000:08:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.765249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 5 with properties: 
pciBusID: 0000:0b:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.767875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 6 with properties: 
pciBusID: 0000:0c:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.775241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 7 with properties: 
pciBusID: 0000:0d:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.775399: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-12 17:09:29.775487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-01-12 17:09:29.775555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-01-12 17:09:29.775600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-12 17:09:29.775628: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-12 17:09:29.775659: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-01-12 17:09:29.775697: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-01-12 17:09:29.775730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-01-12 17:09:29.845646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-01-12 17:09:29.845773: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-12 17:09:36.161719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-12 17:09:36.161820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 1 2 3 4 5 6 7 
2021-01-12 17:09:36.161842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N Y Y Y Y Y Y Y 
2021-01-12 17:09:36.161855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 1:   Y N Y Y Y Y Y Y 
2021-01-12 17:09:36.161869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 2:   Y Y N Y Y Y Y Y 
2021-01-12 17:09:36.161880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 3:   Y Y Y N Y Y Y Y 
2021-01-12 17:09:36.161891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 4:   Y Y Y Y N Y Y Y 
2021-01-12 17:09:36.161907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 5:   Y Y Y Y Y N Y Y 
2021-01-12 17:09:36.161922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 6:   Y Y Y Y Y Y N Y 
2021-01-12 17:09:36.161935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 7:   Y Y Y Y Y Y Y N 
2021-01-12 17:09:36.202960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10910 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:04:00.0, compute capability: 7.0)
2021-01-12 17:09:36.209779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10910 MB memory) -> physical GPU (device: 1, name: TITAN V, pci bus id: 0000:05:00.0, compute capability: 7.0)
2021-01-12 17:09:36.224329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10910 MB memory) -> physical GPU (device: 2, name: TITAN V, pci bus id: 0000:06:00.0, compute capability: 7.0)
2021-01-12 17:09:36.233402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10910 MB memory) -> physical GPU (device: 3, name: TITAN V, pci bus id: 0000:07:00.0, compute capability: 7.0)
2021-01-12 17:09:36.247370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10910 MB memory) -> physical GPU (device: 4, name: TITAN V, pci bus id: 0000:08:00.0, compute capability: 7.0)
2021-01-12 17:09:36.257711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10910 MB memory) -> physical GPU (device: 5, name: TITAN V, pci bus id: 0000:0b:00.0, compute capability: 7.0)
2021-01-12 17:09:36.269125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10910 MB memory) -> physical GPU (device: 6, name: TITAN V, pci bus id: 0000:0c:00.0, compute capability: 7.0)
2021-01-12 17:09:36.280726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10910 MB memory) -> physical GPU (device: 7, name: TITAN V, pci bus id: 0000:0d:00.0, compute capability: 7.0)
[LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU'), LogicalDevice(name='/device:GPU:2', device_type='GPU'), LogicalDevice(name='/device:GPU:3', device_type='GPU'), LogicalDevice(name='/device:GPU:4', device_type='GPU'), LogicalDevice(name='/device:GPU:5', device_type='GPU'), LogicalDevice(name='/device:GPU:6', device_type='GPU'), LogicalDevice(name='/device:GPU:7', device_type='GPU')]
```

We didn't really observe much speedups. The application shouldn't be IO bounded cuz we're training CIFAR10. The whole dataset is stored in memory. 

And yes, I'll go ahead and try TF profiler and get back here. Thanks for the help. @nightldj @ZacharyGarrett Thanks guys! After trying on a few machines it works on one of them configuring devices like this. @nightldj @ZacharyGarrett Is there a way to use multiple GPUs for the server right now? Our research code might benefit from that since the majority of our compression bottleneck is happening on the server. If we could use multiple GPUs it would definite be great. @zehao-sean-huang You can pin server computation on *one* GPU by the `server_tf_device` argument.  There is no native TFF way to do it. One possibility I can think of is to write `tf.distribute` code for server computation, but we have never tested the usage before. Hi, I haven't read the entire thread, so I may be missing some context, but just to make sure all bases are covered, I want to mention that typically, TFF orchestration loop looks something along the lines of the following, with the ""distributed"" part of each round modeled as a computation in TFF (first line), and potentially ""potential_other_server_work"" (the second line) being arbitrary Python code that could e.g., use tf.distribute (or any other TF or non-TF APIs). If there's a heavyweight server-side training to be done in-between computation rounds on lots of server-side data, it would make sense to utilize whatever existing non-TFF infrastructure already exists for this purpose, and just invoke it directly from Python.

for ... in ...:
  round_output, ... = computation_round_modeled_in_tff(server_state, ....)
  server_state, ... = potential_other_server_work(round_output, ....)

Speeding up server-side parts of TFF computation proper is definitely on the table, but we'd need the programmer to be able to somehow communicate to TFF the information about what to parallelize (or make it TF runtime's responsibility to parallelize on the fly - but that could be hard without some additional information about the programmer's intent). It would probably be most actionable to unpack the motivation for this in the context of some specific use cases, workloads, and computation patterns.

Natively incorporating all of tf.distribute into TFF is problematic for architectural reasons. TFF wants the programmer to declare logically what to run in a manner that affords some deployment flexibility, and treat deployment details (specific physical devices, distribution strategies, etc.) as a separate concern, handled at a later stage (typically by the runtime). Not all commonly used APIs are compatible with this out of the box.
Please take a look at our colab tutorial for GPU usage https://www.tensorflow.org/federated/tutorials/simulations_with_accelerators. I will close this issue, but feel free to reopen if it is not resolved.",10,2021-01-11 07:22:32,2021-04-17 03:52:17,2021-04-17 03:52:17
https://github.com/tensorflow/federated/issues/972,[],a problem of parameter,"a problem of parameterIn TFF, which parameter defines how many clients are selected for training in each round? TFF version is 0.15GitHub Issues is more of a forum for discussing bugs, feature requests, and other issues with the code/software. The [`tensorflow-federated`](https://stackoverflow.com/questions/tagged/tensorflow-federated) tag on StackOverflow is a better places for questions about how to _use_ TFF, would you mind re-asking your question there?",1,2020-12-21 07:53:26,2020-12-21 15:41:15,2020-12-21 15:41:14
https://github.com/tensorflow/federated/issues/951,[],A problem of tff.learning. build _ federated _ averaging _ process?,"A problem of tff.learning. build _ federated _ averaging _ process?Why is the default parameter of server_optimizer_fn in TFF. learning. build _ federated _ averaging _ process SGD instead of using weighted average as mentioned in the paperCorrect that `tff.learning.build_federated_averaging_process` isn't implemented exactly as averaging of the client model weights. However it is effectively the same algorithm when using the default server optimizer parameter of `tf.keras.optimizers.SGD(learning_rate=1.0)`.

Instead, the clients send back _the difference_ between their final model and the initial model (client ""model_update""), rather than the weights themselves. This helps with things like lossy compression techniques, as smaller magnitude values encode better and/or with less error. 

These updates are averaged, then applied to the server as a pseudo-gradient using the server optimizer. When the server optimizer is SGD with a learning rate of `1.0` this recovers the same semantics as averaging the model parameters themselves. Loosely, if we take `W_i` as the global model at the start of round `i`, and `W_j` is a client `j`'s model after local training, the model round `i+1` is shown as:

`W_i+1 = W_i + 1.0 * Average_j(W_j - W_i) = Average_j(W_j)`

By implementing this way, its easy to experiment with more sophisticated algorithms for optimization on the server. See https://arxiv.org/abs/2003.00295 for some examples.",1,2020-12-14 00:50:04,2020-12-14 14:48:10,2020-12-14 14:48:10
https://github.com/tensorflow/federated/issues/930,['bug'],import error in google colab,"import error in google colabimport error
[https://colab.research.google.com/drive/1LnDOu076TBeaJT6yjnnNkSyim_GdpCxD?usp=sharing](url)
at the same time, I can not open the tutorial:[https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb](url)

My code:
`import collections  
import numpy as np   
import tensorflow as tf   
import tensorflow_federated as tff   
np.random.seed(0)   
tff.federated_computation(lambda: 'Hello, World!')()  
`


ImportError                              Traceback (most recent call last)
<ipython-input-7-a23308ec3f7c> in <module>()
      3 import numpy as np
      4 import tensorflow as tf
----> 5 import tensorflow_federated as tff
      6 
      7 np.random.seed(0)

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/types/experimental/__init__.py in <module>()
      8 import sys as _sys
      9 
---> 10 from tensorflow.python.types.core import TensorLike
     11 
     12 del _print_function

ImportError: cannot import name 'TensorLike'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
""Open Examples"" button below.
I just verifed that [the tutorial](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb) is working for me. When you say you can't open it, what error are you getting?

Perhaps you need something like this in your colab to get the latest version of TFF and TF?
```
!pip uninstall --yes tensorboard tb-nightly

!pip install --quiet --upgrade tensorflow_federated_nightly
!pip install --quiet --upgrade nest_asyncio
!pip install --quiet tb-nightly  # or tensorboard, but not both

import nest_asyncio
nest_asyncio.apply()
```
Closing after no response for 2 months. Please feel free to re-open if the behavior can be reproduced.",2,2020-12-01 08:33:05,2021-03-31 15:15:20,2021-03-31 15:15:19
https://github.com/tensorflow/federated/issues/929,['bug'],a problem of bazel,"a problem of bazel I use 'bazel run computation.proto'  command .But it doesn't generate 'computation_pb2' pyhtonFor Bazel, I think this will want the `build` command (https://docs.bazel.build/versions/3.7.0/guide.html#build), rather than `run`. Bazel also desires the target label (`:computation_py_b2`), rather than the file name. More information about using Bazel can be found at https://docs.bazel.build/.

In this case, try:

```shell
$ bazel build tensorflow_federated/proto/v0:computation_py_pb2
```",1,2020-11-28 10:28:01,2020-11-30 15:00:35,2020-11-30 15:00:34
https://github.com/tensorflow/federated/issues/928,['bug'],a problem of import,"a problem of importcannot import name 'computation_pb2
Could we add additional information to the issue? It will greatly help debugging.

- What command-line was used that surfaced this error?
- What versions of TFF was being used?
- What environment was this being run in? (OS, any virtual enviroments, Python version, etc)同问题This is likely a duplicate of https://github.com/tensorflow/federated/issues/770.

Please see https://github.com/tensorflow/federated/issues/770#issuecomment-540058686 and re-open if this does not fix the issue.",3,2020-11-28 07:56:02,2021-03-31 15:18:56,2021-03-31 15:18:56
https://github.com/tensorflow/federated/issues/924,[],Custom Keras Model with TFF and Optimizers,"Custom Keras Model with TFF and OptimizersHi,

I wanted to know is it possible to use custom keras model with TFF? I am not sure if the training behavior will be ok. I have this model:

```
 class CustomModel(tf.keras.Model):
  def train_step(self, data):
      x, y = data

      with tf.GradientTape() as tape:
          y_pred = self(x, training=True)
          loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)

      trainable_vars = self.trainable_variables
      gradients = tape.gradient(loss, trainable_vars)
      self.optimizer.apply_gradients(zip(gradients, trainable_vars))
      self.compiled_metrics.update_state(y, y_pred)
      return {m.name: m.result() for m in self.metrics}
```

```
def create_keras_model():
  inputs = tf.keras.Input(shape=(784,))
  outputs = tf.keras.layers.Dense(10, activation=""softmax"")(inputs)
  model = CustomModel(inputs, outputs)
  return model
```

```
def model_fn():
  keras_model = create_keras_model()
  return tff.learning.from_keras_model(
      keras_model,
      input_spec=preprocessed_example_dataset.element_spec,
      loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
```

Another question is there are 2 optimizers:
```
iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))
```
In the earlier versions of TFF there used to only one optimizer (on client). I read the Federated Learning paper, but did not understand why do we need to have an optimizer on server side. Could you please share some paper or relevant doc which provides information on this? It looks like the code wants to a custom training loop for the ""inner"" (on client) optimizer. Rather than implement this inside the Model, implementing this inside the training `tff.templates.IterativeProcess` maybe more natural?

There is a simplified, flat view of a Federatd Averaging implementation in 
https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/examples/simple_fedavg

The client training loop is here: https://github.com/tensorflow/federated/blob/826b66f3f0964bc3dec14e30286b5412bfeffb64/tensorflow_federated/python/examples/simple_fedavg/simple_fedavg_tf.py#L192
It might be worth trying to fork that code and modify?

Side note on subclassed `tf.keras.Model`: they are not supported. This is because TFF requires being able to serialize all computation logic, as the code is intended to run on edge devices. Unfortunately, Keras' subclass models do not support this, from https://keras.io/guides/model_subclassing (which now seems to be 404) use to say: 

> However, in subclassed models, the model's topology is defined as Python code (rather than as a static graph of layers). That means the model's topology cannot be inspected or serialized. As a result, the following methods and attributes are not available for subclassed models:
>
> model.inputs and model.outputs. model.to_yaml() and model.to_json() model.get_config() and model.save().

https://keras.io/guides/making_new_layers_and_models_via_subclassing/

In general we recommend Keras' Functional API for models: https://keras.io/guides/functional_api/Thanks @ZacharyGarrett  for the quick response. 

> ""It looks like the code wants to a custom training loop for the ""inner"" (on client) optimizer. Rather than implement this inside the Model, implementing this inside the training tff.templates.IterativeProcess maybe more natural?""

It seems like TFF is going in the opposite direction of TF2.0 where it is highly marketed to override train_step function. I tested the code I posted earlier with [this TFF tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) and it seems to be working fine and indeed I'm using a Keras' Functional API. 

Do you still recommend using [simple_fedavg](https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/examples/simple_fedavg)?",2,2020-11-10 12:35:56,2020-11-16 18:47:01,2020-11-16 18:47:01
https://github.com/tensorflow/federated/issues/922,[],Non-IID data distribution for a custom dataset,"Non-IID data distribution for a custom datasetCurrent built in API's support only few datasets in a Non-IID setting. How can we create a Non-IID data distribution for a custom text dataset?@avinashsai I have the same problem.@CSJDeveloper, @avinashsai 

There are a few different ways that this can be done, generally depending on the data you have now. [This StackOverflow post](https://stackoverflow.com/questions/58965488/how-to-create-federated-dataset-from-a-csv-file) may be useful.

One thing to note--if your existing dataset does not contain a natural user-keyed structure, you will have to first determine a strategy for creating synthetic non-iid-ness. TFF distributes a [synthetic partitioning of CIFAR100](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/cifar100/load_data) which may be helpful as inspiration.

Generally, we prefer using StackOverflow for usage questions and federated computing questions in general, and reserving GitHub issues for bugs. This is an excellent question! Closing, however, for this reason.",2,2020-11-09 10:57:19,2020-12-07 05:05:35,2020-12-07 05:05:35
https://github.com/tensorflow/federated/issues/920,[],"ValueError: Expected a TensorFlow computation, got lambda.","ValueError: Expected a TensorFlow computation, got lambda.I run this line

```{python}
# Test the TFF is working:
tff.federated_computation(lambda: 'Hello, World!')()
```

and this error occurs: 

```---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-68-a116efda9633> in <module>()
     11 
     12 # Test the TFF is working:
---> 13 tff.federated_computation(lambda: 'Hello, World!')()

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/tensorflow_deserialization.py in deserialize_and_call_tf_computation(computation_proto, arg, graph)
     65   if computation_oneof != 'tensorflow':
     66     raise ValueError(
---> 67         'Expected a TensorFlow computation, got {}.'.format(computation_oneof))
     68   py_typecheck.check_type(graph, tf.Graph)
     69   with graph.as_default():

ValueError: Expected a TensorFlow computation, got lambda.
```

It occurs when I run [Federated_Learning_for_Image_Classification.ipynb](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb#scrollTo=-gm-yx2Mr_bl) in Google Colab today. I think there's something wrong with Google Colab or tff yesterday. 
This line can run properly today. I had the same issue. I used `del interactive_process` to delete all the results of `tff.learning.build_federated_averaging_process` from my notebook, and I didn't run into the issue again.",2,2020-11-03 08:49:02,2020-12-28 22:03:27,2020-11-04 13:21:02
https://github.com/tensorflow/federated/issues/917,['bug'],"Expected TFF type {float32*}@CLIENTS, which is not assignable from <<float32,float32>,<float32>,<float32,float32,float32>>.","Expected TFF type {float32*}@CLIENTS, which is not assignable from <<float32,float32>,<float32>,<float32,float32,float32>>.**Describe the bug**
A clear and concise description of what the bug is. It is often helpful to
provide a link to a [colab](https://colab.research.google.com/) notebook that
reproduces the bug.

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
* Python package versions (e.g., TensorFlow Federated, TensorFlow):
* Python version:
* Bazel version (if building from source):
* CUDA/cuDNN version:
* What TensorFlow Federated execution stack are you using?

Note: You can collect the Python package information by running `pip3 freeze`
from the command line and most of the other information can be collected using
TensorFlows environment capture
[script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).

**Expected behavior**
A clear and concise description of what you expected to happen.

**Additional context**
Add any other context about the problem here.
From the title here, this sounds like TFF's static typesystem working as intended. Perhaps posting the code raising this error on StackOverflow with the tensorflow-federated tag would be the best next step here.I encountered this error when I learned the tutorial [https://github.com/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_1.ipynb](url)
![Uploading Screenshot from 2020-10-29 13-49-51.png…]()
**get_global_temperature_average([[68.0, 70.0], [71.0], [68.0, 72.0, 70.0]])**

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-34-95c690a2d13f> in <module>
----> 1 get_global_temperature_average([[68.0, 70.0], [71.0], [68.0, 72.0, 70.0]])

~/.local/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in __call__(self, *args, **kwargs)
    518   def __call__(self, *args, **kwargs):
    519     context = self._context_stack.current
--> 520     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
    521     return context.invoke(self, arg)
    522 

~/.local/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in pack_args(parameter_type, args, kwargs, context)
    315       else:
    316         arg = pack_args_into_struct(args, kwargs, parameter_type, context)
--> 317       return context.ingest(arg, parameter_type)
    318 
    319 

~/.local/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/tensorflow_context/tensorflow_computation_context.py in ingest(self, val, type_spec)
     39 
     40   def ingest(self, val, type_spec):
---> 41     type_analysis.check_type(val, type_spec)
     42     return val
     43 

~/.local/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/types/type_analysis.py in check_type(value, type_spec)
     89   value_type = type_conversions.infer_type(value)
     90   if not type_spec.is_assignable_from(value_type):
---> 91     raise TypeError(
     92         'Expected TFF type {}, which is not assignable from {}.'.format(
     93             type_spec, value_type))

TypeError: Expected TFF type {float32*}@CLIENTS, which is not assignable from <<float32,float32>,<float32>,<float32,float32,float32>>.",3,2020-10-27 07:10:13,2020-10-29 05:52:53,2020-10-29 05:04:10
https://github.com/tensorflow/federated/issues/914,['bug'],TensorBoard errors in Federated Learning for Image Classification tutorial,"TensorBoard errors in Federated Learning for Image Classification tutorial**Describe the bug**
Notebook: [Federated Learning for Image Classification Tutoria](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb#scrollTo=8BKyHkMxKHfV)l
Command:
```
#@test {""skip"": true}
%tensorboard --logdir /tmp/logs/scalars/ --port=0

ERROR: Failed to launch TensorBoard (exited with 1).
Contents of stderr:
2020-10-06 17:10:06.022164: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia
2020-10-06 17:10:06.022223: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 8, in <module>
    sys.exit(run_main())
  File ""/usr/local/lib/python3.6/dist-packages/tensorboard/main.py"", line 75, in run_main
    app.run(tensorboard.main, flags_parser=tensorboard.configure)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python3.6/dist-packages/tensorboard/program.py"", line 290, in main
    return runner(self.flags) or 0
  File ""/usr/local/lib/python3.6/dist-packages/tensorboard/program.py"", line 306, in _run_serve_subcommand
    server = self._make_server()
  File ""/usr/local/lib/python3.6/dist-packages/tensorboard/program.py"", line 416, in _make_server
    ingester.deprecated_multiplexer,
  File ""/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py"", line 149, in TensorBoardWSGIApp
    experimental_middlewares,
  File ""/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py"", line 257, in __init__
    ""Duplicate plugins for name %s"" % plugin.plugin_name
ValueError: Duplicate plugins for name projector 
```

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
* Python package versions (TensorFlow Federated, TensorFlow): TensorFlow Federated, TensorBoard
* Python version:python3.6
* Bazel version (if building from source):N/A using google collab environment
* CUDA/cuDNN version:
* What TensorFlow Federated execution stack are you using?

Note: You can collect the Python package information by running `pip3 freeze`
from the command line and most of the other information can be collected using
TensorFlows environment capture
[script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).

**Expected behavior**
I expected tensorboard to start.  I got an error that reflects issues in the underlying environment instead.

**Additional context**
Add any other context about the problem here.
I'm hoping to have a fix submitted soon, in the meantime I think changing the first cell along these lines should fix the issue:

```
# tensorflow_federated_nightly also bring in tf_nightly, which
# can causes a duplicate tensorboard install, leading to errors.
!pip uninstall --yes tensorboard tb-nightly

!pip install --quiet --upgrade tensorflow_federated_nightly
!pip install --quiet --upgrade nest_asyncio
!pip install --quiet tb-nightly  # or tensorboard, but not both
```",1,2020-10-06 17:14:41,2020-11-02 16:48:04,2020-11-02 16:48:04
https://github.com/tensorflow/federated/issues/912,['bug'],federated/tensorflow_federated/python/examples/simple_fedavg/emnist_fedavg_main.py does not run from fresh install,"federated/tensorflow_federated/python/examples/simple_fedavg/emnist_fedavg_main.py does not run from fresh install**Describe the bug**
Running federated/tensorflow_federated/python/examples/simple_fedavg/emnist_fedavg_main.py errors out with a sizeable traceback that I've included [here](https://gist.github.com/JustoA/d9b5ab836496369d7a210297eed30869) when running from fresh install. 
The error that I think is the cause is: `TypeError: ReferenceResolvingExecutor asked to create call with incompatible type specifications. Function takes an argument of type <model_weights=<trainable=<float32[5,5,1,32],float32[32],float32[5,5,32,64],float32[64],float32[3136,512],float32[512],float32[512,10],float32[10]>,non_trainable=<>>,optimizer_state=<int64>,round_num=int32>, but was supplied an argument of type <model_weights=<non_trainable=<>,trainable=<>>,optimizer_state=<int64>,round_num=int32>`.

**Environment:**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04.1 LTS
* Python package versions (e.g., TensorFlow Federated, TensorFlow): 
Tensorflow 2.2.1
tensorflow-federated-nightly 0.16.1.dev20200928
tf-estimator-nightly==2.4.0.dev2020092701
tf-nightly==2.4.0.dev20200927
tfa-nightly==0.12.0.dev20200927184204

* Python version: 3.8.2
* Bazel version (if building from source):  Bazelisk version: v1.6.1
* CUDA/cuDNN version: N/A
* What TensorFlow Federated execution stack are you using?
The default one, I'm not entirely sure what this means.

**Expected behavior**
I expected the example file to run without error.

**Additional context**
I'm running this within a pycharm venv. I also had to change some imports in the example directory from
`from tensorflow_federated.python.examples.simple_fedavg import simple_fedavg_tf
from tensorflow_federated.python.examples.simple_fedavg import simple_fedavg_tff`

to

`from federated.tensorflow_federated.python.examples.simple_fedavg import simple_fedavg_tf
from federated.tensorflow_federated.python.examples.simple_fedavg import simple_fedavg_tff`.

I've also tried installing just from pip, but the same error occurs.FYI This may be a Python 3.8 Issue. Running with 3.6 works just fine. @michaelreneer , could you comment on the python version issue? Thanks!@JustoA I think this is correct. I have not been able to dig into this yet, but I have seen failures with Python 3.8 before in some of our downstream dependencies and possibly in TFF as well. We do need to track this issue, but right now you have a work around correct?@michaelreneer Yes, the workaround is running Python 3.6 instead of 3.8.@JustoA Hi, we have a new release and now tested under python 3.8. Could you try it our again, or close this issue if it is no longer an interest? Thanks! I'll get on this ASAPYep, can confirm it's working as intended! Thanks y'all",7,2020-09-28 17:45:46,2020-11-09 01:22:20,2020-11-09 01:22:20
https://github.com/tensorflow/federated/issues/903,['bug'],Memory explosion,"Memory explosionWhile running the following command:
`bazel run main:federated_trainer -- --task=cifar100 --total_rounds=1500 --client_optimizer=sgd --client_learning_rate=0.1 --client_batch_size=20 --server_optimizer=sgd --server_learning_rate=1 --clients_per_round=10 --client_epochs_per_round=1 --experiment_name=cifar100_fedavg`
the VRAM usage explodes.
It quickly reaches 8.9 GB in the first round. It remains stable, until around the 125. round, when it doubles to 16.9 GB. This phenomenon repeats itself until the training crashes.Hi @matech96. Can you verify that you are using the latest version of TFF? If so, can you see what happens when you set `clients_per_round=1`?Hi, @zcharles8! I'm using version 0.16.1, which is the latest as I can tell. It is reproducible with `clients_per_round=1` as well. It starts off with 4.9 GB, then around the 70. round, jumps to 8.9 GB.I also got into this memory explosion issue when running TFF 0.16.1 on GPU (a single GPU). I would be happy to provide more details if neededI am marking this as resolved (see google-research/federated#29 for context).",4,2020-09-08 06:26:24,2021-04-09 17:26:28,2021-04-09 17:26:28
https://github.com/tensorflow/federated/issues/902,['bug'],Can't run 'federated-learning-for-text-generation.ipynb' colab,"Can't run 'federated-learning-for-text-generation.ipynb' colabI tried to run the federated_learning_for_text_generation.ipynb but I am getting an error when running the cell 

```
import collections
import functools
import os
import time

import numpy as np
import tensorflow as tf
import tensorflow_federated as tff

tf.compat.v1.enable_v2_behavior()

np.random.seed(0)

# Test the TFF is working:
tff.federated_computation(lambda: 'Hello, World!')()
```

```
RuntimeError                              Traceback (most recent call last)
<ipython-input-1-8e1fcc795c0f> in <module>()
     13 
     14 # Test the TFF is working:
---> 15 tff.federated_computation(lambda: 'Hello, World!')()

8 frames
/usr/lib/python3.6/asyncio/base_events.py in run_forever(self)
    426         if events._get_running_loop() is not None:
    427             raise RuntimeError(
--> 428                 'Cannot run the event loop while another loop is running')
    429         self._set_coroutine_wrapper(self._debug)
    430         self._thread_id = threading.get_ident()

RuntimeError: Cannot run the event loop while another loop is running
```Try #842 ?Great, thanks. I will try it out.Closing this; thank you @hongliny for referencing #842!",3,2020-09-07 13:11:28,2020-10-29 05:08:07,2020-10-29 05:08:07
https://github.com/tensorflow/federated/issues/897,['bug'],`tff.learning.from_keras_model` does not accept subclassed Keras model,"`tff.learning.from_keras_model` does not accept subclassed Keras model**Describe the bug**
`tff.learning.from_keras_model()` does not accept subclassed keras model. See this [colab](https://colab.research.google.com/drive/1dPJlo3e6vVHXS9O4Pm3_Qq2c_h0VtRaP?usp=sharing).

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
* Python package versions (e.g., TensorFlow Federated, TensorFlow): Colab TF-nightly TFF-nightly
* Python version: python 3.6
* Bazel version (if building from source): N/A
* CUDA/cuDNN version: N/A
* What TensorFlow Federated execution stack are you using? N/A

**Expected behavior**
`tff.learning.from_keras_model()` properly accepts subclassed keras model, or provide info on the doc if this is not possible?

**Additional context**
The issue seems to be due to `from_keras_model()` consuming `tf.keras.model.output` (see [pointer](https://github.com/tensorflow/federated/blob/v0.16.1/tensorflow_federated/python/learning/keras_utils.py#L260)) which is not written when the model is subclassed.Thanks Honglin! One option is to write your own keras model wrapper, or tff.learning.Model. It should not be very difficult, but you may loose some flexibility on things like keras metricss. Examples can be found at https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/examples/simple_fedavg/simple_fedavg_tf.py#L38
and https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/examples/simple_fedavg/simple_fedavg_test.py#L137This behavior is definitely TFF working as intended. TFF is strongly typed and neds to be able to reason about the output shapes and types of the functions it invokes.

But definitely we should update our documentation. Writing such a change now.",2,2020-09-02 23:22:50,2021-02-17 15:55:35,2021-02-17 15:55:35
https://github.com/tensorflow/federated/issues/896,['bug'],`tff.simulation.datasets.emnist.load_data()` extract(unzip) the dataset every time it runs,"`tff.simulation.datasets.emnist.load_data()` extract(unzip) the dataset every time it runs**Describe the bug**
`tff.simulation.datasets.emnist.load_data()` will extract(unzip) the `h5` dataset **every time** it runs. The cache mode will only cache the `.tar.bz2` file (see [here](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/simulation/datasets/stackoverflow.py#L96)). For large dataset (say stackoverflow) it will take quite long time to extract. 

Also there is thread conflict issue if multiple parallel runs attempt to extract the same file.

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
* Python package versions (e.g., TensorFlow Federated, TensorFlow): TF 2.3, TFF 0.16.1
* Python version: 3.7
* Bazel version (if building from source): 3.4.1
* CUDA/cuDNN version: N/A
* What TensorFlow Federated execution stack are you using? N/A

**Expected behavior**
`h5` dataset to be properly cached.

**Additional context**
N/Acc @zcharles8 (as per git blame) Can you please help/route to code owner? Thanks!Hi @hongliny. I'll take a look. We're happy to have contributions on this front, but otherwise I'll add this to my to-do triage.This should be fixed by https://github.com/tensorflow/federated/commit/7bb20162f96e3d5bc28ce50b206de0138d9894eb

The datasets should be uncompressed once, during download. After that the uncompressed files are cached and re-used.",3,2020-09-02 23:17:28,2021-02-20 15:59:19,2021-02-20 15:59:19
https://github.com/tensorflow/federated/issues/895,[],Failed to build from source,"Failed to build from source**While trying to build from source according to the [instructions](https://github.com/tensorflow/federated/blob/master/docs/install.md), I encountered the following error:**

`ERROR: /home/wzy/TF_privacy/Federated_Learning/federated/tensorflow_federated/tools/development/BUILD:9:10: failed to create symbolic link 'tensorflow_federated/tools/development/build_pip_package': file 'tensorflow_federated/tools/development/build_pip_package.sh' is not executable Target //tensorflow_federated/tools/development:build_pip_package failed to build Use --verbose_failures to see the command lines of failed build steps.
`

**Here is the full screenshot of the output:** 
![image](https://user-images.githubusercontent.com/14258909/91701163-78fbf500-eba9-11ea-8a63-27aca0a654e4.png)

**My configuration  are as follows:**

- tensorflow==2.0.0
- Ubuntu 18.04.3
- python = 3.7.5



Can you share the output of running the following command from the repository directory.

```bash
ls -la ""tensorflow_federated/tools/development""
```Sure. Here is the corresponding output of the script:
![image](https://user-images.githubusercontent.com/14258909/91780285-ac7e6400-ec29-11ea-81e8-b0b56523e022.png)

So the problem is that those scripts are not executable. You can find about more about `chmod` here https://linux.die.net/man/1/chmod.

I think the simplest option is to just run...

```
chmod +x tensorflow_federated/tools/development/build_pip_package.sh
```

Please let me know if this resolves your issue.> So the problem is that those scripts are not executable. You can find about more about `chmod` here https://linux.die.net/man/1/chmod.
> 
> I think the simplest option is to just run...
> 
> ```
> chmod +x tensorflow_federated/tools/development/build_pip_package.sh
> ```
> 
> Please let me know if this resolves your issue.

It worked for me. Thank you very much !",4,2020-08-31 08:47:27,2020-09-02 00:19:06,2020-09-02 00:19:06
https://github.com/tensorflow/federated/issues/893,[],Unable to interpret an argument of type numpy.ndarray as a type spec.,"Unable to interpret an argument of type numpy.ndarray as a type spec.Hi, 
I am receiving the following error, ""Unable to interpret an argument of type numpy.ndarray as a type spec."" The code works fine with TF1.x but for some reason, in TF2.x, I get the following error.   Already tried data.astype(dtype=float). Any view would be great! 

 File ""C:\Users\user\abc.py"", line 1072, in <module>
    trainer_Itr_Process = tff.learning.build_federated_averaging_process(model_fn_Federated,client_optimizer_fn=(lambda : tf.keras.optimizers.SGD(learning_rate=0.01)))

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\learning\federated_averaging.py"", line 218, in build_federated_averaging_process
    aggregation_process=aggregation_process)

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\learning\framework\optimizer_utils.py"", line 732, in build_model_delta_optimizer_process
    aggregation_process=aggregation_process)

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\learning\framework\optimizer_utils.py"", line 364, in _build_one_round_computation
    dataset_type = tff.SequenceType(dummy_model_for_metadata.input_spec)

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\api\computation_types.py"", line 421, in __init__
    self._element = to_type(element)

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\api\computation_types.py"", line 704, in to_type
    return StructWithPythonType(spec, type(spec))

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\api\computation_types.py"", line 382, in __init__
    super().__init__(elements)

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\api\computation_types.py"", line 340, in __init__
    structure.Struct.__init__(self, elements)

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\common_libs\structure.py"", line 79, in __init__
    for idx, e in enumerate(elements):

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\api\computation_types.py"", line 338, in <genexpr>
    elements = (_map_element(e) for e in elements)

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\api\computation_types.py"", line 331, in _map_element
    return (e[0], to_type(e[1]))

  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\api\computation_types.py"", line 716, in to_type
    py_typecheck.type_string(type(spec))))

TypeError: Unable to interpret an argument of type numpy.ndarray as a type spec.

Environment: Python 3.7.7
The pip freeze: 
Keras==2.4.3
numpy==1.19.1
tensorflow==2.2.0
tensorflow-addons==0.10.0
tensorflow-estimator==2.2.0
tensorflow-federated==0.16.1
tensorflow-model-optimization==0.3.0Hi @elifustundag 

It looks like this is failing attempting to pull a TFF typespec off of the model_fn to me. Can we see the definition of `model_fn_Federated`?Closing due to lack of response.",2,2020-08-21 12:12:56,2021-02-17 06:07:57,2021-02-17 06:07:57
https://github.com/tensorflow/federated/issues/892,['bug'],Got error when run personalization experiment,"Got error when run personalization experiment**Describe the bug**
A clear and concise description of what the bug is. It is often helpful to
provide a link to a [colab](https://colab.research.google.com/) notebook that
reproduces the bug.

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
* Python package versions (e.g., TensorFlow Federated, TensorFlow): Tensoflow-gpu 2.2, federated 0.16.1(From pypi)
* CUDA/cuDNN version: 10.1
I tried to run personalization experiment unittest but I got some errors. [https://github.com/tensorflow/federated/blob/993493122ea908173eba68525482d5d522db236c/tensorflow_federated/python/research/personalization/p13n_utils_test.py](https://github.com/tensorflow/federated/blob/993493122ea908173eba68525482d5d522db236c/tensorflow_federated/python/research/personalization/p13n_utils_test.py#L1)

```
Testing started at 0:33 ...
ssh://liuyuan@122.207.82.54:14000/home/liuyuan/programs/miniconda3/envs/tf2/bin/python -u /home/liuyuan/.pycharm_helpers/pycharm/_jb_unittest_runner.py --target p13n_utils_test.P13NUtilsTest.test_build_personalize_fn_succeeds_with_valid_args
Launching unittests with arguments python -m unittest p13n_utils_test.P13NUtilsTest.test_build_personalize_fn_succeeds_with_valid_args in /home/liuyuan/shu_codes/federated_learning_tff/personalization

2020-08-13 00:33:25.201227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-08-13 00:33:33.437511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-13 00:33:33.437864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-13 00:33:33.440622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-13 00:33:33.443281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-13 00:33:33.443706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-13 00:33:33.446540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-13 00:33:33.448197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-13 00:33:33.454645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-13 00:33:33.456487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-13 00:33:33.458319: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-08-13 00:33:33.505750: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2100000000 Hz
2020-08-13 00:33:33.510571: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d7f3df9770 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-13 00:33:33.510589: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-13 00:33:33.511533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-13 00:33:33.511581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-13 00:33:33.511598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-13 00:33:33.511614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-13 00:33:33.511631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-13 00:33:33.511647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-13 00:33:33.511663: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-13 00:33:33.511680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-13 00:33:33.513154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-13 00:33:33.513194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-13 00:33:33.624360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-13 00:33:33.624391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-08-13 00:33:33.624397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-08-13 00:33:33.626161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10203 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5)
2020-08-13 00:33:33.628552: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d7f7d467b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-13 00:33:33.628570: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:718: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  if not isinstance(wrapped_dict, collections.Mapping):
WARNING:tensorflow:From /home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.

Error
Traceback (most recent call last):
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/unittest/case.py"", line 59, in testPartExecutor
    yield
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/unittest/case.py"", line 628, in run
    testMethod()
  File ""/home/liuyuan/shu_codes/federated_learning_tff/personalization/p13n_utils_test.py"", line 71, in test_build_personalize_fn_succeeds_with_valid_args
    p13n_metrics = p13n_fn(model=model, train_data=dataset, test_data=dataset)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 644, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1665, in _filtered_call
    self.captured_inputs)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1746, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 598, in call
    ctx=ctx)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node test_data/_32}}]]
	 [[Func/while/body/_11/input/_134/_52]]
  (1) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node test_data/_32}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_personalize_fn_1025]

Function call stack:
personalize_fn -> personalize_fn
```@ddayzzz Could you update TF to 2.3 to see if the error still exists? Thanks!> @ddayzzz Could you update TF to 2.3 to see if the error still exists? Thanks!

I upgrade tensorflow(gpu version) to 2.3 from pypi, but i still got the same errors.
```
Launching unittests with arguments python -m unittest p13n_utils_test.P13NUtilsTest.test_build_personalize_fn_succeeds_with_valid_args in /home/liuyuan/shu_codes/federated_learning_tff/personalization

2020-08-15 18:30:48.587939: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). 
 The versions of TensorFlow you are currently using is 2.3.0 and is not supported. 
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. 
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  UserWarning,
2020-08-15 18:30:49.779733: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-08-15 18:30:57.406770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:b2:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-15 18:30:57.406905: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-15 18:30:57.412322: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-15 18:30:57.415537: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-15 18:30:57.415976: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-15 18:30:57.418986: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-15 18:30:57.420530: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-15 18:30:57.426628: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-15 18:30:57.428847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-15 18:30:57.430272: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-15 18:30:57.478005: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz
2020-08-15 18:30:57.486456: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56144dbf0470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-15 18:30:57.486533: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-15 18:30:57.637296: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56144fcc6590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-15 18:30:57.637361: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-08-15 18:30:57.640469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:b2:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-15 18:30:57.640593: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-15 18:30:57.640646: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-15 18:30:57.640677: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-15 18:30:57.640707: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-15 18:30:57.640737: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-15 18:30:57.640766: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-15 18:30:57.640797: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-15 18:30:57.644716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-15 18:30:57.644789: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-15 18:30:58.383153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-15 18:30:58.383202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-08-15 18:30:58.383211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-08-15 18:30:58.384784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10066 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:b2:00.0, compute capability: 7.5)

Error
Traceback (most recent call last):
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/unittest/case.py"", line 59, in testPartExecutor
    yield
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/unittest/case.py"", line 628, in run
    testMethod()
  File ""/home/liuyuan/shu_codes/federated_learning_tff/personalization/p13n_utils_test.py"", line 75, in test_build_personalize_fn_succeeds_with_valid_args
    loop()
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node BatchDatasetV2/_18}}]]
  (1) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node BatchDatasetV2/_18}}]]
	 [[Func/while/body/_1/input/_51/_46]]
0 successful operations.
0 derived errors ignored. [Op:__inference_loop_85]

Function call stack:
loop -> loop



Assertion failed


Ran 1 test in 9.051s

FAILED (errors=1)

Process finished with exit code 1

Assertion failed

Assertion failed

```@ddayzzz Sorry that you still hit the issue. Could you try tf-nightly if possible? We cannot currently reproduce the issue, but I think I have seen it before. It is a bug inherited from TF on GPUs, and the bug has been fixed in TF. I am not sure if the fix has been in the released version yet, but TF-nightly should be good. > @ddayzzz Sorry that you still hit the issue. Could you try tf-nightly if possible? We cannot currently reproduce the issue, but I think I have seen it before. It is a bug inherited from TF on GPUs, and the bug has been fixed in TF. I am not sure if the fix has been in the released version yet, but TF-nightly should be good.

Install `tf-nightly` or `tf-nightly-gpu` ?> @ddayzzz Sorry that you still hit the issue. Could you try tf-nightly if possible? We cannot currently reproduce the issue, but I think I have seen it before. It is a bug inherited from TF on GPUs, and the bug has been fixed in TF. I am not sure if the fix has been in the released version yet, but TF-nightly should be good.

Thank you. tensorflow-federated works properly under tf 2.3.0 and tf-nightly.",5,2020-08-13 01:53:39,2020-08-16 08:36:49,2020-08-16 08:36:49
https://github.com/tensorflow/federated/issues/890,['bug'],research/targeted_attack: emnist targeted attack doesnt run,"research/targeted_attack: emnist targeted attack doesnt run**Describe the bug**
A clear and concise description of what the bug is. It is often helpful to
provide a link to a [colab](https://colab.research.google.com/) notebook that
reproduces the bug.

After cloning TensorFlow/federated and attempting to run emnist_with_targeted_attack, I get the following error:

TypeError: Expected tensorflow_federated.python.common_libs.structure.Struct or tensorflow_federated.python.learning.model_utils.ModelWeights, found tensorflow.python.autograph.impl.api.ModelWeights.

The issue is caused by the following line in the evaluate method:
tff.learning.assign_weights_to_keras_model(keras_model, state.model)

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
* Python package versions (e.g., TensorFlow Federated, TensorFlow):
tf: 2.2.0
tff: 0.16.1
* Python version:
Python: 3.7.6
* Bazel version (if building from source):
* CUDA/cuDNN version:
* What TensorFlow Federated execution stack are you using?

Note: You can collect the Python package information by running `pip3 freeze`
from the command line and most of the other information can be collected using
TensorFlows environment capture
[script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).

**Expected behavior**
A clear and concise description of what you expected to happen.

I expected the example code to run without errors, and return the test metrics when the evaluate method is called.

**Additional context**
Add any other context about the problem here.

Let me know if you need any additional information, or if I am doing something wrong.
Thank you in advance. I'll take a look internally--thanks for the report!Thank you

Yesterday I was not using Bazel to run the code, so that might be where the problem was coming from. I saw in other posts that Bazel is normally required for building and running the research code.

But I attempted to build emnist_with_targeted_attack with Bazel 3.3.1 using the following command:

bazel build //tensorflow_federated/python/research/targeted_attack:emnist_with_targeted_attack

I get the following error: 
ERROR: An error occurred during the fetch of repository 'local_config_python':
Python Configuration Error: Invalid python library path:

I have given the correct path to the folder containing the python.exe. I am confused as to what is wrong. Please let me know if I am doing something wrong or if you need additional information. It could just be a Bazel configuration issue that I need to work out. I am just trying to build and run the examples in the research folder. Are there any instructions for using Windows to build and run the research examples? I greatly appreciate your help. If I need to switch to a Linux OS I can try to do that as well. 
You are for sure correct that there is a bug here--there have been some internal changes that *should* make TFF user experience generally easier, but you are hitting a thorny edge case. Working on a fix now.

Thanks again for the report!Okay, thank you for your help. Let me know what else I need to do.That fixed the error. Thank you again.",5,2020-08-05 17:02:16,2020-08-06 21:09:46,2020-08-06 20:44:58
https://github.com/tensorflow/federated/issues/886,['bug'],Mandatory installation of tensorflow-cpu 1.15.3 when installing tensorflow_federated 0.2.0,"Mandatory installation of tensorflow-cpu 1.15.3 when installing tensorflow_federated 0.2.0I'm trying to install tensorflow_federated 0.2.0 on my PC using the command pip install tensorflow_federated==0.2.0. But after the installation, my python starts using a newly added tensorflow-cpu 1.15.3 instead of the tensorflow-gpu 1.13.1 I installed before, it seems that the tensorflow-cpu 1.15.3 is installed during the installation of tensorflow_federated 0.2.0. Is there any method that can avoid the installation of tensorflow-cpu during the installation of tensorflow_federated? I'm using ubuntu 18.04.
![1](https://user-images.githubusercontent.com/32090473/89012524-f29f8980-d344-11ea-8622-0a12801f3170.png)
Is there a reason you are trying to use an old version of TFF?

Here is the [pip reference](https://pip.pypa.io/en/stable/reference/), you can ignore dependencies with the `--no-dependencies` flag.",1,2020-07-31 07:46:14,2020-08-03 20:26:11,2020-08-03 20:26:11
https://github.com/tensorflow/federated/issues/884,['bug'],asyncio loop error in Google Colab,"asyncio loop error in Google ColabSo as of today I've been getting the following asyncio error when running in Google Colab:

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-6-a23308ec3f7c> in <module>()
      7 np.random.seed(0)
      8 
----> 9 tff.federated_computation(lambda: 'Hello, World!')()

8 frames
/usr/lib/python3.6/asyncio/base_events.py in run_forever(self)
    426         if events._get_running_loop() is not None:
    427             raise RuntimeError(
--> 428                 'Cannot run the event loop while another loop is running')
    429         self._set_coroutine_wrapper(self._debug)
    430         self._thread_id = threading.get_ident()

RuntimeError: Cannot run the event loop while another loop is running
```

It suddenly showed up in a project which has been working fine before. 
When running the tutorial `Federated_Learning_for_Image_Classification.ipynb` notebook, it also shows up in the first 'Hello world' federated test computation. I tried several types of runtimes and factory reset of the runtime. 

Since it can easily be replicated; anyone have an idea why this suddenly shows up?Colab has recently updated their runtime to use a new version of various dependencies, including tornado. This has hopefully been fixed in `master` as of #1f9daf6

Does this still occur when visiting https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynbThis new notebook w/ the asyncio patch seems to work! 
Might want to change the 'run in colab' link at https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification cause it still directs to the v0.15.0 notebook without the patch.

Thanks anyway!Thanks for confirming. Yup, we'll look at updating the tensorflow.org links.Please check out v0.16.0, or master in GitHub. Also the published tutorials should be pointing at master now.",4,2020-07-27 15:55:54,2020-07-30 03:42:15,2020-07-30 03:42:14
https://github.com/tensorflow/federated/issues/882,['bug'],Empty TFF runtime docker image when running dockerized TFF on GCP,"Empty TFF runtime docker image when running dockerized TFF on GCP**Describe the bug**

I tried to set up a TFF Docker runtime on GCP by following instructions on the official TFF website ( https://www.tensorflow.org/federated/gcp_setup ) . I encountered an error when running the TFF runtime container via the following command:

$ docker run \
    --detach \
    --name=runtime \
    --publish=8000:8000 \
    gcr.io/tensorflow-federated/runtime

The error message is: 

Unable to find image 'gcr.io/tensorflow-federated/runtime:latest' locally docker: Error response from daemon: manifest for gcr.io/tensorflow-federated/runtime:latest not found: manifest unknown: Failed to fetch ""latest"" from request ""/v2/tensorflow-federated/runtime/manifests/latest"".

Additional diagnose:

  It seems that the docker daemon reported this error because the URL appeared in the official document, i.e. gcr.io/tensorflow-federated/runtime is redirecting to pantheon.corp.google.com/gcr/images/tensorflow-federated/GLOBAL/runtime?gcrImageListsize=30  and it is an empty GCP storage bucket at this moment. 


**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ""Container-Optimized"" boot image provided by GCP
* Python package versions (e.g., TensorFlow Federated, TensorFlow): N/A
* Python version: N/A
* Bazel version (if building from source): N/A
* CUDA/cuDNN version: N/A
* What TensorFlow Federated execution stack are you using?
TensorFlow Federated runtime container suggested on the official TFF website ( https://www.tensorflow.org/federated/gcp_setup )

**Expected behavior**

docker run command should end without error, with TFF runtime container up and rumming.

**Additional context**
N/A
Doesn't look like the image: gcr.io/tensorflow-federated/runtime:latest' exists.

@jingtaow For public GCR registries you can enter the URL in a browser to browse the contents of the registry.",1,2020-07-21 22:08:07,2020-08-14 17:38:18,2020-08-14 17:38:18
https://github.com/tensorflow/federated/issues/881,['bug'],Docs of traced functions link to wrong source code location,"Docs of traced functions link to wrong source code location**Describe the bug**
Traced methods/functions have broken links to source code on tensorflow.org/federated/api_docs. For example, in [this method](https://www.tensorflow.org/federated/api_docs/python/tff/framework/EagerTFExecutor#create_call), clicking the `View Source` button redirects me to a [wrapper function](url) `async_trace` local to the `tracing.trace` decorator. The same is true of all other traced functions I tried this with.

**Expected behavior**
I would expect the above to direct me to [this place](https://github.com/tensorflow/federated/blob/v0.15.0/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py#L467-L497) in the GH source code
I am working on a fix for this.",1,2020-07-21 15:45:16,2020-08-03 20:22:19,2020-08-03 20:22:19
https://github.com/tensorflow/federated/issues/878,['bug'],Failed to build the TensorFlow Federated pip package ,"Failed to build the TensorFlow Federated pip package **Describe the bug**
Failed to install TFF from build by following the tutorials of `https://github.com/tensorflow/federated/blob/master/docs/install.md`.

The step 5 `pip install --requirement ""requirements.txt""` will install `tf-nightly`, whereas step 9 `pip install --upgrade ""/tmp/tensorflow_federated/tensorflow_federated-""*"".whl""` will install `tensorflow-2.2.0`. This leads to a version conflict error while executing step 10 :
```
python -c ""import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())""
```
while leads to (Note that the error itself is not relevant but is due to version conflict) 
```
module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'
```

I tried to uninstall `tensorflow` package afterwards but the dependency is still broken
```
AttributeError: module 'tensorflow' has no attribute 'function'
```

**Environment (please complete the following information):**
* OS Platform and Distribution: Linux Debian 9 from GCP `common-cpu` package
* Python package versions: N/A
* Python version: 3.7.6
* Bazel version: 3.3.1
* CUDA/cuDNN version: N/A
* What TensorFlow Federated execution stack are you using? N/A

**Expected behavior**
Can `import tensorflow_federated` after installation

@hongliny Can you try skipping steps 4-6 in that doc. I think the issue is that after step 6 you would need to deactivate the current virtualenv, but honestly these steps are not really helpful so I'd simply skip them.",1,2020-07-10 18:53:55,2020-07-14 20:41:17,2020-07-14 20:41:17
https://github.com/tensorflow/federated/issues/876,['bug'],Failing test related to encoding_utils,"Failing test related to encoding_utils**Describe the bug**
Test related to `tff.learning.framework.encoding_utils` fails on master.  The exact test is `test_broadcast_from_model`, which is specifically testing the factory function `tff.learning.framework.encoding_utils.build_encoded_broadcast_from_model`. I've pasted the error message & traceback below.

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.15
* Python package versions (e.g., TensorFlow Federated, TensorFlow): TFF master @ 259af6a3307ad01af27417cd1b910216e7c2b84c; `tf-nightly==2.3.0.dev20200625`
* Python version: 3.7
* Bazel version (if building from source): 0.26.1

Full list of TF dependencies:
```
tb-nightly==2.3.0a20200702
tensorboard-plugin-wit==1.7.0
tensorflow-model-optimization==0.3.0
tensorflow-privacy==0.3.0
tf-estimator-nightly==2.4.0.dev2020070201
tf-nightly==2.3.0.dev20200625
tfa-nightly==0.11.0.dev20200630231055
```

**Expected behavior**
Tests should pass.

**Additional context**
```
======================================================================
FAIL: test_broadcast_from_model (__main__.EncodingUtilsTest)
test_broadcast_from_model (__main__.EncodingUtilsTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_jasonmancuso/d42bb2c3e756badbc3917bcb81e95d65/sandbox/darwin-sandbox/543/execroot/org_tensorflow_federated/bazel-out/darwin-opt/bin/tensorflow_federated/python/learning/framework/encoding_utils_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/learning/framework/encoding_utils_test.py"", line 74, in test_broadcast_from_model
    self.assertLen(w, 2)
  File ""/Users/jasonmancuso/tff-aggregations/venv/lib/python3.7/site-packages/absl/testing/absltest.py"", line 866, in assertLen
    container_repr, len(container), expected_len), msg)
  File ""/Users/jasonmancuso/tff-aggregations/venv/lib/python3.7/site-packages/absl/testing/absltest.py"", line 1704, in fail
    return super(TestCase, self).fail(self._formatMessage(prefix, msg))
AssertionError: [<warnings.WarningMessage object at 0x13baba650>, <warnings.WarningMessage object at 0x13bd687d0>, <warnings.WarningMessage object at 0x13bc912d0>, <warnings.WarningMessage object at 0x13be524d0>] has length of 4, expected 2.

----------------------------------------------------------------------
```
I just noticed this message from tf-addons:

```
UserWarning: You are currently using a nightly version of TensorFlow (2.3.0-dev20200625).
TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not.
If you encounter a bug, do not file an issue on GitHub.
```

If this is not an issue that should be reported here, please feel free to close without response.This is surely a good issue to post here, don't worry, and this functionality should work.

I was not able to reproduce it now, but I noticed this target was briefly broken around the time matching your tf-nightly build date. I'll assume this is a transient bug due to an unlucky cut, please update the tf-nightly package and reopen if it is not fixed.",2,2020-07-02 20:20:22,2020-07-03 07:47:35,2020-07-03 07:47:34
https://github.com/tensorflow/federated/issues/874,['bug'],Does TFF support windows10 system?,"Does TFF support windows10 system?Hi,
I installed tensorflow-federated on windows10, and run [this](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/examples/remote_executor_example.py) tutorial. But I got the error of `module 'tensorflow_federated' has no attribute 'framework'`.

Python package versions are:
`python=3.6
tensorflow=1.3.1
tensorflow-federated=0.3.0
`
Is it because the OS is wrong? Whether TFF supports win10? Thanks.The current release of TFF is 0.14.0. Please update `tensorflow-federated` and try again.Closing this issue, I believe that @roselander suggestion should fix.",2,2020-06-28 02:09:29,2020-07-27 16:41:46,2020-07-27 16:40:59
https://github.com/tensorflow/federated/issues/872,['bug'],ERROR: No matching distribution found for tensorflow-addons~=0.9.1 (from tensorflow_federated),"ERROR: No matching distribution found for tensorflow-addons~=0.9.1 (from tensorflow_federated)**Describe the bug**
- pip install tensorflow_federated occur error 'ERROR: No matching distribution found for tensorflow-addons~=0.9.1 (from tensorflow_federated)'

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 19.3
* Python package versions (e.g., TensorFlow Federated, TensorFlow): Tensorflow Federated 0.14.0, Tensorflow 2.2.0
* Python version: Python 3.8.3
* Bazel version (if building from source): NA
* CUDA/cuDNN version: NA
* What TensorFlow Federated execution stack are you using?
absl-py==0.9.0
astunparse==1.6.3
cachetools==4.1.0
certifi==2020.6.20
chardet==3.0.4
gast==0.3.3
google-auth==1.18.0
google-auth-oauthlib==0.4.1
google-pasta==0.2.0
grpcio==1.30.0
h5py==2.10.0
idna==2.9
Keras-Preprocessing==1.1.2
Markdown==3.2.2
numpy==1.19.0
oauthlib==3.1.0
opt-einsum==3.2.1
protobuf==3.12.2
pyasn1==0.4.8
pyasn1-modules==0.2.8
requests==2.24.0
requests-oauthlib==1.3.0
rsa==4.6
scipy==1.4.1
six==1.15.0
tensorboard==2.2.2
tensorboard-plugin-wit==1.6.0.post3
tensorflow==2.2.0
tensorflow-estimator==2.2.0
termcolor==1.1.0
urllib3==1.25.9
Werkzeug==1.0.1
wrapt==1.12.1

**Expected behavior**
A clear and concise description of what you expected to happen.
1. Create new virtual environment using Python 3.8
```bash
python3 -m venv venv
source venv/bin/activate
```
2. pip install tensorflow_federated
```bash
pip3 install --upgrade pip
pip3 install tensorflow_federated
```
3. Error!
```bash
ERROR: Could not find a version that satisfies the requirement tensorflow-addons~=0.9.1 (from tensorflow_federated->-r tff-app/requirements.txt (line 4)) (from versions: 0.10.0)
ERROR: No matching distribution found for tensorflow-addons~=0.9.1 (from tensorflow_federated->-r tff-app/requirements.txt (line 4))
```

**Additional context**
Add any other context about the problem here.
Hi @munhyunsu 

Can you run this command in your virtual environment and share the output.

`pip install tensorflow-addons~=0.9.1`

Note that this should [succeed](https://pypi.org/project/tensorflow-addons/#history), but I expect it to fail in your environment for some reason. My guess is that you have `tensorflow-addons` installed in your system or user level site-packages. If that is the case you can use the `--ignore-installed` flag on the `pip` command to install the older version in your virtual environment.Hi @michaelreneer 

I run this command,
`pip3 install tensorflow-addons~=0.9.1` or `pip install --ignore-installed tensorflow-addons~=0.9.1`.
It produce same error that 
```bash
ERROR: Could not find a version that satisfies the requirement tensorflow-addons~=0.9.1 (from versions: 0.10.0)
ERROR: No matching distribution found for tensorflow-addons~=0.9.1
```

I guess it's a problem with Python 3.8.
The [tensorflow-addons Github](https://github.com/tensorflow/addons) states that the 0.9.1 version supports only Python 3.7 and the 0.10.0 version supports Python 3.8.
If I install only tensorflow-addons without tensorflow_federated in Python 3.8, `pip3 install tensorflow-addons` succeeds.
However, `pip3 install tensorflow-addons~=0.9.1` fails.
Hi @munhyunsu

That makes sense. It seems like the set of TFF requirements as it currently stands is not compatible with Python 3.8.

https://github.com/tensorflow/addons/blob/r0.9/setup.py#L87
https://github.com/tensorflow/federated/blob/v0.14.0/tensorflow_federated/tools/development/setup.py#L101

However, it does appear that `tensorflow-addons` is compatible with Python 3.7.

https://github.com/tensorflow/addons/blob/r0.10/setup.py#L92

Either way, you could work around this issue by constructing your virtual environment with Python 3.7?Hi @michaelreneer 

Of course it is possible with 3.7. :D

I just tried it because I wanted to try out the latest features of Python 3.8 in my code.
I'll later change to Python 3.8 if the dependency changes to a version that supports `tensorflow-addons` used by `tensorflow_federated`.
Thanks for the help.
I didn't open `setup.py` and immediately installed it with `pip3`, so I didn't know that Python 3.8 is not supported yet.

PS. But, if this happens, shouldn't it be marked somewhere that Python 3.8 doesn't support TFF yet? I think someone has the same problem as me.
For example, write down the Python version in the TFF-TF version mapping table in the README.
Like a tensorflow-addons README!

Thanks again!",4,2020-06-25 01:38:46,2020-07-06 02:18:39,2020-07-06 02:18:39
https://github.com/tensorflow/federated/issues/869,['bug'],Kernel freezes after running tff.federated_computation,"Kernel freezes after running tff.federated_computation**Describe the bug**
A clear and concise description of what the bug is. It is often helpful to
provide a link to a [colab](https://colab.research.google.com/) notebook that
reproduces the bug.

**Environment (please complete the following information):** 
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
* Python package versions (e.g., TensorFlow Federated, TensorFlow): 
      tensorflow-2.2.0, tf-federated - 0.14.0
* Python version: 3.7
* Bazel version (if building from source):
* CUDA/cuDNN version: 10.1, >418.x
* What TensorFlow Federated execution stack are you using? 

Note: You can collect the Python package information by running `pip3 freeze`
from the command line and most of the other information can be collected using
TensorFlows environment capture
[script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).

**Expected behavior**
Want to set up tensorflow federated with GPU support in my local machine. Installation had no issues. But I am not run the code with tensorflow_federated

**Additional context**

_Code:_

> import collections
> import os
> import numpy as np
> import glob
> import matplotlib.pyplot as plt
> from pathlib import Path
> 
> import tensorflow as tf
> from tensorflow import reshape, nest, config
> from tensorflow.keras import losses, metrics, optimizers
> import tensorflow_federated as tff
> from tensorflow.keras.models import Sequential
> from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
> 
> import nest_asyncio
> nest_asyncio.apply()
> 
> tff.federated_computation(lambda: 'hi')()
> np.random.seed(0)


After running this, I am not able to get the outputs for consecutive cells. The kernel freezes and I have it kill it everytime.

**LOGS:**


` (venv) (tensorflo-local) C:\Users\gokul\Codebase_SDD\Machine Learning\Tensorflow\federated\ToyFederatedLearning\TensorFlow\Federated>jupyter notebook
[I 23:34:06.172 NotebookApp] Serving notebooks from local directory: C:\Users\gokul\Codebase_SDD\Machine Learning\Tensorflow\federated\ToyFederatedLearning\TensorFlow\Federated
[I 23:34:06.172 NotebookApp] The Jupyter Notebook is running at:
[I 23:34:06.172 NotebookApp] http://localhost:8888/?token=3aa46721913631f4bad5ba9e8c46011fd51c777c5cd99845
[I 23:34:06.174 NotebookApp]  or http://127.0.0.1:8888/?token=3aa46721913631f4bad5ba9e8c46011fd51c777c5cd99845
[I 23:34:06.174 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 23:34:06.262 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///C:/Users/gokul/AppData/Roaming/jupyter/runtime/nbserver-7068-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=3aa46721913631f4bad5ba9e8c46011fd51c777c5cd99845
     or http://127.0.0.1:8888/?token=3aa46721913631f4bad5ba9e8c46011fd51c777c5cd99845
[W 23:34:10.009 NotebookApp] Notebook TFFederated.ipynb is not trusted
[I 23:34:10.690 NotebookApp] Kernel started: 0ff077b0-7063-4cd9-a1a3-67175a839873
2020-06-09 23:34:26.636662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-06-09 23:34:32.733418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-06-09 23:34:32.823969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 with Max-Q Design computeCapability: 6.1
coreClock: 1.48GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2020-06-09 23:34:32.834014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-06-09 23:34:32.842948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-06-09 23:34:32.851389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-06-09 23:34:32.857673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-06-09 23:34:32.865883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-06-09 23:34:32.873915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-06-09 23:34:32.887805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-06-09 23:34:32.892708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-06-09 23:34:32.896278: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-06-09 23:34:32.907407: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23689ec41d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-06-09 23:34:32.913272: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-06-09 23:34:32.918127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 with Max-Q Design computeCapability: 6.1
coreClock: 1.48GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2020-06-09 23:34:32.927971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-06-09 23:34:32.933083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-06-09 23:34:32.938547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-06-09 23:34:32.943804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-06-09 23:34:32.948171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-06-09 23:34:32.953024: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-06-09 23:34:32.959809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-06-09 23:34:32.964122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-06-09 23:34:33.599328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-06-09 23:34:33.604817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-06-09 23:34:33.607803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-06-09 23:34:33.611780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4700 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-06-09 23:34:33.624057: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x236b4085280 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-06-09 23:34:33.630275: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1060 with Max-Q Design, Compute Capability 6.1

`
Check your tornado version - https://github.com/jupyter/notebook/issues/3397

Reverting to the old version did the trick for meThanks @ambrishrawat 
Had fixed it already. Referred the same link.

This can be fixed by downgrading the jupyter notebook and tornado version.

Default versions of notebook >= 6.x and tornado >= 6.0

By running the command,

pip install jupyter notebook==5.7.8 tornado==4.5.0

This issue is fixed",2,2020-06-10 06:44:45,2020-06-13 06:27:24,2020-06-13 06:27:23
https://github.com/tensorflow/federated/issues/868,['bug'],ERROR: No matching distribution found for tensorflow-addons~=0.9.1 (from tensorflow_federated),"ERROR: No matching distribution found for tensorflow-addons~=0.9.1 (from tensorflow_federated)**Describe the bug**
A clear and concise description of what the bug is. It is often helpful to
provide a link to a [colab](https://colab.research.google.com/) notebook that
reproduces the bug.

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
* Python package versions (e.g., TensorFlow Federated, TensorFlow): tensorflow = 2.2.0, tffederated=latest
* Python version: 3.8
* Bazel version (if building from source): N/A
* CUDA/cuDNN version: 10.1/ 7.2
* What TensorFlow Federated execution stack are you using? 

Note: You can collect the Python package information by running `pip3 freeze`
from the command line and most of the other information can be collected using
TensorFlows environment capture
[script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).

**Expected behavior**
A clear and concise description of what you expected to happen.
I tried to install tensorflow_federated on my machine and I get his error. I followed all the required steps to set this up.
Cmd: pip install --upgrade --tensorflow_federated

`Collecting tensorflow_federated
  Using cached tensorflow_federated-0.14.0-py2.py3-none-any.whl (452 kB)
Collecting tensorflow-model-optimization~=0.3.0
  Using cached tensorflow_model_optimization-0.3.0-py2.py3-none-any.whl (165 kB)
Collecting portpicker~=1.3.1
  Using cached portpicker-1.3.1.tar.gz (18 kB)
Requirement already satisfied, skipping upgrade: attrs~=19.3.0 in c:\users\gokul\appdata\local\continuum\anaconda3\envs\tensorflo\lib\site-packages (from tensorflow_federated) (19.3.0)
Requirement already satisfied, skipping upgrade: absl-py~=0.9.0 in c:\users\gokul\appdata\local\continuum\anaconda3\envs\tensorflo\lib\site-packages (from tensorflow_federated) (0.9.0)
Requirement already satisfied, skipping upgrade: tensorflow~=2.2.0 in c:\users\gokul\appdata\local\continuum\anaconda3\envs\tensorflo\lib\site-packages (from tensorflow_federated) (2.2.0)
Collecting semantic-version~=2.8.5
  Using cached semantic_version-2.8.5-py2.py3-none-any.whl (15 kB)
Collecting retrying~=1.3.3
  Using cached retrying-1.3.3.tar.gz (10 kB)
Requirement already satisfied, skipping upgrade: numpy~=1.18.4 in c:\users\gokul\appdata\local\continuum\anaconda3\envs\tensorflo\lib\site-packages (from tensorflow_federated) (1.18.5)
Collecting tensorflow-privacy~=0.3.0
  Using cached tensorflow_privacy-0.3.0-py3-none-any.whl (84 kB)
Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in c:\users\gokul\appdata\local\continuum\anaconda3\envs\tensorflo\lib\site-packages (from tensorflow_federated) (2.10.0)
Collecting dm-tree~=0.1.1
  Using cached dm_tree-0.1.5-cp38-cp38-win_amd64.whl (83 kB)
Collecting cachetools~=3.1.1
  Using cached cachetools-3.1.1-py2.py3-none-any.whl (11 kB)
**ERROR: Could not find a version that satisfies the requirement tensorflow-addons~=0.9.1 (from tensorflow_federated) (from versions: 0.10.0)
ERROR: No matching distribution found for tensorflow-addons~=0.9.1 (from tensorflow_federated)**`

Fixed by uninstalling and installing tf and tff",1,2020-06-08 22:18:24,2020-06-09 00:13:03,2020-06-09 00:13:02
https://github.com/tensorflow/federated/issues/867,['bug'],Script aborts due to ModuleNotFoundError,"Script aborts due to ModuleNotFoundError**Describe the bug**
I am trying to execute the `run_federated.py` script but it throws the following error  
  
    
    Traceback (most recent call last):
        File ""run_federated.py"", line 24, in <module>
            from tensorflow_federated.python.research.differential_privacy import dp_utils
    ModuleNotFoundError: No module named 'tensorflow_federated.python.research'

This is strange because I have a conda environment with all modules installed. Then why I am getting import errors? 


**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
* Python package versions (e.g., TensorFlow Federated, TensorFlow): TF federated 0.14.0
* Python version: 3.7.7

Hi @jaintj95 

Research is not included in the pip package--to run those scripts, you will need to build TFF from source--instructions are included [here](https://github.com/tensorflow/federated/blob/master/docs/install.md#build-the-tensorflow-federated-pip-package).I am going to go ahead and close this @jaintj95 please feel free to reopen if @jkr26 explanation did not solve your issue.",2,2020-06-06 05:58:20,2020-06-15 18:20:18,2020-06-15 18:20:18
https://github.com/tensorflow/federated/issues/866,['bug'],Installation failing,"Installation failing**Describe the bug**
I am trying to install tf federated using pip and the installation fails 
```ERROR: Could not find a version that satisfies the requirement tensorflow-addons~=0.9.1 (from tensorflow_federated) (from versions: 0.10.0)
ERROR: No matching distribution found for tensorflow-addons~=0.9.1 (from tensorflow_federated)
```
![fed_tf](https://user-images.githubusercontent.com/36858630/83936041-116a1f80-a7dd-11ea-9113-6b9dcd734c86.png)


**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10


It looks like tf-federated isn't compatible with Python 3.8 
I fixed the installation issue by install python 3.7 in my conda env 
Closing the issue now",1,2020-06-06 04:34:30,2020-06-06 05:53:53,2020-06-06 05:53:53
https://github.com/tensorflow/federated/issues/861,['bug'],Cannot run the event loop while another loop is running,"Cannot run the event loop while another loop is runningpython 3.7.7
all are the latest upgrade

when I try to run


```
import collections
import numpy as np
import tensorflow as tf
import tensorflow_federated as tff

tf.compat.v1.enable_v2_behavior()

np.random.seed(0)

tff.federated_computation(lambda: 'Hello, World!')()
```


RuntimeError: Cannot run the event loop while another loop is running


And I try this. then second-line not running


```
import nest_asyncio
nest_asyncio.apply()
tff.federated_computation(lambda: 'Hello, World!')()
Out[4]: b'Hello, World!'
```


then this line not working or not printing
`print(""hello"")`
no output it's just stack

![problem1](https://user-images.githubusercontent.com/26671669/83523238-b2b65480-a503-11ea-9641-3c8f279cc0a3.png)
Were you able to fix this?
Facing the same error.My error logs:

`---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-5-9c097e9baec9> in <module>
----> 1 tff.federated_computation(lambda: 'hi')()

~\AppData\Local\Continuum\anaconda3\envs\tflocal\lib\site-packages\tensorflow_federated\python\core\impl\utils\function_utils.py in __call__(self, *args, **kwargs)
    562     context = self._context_stack.current
    563     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 564     return context.invoke(self, arg)
    565 
    566 

~\AppData\Local\Continuum\anaconda3\envs\tflocal\lib\site-packages\retrying.py in wrapped_f(*args, **kw)
     47             @six.wraps(f)
     48             def wrapped_f(*args, **kw):
---> 49                 return Retrying(*dargs, **dkw).call(f, *args, **kw)
     50 
     51             return wrapped_f

~\AppData\Local\Continuum\anaconda3\envs\tflocal\lib\site-packages\retrying.py in call(self, fn, *args, **kwargs)
    204 
    205             if not self.should_reject(attempt):
--> 206                 return attempt.get(self._wrap_exception)
    207 
    208             delay_since_first_attempt_ms = int(round(time.time() * 1000)) - start_time

~\AppData\Local\Continuum\anaconda3\envs\tflocal\lib\site-packages\retrying.py in get(self, wrap_exception)
    245                 raise RetryError(self)
    246             else:
--> 247                 six.reraise(self.value[0], self.value[1], self.value[2])
    248         else:
    249             return self.value

~\AppData\Local\Continuum\anaconda3\envs\tflocal\lib\site-packages\six.py in reraise(tp, value, tb)
    701             if value.__traceback__ is not tb:
    702                 raise value.with_traceback(tb)
--> 703             raise value
    704         finally:
    705             value = None

~\AppData\Local\Continuum\anaconda3\envs\tflocal\lib\site-packages\retrying.py in call(self, fn, *args, **kwargs)
    198         while True:
    199             try:
--> 200                 attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
    201             except:
    202                 tb = sys.exc_info()

~\AppData\Local\Continuum\anaconda3\envs\tflocal\lib\site-packages\tensorflow_federated\python\core\impl\executors\execution_context.py in invoke(self, comp, arg)
    202         return event_loop.run_until_complete(
    203             tracing.wrap_coroutine_in_current_trace_context(
--> 204                 _invoke(executor, comp, arg)))

~\AppData\Local\Continuum\anaconda3\envs\tflocal\lib\asyncio\base_events.py in run_until_complete(self, future)
    469         future.add_done_callback(_run_until_complete_cb)
    470         try:
--> 471             self.run_forever()
    472         except:
    473             if new_task and future.done() and not future.cancelled():

~\AppData\Local\Continuum\anaconda3\envs\tflocal\lib\asyncio\base_events.py in run_forever(self)
    426         if events._get_running_loop() is not None:
    427             raise RuntimeError(
--> 428                 'Cannot run the event loop while another loop is running')
    429         self._set_coroutine_wrapper(self._debug)
    430         self._thread_id = threading.get_ident()

RuntimeError: Cannot run the event loop while another loop is running`This can be fixed by using nest_asyncio.
If you face kernel freezing error, try downgrading the jupyter notebook and tornado version.

Default versions of notebook >= 6.x and tornado >= 6.0

By running the command,

`pip install jupyter notebook==5.7.8 tornado==4.5.0`

This issue is fixedInstall old version of notebook and tornado is not a proper solution developer need fix this issue as well as.@imrankhan441 More detailed information on the issue can be found in our documentation [here](https://github.com/tensorflow/federated/blob/master/docs/tutorials/README.md#running-the-tff-tutorials-in-jupyter-notebooks). In short this is not an issue with TFF, but rather a feature of Jupyter.  Alternatively you could run your notebooks using https://colab.research.google.com/.",5,2020-06-02 09:21:56,2020-06-15 18:15:28,2020-06-15 18:15:28
https://github.com/tensorflow/federated/issues/860,['bug'],tf_computation returns garbage when used with `**` operator,"tf_computation returns garbage when used with `**` operator**Describe the bug**
```python
@tff.tf_computation(tf.int32) # or tf.float32
def pow10(n):
    return n ** 10

pow10(10) # returns garbage (different wrong values)
```
The code above does not throw any error, but quietly returns garbage
(even if `**` is not supported, it should throw an error)

**Environment (please complete the following information):**
* OS Platform and Distribution: windows 10
* Python package versions (e.g., TensorFlow Federated, TensorFlow):
tensorflow@2.2 tff@0.14
* Python version: 3.7
This seems to be integer overflow unrelated to TFF -- using `tf.int64` works as I would expect in this case.🤦‍♂️ sorry",2,2020-06-01 09:42:02,2020-06-03 09:08:19,2020-06-03 07:52:40
https://github.com/tensorflow/federated/issues/854,[],A problem of evaluation,"A problem of evaluationI'd like to use my own model and dataset to try TFF, when evaluating ,it taks a long time and I can't see the result. Is there any log file or something I can track the problem?
My dataset is the smart information of hard-drive.


![image](https://user-images.githubusercontent.com/65224044/82646930-f8dc0000-9c47-11ea-8dfa-842c71bfeeee.png)

It seems to me that this question might be best suited on StackOverflow, as it seems more like a usage question than a bug report or feature request. Would you mind posting there and tagging with tensorflow-federated?I have solved this problem.  When I run my code  by using "" python   ***.py"" ,it's ok",2,2020-05-22 08:42:25,2020-05-25 09:55:19,2020-05-25 09:55:19
https://github.com/tensorflow/federated/issues/851,['bug'],Performance drops significantly with 2 federated code,"Performance drops significantly with 2 federated codeHi there, 
I am using federated setting for my customize dataset. I try the same data, same model (load pretrained model). However, results are very different between two code as follow:
1. 41.9% accuracy with `https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb`
2. 17% accuracy with `https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/optimization/shakespeare/run_federated.py`
I was trying different configurations for each model and the accuracy reported here is the best for each model.
I wonder whether the way of aggregating and assigning weights between clients and server are different between these code that makes the performances different.  
Have anyone got the same issues and is there any reason ? ThanksHi @PhungLai728. Can you describe what hyperparameters you were using with the two approaches? The choice of hyperparameters can make a huge difference, especially the choice of learning rates.

As you note, the tutorial uses a pre-trained RNN. However, currently `shakespeare/run_federated.py` trains from scratch. Did you also use the pre-trained model in `shakespeare/run_federated.py`?

As for differences in the code, the tutorial uses `tff.learning.build_federated_averaging_process` while `shakespeare/run_federated.py` uses `https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/optimization/shared/fed_avg_schedule.py`. While there are a few subtle differences between them, the primary difference is that the latter allows for learning rate scheduling. In the absence of using that, the two should perform comparably, barring randomness due to which clients are sampled in each round.Hi there, 
For both, I used pretrained model. And I mostly changed the learning rate for each model and choose the best one. The learning rate I tried is [0.01, 0.05, 0.1] for both. I also try learning rate schedule for the `tutorials/federated_learning_for_text_generation.ipynb`. My dataset has 208 users, and I used 50 clients for each training step when using `run_federated.py`. 
Thanks for the follow-up. It's difficult to tell what's happening without seeing code snippets, but `shakespeare/run_federated.py` has a whole suite of flags with specific default values, so it could be that there's still some difference in configuration between the two.

That being said, `shakespeare/run_federated.py` was designed for a specific dataset and model. If you're finding success in using the tutorial's setup, I would encourage you to continue using that  to train your model.

Currently, I don't see any indication of a bug in the `shakespeare/run_federated.py`, but please let me know if you still believe there is one.Hi @PhungLai728. I'm closing the issue for now, but if you see evidence that there is a bug in `shakespeare/run_federated.py`, please feel free to reopen this issue.",4,2020-05-19 12:49:36,2020-05-26 18:41:32,2020-05-26 18:41:31
https://github.com/tensorflow/federated/issues/850,[],Cannot run the event loop while another loop is running,"Cannot run the event loop while another loop is runningHi,
  A problem appeared when I use TFF run this code:
  tff.federated_computation(lambda: 'Hello, World!')()
 The error  ""Cannot run the event loop while another loop is running""
I used Jupyter notebook, the version of tensorflow_federated is 0.13.1Have a look at this one: #842 Also you can take a look at https://github.com/tensorflow/federated/blob/master/docs/tutorials/README.mdYou can downgrade tornado to 4.5.3 to solve this problem. 
`pip install tornado==4.5.3`
more information: https://github.com/jupyter/notebook/issues/3397",3,2020-05-18 07:15:31,2020-05-28 01:44:19,2020-05-18 16:47:12
https://github.com/tensorflow/federated/issues/846,[],Possible bug in cifar100 data loading module,"Possible bug in cifar100 data loading modulehttps://github.com/tensorflow/federated/blob/269e70d9a0ecc5b73f38cab7403aae9676dbc9e2/tensorflow_federated/python/research/optimization/cifar100/dataset.py#L37

Given that 'crop_shape' is set as
`crop_shape = CIFAR_SHAPE = (32, 32, 3)`

The references line seems to crop the image to shape [32, 3] instead of [32, 32]. Perhaps the code should instead be

`image, target_height=crop_shape[0], target_width=crop_shape[1]) `
Hi Praneeth! Thanks for the close investigation. Further down, you can see that the batch size is put at the beginning of the train image shape: 

https://github.com/tensorflow/federated/blob/269e70d9a0ecc5b73f38cab7403aae9676dbc9e2/tensorflow_federated/python/research/optimization/cifar100/dataset.py#L77-L82

We also have some tests that make sure we have the right crop size: https://github.com/tensorflow/federated/blob/269e70d9a0ecc5b73f38cab7403aae9676dbc9e2/tensorflow_federated/python/research/optimization/cifar100/dataset_test.py#L38-L46

We appreciate extra eyes on this code, so please let us know if you do think there's any other issues.",1,2020-05-13 12:41:43,2020-05-14 16:04:43,2020-05-14 16:04:43
https://github.com/tensorflow/federated/issues/844,['bug'],protobuf issues: computation_pb2 and protobuf version issues,"protobuf issues: computation_pb2 and protobuf version issues**Describe the bug**


1. I installed protobuf (protoc) on ubuntu and compiled computation.proto file which created computation_pb2.py. The location of computation_pb2.py file is same as computation.proto file which is tensorflow_federated/proto/v0

2. when I run this command:

from tensorflow_federated.python.core.api.computations import federated_computation

I get this error and trace:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/federated/tensorflow_federated/__init__.py"", line 21, in <module>
    from tensorflow_federated.python.core.api.computation_base import Computation
  File ""/home/ubuntu/federated/tensorflow_federated/python/core/__init__.py"", line 17, in <module>
    from tensorflow_federated.python.core.api.computation_base import Computation
  File ""/home/ubuntu/federated/tensorflow_federated/python/core/api/__init__.py"", line 25, in <module>
    from tensorflow_federated.python.core.api.computations import federated_computation
  File ""/home/ubuntu/federated/tensorflow_federated/python/core/api/computations.py"", line 17, in <module>
    from tensorflow_federated.python.core.impl.wrappers import computation_wrapper_instances
  File ""/home/ubuntu/federated/tensorflow_federated/python/core/impl/wrappers/computation_wrapper_instances.py"", line 18, in <module>
    from tensorflow_federated.python.core.impl import computation_impl
  File ""/home/ubuntu/federated/tensorflow_federated/python/core/impl/computation_impl.py"", line 17, in <module>
    from tensorflow_federated.proto.v0 import computation_pb2 as pb
  File ""/home/ubuntu/federated/tensorflow_federated/proto/v0/computation_pb2.py"", line 22, in <module>
    create_key=_descriptor._internal_create_key,
AttributeError: module 'google.protobuf.descriptor' has no attribute '_internal_create_key'


Just to note, I followed the install instructions and tff is installed correctly as I can run this command successfully:

python -c ""import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())""

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4
* Python package versions (e.g., TensorFlow Federated, TensorFlow): tensorflow==2.1.0, tensorflow-federated==0.13.1
* Python version: 3.6.9
* Bazel version (if building from source): 3.1.0
* CUDA/cuDNN version: Not using (CPU machine only)
* What TensorFlow Federated execution stack are you using? I am running code under research repository so I am not sure how to answer this.


**Expected behavior**
Since proto file was compiled into .py file Import should just work

**Additional context**
protoc version: libprotoc 3.11.4
computation_pb2.py is present in the dir tensorflow_federated/proto/v0/
I manually compiled .proto file using protoc but it was not mentioned in the install readme. 
Perhaps there is some version mismatch with protobuf. Thanks for the report. Sounds like there maybe some holes in the documentation.

It will help debugging if we can break up Steps 1 and 2 into the full set of commands that others can copy to reproduce locally. A few questions:

In **Step 1**: Does ""compiled computation.proto"" means directly invoking the `protoc` compiler on `tensorflow_federated/proto/v0/computation.proto`, or does it mean building the proto using `bazel build tensorflow_federated/proto/v0/:executor_py_pb2`?

In **Step 2**: Where is the command run (python interpreter shell, python script, jupyter notebook)? If the later two, are they creating using bazel build rules? What is the current working directory?

**Side question**: could you help us understand the intent for directly interacting with the protocol buffer message definition? In general TFF treats that as an internal implementation detail. Understanding the use case can better inform the API design.

Hello,

In Step 1: Does ""compiled computation.proto"" means directly invoking the protoc compiler on tensorflow_federated/proto/v0/computation.proto, or does it mean building the proto using bazel build tensorflow_federated/proto/v0/:executor_py_pb2?

Ans: I used protoc compiler directly.

In Step 2: Where is the command run (python interpreter shell, python script, jupyter notebook)? If the later two, are they creating using bazel build rules? What is the current working directory?

Ans: I am using python interpreter shell. Current working directory is federated root directory.

Side question: could you help us understand the intent for directly interacting with the protocol buffer message definition? In general TFF treats that as an internal implementation detail. Understanding the use case can better inform the API design.

Ans: I was trying to run the code under python/research/target_attack/attacked_fedavg.py and there is this import command which fails:

from tensorflow_federated.python.research.targeted_attack.aggregate_fn import build_stateless_mean

I digged further on why it fails and that lead me to interact with protocol buffer. So API design is okay, I dont have a need to interact with protocol buffer.

Hmm, and think there maybe a few pieces being conflated here.

Running from an interpreter in the root directory of the repo can cause issues. This will cause Python to searching for `tensorflow_federated` in the directory with the same name in the repo. If the code is expecting to import the pip package (which I believe the targeted_attack code is) this will fail. This is likely why its complaining that the proto message import is missing, normally this file is created during the bazel build.

TFF generally requires using bazel to build/run the Python scripts, rather than calling the Python interpreter on the scripts directly (this may work some times, but is unsupported). Bazel creates an environment where all the necessary imports are available.

What commandline was used to run the code under `python/research/target_attack/attacked_fedavg.py`?I was trying to bypass bazel build - was not clear to me that it is required.

I tried running attacked_fedavg_test.py (again not clear to me how to run the files in the research repository)

root directory: federated/

python3 tensorflow_federated/python/research/target_attack/attacked_fedavg_test.py

I ran into import issue: 
 computation_pb2 file is not found for the proto file.

That led me to search how to compile protobuf files and I ended up using protoc to do that. However, doing this lead to some probably protobuf version issue.

I am following the installation guide more methodically (using bazel). Will update the issue shortly.I used bazel to build (all 10 steps ran okay).

However not clear to me how can I run the code in research repo:

tensorflow_federated/python/research/target_attack/attacked_fedavg_test.py

Thanks for the follow-up. Sounds like the TFF documentation could be better to more explicitly call out that bazel is required, perhaps both in the getting started and perhaps in the README.md inside the `research/` directory.

To run the test, could you try:

```shell
bazel test tensorflow_federated/python/research/target_attack:attacked_fedavg_test
```

If may simply only print whether it ""passed"" or ""failed"". To get more info, add the flag [`--test_output=all`](https://docs.bazel.build/versions/master/command-line-reference.html#flag--test_output)

when I run the command

bazel test tensorflow_federated/python/research/target_attack:attacked_fedavg_test

I get following error:

ERROR: Skipping 'tensorflow_federated/python/research/target_attack:attacked_fedavg_test': no such package 'tensorflow_federated/python/research/target_attack': BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.
 - /home/ubuntu/federated/tensorflow_federated/python/research/target_attack
ERROR: no such package 'tensorflow_federated/python/research/target_attack': BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.

BUILD file is present in the folder python/research/targeted_attack. 

Questions:

1. Is this happening due to incorrect install/setup on my machine. If yes, please feel free to close the issue.

2. If not, can you suggest why this is happening?
I am able to run the test case. There was a typo in the name.

Closing the issue.",8,2020-05-11 20:25:27,2020-05-12 21:00:17,2020-05-12 21:00:16
https://github.com/tensorflow/federated/issues/842,['bug'],RuntimeError: Cannot run the event loop while another loop is running,"RuntimeError: Cannot run the event loop while another loop is running**Describe the bug**
Not able to run the tutorial on local jupyter notebook. However, the same code runs on python shell.

Error Log:
```---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-1-9351b5094f96> in <module>
      9 np.random.seed(0)
     10 
---> 11 tff.federated_computation(lambda: 'Hello, World!')()

~/Learning/tf-federated/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in __call__(self, *args, **kwargs)
    559     context = self._context_stack.current
    560     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 561     return context.invoke(self, arg)
    562 
    563 

~/Learning/tf-federated/venv/lib/python3.7/site-packages/retrying.py in wrapped_f(*args, **kw)
     47             @six.wraps(f)
     48             def wrapped_f(*args, **kw):
---> 49                 return Retrying(*dargs, **dkw).call(f, *args, **kw)
     50 
     51             return wrapped_f

~/Learning/tf-federated/venv/lib/python3.7/site-packages/retrying.py in call(self, fn, *args, **kwargs)
    204 
    205             if not self.should_reject(attempt):
--> 206                 return attempt.get(self._wrap_exception)
    207 
    208             delay_since_first_attempt_ms = int(round(time.time() * 1000)) - start_time

~/Learning/tf-federated/venv/lib/python3.7/site-packages/retrying.py in get(self, wrap_exception)
    245                 raise RetryError(self)
    246             else:
--> 247                 six.reraise(self.value[0], self.value[1], self.value[2])
    248         else:
    249             return self.value

~/Learning/tf-federated/venv/lib/python3.7/site-packages/six.py in reraise(tp, value, tb)
    701             if value.__traceback__ is not tb:
    702                 raise value.with_traceback(tb)
--> 703             raise value
    704         finally:
    705             value = None

~/Learning/tf-federated/venv/lib/python3.7/site-packages/retrying.py in call(self, fn, *args, **kwargs)
    198         while True:
    199             try:
--> 200                 attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
    201             except:
    202                 tb = sys.exc_info()

~/Learning/tf-federated/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/execution_context.py in invoke(self, comp, arg)
    201         return event_loop.run_until_complete(
    202             tracing.run_coroutine_in_ambient_trace_context(
--> 203                 _invoke(executor, comp, arg)))

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py in run_until_complete(self, future)
    569         future.add_done_callback(_run_until_complete_cb)
    570         try:
--> 571             self.run_forever()
    572         except:
    573             if new_task and future.done() and not future.cancelled():

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py in run_forever(self)
    527         if events._get_running_loop() is not None:
    528             raise RuntimeError(
--> 529                 'Cannot run the event loop while another loop is running')
    530         self._set_coroutine_origin_tracking(self._debug)
    531         self._thread_id = threading.get_ident()

RuntimeError: Cannot run the event loop while another loop is running
```

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15.3 
* Python package versions (e.g., TensorFlow Federated, TensorFlow): TFF - 0.13.1, TF - 2.1.0
* Python version: 3.7.3
* Bazel version (if building from source):
* CUDA/cuDNN version: NA
* What TensorFlow Federated execution stack are you using?

Note: You can collect the Python package information by running `pip3 freeze`
from the command line and most of the other information can be collected using
TensorFlows environment capture
[script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).

**Expected behavior**
A clear and concise description of what you expected to happen.
```>>> import collections
>>> 
>>> import numpy as np
>>> import tensorflow as tf
>>> import tensorflow_federated as tff
>>> 
>>> tf.compat.v1.enable_v2_behavior()
>>> 
>>> np.random.seed(0)
>>> 
>>> tff.federated_computation(lambda: 'Hello, World!')()
2020-05-04 11:40:33.337101: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-04 11:40:33.356723: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8047f54b70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-04 11:40:33.356741: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
b'Hello, World!'```

**Additional context**
Add any other context about the problem here.
It is a jupyter issue not tff issue. Fix: I added the following 

```
!pip install nest_asyncio
import nest_asyncio
nest_asyncio.apply()
```
Ref: https://markhneedham.com/blog/2019/05/10/jupyter-runtimeerror-this-event-loop-is-already-running/Thats correct, the TFF Notebooks work http://colab.research.google.com/, but running in a local Jupyter runtime requires installing the `nest_asyncio` package.",2,2020-05-04 06:17:52,2020-05-11 20:39:38,2020-05-11 20:39:38
https://github.com/tensorflow/federated/issues/837,['bug'],User-level DP with emnist and stackoverflow not running!,"User-level DP with emnist and stackoverflow not running!I'd like to reproduce the results of User-level DP in the Learning Differentially Private Recurrent Language Models paper [H. Brendan McMahan et al. ICLR 2018]. But the Reddit dataset is not publicly available. So I try with emnist and stackoverflow that are provided in the implementation.

But I am not able to run the model with these datasets. (I am using the latest version tff 0.13.1 &	tensorflow 2.1.0).
1. With stackoverflow, I try to run with `bazel run //tensorflow_federated/..../stackoverflow:run_federated`. But after downloading the data, it stucks there for a while, then it shows an error `RuntimeError: generator raised StopIteration`. 
+ I am not sure how to make it run. Could you please help?
+ How can I adapt the code with other datasets and models (not dataset provided in the code) ? Is there a function to preprocess the data so that we obtain the same format with dataset in `simulation` ?

2. With emnist, I try to run directly with Python, and it 'seems to work'. But when it runs to `training_loop`, when it called `py_typecheck`, it raises an error `TypeError: Expected anonymous_tuple.AnonymousTuple, found tensorflow_federated.python.common_libs.anonymous_tuple.AnonymousTuple.`
+ How can I pass this error? 

3. I searched around and found this github `https://github.com/TalwalkarLab/leaf`. Is the Reddit data here as same as Reddit data used in the [H. Brendan McMahan et al. ICLR 2018] ? 

Thanks!Could you try the following for EMNIST/Stackoverflow for sanity check?

bazel run run_federated -- --clients_per_round 2 --uniform_weighting --noise_multiplier 0.1  --total_rounds 2 --client_optimizer sgd --client_learning_rate 0.1 --server_optimizer sgd --server_learning_rate 1.0 --root_output_dir /tmp/dp-opt/emnist --experiment_name debugI am able to make it run. Thanks.",2,2020-04-28 03:12:52,2020-05-04 03:14:42,2020-05-04 03:14:42
https://github.com/tensorflow/federated/issues/832,['bug'],Code doesn't run on GPU,"Code doesn't run on GPU**Describe the bug**

I am building my own project with tensorflow federated learning API.
When running my code, the GPU is visible (as shown below), but the computation of federated learining was not done on GPU.

**System output to show that the gpu is indeed added**
```
2020-04-20 21:36:49.347491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-04-20 21:36:49.347853: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-20 21:36:49.355351: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3499685000 Hz
2020-04-20 21:36:49.356094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55957a6a65e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-20 21:36:49.356124: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-04-20 21:36:49.421125: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55957a6c9c90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-04-20 21:36:49.421170: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX TITAN X, Compute Capability 5.2
2020-04-20 21:36:49.422796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX TITAN X computeCapability: 5.2
coreClock: 1.076GHz coreCount: 24 deviceMemorySize: 11.93GiB deviceMemoryBandwidth: 313.37GiB/s
2020-04-20 21:36:49.422898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-20 21:36:49.422959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-20 21:36:49.423015: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-04-20 21:36:49.423068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-04-20 21:36:49.423182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-04-20 21:36:49.423287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-04-20 21:36:49.423351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-20 21:36:49.427345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-04-20 21:36:49.427454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-20 21:36:49.430516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-20 21:36:49.430573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-04-20 21:36:49.430624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-04-20 21:36:49.434605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11498 MB memory) -> physical GPU (device: 0, name:
 GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
```

**Environment (please complete the following information):**
* Linux Ubuntu 16.04
* Python package versions

When running `pip freeze | grep tensorflow`, I got:
```
tensorflow==2.1.0
tensorflow-addons==0.8.3
tensorflow-estimator==2.1.0
tensorflow-federated==0.13.1
tensorflow-model-optimization==0.2.1
tensorflow-privacy==0.2.2
```
When running `pip3 freeze | grep tensorflow`, I got:
```
tensorflow==2.1.0
tensorflow-estimator==2.1.0
tensorflow-gpu==2.1.0
```
* Python version: Python3.7

* CUDA/cuDNN version:
CUDA Version 10.2.89

**Expected behavior**
Here is my code to perform federated training:
```
    federated_train_data = [dataset, dataset]
    trainer = tff.learning.build_federated_averaging_process(model_fn,\
            client_optimizer_fn=lambda: keras.optimizers.Adam(lr=args.lr, clipnorm=0.001),
            server_optimizer_fn=lambda: keras.optimizers.Adam(lr=args.lr, clipnorm=0.001))

    state = trainer.initialize()
    for i in range(1000):
        state, metrics = trainer.next(state, federated_train_data)
        print('round  {}, metrics={}'.format(str(i), metrics))
```
The training can be done successfully, but not on GPU. Training speed is very slow.

**Other information**
I install the environment following the instruction, by typing 'pip install --upgrade tensorflow_federated'.

An interesting thing is that when I use 'fit()' to perform model training shown above rather than using federated learning, the computation can be done on GPU.

Many thanks for your help!
When I run the demo code for image classification: https://paste.ubuntu.com/p/sPtqngFPWP/
the GPU is also successfully loaded, but the federated training still cannot be done on GPU.

I add the following at the beginning of demo code.
`print(""Num GPUs Available: "", len(
    tf.config.experimental.list_physical_devices('GPU')))` 

Here is what I got:
```
2020-04-21 20:42:04.618447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
Num GPUs Available:  1
2020-04-21 20:42:04.657577: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-04-21 20:42:04.695003: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2020-04-21 20:42:04.700067: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa46034f000 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-21 20:42:04.700136: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-04-21 20:42:04.738584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1b:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2020-04-21 20:42:04.738835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-21 20:42:04.738929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-21 20:42:04.739001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-04-21 20:42:04.739063: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-04-21 20:42:04.739123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-04-21 20:42:04.739184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-04-21 20:42:04.739241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-21 20:42:04.858617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-04-21 20:42:04.858820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-21 20:42:07.206119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-21 20:42:07.206172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-04-21 20:42:07.206183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-04-21 20:42:07.508719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11162 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:1b:00.0, compute capability: 7.0)
2020-04-21 20:42:07.515198: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa460e1ecb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-04-21 20:42:07.515249: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN V, Compute Capability 7.0
Num GPUs Available:  1
```
The output above should indicate that the GPU was available. But the training was not performed on GPU, and the GPU util rate is always 0%.
![image](https://user-images.githubusercontent.com/29408526/79868306-50513b00-8412-11ea-8b74-2b43fcc4fd02.png)

Any idea about this problem?   Many thanks and appreciate！@liuquande Thanks for the detailed description. tff.learning.federated_averaging currently uses features from TF that will default to CPUs and it is expected to be fixed in the next TF version. 

For now, to have a better utility of GPUs, my suggestion would be to look at the simple_fedavg (https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/simple_fedavg) implementation to see if you can use the federated averaging algorithms there. It should only require very small changes to your code. 

Also note that multi-GPU support is under development. We do not have a solution for now.Hi @nightldj, thanks for the suggestion.

As suggested, I try to run the simple_fedavg demo to see the GPU util rate. ((https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/research/simple_fedavg)))

When running the code (without any change) using bazel :
`
bazel run tensorflow_federated/python/research/simple_fedavg:emnist_fedavg_main
`

I got the following error:
```
iceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>, <TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types
: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>, <TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>, <TensorSliceD
ataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>, <TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: Or
deredDict([(label, tf.int32), (pixels, tf.float32)])>] with type list

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/data/home/qdliu/.cache/bazel/_bazel_qdliu/0f8dd53c2bb702a191b3aefae090b80a/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.run
files/org_tensorflow_federated/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.py"", line 181, in <module>
    app.run(main)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/data/home/qdliu/.cache/bazel/_bazel_qdliu/0f8dd53c2bb702a191b3aefae090b80a/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.run
files/org_tensorflow_federated/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.py"", line 146, in main
    train_data, test_data = get_emnist_dataset()
  File ""/data/home/qdliu/.cache/bazel/_bazel_qdliu/0f8dd53c2bb702a191b3aefae090b80a/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.run
files/org_tensorflow_federated/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.py"", line 79, in get_emnist_dataset
    emnist_test.create_tf_dataset_from_all_clients())
  File ""/data/home/qdliu/.cache/bazel/_bazel_qdliu/0f8dd53c2bb702a191b3aefae090b80a/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.run
files/org_tensorflow_federated/tensorflow_federated/python/simulation/client_data.py"", line 112, in create_tf_dataset_from_all_clients
    nested_dataset = tf.data.Dataset.from_tensor_slices(client_datasets)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 644, in from_tensor_slices
    return TensorSliceDataset(tensors)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 2784, in __init__
    element = structure.normalize_element(element)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py"", line 98, in normalize_element
    ops.convert_to_tensor(t, name=""component_%d"" % i))
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1314, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 317, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 258, in constant
    allow_broadcast=True)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 266, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Attempt to convert a value (<TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>) with an unsupported type (<class
 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>) to a Tensor.
```
To avoid this error, I try to build the validation data with the code from image classification tutorial like follows:
```
def preprocess(dataset):

  def batch_format_fn(element):
    """"""Flatten a batch `pixels` and return the features as an `OrderedDict`.""""""
    return collections.OrderedDict(
        x=tf.expand_dims(element['pixels'], -1), y=element['label'])

  return dataset.repeat(50).batch(20).map(batch_format_fn).prefetch(20)

def get_emnist_dataset():
  """"""Loads and preprocesses the EMNIST dataset.

  Returns:
    A `(emnist_train, emnist_test)` tuple where `emnist_train` is a
    `tff.simulation.ClientData` object representing the training data and
    `emnist_test` is a single `tf.data.Dataset` representing the test data of
    all clients.
  """"""
  emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data(
      only_digits=True)

  def element_fn(element):
    return collections.OrderedDict(
        x=tf.expand_dims(element['pixels'], -1), y=element['label'])

  def preprocess_train_dataset(dataset):
    # Use buffer_size same as the maximum client dataset size,
    # 418 for Federated EMNIST
    return dataset.map(element_fn).shuffle(buffer_size=418).repeat(
        count=FLAGS.client_epochs_per_round).batch(
            FLAGS.batch_size, drop_remainder=False)

  def preprocess_test_dataset(dataset):
    return dataset.batch(FLAGS.test_batch_size, 
      drop_remainder=False).map(element_fn)

  emnist_test = preprocess(emnist_train.create_tf_dataset_for_client(
    emnist_train.client_ids[0]))

  emnist_train = emnist_train.preprocess(preprocess_train_dataset)

  return emnist_train, emnist_test
```

Then I get another error: 
```
Traceback (most recent call last):
  File ""/data/home/qdliu/.cache/bazel/_bazel_qdliu/0f8dd53c2bb702a191b3aefae090b80a/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.runfiles/org_tensorflow_federated/tensorflow_federated/python/common_libs/tracing.py"", line 337, in span
    span_gen.send(TracedSpan())
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/data/home/qdliu/.cache/bazel/_bazel_qdliu/0f8dd53c2bb702a191b3aefae090b80a/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.runfiles/org_tensorflow_federated/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.py"", line 183, in <module>
    app.run(main)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/data/home/qdliu/.cache/bazel/_bazel_qdliu/0f8dd53c2bb702a191b3aefae090b80a/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.runfiles/org_tensorflow_federated/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.py"", line 159, in main
    server_state = iterative_process.initialize()
  File ""/data/home/qdliu/.cache/bazel/_bazel_qdliu/0f8dd53c2bb702a191b3aefae090b80a/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/utils/function_utils.py"", line 563, in __call__
    return context.invoke(self, arg)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/retrying.py"", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/retrying.py"", line 206, in call
    return attempt.get(self._wrap_exception)
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/retrying.py"", line 247, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File ""/data/home/qdliu/.cache/bazel/_bazel_qdliu/0f8dd53c2bb702a191b3aefae090b80a/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.runfiles/six/__init__.py"", line 693, in reraise
    raise value
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/site-packages/retrying.py"", line 200, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File ""/data/home/qdliu/.cache/bazel/_bazel_qdliu/0f8dd53c2bb702a191b3aefae090b80a/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/simple_fedavg/emnist_fedavg_main.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 203, in invoke
    _invoke(executor, comp, arg)))
  File ""/home/qdliu/anaconda/envs/tff0.13/lib/python3.7/contextlib.py"", line 119, in __exit__
    next(self.gen)
RuntimeError: generator raised StopIteration

```

Could you please give me some more advices?
Thank you very much for your time and help!> @liuquande Thanks for the detailed description. tff.learning.federated_averaging currently uses features from TF that will default to CPUs and it is expected to be fixed in the next TF version.

Could you provide a link to the issue in the TF repo?
I'd like to look for their fix and maybe compile TF myself in order to use it sooner rather than later.
@pepper-jk I am not pretty sure how to map internal report to public issue, but TF code after Apr 16 should have fixed the problem. You can compile any TF version after Apr 16.  

@liuquande simple_fedavg runs OK on my side. My guess is that you are running the most recent version of simple_fedavg with the pip installed TFF. Could you try run an older version of simple_fedavg, like the following one that has the v0.13.1 tag on it? https://github.com/tensorflow/federated/commit/464c2758a03a70dfe36a7df1919840a79983a7aa#diff-37f984101ae57f942ed47cb6102d4190
Or compile the most recent TFF?

 @nightldj
So I build tensorflow master at [0493a0](https://github.com/tensorflow/tensorflow/commit/0493a020d48081b6f2afb53a1ac9d45b2861c03d) (pulled yesterday) and there is no significant improvement in performance.

Do I also need to build tf-federated from source for this?

It seems to use the GPU but only every few seconds and only at 70%.

Please note that the experiment from 2 days ago is also still running and today I also noticed that it utilizes the GPU from time to time. So the actual GPU usage per instance is more around 35%.

@liuquande Have you tried `watch -n 1 nvidia-smi` to monitor the GPU usage?@pepper-jk 
Yes, I was monitoring the GPU usage all the time when running the demo code, but my usage is 0% (sometime could be 6% or 7%).
Did you run the demo code or your own code? I mean the program that can successfully use GPU for 70%.It's our own code.

Just wanted to make sure you did not miss the GPU usage, because I did at first with just manually using `nvidia-smi`.@pepper-jk 
Thanks for your reminding.
What's your GPU usage before installing tensorflow master.
Could you please try the image classification demo here ""https://paste.ubuntu.com/p/sPtqngFPWP/"",
and have a look at the GPU usage.

When running my own code, sometimes I can also observe a higher usage such as 30%, but in most time, the usage is 0%.Sadly I can not. Our GPU memory is full at the time with since our experiments are currently running anyway. We need the results and in case it turns out the GPU can't be utilized completely. So we need to keep it running like this.

However we only changed the [tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) code very slightly, for example to use tensorflow-privacy as well.
So I doubt we have a much different setup then you do.
What we probably do differently is that we use the full dataset.

For more information on our dataset setup look in this [issue](https://github.com/tensorflow/federated/issues/833).

> When running my own code, sometimes I can also observe a higher usage such as 30%, but in most time, the usage is 0%.

Yes, like I said we at most get 35% from one instance (we have two running, please see above).@pepper-jk 
Many thanks for your informaiton.
Did you think installing the up-to-date tensorflow master improve your GPU usage?From what I can see no.
However maybe @nightldj will have some more information.

I'm currently compiling the master of today, seeing if this will change anything, but I doubt it.

Any luck with the ""simple avg"" function, that was named as an alternative?Sorry what do you mean by:
>Any luck with the ""simple avg"" function, that was named as an alternative?


I can now successfully run the 'simple_ave' demo, but also the GPU usage is low.
Sometime it could be 20% but in most time it is 0%.
You basically answered my question.
I was wondering if the performance improved.@pepper-jk It is a bit tricky to use the local TF. Sorry if I confused you at the beginning. You probably have to build TFF following the instructions here https://www.tensorflow.org/federated/install#build_the_tensorflow_federated_pip_package. Also, you have to make sure TF is installed in your local python environment, or build a pip package to be installed in the TFF environment. 

@liuquande simple_fedavg has a relatively small workload which may not saturate GPU utility. I am not sure how to correctly measure here, but there are a few  things you can do to check if GPU is used: (1) compare GPU and CPU to verify speedup (2) compare with pure TF on GPU (3) You can make the workload much larger by using a deeper model in simple_fedavg. > @pepper-jk It is a bit tricky to use the local TF. Sorry if I confused you at the beginning. You probably have to build TFF following the instructions here https://www.tensorflow.org/federated/install#build_the_tensorflow_federated_pip_package. Also, you have to make sure TF is installed in your local python environment, or build a pip package to be installed in the TFF environment.

I have build TF and installed it as a pip package in my venv. I did not have any problems with TFF detecting the package, since it is labeled as version 2.1.

For building I used [these instructions](https://www.tensorflow.org/install/source). I enabled CUDA and TensorRT and build the pip package labeled as stable.

What I was asking is, if the fix you referred to was perhaps made in TFF - not TF like you said. And if this is the reason why I did not see any improvements?

If so I will gladly also build TFF from scratch.@pepper-jk Thanks for verifying the TF installation. The fix is from TF, not TFF. Do you mean the TF version in TFF after your local TF installation is 2.1 can confirm that the correct TF is used? I am not the expert on installation stuff. If it is not too much trouble, would you mind building TFF following https://www.tensorflow.org/federated/install#build_the_tensorflow_federated_pip_package? You can stop after step #5 if you only want to test and donot want to build a pip package. 

Would you also share some evidence on 'did not see any improvements'? Sorry if you have mentioned before. 

And thanks for exploring the usage of TFF.> Would you also share some evidence on 'did not see any improvements'? Sorry if you have mentioned before.

Here you go from my earlier post:

> It seems to use the GPU but only every few seconds and only at 70%.
> 
> Please note that the experiment from 2 days ago is also still running and today I also noticed that it utilizes the GPU from time to time. So the actual GPU usage per instance is more around 35%

> If it is not too much trouble, would you mind building TFF following https://www.tensorflow.org/federated/install#build_the_tensorflow_federated_pip_package?

I can do that. But what exactly is the goal with this? Just running the tests with my locally build TF package?@pepper-jk I guess I just want to double check that the fixed TF is used in TFF. In my humble opinion, GPU utility depends on a lot of factors. And I would look for more evidence: (1) compare GPU and CPU to verify speedup (2) compare with pure TF on GPU (3) Sanity check on model training workload and data pipeline.@nightldj does it mean that if we install TFF via pip, it is possible to train model on GPU? is training on the GPU is default now?@aqibsaeed The default of TFF is similar to TF: if GPU is provided, TFF/TF will try to use it. But TFF on GPUs is under development and have not been broadly tested. We have been working on the performance and made some progresses, but we are not at a point to confidently advertise GPU usage for now. > @aqibsaeed The default of TFF is similar to TF: if GPU is provided, TFF/TF will try to use it. But TFF on GPUs is under development and have not been broadly tested. We have been working on the performance and made some progresses, but we are not at a point to confidently advertise GPU usage for now.

So any update? When can we expect these improvements?
Is there something we could contribute to speed up the process? Maybe you want to open some more detailed issues on this topic that specifies required implementations to solve this?

EDIT: Also does it matter what kind of GPU we use? Nvidia or AMD? CUDA or ROCm? So far I only tried CUDA.> @pepper-jk I guess I just want to double check that the fixed TF is used in TFF. In my humble opinion, GPU utility depends on a lot of factors. And I would look for more evidence: (1) compare GPU and CPU to verify speedup (2) compare with pure TF on GPU (3) Sanity check on model training workload and data pipeline.

@nightldj You might be right about the tests though:

```
Executed 115 out of 136 tests: 35 tests pass and 101 fail locally.
```

[full log](https://pastebin.com/SNfpBhtH)

I switched to tf-nightly for a clean install of up to date tf instead of building it myself. And I pulled the most current tff to run the tests.
I had similar results with my personal tf build and ""older"" versions of tff about a week ago.

So any advice to fix this?
Is the GPU usage related to these failed tests?@pepper-jk We do know some GPU incompatible issues, and I have workaround committed in https://github.com/tensorflow/federated/commit/aec461ba9fee36a05704748041af2f94cc5e31c0. All TFF tests should run without error on GPU after the fix. 

All my GPU tests are on P100/V100 with CUDA. At the head of TFF and TF, I have found one GPU works fine except for the known problem I tagged in test.  Multi-GPU is under development and we have to work with TF on that. > @pepper-jk We do know some GPU incompatible issues

Please open some new issues here on github with more detailed information, so we might contribute to the solution. Or point to existing ones, if I missed them.

> and I have workaround committed in [aec461b](https://github.com/tensorflow/federated/commit/aec461ba9fee36a05704748041af2f94cc5e31c0).

This is merely a **workaround** for testing not the issues itself.

> All TFF tests should run without error on GPU after the fix.

Okay, thanks for the information. I will try to get all the tests running on my system before further testing.

> All my GPU tests are on P100/V100 with CUDA. At the head of TFF and TF, I have found one GPU works fine except for the known problem I tagged in test. Multi-GPU is under development and we have to work with TF on that.

We are not asking for multi GPU support. From what I understand we are all using single GPU setups. We are simple stating that the GPU is barely being utilized.I've got more information on the failing tests.

<details><summary>I looked into some of the logs in .cache and found this error repeatedly:</summary>
<p>

```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow_federated/python/core/impl/executors:sizing_executor_test
-----------------------------------------------------------------------------
2020-05-12 09:19:11.112671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so
.10.1
Traceback (most recent call last):
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/721/execroot/org_tensorflow_fed
erated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflo
w_federated/python/core/impl/executors/sizing_executor_test.py"", line 21, in <module>
    import tensorflow as tf
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 53, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py"", line 7, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/google/protobuf/__init__.py"", line 37, in <module>
    __import__('pkg_resources').declare_namespace(__name__)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2927, in <module>
    @_call_aside
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2913, in _call_aside
    f(*args, **kwargs)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2952, in _initialize_master_working_set
    add_activation_listener(lambda dist: dist.activate())
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 956, in subscribe
    callback(dist)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2952, in <lambda>
    add_activation_listener(lambda dist: dist.activate())
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2515, in activate
    declare_namespace(pkg)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2097, in declare_namespace
    _handle_ns(packageName, path_item)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2047, in _handle_ns
    _rebuild_mod_path(path, packageName, module)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2066, in _rebuild_mod_path
    orig_path.sort(key=position_in_sys_path)
AttributeError: '_NamespacePath' object has no attribute 'sort'
```

</p>
</details>

I **fixed** it with: `pip install --upgrade pip setuptools`

I'd suggest adding the upgrade of `setuptools`to your installation instructions.

At least to the self building instructions.

Before:
```Executed 116 out of 136 tests: 21 tests pass and 115 fail locally.```
After:
```Executed 115 out of 136 tests: 48 tests pass and 88 fail locally.```

The other tests seem to be failing since the GPU is out of memory.
Once the server has less load, I'll run the tests again to confirm this.

[full log](https://pastebin.com/j3hv9dZ9)

<details><summary>example error log for OOM:</summary>
<p>

```
[  FAILED  ] SizingExecutorTest.test_executor_stacks_big_caching_stack
======================================================================
ERROR: test_executor_stacks_big_caching_stack (__main__.SizingExecutorTest)
test_executor_stacks_big_caching_stack (__main__.SizingExecutorTest)
test_executor_stacks_big_caching_stack([<class 'tensorflow_federated.python.core.impl.executors.sizing_executor.SizingExecutor'>, <class 'tensorflow_federated.python.core.impl.executors.caching_executor.CachingExecutor'>, <class 'tensorflow_federated.python.core.impl.executors.reference_resolving_executor.ReferenceResolvingExecutor'>, <class 'tensorflow_federated.python.core.impl.executors.caching_executor.CachingExecutor'>, <class 'tensorflow_federated.python.core.impl.executors.caching_executor.CachingExecutor'>, <class 'tensorflow_federated.python.core.impl.executors.reference_resolving_executor.ReferenceResolvingExecutor'>, <class 'tensorflow_federated.python.core.impl.executors.caching_executor.CachingExecutor'>])
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/absl/testing/parameterized.py"", line 265, in bound_param_test
    test_method(self, *testcase_params)
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/sizing_executor_test.py"", line 238, in test_executor_stacks
    asyncio.get_event_loop().run_until_complete(_make())
  File ""/usr/local/lib/python3.6/asyncio/base_events.py"", line 467, in run_until_complete
    return future.result()
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/sizing_executor_test.py"", line 235, in _make
    v1 = await ex.create_value(outer_type_val, outer_type)
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/sizing_executor.py"", line 107, in create_value
    target_val = await self._target.create_value(value, type_spec)
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/caching_executor.py"", line 251, in create_value
    raise e
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/caching_executor.py"", line 243, in create_value
    await cached_value.target_future
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 283, in create_value
    *[self.create_value(val, t) for (_, val), t in zip(v_el, type_spec)])
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 283, in create_value
    *[self.create_value(val, t) for (_, val), t in zip(v_el, type_spec)])
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 290, in create_value
    value, type_spec))
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/caching_executor.py"", line 251, in create_value
    raise e
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/caching_executor.py"", line 243, in create_value
    await cached_value.target_future
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/caching_executor.py"", line 251, in create_value
    raise e
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/caching_executor.py"", line 243, in create_value
    await cached_value.target_future
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 290, in create_value
    value, type_spec))
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/caching_executor.py"", line 251, in create_value
    raise e
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/caching_executor.py"", line 243, in create_value
    await cached_value.target_future
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 406, in create_value
    return EagerValue(value, self._tf_function_cache, type_spec, self._device)
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 309, in __init__
    type_spec, device)
  File ""/home/user/.cache/bazel/_bazel_user/a5689464ec704691e1f2141aa12ce976/sandbox/linux-sandbox/929/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/core/impl/executors/sizing_executor_test.runfiles/org_tensorflow_federated/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 262, in to_representation_for_type
    value = tf.convert_to_tensor(value, dtype=type_spec.dtype)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1432, in convert_to_tensor_v2
    as_ref=False)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1490, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 328, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 263, in constant
    allow_broadcast=True)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 271, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 96, in convert_to_eager_tensor
    ctx.ensure_initialized()
  File ""/home/user/code/venv_clean/lib/python3.6/site-packages/tensorflow/python/eager/context.py"", line 533, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory

----------------------------------------------------------------------
Ran 1 test in 20.471s

FAILED (errors=1)
```

</p>
</details>

**EDIT:** still getting the same issue with a completely free GPU and 12GB of graphic memory.

current state:
```
Executed 88 out of 136 tests: 63 tests pass and 73 fail locally.
```

It turns out, that tensorflow-federated does all its tests in parallel, thus consuming all the graphic memory (12GB in my case) and then some of the tests fail, since the GPU is OOM.> @pepper-jk We do know some GPU incompatible issues, and I have workaround committed in [aec461b](https://github.com/tensorflow/federated/commit/aec461ba9fee36a05704748041af2f94cc5e31c0). All TFF tests should run without error on GPU after the fix.

<details><summary>The computation test you are referring to is passing on our server.</summary>
<p>

```
$ bazel test //tensorflow_federated/python/core/api:computations_test
INFO: Analyzed target //tensorflow_federated/python/core/api:computations_test (0 packages loaded, 0 targets configured).
INFO: Found 1 test target...
Target //tensorflow_federated/python/core/api:computations_test up-to-date:
  bazel-bin/tensorflow_federated/python/core/api/computations_test
INFO: Elapsed time: 38.514s, Critical Path: 38.25s
INFO: 10 processes: 10 linux-sandbox.
INFO: Build completed successfully, 11 total actions
//tensorflow_federated/python/core/api:computations_test                 PASSED in 38.2s
  Stats over 10 runs: max = 38.2s, min = 25.4s, avg = 30.2s, dev = 5.1s

INFO: Build completed successfully, 11 total actions
```

</p>
</details>

I was able to run the test on its own, so I do not run into the issue of the GPU running out of memory due to the other tests that are run in parallel.

Now I'll take a look at the other tests mocked in the commit.

Once I can confirm the `tf-nightly` install is sufficient to run the current tf-federated master, I assume the next step would be to revert the commit you mentioned (aec461ba9fee36a05704748041af2f94cc5e31c0) and run the tests again.

Then from this test results I'll look into the code that needs fixing, but clear issue descriptions on the implementation needed would still be much appreciated. Even general ideas or resources.> @pepper-jk We do know some GPU incompatible issues, and I have workaround committed in [aec461b](https://github.com/tensorflow/federated/commit/aec461ba9fee36a05704748041af2f94cc5e31c0). All TFF tests should run without error on GPU after the fix.

In your TODO you state:
```
# TODO(b/137602785): bring GPU test back after the fix for `wrap_function`
```

Do you refer to [this issue](https://github.com/tensorflow/tensorflow/issues/33585) regarding the `wrap_function`?Thanks for your test and continuous interests. Sorry you meet the parallel test issue on GPU. We fixed that internally, but not sure how to configure that without Google infra. 

 This commit 15fb10008921447690d1ea5a42829b9f9a7e832b might have fixed some of the GPU tests, but probably not all of them. If you could help identify which ones are fixed, we would appreciate it. 

The TODO refers to an internal issue. The closest one might be https://github.com/tensorflow/tensorflow/issues/34519
https://github.com/tensorflow/tensorflow/issues/34112 on the TF side. Any Updates?We have been using single GPU for a while. We will be surprised to know single GPU does not work with the recent release of TF and TFF. The performance might not be fully optimized and we have a few pending changes to improve, but single GPU should work as expected, i.e., much faster than CPU, and on par with pure TF. I am still facing this issue. I am currently training a classifier on my own large dataset.

In a central setup, my keras model is trained successfully consuming the GPU.

When using TFF, the GPU is not used at all, so the training is not feasible.
![image](https://user-images.githubusercontent.com/6350053/93001786-d725c200-f531-11ea-929d-d7358f4d5e28.png)

@nightldj I also tested it with colab and the image classification tutorial.
According to the wandb analyzer, the GPU is not used.
![image](https://user-images.githubusercontent.com/6350053/94151112-4f697d00-fe7a-11ea-8200-26a146d1cd00.png)

@mirkohin1991 sorry just saw this thread. Please take a look at our colab tutorial for GPU usage https://www.tensorflow.org/federated/tutorials/simulations_with_accelerators. I will close this issue, but feel free to reopen if it is not resolved.",34,2020-04-20 14:13:51,2021-04-17 03:55:13,2021-04-17 03:55:13
https://github.com/tensorflow/federated/issues/831,['bug'],Documentaiton: optimization bazel,"Documentaiton: optimization bazelOn this markdown file [https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/optimization/README.md](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/optimization/README.md)
there is the following command: `pip install bazel`.

I haven't found a bazel pip package.Thanks for the careful reading! Your assessment is correct. We have updated the documentation regarding Bazel installation. Hope that helps!",1,2020-04-18 21:13:17,2020-04-20 18:29:03,2020-04-20 18:29:03
https://github.com/tensorflow/federated/issues/827,['bug'],tensorflow_gpu 2.1 and dependencies,"tensorflow_gpu 2.1 and dependencies**Description of the bug**
When I install tensorflow federated after tensorflow gpu, tensorflow will no longer recognize the GPU. I have a simple script using `tf.test.gpu_device_name())` to test if a gpu is available, but it complained about missing libs: 'libnvinfer.so.6', 'libnvinfer_plugin.so.6', and 

After a little digging I found out that the libs `libcudnn7` and `libnvinfer6` were not installed or at least not in the right version to match cuda 10.0 so I downgraded them.

However it looks like 'libcufft.so.10', 'libcurand.so.10', 'libcusolver.so.10' and 'libcusparse.so.10' are missing.

What dependency did I miss?

Is downgrading the only solution?

Should I instead upgrade to cuda 10.1 or even 10.2?

**Environment:**
* OS: Linux Ubuntu 16.04
* Python package versions:
  * fensorflow_federated 0.13.1
  * tensorflow 2.1.0
  * Tensorflow_gpu 2.1.0
* Python version: 3.6.3
* CUDA/cuDNN version:
  * Driver Version: 410.129
  * CUDA Version: 10.0
  * libcudnn7 version: 7.6.4.38-1+cuda10.0
  * libnvinfer6 version: 6.0.1-1+cuda10.0
* What TensorFlow Federated execution stack are you using? I'm not sure about this question

**output**
first output from my script:
```
2020-04-14 16:44:56.772603: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64
2020-04-14 16:44:56.772704: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64
2020-04-14 16:44:56.772717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-04-14 16:44:57.378167: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-14 16:44:57.396282: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000285000 Hz
2020-04-14 16:44:57.400402: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x497f900 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-14 16:44:57.400453: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-04-14 16:44:57.405387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-04-14 16:44:59.734901: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a389b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-04-14 16:44:59.734975: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5
2020-04-14 16:44:59.736434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:82:00.0 name: Tesla K40m computeCapability: 3.5
coreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s
2020-04-14 16:44:59.736850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-14 16:44:59.739934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-14 16:44:59.740102: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64
2020-04-14 16:44:59.740237: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64
2020-04-14 16:44:59.740363: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64
2020-04-14 16:44:59.740485: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64
2020-04-14 16:44:59.747440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-14 16:44:59.747473: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-04-14 16:44:59.747509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-14 16:44:59.747526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-04-14 16:44:59.747540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
No GPU found
```

I downgraded the libs like this:
```
sudo apt-get install --no-install-recommends cuda-10-0 libcudnn7=7.6.4.38-1+cuda10.0 libcudnn7-dev=7.6.4.38-1+cuda10.0 libnvinfer6=6.0.1-1+cuda10.0 libnvinfer-dev=6.0.1-1+cuda10.0 libnvinfer-plugin6=6.0.1-1+cuda10.0
```

After downgrading the libs I got this:
```
2020-04-15 16:58:51.794868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-15 16:58:52.126343: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
WARNING:tensorflow:From gpu_test.py:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2020-04-15 16:58:53.077338: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-15 16:58:53.093169: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000285000 Hz
2020-04-15 16:58:53.098709: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4136490 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-15 16:58:53.098763: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-04-15 16:58:53.105241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-04-15 16:58:53.301034: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x41ef140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-04-15 16:58:53.301114: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5
2020-04-15 16:58:53.302911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:82:00.0 name: Tesla K40m computeCapability: 3.5
coreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s
2020-04-15 16:58:53.303556: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-15 16:58:53.307593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-15 16:58:53.307836: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-04-15 16:58:53.307972: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-04-15 16:58:53.308097: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-04-15 16:58:53.308217: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-04-15 16:58:53.308285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-15 16:58:53.308304: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-04-15 16:58:53.308338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-15 16:58:53.308356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-04-15 16:58:53.308371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
No GPU found
[...]
```

**Expected behavior**
-

**Additional context**
Downgrading to tensorflow_gpu==2.0 fixed the issue, but then I would also have to downgrade to tensorflow_federated==0.11.0 and for our project we need at least version 0.12.0.Hi @pepper-jk, thanks for your interest in TensorFlow Federated.

So this is interesting. Note that [tensorflow](https://pypi.org/project/tensorflow/) and [tensorflow-gpu](https://pypi.org/project/tensorflow-gpu/) have different project names. But they both function as the `tensorflow` package when you install them into the same environment. Also note that the `tf-nightly` behaves this way as well. This is supposed to be a feature of Python, but it can be confusing.

I think that you simply no longer need to install `tensorflow_gpu` given the instructions found here https://www.tensorflow.org/install/gpu. Can you try uninstalling this package or creating a new environment without it and try and re-run your TFF code.Thanks for the quick answer.

**Disclaimer**
I can't 100% reproduce the issue at the moment, since the server I'm working on is used by other users at the moment and apparently they use different lib versions (see below). So do not take this as the final reply to your comment. Just wanted to let you know, I'm here and greatefull for your support.

I will get back to you once I can do further testing.

However I still get a similar issue with a ""clean"" venv with just tf_federated installed:
```
2020-04-16 19:01:03.248229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-16 19:01:03.248547: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvrtc.so.10.2: cannot open shared object file: No such file or directory
2020-04-16 19:01:03.248561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-04-16 19:01:04.009028: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-04-16 19:01:04.013507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:82:00.0 name: Tesla K40m computeCapability: 3.5
coreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s
2020-04-16 19:01:04.013689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-16 19:01:04.013731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-16 19:01:04.013786: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-04-16 19:01:04.013837: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-04-16 19:01:04.013884: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-04-16 19:01:04.013930: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-04-16 19:01:04.013956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-16 19:01:04.013964: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
No GPU found
```

the current library versions changed to:
* libcudnn7 version: 7.6.5.32-1+cuda10.2
* libnvinfer6 version: 6.0.1-1+cuda10.2@pepper-jk Have you gone through all the GPU installation steps found at https://www.tensorflow.org/install/gpu?No, since I did not set up the driver and cuda.

My job was to setup the libraries, but I got permission to downgrade the installed libs. I will do so tomorrow.It seems possible that we updated the TF that you were using and thus you need to update the cuda library. I think you should install the [software requirements](https://www.tensorflow.org/install/gpu#software_requirements) and then follow the instructions to [setup linux](https://www.tensorflow.org/install/gpu#linux_setup).Yes, that would be the ideal solution. However I do not have green light yet to change the cuda version on the server, or the driver for that matter.

Once I get the green light I'll happily do that.

Regarding [your earlier post](https://github.com/tensorflow/federated/issues/827#issuecomment-614772302) about the fresh venv. I was able to downgrade the packages again and even with a ""clean"" venv, I get the same errors (see output below).

Which library does contain the shared libs `libcufft.so.10`, `libcurand.so.10`, `libcusolver.so.10`, and `libcusparse.so.10`?

Or do they only come with newer versions of cuda and its libraries?

output:
```
2020-04-17 08:44:10.129937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-17 08:44:10.131621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
2020-04-17 08:44:10.830622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-04-17 08:44:10.834524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:82:00.0 name: Tesla K40m computeCapability: 3.5
coreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s
2020-04-17 08:44:10.834739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-17 08:44:10.836352: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-17 08:44:10.836481: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-04-17 08:44:10.836534: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-04-17 08:44:10.836585: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-04-17 08:44:10.836633: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-04-17 08:44:10.836659: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-17 08:44:10.836668: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
No GPU found
```> It seems possible that we updated the TF that you were using and thus you need to update the cuda library. I think you should install the [software requirements](https://www.tensorflow.org/install/gpu#software_requirements) and then follow the instructions to [setup linux](https://www.tensorflow.org/install/gpu#linux_setup).

Thanks for the help.

I followed the instructions and now it works perfectly.

```
$ python gpu_test.py            
2020-04-20 15:52:20.236822: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-20 15:52:20.439014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
2020-04-20 15:52:26.833957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-04-20 15:52:29.211652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:82:00.0 name: Tesla K40m computeCapability: 3.5
coreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s
2020-04-20 15:52:29.211765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-20 15:52:29.211873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-20 15:52:29.286326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-04-20 15:52:29.325932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-04-20 15:52:29.810698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-04-20 15:52:29.858476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-04-20 15:52:29.858591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-20 15:52:29.861818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
GPU found
```

## For future reference:

Please note that installing the `cude-10-1` package on ubuntu 16.04 will require a newer driver than `nvidia-418`, whiche the guide promotes, I assume it is the newest. In my case that was `nvidia-440` However apt will automatically install it an remove the old driver you just installed with the guide.

Also in `nvidia-smi` it shows cuda version 10.2 for some reason. I'm assuming one could just install the newest cuda version available and it would still work, even though both [the guide](https://www.tensorflow.org/install/gpu#linux_setup) and  [tensorflows website](https://www.tensorflow.org/install/source#tested_build_configurations#Linux#GPU) states that cuda 10.1 is the only officially supported version.

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K40m          Off  | 00000000:82:00.0 Off |                    0 |
| N/A   40C    P0    67W / 235W |      0MiB / 11441MiB |     94%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

`apt` also states that Cuda 10.1 is installed:

```
$ sudo apt-cache policy cuda-10-1
cuda-10-1:
  Installed: 10.1.243-1
  Candidate: 10.1.243-1
  Version table:
 *** 10.1.243-1 500
        500 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages
        100 /var/lib/dpkg/status
     10.1.168-1 500
        500 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages
     10.1.105-1 500
        500 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages
$ sudo apt-cache policy cuda-10-2
cuda-10-2:
  Installed: (none)
  Candidate: 10.2.89-1
  Version table:
     10.2.89-1 500
        500 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages
```",7,2020-04-16 14:19:05,2020-04-20 14:18:08,2020-04-20 14:18:08
https://github.com/tensorflow/federated/issues/825,['bug'],Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU,"Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPUI've run into an error when trying to run a CNN model with a max-pooling operation in my tff experiment.

I have 2 Nvidia GPU's on my machine and my tff library was able to utilize them if I use a DNN model without max-pooling.

As you can see with the log below, My script was able to load the 2 GPUs but I get an error which says I'm using the CPU.

```
2020-04-15 00:12:16.118122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-15 00:12:16.118155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-15 00:12:16.118206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-04-15 00:12:16.118225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-04-15 00:12:16.118254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-04-15 00:12:16.118284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-04-15 00:12:16.118303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-15 00:12:16.119743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1
2020-04-15 00:12:16.119789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-15 00:12:16.119811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 
2020-04-15 00:12:16.119817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N 
2020-04-15 00:12:16.119823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N 
2020-04-15 00:12:16.120866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5707 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:04:00.0, compute capability: 3.5)
2020-04-15 00:12:16.121361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 5707 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5)
2020-04-15 00:12:18.113550: E tensorflow/core/common_runtime/executor.cc:654] Executor failed to create kernel. Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU
	 [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential/max_pooling1d/MaxPool}}]]
2020-04-15 00:12:18.120562: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at iterator_ops.cc:611 : Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU
	 [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential/max_pooling1d/MaxPool}}]]
Traceback (most recent call last):
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1367, in _do_call
    return fn(*args)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1352, in _run_fn
    target_list, run_metadata)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1445, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU
	 [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential/max_pooling1d/MaxPool}}]]
	 [[subcomputation/StatefulPartitionedCall_1/ReduceDataset]]
	 [[subcomputation/StatefulPartitionedCall_1/ReduceDataset/_42]]
  (1) Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU
	 [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential/max_pooling1d/MaxPool}}]]
	 [[subcomputation/StatefulPartitionedCall_1/ReduceDataset]]
0 successful operations.
0 derived errors ignored.
```

The model I used is

```
def create_keras_model():
    return tf.keras.models.Sequential([
        tf.keras.layers.Input(shape=(segment_size, num_input_channels)),
        tf.keras.layers.Conv1D(196,  16, activation='relu'),
        tf.keras.layers.MaxPool1D(4),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units=1024, activation='relu'),
        tf.keras.layers.Dropout(dropout_rate),
        tf.keras.layers.Dense(activityCount, activation='softmax'),
    ])
```

**Environment:**
* OS Platform and Distribution:  ""Debian GNU/Linux 10 (buster)""
* Python package versions (e.g., TensorFlow Federated, TensorFlow):  TensorFlow 2.1.1, TFF 0.13.1
* Python version: 3.7
* Bazel version (if building from source): 
* CUDA/cuDNN version: Cuda: 10.1 / CUDNN 7.6.5
* What TensorFlow Federated execution stack are you using?: Default

**Expected behavior**
To execute normally without a max-pooling op error

Hi @Sannaraek 

It looks like you are getting that error out of TensorFlow. Can you confirm that you can create and utilize this model without TensorFlow Federated? I'm wondering if this is an issue in keras.Hello @michaelreneer 

I just tried to train a Keras Tensorflow CNN model with max pooling. I used the same code for the model.
Everything seems to work fine on the Tensorflow side.



Below is the second part of the error message I get when I try to run my code in a TFF experiment.

```
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""fedavgucibalancedworkingon.py"", line 242, in <module>
    state, metrics = trainer.next(state, train_data)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py"", line 561, in __call__
    return context.invoke(self, arg)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 697, in invoke
    result = computed_comp.value(computed_arg)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 847, in <lambda>
    return ComputedValue(lambda x: self._compute(comp.result, _wrap(x)),
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 745, in _compute
    return self._compute_tuple(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 800, in _compute_tuple
    computed_v = self._compute(v, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 743, in _compute
    return self._compute_call(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 782, in _compute_call
    computed_arg = self._compute(comp.argument, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 745, in _compute
    return self._compute_tuple(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 800, in _compute_tuple
    computed_v = self._compute(v, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 743, in _compute
    return self._compute_call(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 782, in _compute_call
    computed_arg = self._compute(comp.argument, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 745, in _compute
    return self._compute_tuple(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 800, in _compute_tuple
    computed_v = self._compute(v, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 743, in _compute
    return self._compute_call(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 782, in _compute_call
    computed_arg = self._compute(comp.argument, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 745, in _compute
    return self._compute_tuple(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 800, in _compute_tuple
    computed_v = self._compute(v, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 753, in _compute
    return self._compute_block(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 859, in _compute_block
    local_val = self._compute(local_comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 745, in _compute
    return self._compute_tuple(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 800, in _compute_tuple
    computed_v = self._compute(v, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 743, in _compute
    return self._compute_call(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 782, in _compute_call
    computed_arg = self._compute(comp.argument, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 745, in _compute
    return self._compute_tuple(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 800, in _compute_tuple
    computed_v = self._compute(v, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 743, in _compute
    return self._compute_call(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 782, in _compute_call
    computed_arg = self._compute(comp.argument, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 745, in _compute
    return self._compute_tuple(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 800, in _compute_tuple
    computed_v = self._compute(v, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 743, in _compute
    return self._compute_call(comp, context)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 789, in _compute_call
    result = computed_fn.value(computed_arg)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 875, in <lambda>
    lambda x: my_method(fit_argument(x, arg_type, context), context),
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 952, in _federated_map
    fn(ComputedValue(x, mapping_type.parameter)).value for x in arg.value[1]
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 952, in <listcomp>
    fn(ComputedValue(x, mapping_type.parameter)).value for x in arg.value[1]
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 773, in <lambda>
    return ComputedValue(lambda x: run_tensorflow(comp, x),
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/reference_executor.py"", line 346, in run_tensorflow
    result_val = tensorflow_utils.fetch_value_in_session(sess, result)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py"", line 1067, in fetch_value_in_session
    flat_computed_tensors = sess.run(flat_tensors)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 960, in run
    run_metadata_ptr)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1183, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1361, in _do_run
    run_metadata)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1386, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU
	 [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential/max_pooling1d/MaxPool}}]]
	 [[subcomputation/StatefulPartitionedCall_1/ReduceDataset]]
	 [[subcomputation/StatefulPartitionedCall_1/ReduceDataset/_42]]
  (1) Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU
	 [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential/max_pooling1d/MaxPool}}]]
	 [[subcomputation/StatefulPartitionedCall_1/ReduceDataset]]
0 successful operations.
0 derived errors ignored.
```
Off the top of my head I don't know what is going on here, but I will take a look and/or find someone who does.

While I am investigating, I notice that you are setting the default executor to the `ReferenceExecutor`. What happens if you don't change the default executor to the `ReferenceExecutor` and re-run, do you get the same error?I'm not very familiar with the `ReferenceExecutor`, might I ask if you can give me an example on how to change it? Sure! I don't think it's going to make a difference BTW, I'm just suggesting that we gather more data. The default executor should not actually be the `ReferenceExecutor`, you must have set it. Similar to how it is set in https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2. So I'm suggesting that you search your code for where you are setting the default executor to something `tff.framework.set_default_executor` and comment that line out.Hi Sannaraek, we are actively working on improving the TFF performance on accelerators (GPU/TPU). Things often get complicated because we have to make sure things still work on device. 

To help us understand more and find a solution, could you help verify that the error is from placing the TF ops on CPUs? You can do it by running your pure TF code in a `tf.device('/cpu:0')` scope. Or even more reliable with CUDA_VISIBLE_DEVICES= when you run the code. 

I guess the ultimate goal is till to get good efficiency on GPUs. Are you using tff.learning.federated_averaging for training, or did you build your customized optimization method?@michaelreneer Thank for your help!
In this case, I get a different error 

```
2020-04-15 21:12:09.708833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5707 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:04:00.0, compute capability: 3.5)
2020-04-15 21:12:09.710369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 5707 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
WARNING:tensorflow:From /home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Traceback (most recent call last):
  File ""fedavgucibalancedworkingon.py"", line 244, in <module>
    state, metrics = trainer.next(state, train_data)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py"", line 561, in __call__
    return context.invoke(self, arg)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/retrying.py"", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/retrying.py"", line 206, in call
    return attempt.get(self._wrap_exception)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/retrying.py"", line 247, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/six.py"", line 703, in reraise
    raise value
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/retrying.py"", line 200, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 203, in invoke
    _invoke(executor, comp, arg)))
  File ""/usr/lib/python3.7/asyncio/base_events.py"", line 584, in run_until_complete
    return future.result()
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 121, in _invoke
    result = await executor.create_call(comp, arg)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 347, in create_call
    return await comp_repr.invoke(self, arg)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 156, in invoke
    return await executor._evaluate(comp_lambda.result, new_scope)  # pylint: disable=protected-access
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 459, in _evaluate
    values = await asyncio.gather(*values)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 447, in _evaluate
    func, arg = await asyncio.gather(func, get_arg())
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 443, in get_arg
    return await self._evaluate(comp.call.argument, scope=scope)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 459, in _evaluate
    values = await asyncio.gather(*values)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 447, in _evaluate
    func, arg = await asyncio.gather(func, get_arg())
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 443, in get_arg
    return await self._evaluate(comp.call.argument, scope=scope)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 459, in _evaluate
    values = await asyncio.gather(*values)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 447, in _evaluate
    func, arg = await asyncio.gather(func, get_arg())
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 443, in get_arg
    return await self._evaluate(comp.call.argument, scope=scope)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 459, in _evaluate
    values = await asyncio.gather(*values)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 464, in _evaluate
    value = await self._evaluate(loc.value, scope)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 459, in _evaluate
    values = await asyncio.gather(*values)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 448, in _evaluate
    return await self.create_call(func, arg=arg)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 345, in create_call
    comp_repr, delegated_arg))
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/caching_executor.py"", line 287, in create_call
    raise e
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/caching_executor.py"", line 280, in create_call
    target_value = await cached_value.target_future
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 87, in create_call
    return await self._delegate(self._target_executor.create_call(comp, arg))
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/federating_executor.py"", line 368, in create_call
    return await coro(arg)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 281, in async_trace
    return await memoizing_decorator(*fn_args, **fn_kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 183, in async_fn
    retval = await fn(*args, **kwargs)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/federating_executor.py"", line 689, in _compute_intrinsic_federated_weighted_mean
    return await executor_utils.compute_federated_weighted_mean(self, arg)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/executors/executor_utils.py"", line 197, in compute_federated_weighted_mean
    zipped_arg.type_signature.member, tf.multiply)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/compiler/building_block_factory.py"", line 1989, in create_binary_operator_with_upcast
    _check_generic_operator_type(type_signature)
  File ""/home/getalp/eks/env/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/compiler/building_block_factory.py"", line 1960, in _check_generic_operator_type
    'more details.'.format(type_spec))
TypeError: The two-tuple you have passed in is incompatible with upcasted binary operators. You have passed the tuple type <<float64[16,6,196],float64[196],float64[10976,1024],float64[1024],float64[1024,6],float64[6]>,float32>, which fails the check that the two members of the tuple are either the same type, or the second is a scalar with the same dtype as the leaves of the first. See `type_utils.is_binary_op_with_upcast_compatible_pair` for more details.
```@nightldj  Hi Zheng

Thank you for your support

I'm currently using `tff.learning.federated_averaging ` for traininng

I've included `tf.device('/cpu:0')` in my script and did a rerun. The same error as the initial was raised.





@Sannaraek , to clarify, are you running the pure TF code without TFF? In your previous comments, you said the pure TF code runs fine. I think pure TF will place ops on GPUs by default if GPUs are provided. You can force TF to place ops on CPU by either tf.device or CUDA_VISIBLE_DEVICES to see if the same error message shows up as in the TFF case. @nightldj , Oh I'm sorry about the mistake, I ran the experiment in a TFF script.

I've run my other pure TF script with  `tf.device('/cpu:0')`included and I confirm that the model is training and working.



 @Sannaraek Seems like there is possibly two things to consider here. Would it be possible to create and share a [colab](https://colab.research.google.com/) that generates these errors?@Sannaraek Thanks for confirming that. To make sure the problem is not from TF ops device placement, would mind running your pure TF code with `CUDA_VISIBLE_DEVICES=` to explicitly make GPU not available? @michaelreneer  That is okay with me, but if possible, Can I send the link of the collab by email to your Github's Gmail account? 
My work is part of a research lab, I'm not sure if it may be okay for me to post it openly at the moment.@nightldj  I've confirmed that with os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1"" or CUDA_VISIBLE_DEVICES='', The model was able to train in a pure TF script.@Sannaraek Thanks again. You could also cc the link to me to take a look, '%s%s@google.com'%(xu, zheng).I've just sent an email with the link to a colab instance. 
Thank you for your time!Some additional info about the machine I'm running on, 

This the result I get when I run a `device_lib.list_local_devices()`

```
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 18359065315922357112
, name: ""/device:XLA_CPU:0""
device_type: ""XLA_CPU""
memory_limit: 17179869184
locality {
}
incarnation: 9431628243468623825
physical_device_desc: ""device: XLA_CPU device""
, name: ""/device:XLA_GPU:0""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 9706336177551290074
physical_device_desc: ""device: XLA_GPU device""
, name: ""/device:XLA_GPU:1""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 17040832476138747248
physical_device_desc: ""device: XLA_GPU device""
, name: ""/device:GPU:0""
device_type: ""GPU""
memory_limit: 5985225933
locality {
  bus_id: 1
  links {
  }
}
incarnation: 9705894900389934487
physical_device_desc: ""device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:04:00.0, compute capability: 3.5""
, name: ""/device:GPU:1""
device_type: ""GPU""
memory_limit: 5985225933
locality {
  bus_id: 2
  numa_node: 1
  links {
  }
}
incarnation: 8085508660445989695
physical_device_desc: ""device: 1, name: GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5""
]

```
@nightldj  some additional information I've found is that the code is able to run on Google colab if I use a TPU but fails when I select a GPU@Sannaraek This is an interesting observation. There is a secret configuration to explicitly set CPU/GPU/TPU in TFF which is under beta test. Would you want to try it to see what happens, and whether it solves the problem? Sorry did not find time to look at your code yet, but I do receive the email and appreciate that. 

at the very beginning of your binary, after import tff, add/replace the following lines

client_devices = tf.config.list_logical_devices('GPU')
server_device = client_devices[0]
tff.framework.set_default_executor(
      tff.framework.local_executor_factory(
          server_tf_device=server_device,
          client_tf_devices=client_devices))
@nightldj Thank you for your help

I tried adding the lines after a tff import but I get the following error (Running on colab)

 ```
 
34 tff.framework.local_executor_factory(
     35 server_tf_device=server_device,
---> 36 client_tf_devices=client_devices))
     37 
     38 

TypeError: local_executor_factory() got an unexpected keyword argument 'server_tf_device'
```@Sannaraek  `server_tf_device` is probably not in the released version yet. I will take a look at your colab today.

Try this 
client_devices = tf.config.list_logical_devices('GPU')
tff.framework.set_default_executor(
tff.framework.local_executor_factory(
tf_devices=client_devices))@Sannaraek I cannot run your colab because of the drive access. This line looks suspicious to me. `tf.keras.backend.set_floatx('float64')`. Is it necessary for your experiment? Could you try comment it out?@nightldj  Thank you! 
I initially added `tf.keras.backend.set_floatx('float64')` because I received a warning:

```
WARNING:tensorflow:Layer conv1d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

WARNING:tensorflow:Layer conv1d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.
```

Thank you very much for your time and help!@Sannaraek For the warning, I would suggest you check your input data and cast them to float32. Float32 is better supported in general. We would also double check our support for float64 in TFF.@Sannaraek 
Hi Sannaraek, did you successfully run your code on GPU? 
I train to run my own code as well as the image classification demo on GPU with tff 0.13.1. 
These codes can be ran successfully and the GPU can be successfully loaded.  But it appears that the training haven't been done on GPU at all because the GPU util is always 0%. 

Could you please share with me your experience about how to make sure the tff training will automatically use GPU first.

Many thanks and appreciate.@liuquande Hi
For my case, I just followed the Tensorflow GPU set-up requirements (https://www.tensorflow.org/install/gpu) 

You use the below to see if TF ses your device
`from tensorflow.python.client import device_lib
``print(device_lib.list_local_devices())`
or
```
print(""Num GPUs Available: "", len(
    tf.config.experimental.list_physical_devices('GPU')))
```

If you have multiple GPU, You can select which to use with the below
`os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""
`Dear @Sannaraek ,
Thanks for your reply.

Acturally the GPU can be successfully loaded when I run the demo code. The problem is that even though the GPU is loaded, the federated training is not done on GPU.

I add the following at the beginning of demo code.
`print(""Num GPUs Available: "", len(
    tf.config.experimental.list_physical_devices('GPU')))` 

Here is what I got:
```
2020-04-21 20:42:04.618447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
Num GPUs Available:  1
2020-04-21 20:42:04.657577: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-04-21 20:42:04.695003: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2020-04-21 20:42:04.700067: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa46034f000 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-21 20:42:04.700136: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-04-21 20:42:04.738584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:1b:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2020-04-21 20:42:04.738835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-21 20:42:04.738929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-21 20:42:04.739001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-04-21 20:42:04.739063: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-04-21 20:42:04.739123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-04-21 20:42:04.739184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-04-21 20:42:04.739241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-21 20:42:04.858617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-04-21 20:42:04.858820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-21 20:42:07.206119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-21 20:42:07.206172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-04-21 20:42:07.206183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-04-21 20:42:07.508719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11162 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:1b:00.0, compute capability: 7.0)
2020-04-21 20:42:07.515198: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa460e1ecb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-04-21 20:42:07.515249: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN V, Compute Capability 7.0
Num GPUs Available:  1
```
The output above should indicate that the GPU was available. But the training was not performed on GPU, and the GPU util rate is always 0%.
![image](https://user-images.githubusercontent.com/29408526/79868306-50513b00-8412-11ea-8b74-2b43fcc4fd02.png)


The demo code I used can be found here:
 https://paste.ubuntu.com/p/sPtqngFPWP/",27,2020-04-14 22:58:14,2020-04-21 12:55:36,2020-04-18 22:14:09
https://github.com/tensorflow/federated/issues/823,[],Jupyter notebook freezes when executing a cell more than once,"Jupyter notebook freezes when executing a cell more than onceHello everyone, 

I am trying to replicate the tutorials about TensorFlow Federated using Jupyter Notebook. I followed the procedure described [here](https://github.com/tensorflow/federated/blob/master/docs/tutorials/README.md) to setup Jupyter. However, when I try to execute a cell more than once, the notebook freezes and I have to restart it. I tried the same on Colab and everything goes well. I can execute a cell as much as I want and it works.

Some additional information:
- Python version: 3.7.7
- Tensorflow version: 2.1.0
- Tensorflow federated version: 0.13.1

I attached some examples. Any idea?

![Screenshot 2020-04-10 at 10 36 40](https://user-images.githubusercontent.com/22998083/78977241-1c277180-7b18-11ea-93b5-87700a7970f8.png)

![Screenshot 2020-04-10 at 10 36 55](https://user-images.githubusercontent.com/22998083/78977243-1cc00800-7b18-11ea-91da-b282747c3420.png)




Thank you!Hi @fodierna 

Hm, this is a tough one for us--generally speaking, we are optimizing for the colab experience, as managing different builds, versions, release cycles and testing etc. is actually impossible due to mutually incompatible configurations.

Assigning to the person who wrote that documentation, as presumably they have the most state here.@fodierna 

The fact that your notebook is freezing sounds like it could be related to Jupyter + asyncio. But I have not hit this issue myself and I do not have an idea for you on how to fix this issue. My suggestion would be to repost this question on Stack Overflow and tag: [tensorflow-federated](https://stackoverflow.com/questions/tagged/tensorflow-federated), [jupyter-notebook](https://stackoverflow.com/questions/tagged/jupyter-notebook), and [python-asyncio](https://stackoverflow.com/questions/tagged/python-asyncio). I think it might be possible that the Jupyter Community might have more insight.

I'm going to go ahead and close this issue. If you learn something that could help the next person please feel free to make a pull and update that document or reopen this issue.",2,2020-04-10 08:49:20,2020-04-14 16:30:46,2020-04-14 16:30:46
https://github.com/tensorflow/federated/issues/822,[],bulid tensorflow federated failed,"bulid tensorflow federated failedI follow the instrctons in the ""https://tensorflow.google.cn/federated/install"".
(1)Install TensorFlow Federated using pip: When I run the command ""python -c ""import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())"""" , there is a SyntaxError: invalid syntax os
(2)Using Docker: When I run the command ""docker build . --tag tensorflow_federated"", it always fails.
These two errors are shown in the shapshot.
<img width=""771"" alt=""78322895-a0bb4280-75a2-11ea-9886-20e256eae28c"" src=""https://user-images.githubusercontent.com/35478590/78515225-9ccc3200-77e7-11ea-9b0e-f27345033443.png"">
@whyseu could you. please include which version of `tensorflow` and `tesnorflow_federated` pip packages are installed (found via `pip freeze`) and the version of python (found via `python --version`).tensorflow 2.1.0; tesnorflow_federated 0.13.1; python 3.5.2The error is occurring on a literal string interpolation ([PEP 498](https://www.python.org/dev/peps/pep-0498/)) which was introduced in Python 3.6.

It seems like the TFF pip package maybe too lenient in declaring acceptable python versions.

Are you able to use a newer version of Python?This should not be an issue with Python 3.6 and greater. The latest tensorflow_federated package on PyPI is not annotated correctly to express the fact that it is dependent on Python 3.6 and greater. This will be fixe in the next release.

In the meantime you can fix this issue by updating to Python 3.6 or greater. Please feel free to reopen this issue if you find that you continue to get the error after updating to Python.",4,2020-04-06 01:19:00,2020-04-14 17:09:31,2020-04-14 17:09:31
https://github.com/tensorflow/federated/issues/820,[],Issue when running stackoverflow_lr,"Issue when running stackoverflow_lrWhen I tried to run the experiment of stackoverflow_lr:

> bazel run stackoverflow_lr:run_federated -- --total_rounds=1500 --client_optimizer=sgd --client_learning_rate=0.5 --server_optimizer=adagrad --server_learning_rate=10 --clients_per_round=10 --client_batch_size=100 --client_epochs_per_round=10 --experiment_name=stackoverflow_fedadagrad_experiment_05_10 --root_output_dir=/export/UserData/hzy/tmp/fed_opt/ --server_adagrad_initial_accumulator_value=0.1 --server_adagrad_epsilon=0.01


I have the following issue: 

> Traceback (most recent call last):                                                                                                                                                                          
>   File ""/home/hzy/.cache/bazel/_bazel_hzy/86f4822ac0d3e01f42858069fe654bbe/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_fed
> erated.runfiles/org_tensorflow_federated/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_federated.py"", line 125, in <module>                                                        
>     app.run(main)                                                                                                                                                                                           
>   File ""/home/hzy/anaconda3/envs/PY3/lib/python3.6/site-packages/absl/app.py"", line 299, in run                                                                                                             
>     _run_main(main, args)                                                                                                                                                                                   
>   File ""/home/hzy/anaconda3/envs/PY3/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main                                                                                                       
>     sys.exit(main(argv))                                                                                                                                                                                    
>   File ""/home/hzy/.cache/bazel/_bazel_hzy/86f4822ac0d3e01f42858069fe654bbe/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_fed
> erated.runfiles/org_tensorflow_federated/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_federated.py"", line 75, in main                                                             
>     num_validation_examples=FLAGS.num_validation_examples)                                                                                                                                                  
>   File ""/home/hzy/.cache/bazel/_bazel_hzy/86f4822ac0d3e01f42858069fe654bbe/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_fed
> erated.runfiles/org_tensorflow_federated/tensorflow_federated/python/research/optimization/stackoverflow_lr/dataset.py"", line 117, in get_stackoverflow_datasets                                            
>     stackoverflow_train, _, stackoverflow_test = tff.simulation.datasets.stackoverflow.load_data(                                                                                                           
>   File ""/home/hzy/.cache/bazel/_bazel_hzy/86f4822ac0d3e01f42858069fe654bbe/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_fed
> erated.runfiles/org_tensorflow_federated/tensorflow_federated/python/simulation/datasets/stackoverflow.py"", line 109, in load_data                                                                          
>     os.path.join(dir_path, 'stackoverflow_train.h5'))                                                                                                                                                       
>   File ""/home/hzy/.cache/bazel/_bazel_hzy/86f4822ac0d3e01f42858069fe654bbe/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_fed
> erated.runfiles/org_tensorflow_federated/tensorflow_federated/python/simulation/hdf5_client_data.py"", line 50, in __init__                                                                                  
>     self._h5_file = h5py.File(self._filepath, ""r"")                                                                                                                                                          
>   File ""/home/hzy/anaconda3/envs/PY3/lib/python3.6/site-packages/h5py/_hl/files.py"", line 312, in __init__                                                                                                  
>     fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)                                                                                                                                             
>   File ""/home/hzy/anaconda3/envs/PY3/lib/python3.6/site-packages/h5py/_hl/files.py"", line 142, in make_fid                                                                                                  
>     fid = h5f.open(name, flags, fapl=fapl)                                                                                                                                                                  
>   File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper                                                                                                                                     
>   File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper                                                                                                                                     
>   File ""h5py/h5f.pyx"", line 78, in h5py.h5f.open                                                                                                                                                            
> OSError: Unable to open file (unable to open file: name = '/home/hzy/.keras/stackoverflow_train.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)                        
> 

tf version: 2.1.0 
tff version: 0.13.1 

Is this a bug caused by tf and tff mismatch @ZacharyGarrett ?
I solved this issue by changing this line: https://github.com/tensorflow/federated/blob/d145e66ebb113423a3c921621e1b11957642ad24/tensorflow_federated/python/research/optimization/stackoverflow_lr/dataset.py#L53 to: words = tf.strings.split(tf.strings.substr(sentence, 0, MAX_SEQ_LEN), sep=' ')
and https://github.com/tensorflow/federated/blob/d145e66ebb113423a3c921621e1b11957642ad24/tensorflow_federated/python/research/optimization/stackoverflow_lr/dataset.py#L59 to: tags = tf.strings.split(tags, sep='|')

Those versions of TFF and TF should be compatible.

@zcharles8 @jkr26 have written a lot of the code in this directory, they might have ideas?@zcharles8 just pushed [this change](https://github.com/tensorflow/federated/commit/22c89fac4aa9e92e591a7de43e705773215986b2), which should have fixed this problem. @slowbull, could you confirm?Hi @slowbull, just wanted to ping this thread again. I've since attempted to make sure that using the code internally and externally produced the same results, and things look good as far as I can tell. Have your issues been resolved?Sorry for the late reply. It works.  @jkr26 @zcharles8",4,2020-04-03 15:34:32,2020-04-25 04:29:19,2020-04-25 04:29:19
https://github.com/tensorflow/federated/issues/819,[],AttributeError: type object 'DatasetV2' has no attribute 'TextLineDataset',"AttributeError: type object 'DatasetV2' has no attribute 'TextLineDataset'Hi, I am following the tutorial 'Federated Learning for Image Classification', but using my own dataset. I got this error.

Each .csv has a 3 column dataset which I'm trying to simulate as a client data for testing federated learning.

here is my code:
```
tf.compat.v1.enable_v2_behavior()
import tensorflow_federated as tff
dataset_paths = {
  'client_0': '/content/drive/My Drive/tmp/a.csv',
  'client_1': '/content/drive/My Drive/tmp/b.csv',
  'client_2': '/content/drive/My Drive/tmp/c.csv',
}

def create_tf_dataset_for_client_fn(id):
   path = dataset_paths.get(id)
   if path is None:
     raise ValueError(f'No dataset for client {id}')
   return tf.data.Dataset.TextLineDataset(path)

source = tff.simulation.ClientData.from_clients_and_fn(
  ['client_0','client_1','client_2'], create_tf_dataset_for_client_fn)
```


I got the error

```
AttributeError                            Traceback (most recent call last)
<ipython-input-9-b3879bd82bf0> in <module>()
     14 
     15 source = tff.simulation.ClientData.from_clients_and_fn(
---> 16   ['client_0','client_1','client_2'], create_tf_dataset_for_client_fn)

2 frames
<ipython-input-9-b3879bd82bf0> in create_tf_dataset_for_client_fn(id)
     11    if path is None:
     12      raise ValueError(f'No dataset for client {id}')
---> 13    return tf.data.Dataset.TextLineDataset(path)
     14 
     15 source = tff.simulation.ClientData.from_clients_and_fn(

AttributeError: type object 'DatasetV2' has no attribute 'TextLineDataset'
```

my environment
tensorboard == '2.2.0'
tensorflow == '2.2.0-rc2'
tensorflow_federated == '0.13.1'
Just to be clear I also tried doing @ZacharyGarrett 's solution on [Stack Overflow](https://stackoverflow.com/questions/60265798/tff-how-define-tff-simulation-clientdata-from-clients-and-fn-function) 
i.e
```
dataset_paths = {
  'client_0': '/tmp/A.csv',
  'client_1': '/tmp/B.csv',
  'client_2': '/tmp/C.csv',
}

def create_tf_dataset_for_client_fn(id):
   path = dataset_paths.get(id)
   if path is None:
     raise ValueError(f'No dataset for client {id}')
   return tf.data.Dataset.TextLineDataset(path)

source = tff.simulation.ClientData.from_clients_and_fn(
  dataset_paths.keys(), create_tf_dataset_for_client_fn)
```


But I got the following error

```
TypeError                                 Traceback (most recent call last)
<ipython-input-13-3fa09c31a2b3> in <module>()
     12 
     13 source = tff.simulation.ClientData.from_clients_and_fn(
---> 14   dataset_paths.keys(), create_tf_dataset_for_client_fn)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/py_typecheck.py in check_type(target, type_spec, label)
     42     raise TypeError('Expected {}{}, found {}.'.format(
     43         '{} to be of type '.format(label) if label is not None else '',
---> 44         type_string(type_spec), type_string(type(target))))
     45   return target
     46 

TypeError: Expected list, found dict_keys.
```> `TypeError: Expected list, found dict_keys.`

This is happening because `ClientData.from_clients_and_fn` takes a `list` of keys, and the code in your example passes `dataset_paths.keys()`. I'd try turning that into a `list`, like this: `list(dataset_paths.keys())`.> > `TypeError: Expected list, found dict_keys.`
> 
> This is happening because `ClientData.from_clients_and_fn` takes a `list` of keys, and the code in your example passes `dataset_paths.keys()`. I'd try turning that into a `list`, like this: `list(dataset_paths.keys())`.

Yes, I understand that! But it again goes to the main issue that I'm facing 

`AttributeError: type object 'DatasetV2' has no attribute 'TextLineDataset'` 
The code appears to use `tf.data.Dataset.TextLineDataset`, but the tensorflow.org documentation seems to say the path is [`tf.data.TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset). Does removing the extra `.Dataset` part help?@kinetickansra Please reopen this issue if @ZacharyGarrett suggestion did not fix your issue.",5,2020-04-01 15:01:53,2020-04-14 16:42:28,2020-04-14 16:42:28
https://github.com/tensorflow/federated/issues/817,[],Two issues when I tried to reimplement the result training emnist using adam,"Two issues when I tried to reimplement the result training emnist using adamThe command line is as follows, where I follow the value in Table 10 of paper ""https://arxiv.org/pdf/2003.00295.pdf"": 
bazel run emnist:run_federated -- --total_rounds=1500 --client_optimizer=sgd --client_learning_rate=0.001 --server_optimizer=adam --server_learning_rate=1.0 --clients_per_round=10 --client_epochs_per_round=1 --experiment_name=emnist_fedadam_experiment --root_output_dir=/export/tmp/fed_opt/

Two issues:
1) the program breaks down after 70 iterations.  Error: ""/home/.cache/bazel/_bazel_hzy/0133413982e33a0797f543bd111e9efb/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/emnist/run_federated.run
files/org_tensorflow_federated/tensorflow_federated/python/simulation/hdf5_client_data.py"", line 71, in create_tf_dataset_for_client                                                                        
    tf_dataset = self._create_dataset(client_id) ""/home/cache/bazel/_bazel_hzy/0133413982e33a0797f543bd111e9efb/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/emnist/run_federated.run
files/org_tensorflow_federated/tensorflow_federated/python/simulation/hdf5_client_data.py"", line 64, in _create_dataset                                                                                     
    self._h5_file[HDF5ClientData._EXAMPLES_GROUP][client_id].items())))                                                                                                                                     
  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper                                                                                                                                     
  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper                                                                                                                                     
  File ""/home/anaconda3/envs/PY3/lib/python3.6/site-packages/h5py/_hl/group.py"", line 177, in __getitem__                                                                                               
    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)                                                                                                                                                 
  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper                                                                                                                                     
  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper                                                                                                                                     
  File ""h5py/h5o.pyx"", line 190, in h5py.h5o.open                                                                                                                                                           
KeyError: 'Unable to open object (bad symbol table node signature)'                                                                                                                                         
""
**Does it mean that there are problems with the data?**

2) **The loss does not converge. The acc is still 0.04137931 after 70 rounds. Are there any other params I need to change?**  (It converges when I change client_learning_rate=0.01, server_learning_rate=0.01. ) But how can I reimplement the result in the paper?1. Could you please post the contents of `pip freeze`? What version fo `tensorflow_federated` and `h5py` are installed?

2. @zcharles8 to confirm: I believe the Adam experiments may need higher `epsilon` values, though they might already be the default?The Adam optimizer argument uses the Keras defaults unless otherwise specified. @slowbull I would recommend using the flag ""--server_adam_epsilon=0.1"" to re-obtain the results from our paper.Using the same parameters but with the epsilon value set to 0.1, I am observing a consistently decreasing loss function. I would also caution against using FedAdam for EMNIST. On the character recognition task, it was the slowest to reach its peak accuracy, and for the autoencoder task, it consistently got stuck or diverged. FedYogi and FedAvgM were generally the best optimizers for EMNIST.> 1. Could you please post the contents of `pip freeze`? What version fo `tensorflow_federated` and `h5py` are installed?
This problem is not observed every run. But it may occur sometimes.                                                                                                                                                                  
> 2. @zcharles8 to confirm: I believe the Adam experiments may need higher `epsilon` values, though they might already be the default?

1) tensorflow-federated==0.13.1, 
h5py==2.8.0
                     
2) @zcharles8  Thanks, epsilon=0.1 should do the trick. I will try yogi also. 
@slowbull Thanks for the information!

Could you possibly try adding a try/except context to see if there is a specific client id that is failing?

Maybe something like this:
```python
try:
  datasets = []
  for client_id in sample(client_data.client_ids):
    datasets.append(client_data.create_tf_dataset_for_client(client_id))
except KeyError as e:
  logging.exception('Failed to create dataset for client_id %s', client_id)
  raise e
```> Using the same parameters but with the epsilon value set to 0.1, I am observing a consistently decreasing loss function. I would also caution against using FedAdam for EMNIST. On the character recognition task, it was the slowest to reach its peak accuracy, and for the autoencoder task, it consistently got stuck or diverged. FedYogi and FedAvgM were generally the best optimizers for EMNIST.

@zcharles8 I tried to use Yogi: 
bazel run emnist:run_federated -- --total_rounds=500 --client_optimizer=sgd --client_learning_rate=0.001 --server_optimizer=yogi --server_learning_rate=3.
162 --clients_per_round=10 --client_epochs_per_round=1 --experiment_name=emnist_fedyogi_experiment_n3_p1f2 --root_output_dir=/export/UserData/tmp/fed_opt/ --server_yogi_initial_accumulator_value=8.178
732e-6
The learning rates are from Table 10 in the paper, client_learning_rate=10^-3,  server_learning_rate=10^0.5 \approx 3.162, and server_yogi_initial_accumulator_value=8.178
732e-6 is computed from emnist/compute_yogi_init.py. The loss does not converge either in this setting. Any hint?


> @slowbull Thanks for the information!
> 
> Could you possibly try adding a try/except context to see if there is a specific client id that is failing?
> 
> Maybe something like this:
> 
> ```python
> try:
>   datasets = []
>   for client_id in sample(client_data.client_ids):
>     datasets.append(client_data.create_tf_dataset_for_client(client_id))
> except KeyError as e:
>   logging.exception('Failed to create dataset for client_id %s', client_id)
>   raise e
> ```

@ZacharyGarrett  It looks like this issue disappears if there is only one process/experiment running on one machine. I will keep you informed when the error pops up.@slowbull Ah, thanks for the additional information. It looks like h5py may not support concurrent access to the same file based on the code comment [here](https://github.com/h5py/h5py/blob/4589052eaa6aa8fec612bb2b73c11006e7ead20f/examples/multiprocessing_example.py#L17).

We may want to look into alternative file formats for the datatasets...@slowbull For all of our adaptive optimizers, we used an epsilon value of 0.1. I believe this was noted in the paper. We should probably emphasize it in the repository too, as it seems to be critical to reaching a good minima.

EDIT: I see that you did use a more targeted Yogi initial accumulator value. The epsilon value is probably the issue here.@zcharles8   Are you referring to this sentence ""For the adaptive optimizers, we fix the numerical stability constant \varepsilon to be 0.01"" ? Thank you very much. Yes, sorry, it was set to 0.01 in all experiments. I went back through the experiment output data to be certain.",10,2020-03-30 05:10:41,2020-04-01 21:22:42,2020-04-01 21:22:42
https://github.com/tensorflow/federated/issues/816,[],Error while using GPU based execution,"Error while using GPU based executionI am trying to experiment with remote executor runtime with the example provided on this link.
https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/examples/remote_executor_example.py

If I using CPU based tensorflow, then everything works fine. However, for GPU based tensorflow 
the follow error occurs and aborts execution:

2020-03-29 16:27:22.904103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-29 16:27:22.904807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 978 MB memory) -> physical GPU (device: 0, name: GRID V100DX-32C, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-03-29 16:27:22.995000: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node partitionedcall_args_0/_2}}]]


How do I solve this ? Have anyone faced similar issues ?
Sorry you hit this! There's an internal bug tracking this issue as well, and it's currently blocked on some work from the mainstream Tensorflow team. I'll try to give an update here when there's more to say, and thanks for your patience!Hi! I also ran into this bug while trying to run tensorflow federated with a GPU base remote executor. In my case the execution does not abort, but hangs.

Is there any way around it?

Some remote executor logs:
```
2020-05-03 12:21:38.334182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10770 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2020-05-03 12:21:38.682825: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node partitionedcall_args_0/_2}}]]
2020-05-03 12:21:38.760455: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node partitionedcall_args_0/_2}}]]
```Sadly still no update internally. :(Hi all! For the version of this we have hit internally, we have been able to work around the underlying TF issue with [this commit](https://github.com/tensorflow/federated/commit/15fb10008921447690d1ea5a42829b9f9a7e832b). Can those who have experienced this issue externally confirm whether or not this resolves their problem? At this point, this would necessitate building TFF from master--but we are looking to have a pip package out ""soon"" (I am on rotation next week and promised to make a release). I will ping this bug back when we have a pip package out as well.This should be fixed as of [TFF 0.14.0](https://github.com/tensorflow/federated/releases/tag/0.14.0), just released. Would it be possible to confirm?We believe this issue to be resolved as of TFF 0.14.0; closing now.",6,2020-03-29 20:38:24,2020-07-08 04:14:54,2020-07-08 04:14:54
https://github.com/tensorflow/federated/issues/815,[],Preprocessing error: TypeError: <lambda>() takes 1 positional argument but 2 were given,"Preprocessing error: TypeError: <lambda>() takes 1 positional argument but 2 were givenHi,

I am attempting the duplicate the EMNIST tutorial with another dataset by using tf.data to read data from tf keras image data generators. The issue arises when mapping the batch_format_fn to each batch of data as the mapping method passes 2 arguments to the batch_format_fn rather than the 1 'element' argument. I inspected the 2 arguments that are getting passed to the batch_format_fn and they were: 

```python
Tensor(""args_0:0"", dtype=float32)
Tensor(""args_1:0"", dtype=float32)
```

But when the same arguments are inspected when running the EMNIST example, it is one argument:

```python
OrderedDict([('label', <tf.Tensor 'args_0:0' shape=() dtype=int32>), ('pixels', <tf.Tensor 'args_1:0' shape=(28, 28) dtype=float32>)])
```

Below is the error:
```python
Traceback (most recent call last):
  File ""src/federated_learning/federated_learning_model/model.py"", line 189, in <module>
    prefetch_buffer
  File ""src/federated_learning/federated_learning_model/model.py"", line 47, in __init__
    prefetch_buffer)
  File ""src/federated_learning/federated_learning_model/model.py"", line 157, in _get_sample_batch
    revert_one_hot=False)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/src/federated_learning/federated_learning_model/tff_utils.py"", line 61, in preprocess
    mapped_dataset = shuffled_dataset.map(batch_format_fn)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 1588, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 3888, in __init__
    use_legacy_function=use_legacy_function)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 3147, in __init__
    self._function = wrapper_fn._get_concrete_function_internal()
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2395, in _get_concrete_function_internal
    *args, **kwargs)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 3140, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 3082, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 237, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:


    TypeError: tf__batch_format_fn() takes 1 positional argument but 2 were given
```

The code below is extracted from the class that I am creating but has all the steps that are leading to the issue.

```python
    def read_client_data(path,
                         img_h,
                         img_w,
                        batch_size,
                        class_mode,
                        dataset='train'):
       path = os.path.join(path, dataset)


        def make_generator():
            train_datagen = image.ImageDataGenerator(rescale=1. / 255)
            train_generator = train_datagen.flow_from_directory(path,
                                                                target_size=[img_h, img_w],
                                                                class_mode=class_mode,
                                                                batch_size=batch_size)
            return train_generator

        train_dataset = tf.data.Dataset.from_generator(
            generator=make_generator,
            output_types=(tf.float32, tf.float32)
        )

        return train_dataset


    def batch_format_fn(element):
        """"""Flatten a batch `pixels` and return the features as an `OrderedDict`.""""""
        # batch = collections.OrderedDict(x=tf.reshape(element['pixels'], [-1, 784]),
        #                                 y=tf.reshape(element['label'], [-1, 1]))
        print('Running batch format ')
        print(element)
        batch = collections.OrderedDict(
            x=tf.reshape(element['pixels'], element['pixels'].shape),
            y=tf.reshape(element['label'], [-1, 1])
        )
        return batch


    def preprocess(dataset,
                   num_epochs,
                   shuffle_buffer,
                   batch_size,
                   prefetch_buffer,
                   revert_one_hot=False):
        # Repeating dataset count times
        repeated_dataset = dataset.repeat(count=num_epochs)
        # Randomly shuffling elements from dataset of size shuffle_buffer
        shuffled_dataset = repeated_dataset.shuffle(shuffle_buffer)
        # Combines consecutive elements of dataset into batches
        # Removed as the data is already in batches
        # batch = shuffled_dataset.batch(batch_size)
        # THE LINE BELOW THROWS THE ERROR
        mapped_dataset = shuffled_dataset.map(batch_format_fn)
        # mapped_dataset = shuffled_dataset.map(lambda self, x: batch_format_fn(x))
        if revert_one_hot:
            # Converting labels to ints
            mapped_dataset = mapped_dataset.map(lambda i, l: (i, tf.argmax(l, axis=1)))
        processed_dataset = mapped_dataset.prefetch(prefetch_buffer)
    return processed_dataset

    client_data = [read_client_data(os.path.join(path_to_data, client),
                                            self.img_h,
                                            self.img_w,
                                            self.batch_size,
                                            self.class_mode)
                              for client in clients]

    preprocessed_data = [preprocess(dataset,
                                            num_epochs,
                                            shuffle_buffer,
                                            batch_size,
                                            prefetch_buffer,
                                            revert_one_hot=False)
                                 for dataset in client_data]
```The code in question is only using `tensorflow` core APIs (specifically setting up a `tf.data` pipeline), no `tensorflow_federated` or `tff` API. Looks like a usage question, rather than a issue/bug with Tensorflow Federated.

The best place for such questions is StackOverflow using the `tensorflow` tag (https://stackoverflow.com/questions/tagged/tensorflow). Could you re-post this question there?",1,2020-03-27 13:31:00,2020-03-27 13:54:08,2020-03-27 13:54:07
https://github.com/tensorflow/federated/issues/814,[],ModuleNotFoundError: No module named 'tensorflow_federated.python.research',"ModuleNotFoundError: No module named 'tensorflow_federated.python.research'The following error occurred while running ""https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/gans/experiments/emnist/run_experiments.py"" (I am attempting to regenerate the results of the [paper](https://arxiv.org/abs/1911.06679)):

`ModuleNotFoundError: No module named 'tensorflow_federated.python.research'`

1. Here is the full list of my anaconda environment information:
![Screenshot from 2020-03-23 21-05-17](https://user-images.githubusercontent.com/14258909/77319901-97340e00-6d4a-11ea-96d2-af819ebfb1ef.png)

2. and I am using Pycharm in Ubuntu 16.04:
![Screenshot from 2020-03-23 21-04-13](https://user-images.githubusercontent.com/14258909/77320002-bf237180-6d4a-11ea-914e-17ae59594c36.png)

3. Last but not least, I have tried to execute the code in the terminal but the error continued to exist, so I'm afraid that the problem is  not concerned with the Pycharm IDE.
![Screenshot from 2020-03-23 21-11-35](https://user-images.githubusercontent.com/14258909/77320343-5a1c4b80-6d4b-11ea-90be-ccd68fe51245.png)

It will be appreciated if you could give me some suggestions.
I believe the research under `tensorflow_federated/python/research/gans/` requires using [bazel](https://bazel.build/) to build and run, similar to the rest of TensorFlow Federated. It likely won't work to run the python script directly.

@saugenst can you confirm? If this is the case, could we update the directory's [README.md](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/gans/README.md) to explicitly state this requirement, and provide some example commandlines?@ZacharyGarrett Thank you for your response very much, and I have installed bazel now. However, while running
 `bazel run \ tensorflow_federated/python/research/gans/experiments/emnist:run_experiments` 
I have encountered some other troubles.
![Screenshot from 2020-03-24 11-54-25](https://user-images.githubusercontent.com/14258909/77386693-5ed21580-6dc6-11ea-87d7-3450c014c3ed.png)
As can be seen, I have tried to create a WORKSPACE using `touch WORKSPACE`, but I don't konw what to do in the next step since I am not familiar with bazel.
Could you please give me some more advices?
Could you try re-running the command _inside_ the folder where the cloned TensorFlow Federated repo is? TensorFlow Federated comes with a [`WORKSPACE`](https://github.com/tensorflow/federated/blob/master/WORKSPACE) file that should do everything needed.

```shell
git clone https://github.com/tensorflow/federated.git ${DESTINATION}
cd ${DESTINATION}
bazel run ...
```


Possibly the instructions for [Build the TensorFlow Federated pip package](https://github.com/tensorflow/federated/blob/master/docs/install.md#build-the-tensorflow-federated-pip-package), but skipping the last steps (stop after step 5), will help get the environment working.The problem was solved based on your suggestions. Thanks a lot!",4,2020-03-23 13:21:06,2020-03-25 09:20:55,2020-03-25 09:20:55
https://github.com/tensorflow/federated/issues/809,[],How to aggregate only part of client model weights?,"How to aggregate only part of client model weights?Not sure if I should ask here again. I encounted similar issue as https://stackoverflow.com/questions/60501245/can-i-broadcast-different-models-for-each-of-the-clients-in-federated-tensorflow, but seems no one answered it. Wondering if tff supports aggregating only part of client model weights? I mean the client model contains two parts, and one part needs to aggregate, the other part needs to keep locally. Thanks!There isn't anything available yet for this that is plug-and-play-- it's possible to do by creating your own `IterativeProcess` and calling `tff.federated_aggregate`/`tff.federated_collect` manually. There are, however, some internal research examples that do this that we're looking to open-source soon. Stay tuned for those, otherwise feel free to ask questions if you decide to use `IterativeProcess` and hit any issues!Thanks!
I have implemented this by transfering weights deltas to python environment by tff.federated_collect, the patch is
[optimizer_utils.txt](https://github.com/tensorflow/federated/files/4455248/optimizer_utils.txt)
Do you think this implementation is suitable?
The following issue is that performance is lowered by about 10X after using tff.federated_collect. Do you have any advice?
The version of tensorflow_federated I used is 0.12.0 and 10 clients are used.
Thanks again!You're in luck! `federate_collect` had a severe performance issue on the simulation stack, and I have a patch locally that fixes it. I'll try to have it in some time this week-- thanks for your patience!

If you're curious, it boils down to the `EagerTFExecutor::create_value` call with values of `SequenceType` calling into [`make_dataset_from_elements`](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py#L928) and then [this numpy call](https://github.com/tensorflow/federated/blob/eb57cca1666670cf167a5468c761cd3724fe4a8f/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py#L902) which can take a *long* time. I believe we can replace that call in this case with a more efficient type conversion, but I'll update back here once I have a finished patch.I've just landed https://github.com/tensorflow/federated/commit/c7f90f11ca17224cdf130206dbd0736f9a8a7318 which gives me massive speedup on some cases of `federated_collect`. It'll be available in the next release. Let me know when you have a chance to try it and if it helps you!It works and I also get massive speedup! Thanks for your great help!Woooo!!! :)",6,2020-03-19 11:24:32,2020-05-08 20:37:13,2020-05-08 20:37:13
https://github.com/tensorflow/federated/issues/808,[],AttributeError: module 'tensorflow' has no attribute 'contrib',"AttributeError: module 'tensorflow' has no attribute 'contrib'The following error occurred while  running (https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/gans/experiments/emnist/classifier/measure_misclassification_of_users.py):

**AttributeError: module 'tensorflow' has no attribute 'contrib'**

There exists a similar issue #224 , but it's actually different from mine
since I have installed  TensorFlow 1.14.0 which is a version under  2.0.
![image](https://user-images.githubusercontent.com/14258909/77029801-aed06700-69d7-11ea-9dd8-47c1903118f1.png)

Could you please give me some suggestions?
Hi @AshiakerWang, it looks like you are using an old version of the TensorFlow Federated package and if you have TensorFlow 1.14.0 then you are also using an old version of the TensorFlow package. Can you update to the latest version of those packages and see if that fixes your issue.

```
pip install --upgrade tensorflow-federated
``` 

This will install the latest version of the TensorFlow Federated package and all the dependencies it requires (including the version fo TensorFlow it needs).thank you so much! I will try it.",2,2020-03-19 03:58:13,2020-03-21 01:31:28,2020-03-21 01:31:27
https://github.com/tensorflow/federated/issues/805,[], RuntimeError: This event loop is already running,"RuntimeError: This event loop is already runningI have been receiving an issue while working through the TFF image classification tutorial (https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) when invoking the initialize computation to construct the server state. 

RuntimeError                              Traceback (most recent call last)
<ipython-input-18-0c0dcd96a774> in <module>
----> 1 state = iterative_process.initialize()

/anaconda3/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in __call__(self, *args, **kwargs)
    590     context = self._context_stack.current
    591     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 592     return context.invoke(self, arg)
    593 
    594 

/anaconda3/lib/python3.6/site-packages/retrying.py in wrapped_f(*args, **kw)
     47             @six.wraps(f)
     48             def wrapped_f(*args, **kw):
---> 49                 return Retrying(*dargs, **dkw).call(f, *args, **kw)
     50 
     51             return wrapped_f

/anaconda3/lib/python3.6/site-packages/retrying.py in call(self, fn, *args, **kwargs)
    204 
    205             if not self.should_reject(attempt):
--> 206                 return attempt.get(self._wrap_exception)
    207 
    208             delay_since_first_attempt_ms = int(round(time.time() * 1000)) - start_time

/anaconda3/lib/python3.6/site-packages/retrying.py in get(self, wrap_exception)
    245                 raise RetryError(self)
    246             else:
--> 247                 six.reraise(self.value[0], self.value[1], self.value[2])
    248         else:
    249             return self.value

/anaconda3/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

/anaconda3/lib/python3.6/site-packages/retrying.py in call(self, fn, *args, **kwargs)
    198         while True:
    199             try:
--> 200                 attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
    201             except:
    202                 tb = sys.exc_info()

/anaconda3/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/execution_context.py in invoke(self, comp, arg)
    186             _ingest(executor, unwrapped_arg, arg.type_signature))
    187       return asyncio.get_event_loop().run_until_complete(
--> 188           _invoke(executor, comp, arg))

/anaconda3/lib/python3.6/asyncio/base_events.py in run_until_complete(self, future)
    469         future.add_done_callback(_run_until_complete_cb)
    470         try:
--> 471             self.run_forever()
    472         except:
    473             if new_task and future.done() and not future.cancelled():

/anaconda3/lib/python3.6/asyncio/base_events.py in run_forever(self)
    423         self._check_closed()
    424         if self.is_running():
--> 425             raise RuntimeError('This event loop is already running')
    426         if events._get_running_loop() is not None:
    427             raise RuntimeError(

RuntimeError: This event loop is already running@cianohagan could you provide details on where this is running?

- colab.research.google.com or Jupyter?
- Version of TFF being run?
- Version of Python?Hi, I just got the same error, for that same code and as well, for this snippet from the algorithms 1 notebook:

```
@tff.federated_computation(tff.FederatedType(tf.float32, tff.CLIENTS))
def get_average_temperature(sensor_readings):
  return tff.federated_mean(sensor_readings)

get_average_temperature([68.5, 70.3, 69.8])
```

I'm running on my local machine, with versions:

Python: 3.7.6
TF: 2.1
TFF: 0.12

Please let me know if there's anything else that could help. Many thanks!

Oops - forgot about the Jupyter question and now I tested, it **does in fact occur in Jupyter only, but not interactively in the Python interpreter!**

:-)If this is happening in Jupyter, could you see the instructions in the [tutorial README](https://github.com/tensorflow/federated/blob/master/docs/tutorials/README.md)? The section [Running the TFF Tutorials in Jupyter Notebooks](https://github.com/tensorflow/federated/blob/master/docs/tutorials/README.md#running-the-tff-tutorials-in-jupyter-notebooks) discusses how to install `nest_asyncio` (which is required only when running within Jupyter).Wonderful, - works! 

Many thanks for the quick resolution!@ZacharyGarrett Thanks for that. I didn't see the instructions in the README about Jupyter. The code now works without any errors.",5,2020-03-16 15:24:11,2020-03-18 11:39:49,2020-03-16 23:16:01
https://github.com/tensorflow/federated/issues/804,[],Bad Time Performance with  a lot of clients,"Bad Time Performance with  a lot of clientsI am using the TF Federated Framework to implement a fare price predictor on the NYC taxi dataset. 
Therefore i split the dataset into a federated clientdataset and use it to train a federated predictor like in the tutorials provided online. 
I noticed that the time needed to train increases rapedly when the amount of clients is increased. 
All data is used for the training and is split across the amount of clients evenly.
With the same network type and same epochs it takes : 

1000 Clients , 500 Per Round : 130 minutes
100 Clients , 50     Per Round : 13 minutes
10 Clinets , 5        Per Round :  5 minutes


Is this something that happens normaly or is this some kind of bug ? Or am i doing something wrong here ? 

![Code](https://user-images.githubusercontent.com/62025839/76345905-70b2b380-6304-11ea-94a0-9c05de4b989a.PNG)
Hi @steffenStra can I ask what version of TFF you are using?Hi @michaelreneer i am using TFF Version 0.12.0 on google colab.Hi @steffenStra I think this seems normal, lets assume that your setup can run 20 clients in parallel, which takes about 5 minutes

```markdown
10 clients: 1 batch (20 clients in parallel) = 5 minutes
50 clients: 2-3 batches of 20 clients in parallel * 5 minutes = 10-15 minutes
500 clients: ~25 batches of 20 clients in parallel * 5 minutes = 125 minutes
```

**You probably want to take a look at setting up a [multi-machine simulation](https://github.com/tensorflow/federated/blob/master/docs/tutorials/high_performance_simulation_with_kubernetes.ipynb).**

Also note that aggregation gets slower with more clients because this operation is serial, not parallel. *You might be able to take a look at creating a custom execution stack and adjusting the [max_fanout](https://www.tensorflow.org/federated/api_docs/python/tff/framework/local_executor_factory)*, which can be done with...

```python
executor_factory = tff.framework.local_executor_factory(
    num_clients=None,
    max_fanout=100,
    clients_per_thread=1)
tff.framework.set_default_executor(executor_factory)
```",3,2020-03-10 18:25:46,2020-06-15 18:23:37,2020-06-15 18:23:37
https://github.com/tensorflow/federated/issues/803,[],Does tff.tf_computation support tf.numpy_function?,"Does tff.tf_computation support tf.numpy_function?When I use tf.numpy_function in optimizer_utils.py(tensorflow_federated/python/learning/framework/optimizer_utils.py) like this

```
tensor_string_type = tff.TensorType(tf.string)
@tff.tf_computation(tensor_string_type)
    def cast_tensor_to_int(k):
        def _numpy_func(x):
            x = int(x) + 1
            return np.array(x, dtype='U100000')

        @tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.string)])
        def tf_function(input):
            y = tf.numpy_function(_numpy_func, [input], tf.string)
            y1 = tf.reshape(y, ())
            return y1
        return tf_function(k)
```

and I call this function in run_one_round_tff, It gives me the errors in client side

> ValueError: callback pyfunc_0 is not found
> 	 [[{{node import/StatefulPartitionedCall/PyFunc}}]] [Op:__inference_wrapped_function_13]
> Function call stack:

I don't know why this happened?
Hi @FancyXun 

I have been unable to reproduce this problem in our current setup (that is, with TFF 0.13.1). See the colab [here](https://colab.research.google.com/drive/15wfY6xv4LZa1oBf-mNjJXJfir1XIwLWy) for a working version of the above.

With a new version of TFF, is this fixed on your end?> Hi @FancyXun
> 
> I have been unable to reproduce this problem in our current setup (that is, with TFF 0.13.1). See the colab [here](https://colab.research.google.com/drive/15wfY6xv4LZa1oBf-mNjJXJfir1XIwLWy) for a working version of the above.
> 
> With a new version of TFF, is this fixed on your end?

Thanks!
Does this code in colab run in client side? Follow https://www.tensorflow.org/api_docs/python/tf/numpy_function , tf.numpy_function can not support  serializeTFF uses its own TensorFlow serialization mechanisms, and in fact the computations the TFF runtime invokes are essentially precisely backed by the same artifact that would run them on device (a TFF [computation.proto](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/proto/v0/computation.proto). If the code runs in the TFF runtime, it will run identically on device.

There are *likely* some things `numpy_function` could do that TFF's serialization mechanisms would fail to capture, but you would see this right away--the code would fail in the runtime. Likely any numpy arrays are baked into the graph as constants, for example.",3,2020-03-09 11:43:49,2020-07-08 04:22:17,2020-07-08 04:22:17
https://github.com/tensorflow/federated/issues/802,[],tf_nightly-1.15.0.dev20190805,"tf_nightly-1.15.0.dev20190805hello, I am looking for `tf_nightly-1.15.0.dev20190805` package on win64 platform. The link in README is not available. Could you please fix that link? thank you.@Zing22 thanks for bringing this to my attention, I'm taking a look at this.@Zing22 I have done some digging and the PyPI team has requested that TF delete old binary due to size limitations. See https://github.com/pypa/pypi-support/issues/224. As a result, these packages are no longer being distributed. Unfortunately, the TFF package was dependent on TF nightly package while code was being migrated from TF1 to TF2. This means that these versions of TFF are not easily pip installed.

I would recommend updating to the latest version of TFF, which does not have this issue.I have done some more digging and the exact requirements are...

- TFF `v0.9.0` -> tf-nightly
- TFF `v0.8.0` -> tf-nightly==1.15.0.dev20190805
- TFF `v0.7.0` -> tf-nightly
- TFF `v0.6.0` -> tf-nightly
- TFF `v0.5.0` -> tf-nightly

Therefore, `pip install tensorflow_federated==0.8.0` will fail. It's likely that the other TFF versions listed above will also fail (at run-time) because they will be pulling in the latest tf-nightly packages which use TF2 API.

Since `v0.10.0` we have specified out TF dependency against tensorflow fixing the major/minor version. Again, I am sorry that this is confusing and troublesome, this was done this way because of the transition from TF1 to TF2.",3,2020-03-09 02:53:05,2020-03-09 18:19:47,2020-03-09 16:37:49
https://github.com/tensorflow/federated/issues/799,[],Loosen dependency on TFA,"Loosen dependency on TFATensorFlow Addons is happy to be a part of the federated package. However, the current packaging restricts the version of TFA to be ~=0.6 which makes it so users can't keep an updated Addons library in their environment. 

This is especially problematic in Colab since TF-Federated is an installed package in the TF2.X environment. This means that by default a quite old version on TFA is installed and if the user tries to upgrade it will cause a dependency error with TF-Federated.

Would it be possible to pin to TFA >= 0.6 in your releases? There shouldn't be any breakage for the functionality that is being called within this repo.

Related: https://github.com/tensorflow/addons/issues/1104TFA has been broken in the past at latest release. I don't think it's generally a good idea to accept major/minor features in a release. This keeps a release of TFF stable, reproducible, and working. We do accept hot fixes, i.e. `tensorflow-addons~=0.7.0`.

See https://semver.org/ for more information.Hi @michaelreneer thats fair and understand why its needed. I'll close this issue since I think it's more of an issue within colab environments, but I have little visibility into whats going on there. As of today it looks as though TFA was updated and TF==2.1.0 but TFF==0.11 which I didn't think was compatible with TF 2.1

I'm not sure how this installation is being done since the installed addons should conflict with TFF

![image](https://user-images.githubusercontent.com/18154355/74973259-1dc69a00-53f1-11ea-8f9c-9bef03bda3d4.png)
@seanpmorgan I am also curious how the installation is being done.

FYI, we fixed an issue today that has allowed us to move our development dependencies to the latest version. I should be able to bump the version of TFA to latest release.

I 100% agree with you that the issue seems like a colab environment building issue. It seems like there should be some requirements.txt that define the colab environment and if a standard pip install fails you shouldn't accept a change to that requirements.txt.",3,2020-02-20 01:37:45,2020-02-22 00:02:47,2020-02-20 19:56:29
https://github.com/tensorflow/federated/issues/798,[],Learning parameters of each simulated device,"Learning parameters of each simulated deviceDoes tensorflow-federated support assigning different hyper-parameters(like batch-size or learning rate) for different simulated devices?
This doesn't seem like a bug/feature request for the code, but rather usage question. This might be better asked at https://stackoverflow.com/questions/tagged/tensorflow-federated. Could you ask there?",1,2020-02-18 16:24:15,2020-02-18 16:28:15,2020-02-18 16:28:14
https://github.com/tensorflow/federated/issues/797,[],"InvalidArgumentError: TypeError: endswith first arg must be bytes or a tuple of bytes, not str","InvalidArgumentError: TypeError: endswith first arg must be bytes or a tuple of bytes, not strHi, I am following the tutorial 'Federated Learning for Image Classification', but using my own dataset. I got this error, when running `iterative_process.next`.

here is my code:
```

par1_train_data_dir = './par1/train'
par2_train_data_dir = './par2/train'
input_shape = (img_height, img_width, 3)

img_gen = ImageDataGenerator(preprocessing_function=preprocess_input)

ds_par1 = tf.data.Dataset.from_generator(
    img_gen.flow_from_directory,  args=[par1_train_data_dir,(img_height,img_width)],
    output_types=(tf.float32, tf.float32), 
    output_shapes=([batch_size,img_height,img_width,3], [batch_size,num_classes])
)
ds_par2 = tf.data.Dataset.from_generator(
    img_gen.flow_from_directory,  args=[par2_train_data_dir,(img_height,img_width)],
    output_types=(tf.float32, tf.float32), 
    output_shapes=([batch_size,img_height,img_width,3], [batch_size,num_classes])
)

dataset_dict={}
dataset_dict['1'] = ds_par1
dataset_dict['2'] = ds_par2

def create_tf_dataset_for_client_fn(client_id):
    return dataset_dict[client_id]

train_data = tff.simulation.ClientData.from_clients_and_fn(['1','2'],create_tf_dataset_for_client_fn)

def make_federated_data(client_data, client_ids):
    return [client_data.create_tf_dataset_for_client(x)
          for x in client_ids]

federated_train_data = make_federated_data(train_data, train_data.client_ids)

images, labels = next(img_gen.flow_from_directory(par1_train_data_dir,batch_size=batch_size,target_size=(img_height,img_width)))

sample_batch = (images,labels)

def create_compiled_keras_model():
    pretrain_model = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', 
                                                input_tensor=tf.keras.layers.Input(shape=(img_height, 
                                                img_width, 3)), pooling=None)

    Inp = Input((img_height, img_width, 3))
    x = pretrain_model(Inp)

    x = GlobalAveragePooling2D()(x)
    x = Dense(1024, activation='relu')(x)
    predictions = Dense(10, activation='softmax')(x)
    model = Model(inputs=Inp, outputs=predictions,name='resnet50_transfer')    
    
    model.compile(
      loss=tf.keras.losses.categorical_crossentropy,
      optimizer=tf.keras.optimizers.SGD(learning_rate=0.02))
    return model

def model_fn():
    keras_model = create_compiled_keras_model()
    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)

iterative_process = tff.learning.build_federated_averaging_process(model_fn)
state = iterative_process.initialize()

NUM_ROUNDS = 11
for round_num in range(2, NUM_ROUNDS):
    state, metrics = iterative_process.next(state, federated_train_data)
    print('round {:2d}, metrics={}'.format(round_num, metrics))
```
I got the error
`InvalidArgumentError: TypeError: endswith first arg must be bytes or a tuple of bytes, not str
`
here is more information
```
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-48-b01b66dc0dcd> in <module>
      1 NUM_ROUNDS = 11
      2 for round_num in range(2, NUM_ROUNDS):
----> 3     state, metrics = iterative_process.next(state, federated_train_data)
      4     print('round {:2d}, metrics={}'.format(round_num, metrics))

~/miniconda3/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in __call__(self, *args, **kwargs)

~/miniconda3/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in pack_args(parameter_type, args, kwargs, context)

~/miniconda3/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in pack_args_into_anonymous_tuple(args, kwargs, type_spec, context)

~/miniconda3/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in ingest(self, arg, type_spec)
    627         intrinsic_defs.FEDERATED_MEAN.uri:
    628             self._federated_mean,
--> 629         intrinsic_defs.FEDERATED_BROADCAST.uri:
    630             self._federated_broadcast,
    631         intrinsic_defs.FEDERATED_COLLECT.uri:

~/miniconda3/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in to_representation_for_type(value, type_spec, callable_handler)
    239     else:
    240       return [
--> 241           to_representation_for_type(v, type_spec.member, callable_handler)
    242           for v in value
    243       ]

~/miniconda3/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in <listcomp>(.0)
    239     else:
    240       return [
--> 241           to_representation_for_type(v, type_spec.member, callable_handler)
    242           for v in value
    243       ]

~/miniconda3/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in to_representation_for_type(value, type_spec, callable_handler)
    198       if tf.executing_eagerly():
    199         return [
--> 200             to_representation_for_type(v, type_spec.element, callable_handler)
    201             for v in value
    202         ]

~/miniconda3/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in <listcomp>(.0)
    197     if isinstance(value, tf.data.Dataset):
    198       if tf.executing_eagerly():
--> 199         return [
    200             to_representation_for_type(v, type_spec.element, callable_handler)
    201             for v in value

~/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in __next__(self)
    620 
    621   def __next__(self):  # For Python 3 compatibility
--> 622     return self.next()
    623 
    624   def _next_internal(self):

~/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in next(self)
    664     """"""Returns a nested structure of `Tensor`s containing the next element.""""""
    665     try:
--> 666       return self._next_internal()
    667     except errors.OutOfRangeError:
    668       raise StopIteration

~/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)
    649             self._iterator_resource,
    650             output_types=self._flat_output_types,
--> 651             output_shapes=self._flat_output_shapes)
    652 
    653       try:

~/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)
   2671   _ctx = _context._context or _context.context()
   2672   if _ctx is not None and _ctx._thread_local_data.is_eager:
-> 2673     try:
   2674       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(
   2675         _ctx._context_handle, _ctx._thread_local_data.device_name,

~/miniconda3/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: TypeError: endswith first arg must be bytes or a tuple of bytes, not str
Traceback (most recent call last):

  File ""/root/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 464, in get_iterator
    self._next_id += 1

KeyError: 2


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/root/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py"", line 221, in __call__
    """"""

  File ""/root/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 585, in generator_py_func

  File ""/root/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 466, in get_iterator
    # NOTE(mrry): Explicitly create an array of `np.int64` because implicit

  File ""/root/miniconda3/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py"", line 540, in flow_from_directory
    interpolation=interpolation

  File ""/root/miniconda3/lib/python3.6/site-packages/keras_preprocessing/image/directory_iterator.py"", line 126, in __init__
    classes, filenames = res.get()

  File ""/root/miniconda3/lib/python3.6/multiprocessing/pool.py"", line 644, in get
    raise self._value

  File ""/root/miniconda3/lib/python3.6/multiprocessing/pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))

  File ""/root/miniconda3/lib/python3.6/site-packages/keras_preprocessing/image/utils.py"", line 216, in _list_valid_filenames_in_directory
    for root, fname in valid_files:

  File ""/root/miniconda3/lib/python3.6/site-packages/keras_preprocessing/image/utils.py"", line 172, in _iter_valid_files
    if fname.lower().endswith('.tiff'):

TypeError: endswith first arg must be bytes or a tuple of bytes, not str


	 [[{{node PyFunc}}]] [Op:IteratorGetNextSync]
```

my environment
```
tensorboard==1.15.0
tensorcache==0.4.2
tensorflow==1.15.2
tensorflow-addons==0.6.0
tensorflow-estimator==1.15.1
tensorflow-federated==0.4.0
```Hi @aslfu 

It seems that your TFF computations are being interpreted through the `ReferenceExecutor`, which was generally intended for unit tests, and the developers of TFF are not planning on doing much work to support it for real workloads.

I would first recommend upgrading your TFF  pip package, as I see you are on version `0.4.0`; TFF is a very fast-moving project with a *lot* of features and improvements in each release (and between releases as well, but that is another story). If you upgrade to `0.12.0`, the `local_executor` (as opposed to the reference executor) will be installed as the default execution context, and this error may (probably, should) silently disappear.

Otherwise (assuming we had this in `0.4.0`, can't quite recall...) try calling `tff.framework.set_default_executor(tff.framework.create_local_executor())` to install the local executor for interpreting your TFF computations; this should remove this error, and you should see a noticeable speedup as well.Hi @jkr26 
I've upgraded tf==2.1.0 and tff==0.12.0, the error disappeared, but I got another error.

It seems that the generator reaches the last batch and does not match the input shape. 

But ImageDataGenerator does not need to set `drop_remainder`. Is there anything wrong with my code?
 I also posted it on [stackoverflow](https://stackoverflow.com/questions/60181180/tensorflow-federated-typeerror-when-using-customized-dataset-and-model)
The question has an accepted answer. close this issue.

```
InvalidArgumentError:  ValueError: `generator` yielded an element of shape (50, 224, 224, 3) where an element of shape (64, 224, 224, 3) was expected.
Traceback (most recent call last):

  File ""/root/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""/root/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 825, in generator_py_func
    ""of shape %s was expected."" % (ret_array.shape, expected_shape))

ValueError: `generator` yielded an element of shape (50, 224, 224, 3) where an element of shape (64, 224, 224, 3) was expected.


	 [[{{node PyFunc}}]]
	 [[import/StatefulPartitionedCall_1/ReduceDataset]] [Op:__inference_wrapped_function_277930]

Function call stack:
wrapped_function
```

can you tell us the solution ?",3,2020-02-12 02:40:47,2020-02-28 12:41:31,2020-02-20 07:39:37
https://github.com/tensorflow/federated/issues/795,[],Text generation colab notebook throws error on keras evaluation,"Text generation colab notebook throws error on keras evaluationHello! I am running the Text generation colab notebook in Python 3 runtime with no hardware acceleration. Without making any modification to the notebook and simply running all cells, the notebook throws an error in the ""Fine-tune the model with Federated Learning"" section on line 21 where it calls the keras_evaluate function. 

https://colab.research.google.com/github/tensorflow/federated/blob/v0.11.0/docs/tutorials/federated_learning_for_text_generation.ipynb#scrollTo=vm_-PU8OFXpY

I have encountered a similar issue with my own work. I think tf keras, unlike TFF, does not accept preprocessed datasets where elements are OrderedDicts. The situation is fixed by switching the preprocess function to take a tuple as instead of an OrderedDict.

Full error is given below: 

Evaluating before training round 0
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-21-6e9921caacc8> in <module>()
     19 
     20 for round_num in range(NUM_ROUNDS):
---> 21   keras_evaluate(state, round_num)
     22   # N.B. The TFF runtime is currently fairly slow,
     23   # expect this to get significantly faster in future releases.

2 frames
<ipython-input-21-6e9921caacc8> in keras_evaluate(state, round_num)
     15   tff.learning.assign_weights_to_keras_model(keras_model, state.model)
     16   print('Evaluating before training round', round_num)
---> 17   keras_model.evaluate(example_dataset, steps=2)
     18 
     19 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v1.py in evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)
    883     self._check_call_args('evaluate')
    884 
--> 885     func = self._select_training_loop(x)
    886     return func.evaluate(
    887         self,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v1.py in _select_training_loop(self, inputs)
    563 
    564     # Experiment training loop with default DS path.
--> 565     if context.executing_eagerly() and self._experimental_run_tf_function:
    566       if self._in_multi_worker_mode():
    567         return training_distributed.DistributionMultiWorkerTrainingLoop(

AttributeError: 'Sequential' object has no attribute '_experimental_run_tf_function'Hi @r-o-s-h-a-n we just released version `v0.12.0`. The new version of the colab tutorial is [here](https://colab.research.google.com/github/tensorflow/federated/blob/v0.12.0/docs/tutorials/federated_learning_for_text_generation.ipynb). Our tests indicate this notebook does not raise errors, could you retry there and see if this issue still occurs?Hi @ZacharyGarrett , the notebook works now. Thanks!",2,2020-02-07 18:17:46,2020-02-12 23:03:38,2020-02-12 23:03:38
https://github.com/tensorflow/federated/issues/793,[],Use data generator to federated framework when train on large dataset ,"Use data generator to federated framework when train on large dataset Hi!

I was very glad to customize my own data and model to federated interfaces and the training converged!

Now I am confused about an issue that in an images classification task, the whole dataset is extreme large and it can't be stored in a single `federated_train_data ` nor be imported to memory for one time. So I need to load the dataset from the hard disk in batches to memory real-timely and use  Keras `model.fit_generator` instead of `model.fit` during training, the approach people use to deal with large data.

I suppose in `iterative_process` shown in image classification tutorial, the model is fitted on a fixed set of data. Is there any way to adjust the code to let it fit to a data generator?I have looked into the source codes but still quite confused. Would be incredibly grateful for any hints.Thanks for asking on SO! Dropping a [link here](https://stackoverflow.com/questions/59835749/implement-data-generator-in-federated-training/59865565#59865565), will close when there is an accepted answer.Hi! Thanks so much for your reply on SO! Unfortunately I still can't work it out. Do you know any way to change the model training process on local clients?@zm17943 could you take a look at the example in [this StackOverflow answer](https://stackoverflow.com/a/60266566/14692)? This does _not_ load all clients at once, only the clients in one round of competition are used at a time, and then the `tf.data.Dataset` also are only loading a subset of their entirely data, as-needed, for training.Thank you! I have looked into the StackOverflow answer, and adjusted my code to load each client at one time. However, I am still confused about the use of real-time data augmentation, for example, can I use tf.Data.Dataset.from_generator to load data into Federated?Hi, I tried to use tf.data.Dataset.from_generator to train federated model. But this step took forever. 
```
iterative_process.next
```
I tried to reduce the batch_size and trainable parameters to get it fast, but still. I was wondering how to diagnose the training process?I have exactly 

> Hi, I tried to use tf.data.Dataset.from_generator to train federated model. But this step took forever.
> 
> ```
> iterative_process.next
> ```
> 
> I tried to reduce the batch_size and trainable parameters to get it fast, but still. I was wondering how to diagnose the training process?

I have the exactly same issue!One thing that I might investigate here: try adding a `ds.take(1)` to your dataset constructors, or raise a `StopIteration` exception from your generator after yielding e.g. a single element.

If TFF is given an infinite `tf.data.Dataset`, it will likely reduce forever, keep pulling elements out of the dataset.

I am thinking this way because if your generator never raises `StopIteration`, I believe [`from_generator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) will treat the dataset as infinite. The docs just linked reference Python's [iterator protocol](https://docs.python.org/3/library/stdtypes.html#iterator-types), which states: 

>  If there are no further items, raise the StopIteration exception.

which therefore implies: if there is no `StopIteration`, there are further items.Yes, I am using `ImageDataGenerator` to create my generator, and it produces infinite batches. 
How to add `StopIteration`  in `ImageDataGenerator` ?  Or do I need to use another generator.We seem to be getting this question along multiple channel, so I think for ease of discoverability we would prefer to consolidate to stackoverflow. Please see the discussion [here](https://stackoverflow.com/questions/60153603/tensorflow-federated-why-my-iterative-process-unable-to-train-rounds/60517814#60517814), and open a question there if that does not suit your needs.

Thanks!",9,2020-01-21 06:56:19,2020-03-06 04:23:17,2020-03-06 04:23:17
https://github.com/tensorflow/federated/issues/790,[],Can't find tff research in released version,"Can't find tff research in released versionTFF research does not seem to be present in the released pip build of tff, despite being present in the code at that tag.   Is that intentional?

A file such as the following is present at tag 0.11.0 https://github.com/tensorflow/federated/blob/v0.11.0/tensorflow_federated/python/research/baselines/emnist/run_federated.py

However, if I try to import research (or run the code in the above file line by line) I get the error below.  (I tried this in a colab).


```
import tensorflow_federated as tff
print(tff.__version__)
from tensorflow_federated.python.research import *

0.11.0
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-7-20193a02f825> in <module>()
      1 import tensorflow_federated as tff
      2 print(tff.__version__)
----> 3 from tensorflow_federated.python.research import *

ModuleNotFoundError: No module named 'tensorflow_federated.python.research'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
""Open Examples"" button below.
---------------------------------------------------------------------------
```Correct, the `research/` directory is not part of the pip package. Code in the `research/` directory is intended for local development and tinkering, and hence is intentionally not part of the pip package.

The official, supported APIs available in the pip package are documented at https://www.tensorflow.org/federated/api_docs/python/tff. All other packages/module imports come without guarantee, they may break in future versions (this includes importing via `tff.python`).",1,2019-12-10 19:12:43,2019-12-10 19:20:01,2019-12-10 19:20:01
https://github.com/tensorflow/federated/issues/786,[],Making federated data!,"Making federated data!Hi,

I have selected a dataset from kaggle which is the ""FIFA 19 complete player dataset"". I wanted to use this dataset as a federated dataset. However, when I was trying to convert the dataset into a federated one, it seems not working for me. I have imported the dataset as Dataframe. 

Does anyone have any suggestions? How do I make this dataset into federated one as they showed in the tutorial section?

Thanks@Krypton3 questions about _how to use_ TFF are often better addressed at StackOverflow with the [TensorFlow Federated tag](https://stackoverflow.com/questions/tagged/tensorflow-federated).

Perhaps the following questions are similar to yours and the answers might apply?

- [Create a custom federated data set in TensorFlow Federated](https://stackoverflow.com/questions/55434004/create-a-custom-federated-data-set-in-tensorflow-federated)
- [Is there a reasonable way to create tff clients datat sets?](https://stackoverflow.com/questions/58004272/is-there-a-reasonable-way-to-create-tff-clients-datat-sets)

@jpgard is currently working on a tutorial for structured data in #782, it might be good to follow that issue as well.+1 to @ZacharyGarrett 's comment. @Krypton3 if you do post this on StackOverflow, perhaps you can share a link and I will see if I can answer the question on StackOverflow.Thank you. I will post this on StackOverflow and use the proper tag. I am working on federated for a few days and hopefully will do my ms thesis on it. However, I need to solve this issue in which I am stuck right now. 

ThanksFYI I think this question has been posted on SO here: https://stackoverflow.com/questions/58965488/how-to-create-federated-dataset-from-a-csv-fileSeems like given the activity on SO, this issue can be closed.",5,2019-11-20 02:45:24,2020-01-16 18:05:07,2020-01-16 18:05:07
https://github.com/tensorflow/federated/issues/785,[],Model with multiple inputs of varying shape,"Model with multiple inputs of varying shapeIs there an example of a tff model with multiple inputs that are not of same shape? I tried the following code snippet to create a tf.data object but it is not working as tf.data api doesn't allow stacking tensors of varying ranks. 

```
def map_fn(example):
  return {'x': [tf.reshape(example['pixels'], [-1]), 
      tf.reshape(np.zeros((32,32) ,dtype=np.float32), [-1])], 'y': example['label']}

def client_data(n):
  ds = source.create_tf_dataset_for_client(source.client_ids[n])
  return ds.repeat(10).map(map_fn).shuffle(500).batch(20)

source, _ = tff.simulation.datasets.emnist.load_data()
client_data(1)
```

this is the raised error: 

`ValueError: Value [<tf.Tensor 'Reshape:0' shape=(784,) dtype=float32>, <tf.Tensor 'Reshape_1:0' shape=(1024,) dtype=float32>] is not convertible to a tensor with dtype <dtype: 'float32'> and shape (2, None).`

This could also be useful:

```
def map_fn(example):
  return {""x"": (tf.reshape(example['pixels'], [-1]), 
                np.zeros((32,32)).astype(np.float32)), 
          
          ""y"": [example['label']]}

def client_data(n):
  ds = source.create_tf_dataset_for_client(source.client_ids[n])
  return ds.repeat(1).map(map_fn).shuffle(500).batch(5)

source, _ = tff.simulation.datasets.emnist.load_data()
train_data = [client_data(n) for n in range(2)]
batch = tf.nest.map_structure(lambda x: x.numpy(), iter(train_data[0]).next())

def model_fn():
  finput = tf.keras.layers.Input((784,), name = ""a"")
  oinput = tf.keras.layers.Input((32,32), name = ""b"")

  ox = tf.keras.layers.Flatten()(oinput)
  concat = tf.keras.layers.Concatenate()([finput, ox])
  output = tf.keras.layers.Dense(10, tf.nn.softmax, 
                                 kernel_initializer='zeros')(concat)
  model = tf.keras.models.Model({""a"": finput, ""b"": oinput}, output)
  model.compile(
      loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.SGD(0.02),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
  return tff.learning.from_compiled_keras_model(model, batch)

trainer = tff.learning.build_federated_averaging_process(model_fn)
```
Error: 
```
ValueError: Shapes must be equal rank, but are 2 and 3
From merging shape 0 with other shapes. for 'Shape/packed' (op: 'Pack') 
with input shapes: [?,784], [?,32,32].
```
In the first example, the value for the `x` key in the `dict` is a `list` of tensors. I believe TensorFlow dataset is going to try and concatenate/stack these into a single tensor, but this will fail because they are not compatible shapes. This is fixed in the second code block by using a `tuple` instead of a `list` for the value of `x`.  This question is more of _how to use_ TensorFlow or TFF and would probably be best asked on StackOverflow so the community can benefit from the discussion even after this issue is closed. [[tensorflow-federated]](https://stackoverflow.com/questions/tagged/tensorflow) and [[tensorflow]](https://stackoverflow.com/questions/tagged/tensorflow) tags have been created for this purpose.

In the second example, it looks like there is an issue with [this line](https://github.com/tensorflow/federated/blob/547eeae6b65f6951504dfb49615a782306a6e097/tensorflow_federated/python/learning/keras_utils.py#L440) in TFF when multiple inputs have been specified. Probably this line needs to be replaced with something like `tf.shape(tf.nest.flatten(inputs)[0])[0]`.Thank you for prompt response. Do you think the issue mentioned in second example will be fixed anytime soon?Yup, I'm hoping to have a PR merged today. Currently running through continuous integration tests.Thank you.Can I use it in Google Colab now? tff installation via pip package is also updated?Sorry, the pip package has not been updated so this isn't usable in Google Colab yet. That will require a new release, and TFF typically release every few weeks. There maybe a release in the next few days, I'll check and follow-up.Hey @ZacharyGarrett, 

Any updates about the release?@aqibsaeed apologies for the delay, thanks for checking in. We we waiting to get a01eadc1b33b31947ec4540689a63e777c933205 merged (which is good to go now). Someone from the team will look at releasing a pip package today (though may take until tomorrow).Thank you @ZacharyGarrett Hi @aqibsaeed , I met the same problem when I try to use multiple inputs in keras model. 
I am using tff 0.6.0 now. 
May I know how did you solve it and in which version is the problem fixed?

Many thanks!",10,2019-11-13 12:40:07,2020-04-19 10:50:27,2019-11-13 18:04:45
https://github.com/tensorflow/federated/issues/780,[],FileCheckpointManager causes exception on windows,"FileCheckpointManager causes exception on windowsHi,

running FileCheckpointManager using a windows machine will cause an exception at line https://github.com/tensorflow/federated/blob/48d5e4860f3349086d018b9553f730c87def2429/tensorflow_federated/python/research/utils/checkpoint_manager.py#L88 :

`re.error: bad escape \c at position`

this happened because `os.path.join` creates a string that contains `\\ckpt_`

(I bypassed https://github.com/tensorflow/federated/issues/779 by commenting out emnist import, and am using the checkpoint_manager by copy-pasting the code. So I just hardcoded a solution for my project, but it is an issue with that code in this repo)@amitport Can you provide a few pieces of information...
* Python version
* Is `path` == '\\ckpt_'? If not can you provide the full value of `path`?
* python 3.6
* path has a prefix but the error happen because how `os.path.join` is adding the default `ckpt`

just run the following on windows to get an exception
```
path = os.path.join(anyPath, 'ckpt')
re.compile(r'{}([0-9]+)$'.format(path)) 
```@amitport Can you provide the value you are using for `anyPath` and what is `path` after you call join?

```python
path = os.path.join(anyPath, 'ckpt')
print(path)
```

Thanksit really does not matter since the exception happen on the `\\c`: `re.error: bad escape \c at position`

in any case mine is something like: `C:\\my_folder`

(the result of `path = os.path.abspath(__file__)` on windows)This issue is more subtle than you are making it out to be (or I am confused) and this is why I am asking you for specifics.

This is what we want:
```shell
>>> re.compile(r'c:\\my_folder')
re.compile('c:\\\\my_folder')
```

All the following generate the error you are experiencing:
```shell
>>> re.compile(r'c:\my_path')
...
re.error: bad escape \m at position 2
>>> re.compile('c:\\my_path')
...
re.error: bad escape \m at position 2
>>> re.compile('c:\my_path')
...
re.error: bad escape \m at position 2
```

Therefore I believe that `os.path.join` returning a string that has an escaped slash is correct. Instead I believe the issue is in combining Python raw strings with Python strings.

```shell
>>> re.compile(r'{}'.format(r'c:\\my_path'))
re.compile('c:\\\\my_path')
>>> re.compile(r'{}'.format('c:\\my_path'))
...
re.error: bad escape \m at position 2
```

I believe the solution is something like...
```shell
>>> re.compile(r'{}'.format('c:\\my_path'.encode('unicode-escape').decode()))
re.compile('c:\\\\my_path')
```

Note this is a Python 3 only fix. I'll follow up with a fix after a little more testing.The two lines I quoted in always cause the issue on windows and it's easy to reproduce. I didn't mean that it is easy to solve or that `path.join` directly causes the issue, but having `\\c` in the next line does.

anyway thanks for the effort",6,2019-11-06 10:29:12,2020-06-15 18:23:05,2020-06-15 18:23:05
https://github.com/tensorflow/federated/issues/779,[],windows support,"windows supportcurrently it is impossible to install through pip or build this package in windows

the core issue is
https://github.com/tensorflow/custom-op/issues/24
which blocks
https://github.com/tensorflow/addons/issues/173
which blocks
various tensorflow-addons windows issues 
which blocks
tensorflow-addons use by the emnist simulation here

the bottom line is that I just want to use tensorflow-federated on windows and I think its windows support should be traced here and perhaps an alternative solution could be provided since I'm not sure anyone is working on the core issue.
(is it possible to avoid or extract tensorflow-addons usage?)
Hi @amitport,

I think we unfortunately are simply unlikely to attempt to add windows support in the near future. There is some nontrivial setup that would have to happen on our side, and it's not currently considered a high priority.

That said, a solution like [virtualbox](https://www.virtualbox.org/) or [docker](https://www.docker.com/) (maybe even [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install-win10)?) should allow you to run TFF on a Windows machine, though there may be some slightly gnarly edges.> That said, a solution like virtualbox or docker (maybe even Windows Subsystem for Linux?) should allow you to run TFF on a Windows machine, though there may be some slightly gnarly edges.

Is there a way which includes GPU use?resolved for me> resolved for me

How did you resolve the problem? I have encountered the same issue here. Could you please give me some suggestions?It just worked for on clean penv install today with tensorflow2.1. I didn't do anything on my part.",5,2019-10-24 13:46:49,2020-05-14 03:55:37,2020-05-13 06:00:04
https://github.com/tensorflow/federated/issues/776,[],How does tff allocate processes or threads?,"How does tff allocate processes or threads?How does tff allocate processes or threads in the code ?If it is parallel, then there will definitely be code to create a process or thread. I looked at the code but didn't see the content, so I was confused how it distributed the data to different clients and trained independently.
Thank you very muchThis question is may be better suited fro the `tensorflow-federated` tag on StackOverflow (https://stackoverflow.com/questions/tagged/tensorflow-federated); TFF uses GitHub issues for bugs and feature requests, and StackOverflow for questions & answers.

 The default simulation execution does not run multi-threaded; this executor is called the _reference executor_ ([code link](https://github.com/tensorflow/federated/blob/8c904235dbf00431a95b5b1c64017aecf14517e6/tensorflow_federated/python/core/impl/reference_executor.py#L552)) which is very straight forward and less performant by design; it is optimizing for _correctness_. This processes each client serially on the local machine.

There is also a _concurrent executor_ ([code link](https://github.com/tensorflow/federated/blob/8c904235dbf00431a95b5b1c64017aecf14517e6/tensorflow_federated/python/core/impl/concurrent_executor.py#L26)) which runs using Python's `asyncio` event loop. Most users will want to create an executor stack with `tff.framework.create_local_executor()`, there are example usages in [this tutorial](https://github.com/tensorflow/federated/blob/v0.10.1/docs/tutorials/simulations.ipynb). This can process multiple clients in parallel on a single machine.

Good progress has been made on a _remote executor_, which will allowing running simulations in a distributed fashion on multiple machine; stay tuned.

Note: this is all for _simulating_ federated learning.",1,2019-10-22 07:56:28,2019-11-07 13:01:03,2019-11-07 13:00:59
https://github.com/tensorflow/federated/issues/774,[],NotFoundError on import after pip install,"NotFoundError on import after pip installI was using an older version of tff and tensorflow 1.x, but I am attempting to update to the newest version of tff and tensorflow 2.x.

I followed the pip installation instructions [here](https://github.com/tensorflow/federated/blob/master/docs/install.md#install-tensorflow-federated-using-pip), and set up and activated a virtual environment.  However, running the command in the optional step 4, to test tff, leads to the following output:

```
$ python -c ""import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())""

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_federated/__init__.py"", line 62, in <module>
    from tensorflow_federated.python import simulation
  File ""/Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_federated/python/simulation/__init__.py"", line 23, in <module>
    from tensorflow_federated.python.simulation import datasets
  File ""/Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_federated/python/simulation/datasets/__init__.py"", line 21, in <module>
    from tensorflow_federated.python.simulation.datasets import emnist
  File ""/Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_federated/python/simulation/datasets/emnist.py"", line 29, in <module>
    import tensorflow_addons.image as tfa_image
  File ""/Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_addons/__init__.py"", line 21, in <module>
    from tensorflow_addons import activations
  File ""/Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_addons/activations/__init__.py"", line 21, in <module>
    from tensorflow_addons.activations.gelu import gelu
  File ""/Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_addons/activations/gelu.py"", line 25, in <module>
    get_path_to_datafile(""custom_ops/activations/_activation_ops.so""))
  File ""/Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/load_library.py"", line 57, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_addons/custom_ops/activations/_activation_ops.so, 6): Symbol not found: __ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceEN4absl14lts_2019_08_0811string_viewEPb
  Referenced from: /Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_addons/custom_ops/activations/_activation_ops.so (which was built for Mac OS X 10.14)
  Expected in: /Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
 in /Users/jpgard/Documents/github/federated/venv/lib/python3.6/site-packages/tensorflow_addons/custom_ops/activations/_activation_ops.so
```

If I comment out the dataset imports (`from tensorflow_federated.python.simulation.datasets import ...`) in tensorflow_federated.python.simulation.datasets.__init__.py`, the hello world command works fine.

Operating system: macOS High Sierra 10.13.6

Output of `pip freeze` inside venv:
```
absl-py==0.8.1
astor==0.8.0
attrs==18.2.0
cachetools==3.1.1
enum34==1.1.6
gast==0.2.2
google-pasta==0.1.7
grpcio==1.22.1
h5py==2.10.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
Markdown==3.1.1
mpmath==1.1.0
numpy==1.17.2
opt-einsum==3.1.0
portpicker==1.3.1
protobuf==3.10.0
scipy==1.3.1
six==1.12.0
tb-nightly==2.1.0a20191016
tensorflow-federated==0.9.0
tensorflow-model-optimization==0.1.3
tensorflow-privacy==0.1.0
termcolor==1.1.0
tf-estimator-nightly==2.0.0.dev2019101601
tf-nightly==2.1.0.dev20191015
tfa-nightly==0.7.0.dev20191010
Werkzeug==0.16.0
wrapt==1.11.2
```Thank you for providing all the context, including the `pip freeze` output!

Commenting out datasets is likely to work because it stops pulling in the TensorFlow Addons (`tfa-nightly`) package, which is used for image transformations in the infinite EMNIST dataset.

For starters, could you try upgrading `tfa-nightly` package (`pip install --upgrade tfa-nightly`)? There looks to be a new package. We've seen similar problems with this package a few days ago, but internal continuous integration is passing this morning.

I also noticed the stack trace says `../_activation_ops.so` **`(which was built for Mac OS X 10.14)`**, but I believe the system is `10.13`? I'm not familiar with macOS, any idea if this might have some incompatibility?Hm, yes, the `tfa-nightly` seems to have been the issue -- no problem when rebuilding from a clean venv now. Thanks!",2,2019-10-16 20:05:27,2019-10-17 16:57:04,2019-10-17 16:57:03
https://github.com/tensorflow/federated/issues/771,[],Unable to build from source on macOS,"Unable to build from source on macOSPlatform specs:
OS: macOS Mojave 10.14.5
Python: 3.7.4
Build: From Source(d02309a)

packages | version
--- | --- 
six|                           1.12.0
tb-nightly|                    2.1.0a20191013
tensorflow-model-optimization| 0.1.3
tensorflow-privacy|            0.1.0
tf-estimator-nightly|          2.0.0.dev2019101401
tf-nightly|                    2.1.0.dev20191014
tfa-nightly|                   0.7.0.dev20191010

Followed the instructions in [building from source](https://github.com/tensorflow/federated/blob/master/docs/install.md#build-the-tensorflow-federated-pip-package).

The following tests fail:
```
//tensorflow_federated/python/core/impl:intrinsic_bodies_test           TIMEOUT in 61.0s
  /private/var/tmp/_bazel_rishavchourasia/26093410bba57464b779c2b8c15ff77a/execroot/org_tensorflow_federated/bazel-out/darwin-opt/testlogs/tensorflow_federated/python/core/impl/intrinsic_bodies_test/test.log
//tensorflow_federated/python/core/utils:federated_aggregations_test    TIMEOUT in 61.0s
  /private/var/tmp/_bazel_rishavchourasia/26093410bba57464b779c2b8c15ff77a/execroot/org_tensorflow_federated/bazel-out/darwin-opt/testlogs/tensorflow_federated/python/core/utils/federated_aggregations_test/test.log
//tensorflow_federated/python/research/robust_aggregation:robust_federated_aggregation_test FAILED in 5.6s
  /private/var/tmp/_bazel_rishavchourasia/26093410bba57464b779c2b8c15ff77a/execroot/org_tensorflow_federated/bazel-out/darwin-opt/testlogs/tensorflow_federated/python/research/robust_aggregation/robust_federated_aggregation_test/test.log
//tensorflow_federated/python/research/simple_fedavg:simple_fedavg_test  FAILED in 5.0s
  /private/var/tmp/_bazel_rishavchourasia/26093410bba57464b779c2b8c15ff77a/execroot/org_tensorflow_federated/bazel-out/darwin-opt/testlogs/tensorflow_federated/python/research/simple_fedavg/simple_fedavg_test/test.log
//tensorflow_federated/python/research/utils:training_loops_test         FAILED in 5.0s
  /private/var/tmp/_bazel_rishavchourasia/26093410bba57464b779c2b8c15ff77a/execroot/org_tensorflow_federated/bazel-out/darwin-opt/testlogs/tensorflow_federated/python/research/utils/training_loops_test/test.log
//tensorflow_federated/python/simulation/datasets:emnist_test            FAILED in 4.0s
  /private/var/tmp/_bazel_rishavchourasia/26093410bba57464b779c2b8c15ff77a/execroot/org_tensorflow_federated/bazel-out/darwin-opt/testlogs/tensorflow_federated/python/simulation/datasets/emnist_test/test.log

Executed 89 out of 89 tests: 83 tests pass and 6 fail locally.
INFO: Build completed, 6 tests FAILED, 404 total actions
```

Hello World [snippet](https://github.com/tensorflow/federated/blob/master/docs/install.md#10-test-tensorflow-federated) also fails.
```
(venv)  rishavchourasia@Rishavs-MacBook-Pro  /tmp/project  python -c ""import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_federated/__init__.py"", line 62, in <module>
    from tensorflow_federated.python import simulation
  File ""/private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_federated/python/simulation/__init__.py"", line 23, in <module>
    from tensorflow_federated.python.simulation import datasets
  File ""/private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_federated/python/simulation/datasets/__init__.py"", line 21, in <module>
    from tensorflow_federated.python.simulation.datasets import emnist
  File ""/private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_federated/python/simulation/datasets/emnist.py"", line 29, in <module>
    import tensorflow_addons.image as tfa_image
  File ""/private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_addons/__init__.py"", line 21, in <module>
    from tensorflow_addons import activations
  File ""/private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_addons/activations/__init__.py"", line 21, in <module>
    from tensorflow_addons.activations.gelu import gelu
  File ""/private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_addons/activations/gelu.py"", line 25, in <module>
    get_path_to_datafile(""custom_ops/activations/_activation_ops.so""))
  File ""/private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/load_library.py"", line 57, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_addons/custom_ops/activations/_activation_ops.so, 6): Symbol not found: __ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceEN4absl14lts_2019_08_0811string_viewEPb
  Referenced from: /private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_addons/custom_ops/activations/_activation_ops.so
  Expected in: /private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib
 in /private/tmp/project/venv/lib/python3.7/site-packages/tensorflow_addons/custom_ops/activations/_activation_ops.so
```Hi @Rishav1,

We have seen issues with some of the earlier `tf-nightly` and `tfa-nightly` versions. Would you be up for trying to see if updating to a newer version has the same issue?

We have seen `tfa-nightly-0.7.0.dev20191017` but `tfa-nightly-0.7.0.dev20191010` has failed in the fast.Issue seems to be resolved in `tfa-nightly-0.7.0.dev20191031`. All bazel tests also passed except a single test timing out.

```
//tensorflow_federated/python/core/impl:intrinsic_bodies_test           TIMEOUT in 61.0s
  /private/var/tmp/_bazel_rishavchourasia/26093410bba57464b779c2b8c15ff77a/execroot/org_tensorflow_federated/bazel-out/darwin-opt/testlogs/tensorflow_federated/python/core/impl/intrinsic_bodies_test/test.log

Executed 89 out of 89 tests: 88 tests pass and 1 fails locally.
INFO: Build completed, 1 test FAILED, 404 total actions
```",2,2019-10-14 12:32:51,2019-11-01 02:54:00,2019-11-01 02:54:00
https://github.com/tensorflow/federated/issues/770,[],cannot import name 'computation_pb2',"cannot import name 'computation_pb2'I tried to connect pycharm to the remote server. The connection was successful. I used the virtual environment tff3. The jupyter remote connection to the virtual environment can make the code run successfully. However, the pycharm failed.
I created a new py file and entered the following statement. 
```
import tensorflow_federated as tff
print(tff.federateprint(tff.federated_computation(lambda: 'Hello World')()))
```
The error message is as follows:

```
ssh://liuxy@*.*.*.*:22/home/liuxy/anaconda3/envs/tff3/bin/python3 -u /home/liuxy/tmp/pycharm_project_567/docs/tutorials/image_classification.py
Traceback (most recent call last):
  File ""/home/liuxy/tmp/pycharm_project_567/docs/tutorials/image_classification.py"", line 1, in <module>
    import tensorflow_federated as tff
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/__init__.py"", line 23, in <module>
    from tensorflow_federated.python.core.api.computation_base import Computation
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/python/core/__init__.py"", line 21, in <module>
    from tensorflow_federated.python.core.api.computation_base import Computation
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/python/core/api/__init__.py"", line 29, in <module>
    from tensorflow_federated.python.core.api.computations import federated_computation
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/python/core/api/computations.py"", line 21, in <module>
    from tensorflow_federated.python.core.impl import computation_wrapper_instances
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/python/core/impl/computation_wrapper_instances.py"", line 22, in <module>
    from tensorflow_federated.python.core.impl import computation_impl
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/python/core/impl/computation_impl.py"", line 21, in <module>
    from tensorflow_federated.proto.v0 import computation_pb2 as pb
ImportError: cannot import name 'computation_pb2' from 'tensorflow_federated.proto.v0' (/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/proto/v0/__init__.py)
```
Thank you very muchHi @phalangee 

We are familiar with this error; this is likely due to trying to launch or run the interpreter from within the same directory in which you have a copy of the source. If you move out of this directory, it should all work. This is because the Python interpreter is looking into the raw source of our `proto` files, which generate Python when TFF is compiled or run with `bazel`, but are not themselves Python.@jkr26  Thank you very much！  it worked for me !Awesome! Happy to help!",3,2019-10-09 14:13:10,2019-10-10 03:36:26,2019-10-10 03:36:26
https://github.com/tensorflow/federated/issues/769,[],Unable to run tutorial code in Python 3,"Unable to run tutorial code in Python 3Hello, I'm receiving the error below when I attempt to run the code from the TFF front page. It's probably worth mentioning that it seems to run fine with python 2.7, but not 3.6.

Tutorial code:
```import tensorflow as tf
tf.compat.v1.enable_v2_behavior()
import tensorflow_federated as tff

source, _ = tff.simulation.datasets.emnist.load_data()
def client_data(n):
  return source.create_tf_dataset_for_client(source.client_ids[n]).map(
      lambda e: {
          'x': tf.reshape(e['pixels'], [-1]),
          'y': e['label'],
  }).repeat(10).batch(20)

train_data = [client_data(n) for n in range(3)]

sample_batch = tf.nest.map_structure(
    lambda x: x.numpy(), iter(train_data[0]).next())

def model_fn():
 model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, tf.nn.softmax, input_shape=(784,),
                          kernel_initializer='zeros')
 ])
 model.compile(
     loss=tf.keras.losses.SparseCategoricalCrossentropy(),
     optimizer=tf.keras.optimizers.SGD(0.1),
     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
 return tff.learning.from_compiled_keras_model(model, sample_batch)

trainer = tff.learning.build_federated_averaging_process(model_fn)
state = trainer.initialize()
for _ in range(5):
  state, metrics = trainer.next(state, train_data)
  print (metrics.loss)
```

Error:
```    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/thai/repos/ehr-project/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2614, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""/Users/thai/repos/ehr-project/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 874, in wrapper
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in converted code:
    relative to /Users/thai/repos/ehr-project/venv/lib/python3.6/site-packages:

    tensorflow_federated/python/tensorflow_libs/tensor_utils.py:115 zero_all_if_any_non_finite
        if all_finite:
    tensorflow_core/python/framework/ops.py:762 __bool__
        self._disallow_bool_casting()
    tensorflow_core/python/framework/ops.py:531 _disallow_bool_casting
        ""using a `tf.Tensor` as a Python `bool`"")
    tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled
        "" decorating it directly with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
```

requirements.txt:
```
tf-nightly==1.15.0.dev20190805
tensorflow_federated
tensorflow-estimator==1.15.1
setuptools
grpcio~=1.22.0
```

Pip freeze:
```
absl-py==0.8.0
astor==0.8.0
attrs==18.2.0
cachetools==3.1.1
enum34==1.1.6
gast==0.3.2
google-pasta==0.1.7
grpcio==1.22.1
h5py==2.10.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
Markdown==3.1.1
numpy==1.17.2
opt-einsum==3.1.0
portpicker==1.3.1
protobuf==3.10.0
six==1.12.0
tb-nightly==1.15.0a20190911
tensorflow-estimator==1.15.1
tensorflow-federated==0.8.0
tensorflow-model-optimization==0.1.3
termcolor==1.1.0
tf-estimator-nightly==2.0.0.dev2019100301
tf-nightly==1.15.0.dev20190805
Werkzeug==0.16.0
wrapt==1.11.2
```

Python version
```Python 3.6.5```Hi @pthai,

Can you advise when line from TFF threw this error? In particular, does it come from `build_federated_averaging_process` or invoking the returned function inside the loop? AutoGraph should be tracking this and it should be *somewhere* (perhaps non-obvious) in the stacktrace.

An alternative option here (probably the easier one) is to simply try and run the notebook code as it exists after [this commit](https://github.com/tensorflow/federated/commit/7f6cd06e0032e8afdc8584fde5b941761bd1e97d) instead; given the recent release of TF2 we just moved our pip package to pull in TF2 instead of 1.15, and converted our code to be fully TF2-compliant. The message about `tf.function` makes me think that this may be an artifact of TF1-style code in a TF2-style world.

Hope this helps!

KeithPinging, is this still an issue?Closing as unable to repro",3,2019-10-04 04:47:22,2019-12-06 20:01:11,2019-12-06 20:01:10
https://github.com/tensorflow/federated/issues/767,[],"TypeError : Expected a TensorFlow computation, found intrinsic.","TypeError : Expected a TensorFlow computation, found intrinsic.Hi, I follow the guidance in the TensorFlow Federated. After Installation, I meet the error : `TypeError : Expected a TensorFlow computation, found intrinsic.` in **federated_learning_for_image_classification.ipynb**

The problem raises when I try to run the code : 

```
state = iterative_process.initialize()
```

I found that there is no helpful answer while Googling the error, so I have the question : **why this error raises?** And **are there any idea to solve the problem ?**

Here is possible reasons (I guess) :
1. I have install the gast in 0.2.2 version  to sovle the warning `Module 'gast' has no attribute 'Num'`
2. I just follow the guidance in `How to Install in Ubuntu` cause I think compiling in Ubuntu maybe meaningless

Here is the log:
```

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-25-0c0dcd96a774> in <module>
----> 1 state = iterative_process.initialize()

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in __call__(self, *args, **kwargs)
    628     context = self._context_stack.current
    629     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 630     return context.invoke(self, arg)
    631 
    632 

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/execution_context.py in invoke(self, comp, arg)
    118   def invoke(self, comp, arg):
    119     return asyncio.get_event_loop().run_until_complete(
--> 120         _invoke(self._executor, comp, arg))

~/anaconda3/lib/python3.7/asyncio/base_events.py in run_until_complete(self, future)
    582             raise RuntimeError('Event loop stopped before Future completed.')
    583 
--> 584         return future.result()
    585 
    586     def stop(self):

~/anaconda3/lib/python3.7/asyncio/futures.py in result(self)
    176         self.__log_traceback = False
    177         if self._exception is not None:
--> 178             raise self._exception
    179         return self._result
    180 

~/anaconda3/lib/python3.7/asyncio/tasks.py in __step(***failed resolving arguments***)
    221                 # We use the `send` method directly, because coroutines
    222                 # don't have `__iter__` and `__next__` methods.
--> 223                 result = coro.send(None)
    224             else:
    225                 result = coro.throw(exc)

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/execution_context.py in _invoke(executor, comp, arg)
     89     py_typecheck.check_type(arg, executor_value_base.ExecutorValue)
     90   comp = await executor.create_value(comp)
---> 91   result = await executor.create_call(comp, arg)
     92   py_typecheck.check_type(result, executor_value_base.ExecutorValue)
     93   result_val = _unwrap(await result.compute())

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in _evaluate(self, comp, scope)
    376         arg = None
    377       return await self.create_call(
--> 378           LambdaExecutorValue(comp.call.function, scope=scope), arg=arg)
    379     elif which_computation == 'selection':
    380       which_selection = comp.selection.WhichOneof('selection')

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    259         arg_type = type_utils.get_argument_type(arg.type_signature)
    260         type_utils.check_assignable_from(param_type, arg_type)
--> 261         return await self.create_call(comp, await self.create_call(arg))
    262     else:
    263       py_typecheck.check_none(arg)

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in _evaluate(self, comp, scope)
    352           comp,
    353           type_utils.get_function_type(
--> 354               type_serialization.deserialize_type(comp.type))))
    355     elif which_computation == 'lambda':
    356 

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/caching_executor.py in create_value(self, value, type_spec)
    240                                  target_future)
    241       self._cache[identifier] = cached_value
--> 242     target_value = await cached_value.target_future
    243     type_utils.check_assignable_from(type_spec, target_value.type_signature)
    244     return cached_value

~/anaconda3/lib/python3.7/asyncio/futures.py in __await__(self)
    261         if not self.done():
    262             raise RuntimeError(""await wasn't used with future"")
--> 263         return self.result()  # May raise too.
    264 
    265     __iter__ = __await__  # make compatible with 'yield from'.

~/anaconda3/lib/python3.7/asyncio/futures.py in result(self)
    176         self.__log_traceback = False
    177         if self._exception is not None:
--> 178             raise self._exception
    179         return self._result
    180 

~/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py in run_code(self, code_obj, result, async_)
   3323                     await eval(code_obj, self.user_global_ns, self.user_ns)
   3324                 else:
-> 3325                     exec(code_obj, self.user_global_ns, self.user_ns)
   3326             finally:
   3327                 # Reset our crash handler in place

<ipython-input-24-45524444b5c9> in <module>
----> 1 state = iterative_process.initialize()
      2 
      3 state, metrics = iterative_process.next(state, federated_train_data)

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in __call__(self, *args, **kwargs)
    628     context = self._context_stack.current
    629     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 630     return context.invoke(self, arg)
    631 
    632 

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/execution_context.py in invoke(self, comp, arg)
    118   def invoke(self, comp, arg):
    119     return asyncio.get_event_loop().run_until_complete(
--> 120         _invoke(self._executor, comp, arg))

~/anaconda3/lib/python3.7/asyncio/base_events.py in run_until_complete(self, future)
    582             raise RuntimeError('Event loop stopped before Future completed.')
    583 
--> 584         return future.result()
    585 
    586     def stop(self):

~/anaconda3/lib/python3.7/asyncio/futures.py in result(self)
    176         self.__log_traceback = False
    177         if self._exception is not None:
--> 178             raise self._exception
    179         return self._result
    180 

~/anaconda3/lib/python3.7/asyncio/tasks.py in __step(***failed resolving arguments***)
    221                 # We use the `send` method directly, because coroutines
    222                 # don't have `__iter__` and `__next__` methods.
--> 223                 result = coro.send(None)
    224             else:
    225                 result = coro.throw(exc)

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/execution_context.py in _invoke(executor, comp, arg)
     89     py_typecheck.check_type(arg, executor_value_base.ExecutorValue)
     90   comp = await executor.create_value(comp)
---> 91   result = await executor.create_call(comp, arg)
     92   py_typecheck.check_type(result, executor_value_base.ExecutorValue)
     93   result_val = _unwrap(await result.compute())

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in _evaluate(self, comp, scope)
    376         arg = None
    377       return await self.create_call(
--> 378           LambdaExecutorValue(comp.call.function, scope=scope), arg=arg)
    379     elif which_computation == 'selection':
    380       which_selection = comp.selection.WhichOneof('selection')

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    259         arg_type = type_utils.get_argument_type(arg.type_signature)
    260         type_utils.check_assignable_from(param_type, arg_type)
--> 261         return await self.create_call(comp, await self.create_call(arg))
    262     else:
    263       py_typecheck.check_none(arg)

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in _evaluate(self, comp, scope)
    352           comp,
    353           type_utils.get_function_type(
--> 354               type_serialization.deserialize_type(comp.type))))
    355     elif which_computation == 'lambda':
    356 

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/caching_executor.py in create_value(self, value, type_spec)
    240                                  target_future)
    241       self._cache[identifier] = cached_value
--> 242     target_value = await cached_value.target_future
    243     type_utils.check_assignable_from(type_spec, target_value.type_signature)
    244     return cached_value

~/anaconda3/lib/python3.7/asyncio/futures.py in __await__(self)
    261         if not self.done():
    262             raise RuntimeError(""await wasn't used with future"")
--> 263         return self.result()  # May raise too.
    264 
    265     __iter__ = __await__  # make compatible with 'yield from'.

~/anaconda3/lib/python3.7/asyncio/futures.py in result(self)
    176         self.__log_traceback = False
    177         if self._exception is not None:
--> 178             raise self._exception
    179         return self._result
    180 

~/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py in run_code(self, code_obj, result, async_)
   3323                     await eval(code_obj, self.user_global_ns, self.user_ns)
   3324                 else:
-> 3325                     exec(code_obj, self.user_global_ns, self.user_ns)
   3326             finally:
   3327                 # Reset our crash handler in place

<ipython-input-23-45524444b5c9> in <module>
----> 1 state = iterative_process.initialize()
      2 
      3 state, metrics = iterative_process.next(state, federated_train_data)

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in __call__(self, *args, **kwargs)
    628     context = self._context_stack.current
    629     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 630     return context.invoke(self, arg)
    631 
    632 

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/execution_context.py in invoke(self, comp, arg)
    118   def invoke(self, comp, arg):
    119     return asyncio.get_event_loop().run_until_complete(
--> 120         _invoke(self._executor, comp, arg))

~/anaconda3/lib/python3.7/asyncio/base_events.py in run_until_complete(self, future)
    582             raise RuntimeError('Event loop stopped before Future completed.')
    583 
--> 584         return future.result()
    585 
    586     def stop(self):

~/anaconda3/lib/python3.7/asyncio/futures.py in result(self)
    176         self.__log_traceback = False
    177         if self._exception is not None:
--> 178             raise self._exception
    179         return self._result
    180 

~/anaconda3/lib/python3.7/asyncio/tasks.py in __step(***failed resolving arguments***)
    221                 # We use the `send` method directly, because coroutines
    222                 # don't have `__iter__` and `__next__` methods.
--> 223                 result = coro.send(None)
    224             else:
    225                 result = coro.throw(exc)

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/execution_context.py in _invoke(executor, comp, arg)
     89     py_typecheck.check_type(arg, executor_value_base.ExecutorValue)
     90   comp = await executor.create_value(comp)
---> 91   result = await executor.create_call(comp, arg)
     92   py_typecheck.check_type(result, executor_value_base.ExecutorValue)
     93   result_val = _unwrap(await result.compute())

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in _evaluate(self, comp, scope)
    376         arg = None
    377       return await self.create_call(
--> 378           LambdaExecutorValue(comp.call.function, scope=scope), arg=arg)
    379     elif which_computation == 'selection':
    380       which_selection = comp.selection.WhichOneof('selection')

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    259         arg_type = type_utils.get_argument_type(arg.type_signature)
    260         type_utils.check_assignable_from(param_type, arg_type)
--> 261         return await self.create_call(comp, await self.create_call(arg))
    262     else:
    263       py_typecheck.check_none(arg)

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in _evaluate(self, comp, scope)
    352           comp,
    353           type_utils.get_function_type(
--> 354               type_serialization.deserialize_type(comp.type))))
    355     elif which_computation == 'lambda':
    356 

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/caching_executor.py in create_value(self, value, type_spec)
    240                                  target_future)
    241       self._cache[identifier] = cached_value
--> 242     target_value = await cached_value.target_future
    243     type_utils.check_assignable_from(type_spec, target_value.type_signature)
    244     return cached_value

~/anaconda3/lib/python3.7/asyncio/futures.py in __await__(self)
    261         if not self.done():
    262             raise RuntimeError(""await wasn't used with future"")
--> 263         return self.result()  # May raise too.
    264 
    265     __iter__ = __await__  # make compatible with 'yield from'.

~/anaconda3/lib/python3.7/asyncio/futures.py in result(self)
    176         self.__log_traceback = False
    177         if self._exception is not None:
--> 178             raise self._exception
    179         return self._result
    180 

~/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py in run_code(self, code_obj, result, async_)
   3323                     await eval(code_obj, self.user_global_ns, self.user_ns)
   3324                 else:
-> 3325                     exec(code_obj, self.user_global_ns, self.user_ns)
   3326             finally:
   3327                 # Reset our crash handler in place

<ipython-input-21-5acbbc43674b> in <module>
----> 1 state = iterative_process.initialize()
      2 
      3 for round_num in range(2, 20):
      4   state, metrics = iterative_process.next(state, federated_train_data)
      5   print('round {:2d}, metrics={}'.format(round_num, metrics))

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in __call__(self, *args, **kwargs)
    628     context = self._context_stack.current
    629     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 630     return context.invoke(self, arg)
    631 
    632 

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/execution_context.py in invoke(self, comp, arg)
    118   def invoke(self, comp, arg):
    119     return asyncio.get_event_loop().run_until_complete(
--> 120         _invoke(self._executor, comp, arg))

~/anaconda3/lib/python3.7/asyncio/base_events.py in run_until_complete(self, future)
    582             raise RuntimeError('Event loop stopped before Future completed.')
    583 
--> 584         return future.result()
    585 
    586     def stop(self):

~/anaconda3/lib/python3.7/asyncio/futures.py in result(self)
    176         self.__log_traceback = False
    177         if self._exception is not None:
--> 178             raise self._exception
    179         return self._result
    180 

~/anaconda3/lib/python3.7/asyncio/tasks.py in __step(***failed resolving arguments***)
    223                 result = coro.send(None)
    224             else:
--> 225                 result = coro.throw(exc)
    226         except StopIteration as exc:
    227             if self._must_cancel:

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/execution_context.py in _invoke(executor, comp, arg)
     89     py_typecheck.check_type(arg, executor_value_base.ExecutorValue)
     90   comp = await executor.create_value(comp)
---> 91   result = await executor.create_call(comp, arg)
     92   py_typecheck.check_type(result, executor_value_base.ExecutorValue)
     93   result_val = _unwrap(await result.compute())

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in _evaluate(self, comp, scope)
    376         arg = None
    377       return await self.create_call(
--> 378           LambdaExecutorValue(comp.call.function, scope=scope), arg=arg)
    379     elif which_computation == 'selection':
    380       which_selection = comp.selection.WhichOneof('selection')

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    259         arg_type = type_utils.get_argument_type(arg.type_signature)
    260         type_utils.check_assignable_from(param_type, arg_type)
--> 261         return await self.create_call(comp, await self.create_call(arg))
    262     else:
    263       py_typecheck.check_none(arg)

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/lambda_executor.py in _evaluate(self, comp, scope)
    352           comp,
    353           type_utils.get_function_type(
--> 354               type_serialization.deserialize_type(comp.type))))
    355     elif which_computation == 'lambda':
    356 

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/caching_executor.py in create_value(self, value, type_spec)
    240                                  target_future)
    241       self._cache[identifier] = cached_value
--> 242     target_value = await cached_value.target_future
    243     type_utils.check_assignable_from(type_spec, target_value.type_signature)
    244     return cached_value

~/anaconda3/lib/python3.7/asyncio/futures.py in __await__(self)
    258         if not self.done():
    259             self._asyncio_future_blocking = True
--> 260             yield self  # This tells Task to wait for completion.
    261         if not self.done():
    262             raise RuntimeError(""await wasn't used with future"")

~/anaconda3/lib/python3.7/asyncio/tasks.py in __wakeup(self, future)
    290     def __wakeup(self, future):
    291         try:
--> 292             future.result()
    293         except Exception as exc:
    294             # This may also be a cancellation.

~/anaconda3/lib/python3.7/asyncio/futures.py in result(self)
    176         self.__log_traceback = False
    177         if self._exception is not None:
--> 178             raise self._exception
    179         return self._result
    180 

~/anaconda3/lib/python3.7/asyncio/tasks.py in __step(***failed resolving arguments***)
    223                 result = coro.send(None)
    224             else:
--> 225                 result = coro.throw(exc)
    226         except StopIteration as exc:
    227             if self._must_cancel:

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/concurrent_executor.py in create_value(self, value, type_spec)
     66   async def create_value(self, value, type_spec=None):
     67     return await self._delegate(
---> 68         self._target_executor.create_value(value, type_spec))
     69 
     70   async def create_call(self, comp, arg=None):

~/anaconda3/lib/python3.7/asyncio/futures.py in __await__(self)
    258         if not self.done():
    259             self._asyncio_future_blocking = True
--> 260             yield self  # This tells Task to wait for completion.
    261         if not self.done():
    262             raise RuntimeError(""await wasn't used with future"")

~/anaconda3/lib/python3.7/asyncio/tasks.py in __wakeup(self, future)
    290     def __wakeup(self, future):
    291         try:
--> 292             future.result()
    293         except Exception as exc:
    294             # This may also be a cancellation.

~/anaconda3/lib/python3.7/asyncio/futures.py in result(self)
    176         self.__log_traceback = False
    177         if self._exception is not None:
--> 178             raise self._exception
    179         return self._result
    180 

~/anaconda3/lib/python3.7/asyncio/tasks.py in __step(***failed resolving arguments***)
    221                 # We use the `send` method directly, because coroutines
    222                 # don't have `__iter__` and `__next__` methods.
--> 223                 result = coro.send(None)
    224             else:
    225                 result = coro.throw(exc)

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/eager_executor.py in create_value(self, value, type_spec)
    369     if not tf.executing_eagerly():
    370       raise RuntimeError('The eager executor may only be used in eager mode.')
--> 371     return EagerValue(value, type_spec, self._device)
    372 
    373   async def create_call(self, comp, arg=None):

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/eager_executor.py in __init__(self, value, type_spec, device)
    274       py_typecheck.check_type(type_spec, computation_types.Type)
    275     self._type_signature = type_spec
--> 276     self._value = to_representation_for_type(value, type_spec, device)
    277 
    278   @property

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/eager_executor.py in to_representation_for_type(value, type_spec, device)
    207         computation_impl.ComputationImpl.get_proto(value), type_spec, device)
    208   if isinstance(value, pb.Computation):
--> 209     return embed_tensorflow_computation(value, type_spec, device)
    210   if isinstance(type_spec, computation_types.TensorType):
    211     if not isinstance(value, tf.Tensor):

~/anaconda3/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/eager_executor.py in embed_tensorflow_computation(comp, type_spec, device)
     66   if which_computation != 'tensorflow':
     67     raise TypeError('Expected a TensorFlow computation, found {}.'.format(
---> 68         which_computation))
     69 
     70   if isinstance(type_spec, computation_types.FunctionType):

TypeError: Expected a TensorFlow computation, found intrinsic.

```

ThanksHi @ZyuHoiMing,

Please see issue #764; long story short, this is a mismatch between the released pip package and master, and if you specify the number of clients in the executor, this should disappear.

We are planning on another release soon, and this issue should simply disappear with that package.

Thanks for your interest in TFF!
Keith",1,2019-09-28 14:45:59,2019-09-29 03:18:38,2019-09-29 03:18:38
https://github.com/tensorflow/federated/issues/766,[],Installation problem,"Installation problem  Hi! Can `Tensorflow_federated` be installed in` CentOS`?
  I'm trying to install this in remote server and can  only install the software by compiling it,because I can't get the ""root"" permission.I'm very embarrassed to ask if you have relevant  installation documentation?
  My server system information is :`CentOS Linux release 7.4.1708 (Core)`
Thank you very much
Hi @phalangee,

Can you create a virtualenv and install the TFF pip package there? In general, as long as you have python already installed, you should be able to start [here](https://www.tensorflow.org/federated/install#2_create_a_virtual_environment), install TFF to your interpreter, and run any programs requiring TFF through this interpreter.

Does this work for your setup?@jkr26  Thank you very much !I just came back from vacation. You are right, I googled how to install conda with non-root permissions. and `conda create -n envname` Then follow the tutorial to install successfully.
Thank you very muchAwesome, happy to help!",3,2019-09-28 08:49:41,2019-10-09 15:36:19,2019-10-09 15:36:18
https://github.com/tensorflow/federated/issues/765,[],The best version of frameworks that support federated,"The best version of frameworks that support federatedI want to know that if I use tensorflow federated, which version of python, tensorflow and keras should I use to minimize the weird bugs.Hi @Pangeensi,

There is a [compatibility chart](https://github.com/tensorflow/federated#compatibility) on the https://github.com/tensorflow/federated with versions of Tensorflow (which should include Keras in the `tf.keras` package) and Tensorflow Federated. As for Python versions, we run continuous tests on Python 2.7 (Linux) and Python 3.6 (Linux and MacOS).

I would check out the steps are [Install TensorFlow Federated](https://github.com/tensorflow/federated/blob/master/docs/install.md), which provide a couple different alternatives for installing.

In the future, this type of question maybe better asked at https://stackoverflow.com/questions/tagged/tensorflow-federated. Please continue to create github issues for the weird bugs you encounter, we'd like to fix those.",1,2019-09-21 01:29:17,2019-09-23 18:04:04,2019-09-23 18:04:04
https://github.com/tensorflow/federated/issues/764,[],"TypeError: Expected a TensorFlow computation, found intrinsic.","TypeError: Expected a TensorFlow computation, found intrinsic.This is from Federated_learning_for_image_classification.ipynb
tensorflow_federated 0.8.0
tensorflow nightly 1.15.0-dev20190805

run as a local notebook. It works on colab!

state = iterative_process.initialize()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-18-0c0dcd96a774> in <module>
----> 1 state = iterative_process.initialize()

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\utils\function_utils.py in __call__(self, *args, **kwargs)
    628     context = self._context_stack.current
    629     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 630     return context.invoke(self, arg)
    631 
    632 

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\execution_context.py in invoke(self, comp, arg)
    118   def invoke(self, comp, arg):
    119     return asyncio.get_event_loop().run_until_complete(
--> 120         _invoke(self._executor, comp, arg))

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\base_events.py in run_until_complete(self, future)
    482             raise RuntimeError('Event loop stopped before Future completed.')
    483 
--> 484         return future.result()
    485 
    486     def stop(self):

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\tasks.py in _step(***failed resolving arguments***)
    180                 result = coro.send(None)
    181             else:
--> 182                 result = coro.throw(exc)
    183         except StopIteration as exc:
    184             if self._must_cancel:

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\execution_context.py in _invoke(executor, comp, arg)
     89     py_typecheck.check_type(arg, executor_value_base.ExecutorValue)
     90   comp = await executor.create_value(comp)
---> 91   result = await executor.create_call(comp, arg)
     92   py_typecheck.check_type(result, executor_value_base.ExecutorValue)
     93   result_val = _unwrap(await result.compute())

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\lambda_executor.py in _evaluate(self, comp, scope)
    376         arg = None
    377       return await self.create_call(
--> 378           LambdaExecutorValue(comp.call.function, scope=scope), arg=arg)
    379     elif which_computation == 'selection':
    380       which_selection = comp.selection.WhichOneof('selection')

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\lambda_executor.py in create_call(self, comp, arg)
    259         arg_type = type_utils.get_argument_type(arg.type_signature)
    260         type_utils.check_assignable_from(param_type, arg_type)
--> 261         return await self.create_call(comp, await self.create_call(arg))
    262     else:
    263       py_typecheck.check_none(arg)

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\lambda_executor.py in _evaluate(self, comp, scope)
    352           comp,
    353           type_utils.get_function_type(
--> 354               type_serialization.deserialize_type(comp.type))))
    355     elif which_computation == 'lambda':
    356 

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\caching_executor.py in create_value(self, value, type_spec)
    240                                  target_future)
    241       self._cache[identifier] = cached_value
--> 242     target_value = await cached_value.target_future
    243     type_utils.check_assignable_from(type_spec, target_value.type_signature)
    244     return cached_value

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\tasks.py in _wakeup(self, future)
    248     def _wakeup(self, future):
    249         try:
--> 250             future.result()
    251         except Exception as exc:
    252             # This may also be a cancellation.

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\tasks.py in _step(***failed resolving arguments***)
    180                 result = coro.send(None)
    181             else:
--> 182                 result = coro.throw(exc)
    183         except StopIteration as exc:
    184             if self._must_cancel:

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\concurrent_executor.py in create_value(self, value, type_spec)
     66   async def create_value(self, value, type_spec=None):
     67     return await self._delegate(
---> 68         self._target_executor.create_value(value, type_spec))
     69 
     70   async def create_call(self, comp, arg=None):

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\futures.py in __iter__(self)
    325         if not self.done():
    326             self._asyncio_future_blocking = True
--> 327             yield self  # This tells Task to wait for completion.
    328         assert self.done(), ""yield from wasn't used with future""
    329         return self.result()  # May raise too.

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\tasks.py in _wakeup(self, future)
    248     def _wakeup(self, future):
    249         try:
--> 250             future.result()
    251         except Exception as exc:
    252             # This may also be a cancellation.

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\futures.py in result(self)
    241         self._log_traceback = False
    242         if self._exception is not None:
--> 243             raise self._exception
    244         return self._result
    245 

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\tasks.py in _step(***failed resolving arguments***)
    178                 # We use the `send` method directly, because coroutines
    179                 # don't have `__iter__` and `__next__` methods.
--> 180                 result = coro.send(None)
    181             else:
    182                 result = coro.throw(exc)

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\eager_executor.py in create_value(self, value, type_spec)
    369     if not tf.executing_eagerly():
    370       raise RuntimeError('The eager executor may only be used in eager mode.')
--> 371     return EagerValue(value, type_spec, self._device)
    372 
    373   async def create_call(self, comp, arg=None):

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\eager_executor.py in __init__(self, value, type_spec, device)
    274       py_typecheck.check_type(type_spec, computation_types.Type)
    275     self._type_signature = type_spec
--> 276     self._value = to_representation_for_type(value, type_spec, device)
    277 
    278   @property

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\eager_executor.py in to_representation_for_type(value, type_spec, device)
    207         computation_impl.ComputationImpl.get_proto(value), type_spec, device)
    208   if isinstance(value, pb.Computation):
--> 209     return embed_tensorflow_computation(value, type_spec, device)
    210   if isinstance(type_spec, computation_types.TensorType):
    211     if not isinstance(value, tf.Tensor):

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\eager_executor.py in embed_tensorflow_computation(comp, type_spec, device)
     66   if which_computation != 'tensorflow':
     67     raise TypeError('Expected a TensorFlow computation, found {}.'.format(
---> 68         which_computation))
     69 
     70   if isinstance(type_spec, computation_types.FunctionType):

TypeError: Expected a TensorFlow computation, found intrinsic.
Hi @dsudzeng 

Thanks for pinging us on this!

We pushed the capability for the local executor to infer cardinalities from its arguments in August 29 in [this change](https://github.com/tensorflow/federated/commit/7107a4a8d474524341250b15da389614b478b1e8#diff-9a0820f4f746826fab7fd2b2f3612342), and released our last version of TFF (0.8.0) on August 20. This means that any ipython notebook currently at master which does not explicitly set its number of clients will behave as a non-federated executor (and e.g. won't know how to handle intrinsics); basically master is out of sync with the pip package.

If you are running these notebooks against the pip package, you will need to explicitly specify the number of clients in the executor stack. Here, `num_clients` should be 10 if I recall correctly; so if you call `tff.framework.set_default_executor(tff.framework.create_local_executor(10))` instead of `tff.framework.set_default_executor(tff.framework.create_local_executor())` at the top level of this notebook, this discrepancy should be resolved. you must add this lignes in notebook tff.framework.set_default_executor(tff.framework.create_local_executor(10)) tff.framework.set_default_executor(tff.framework.create_local_executor())

But 10 means the total number of clients ? or the fraction of clients C that we have to choose, because as you know, in Federated learning, not all the clients are going to participate in training.",2,2019-09-20 21:39:42,2019-12-20 12:01:02,2019-09-24 20:42:21
https://github.com/tensorflow/federated/issues/763,[],Manipulating weight delta element-wisely,"Manipulating weight delta element-wiselySay, for each entry in the weight delta, i would like to find its median among the clients. How can i do that? 
I am trying to build a custom `stateful_delta_aggregate_fn`  and feed that to `tff.learning.build_federated_averaging_process`, as a result, having elemental-wise median updates instead of FedAvg.Could you please ask this question at https://stackoverflow.com/questions/tagged/tensorflow-federated? Questions about how to use TFF, or how to accomplish a task are best asked there.

We use the GitHub issues page for handling bugs in TFF.",1,2019-09-20 15:54:10,2019-10-02 15:29:51,2019-10-02 15:29:51
https://github.com/tensorflow/federated/issues/761,[],"TypeError: Expected collections.abc.Iterable, found method.","TypeError: Expected collections.abc.Iterable, found method.my Installation Environment is :
**python: 3.7.2
tensorflow: 1.14.0
tensorflow-federated : 0.8.0
tf-nightly: 1.15.0.dev20190805**
```
class MnistTrainableModel(MnistModel, tff.learning.TrainableModel):

  @tf.function
  def train_on_batch(self, batch):
    output = self.forward_pass(batch)
    optimizer = tf.train.GradientDescentOptimizer(0.02)
    optimizer.minimize(output.loss, var_list=self.trainable_variables)
    return output  


iterative_process = tff.learning.build_federated_averaging_process(
    MnistTrainableModel)
```
**This code is from federated_learning_for_image_classification.ipynb**
**and I got this error:**
`TypeError: Expected collections.abc.Iterable, found method.`

**more information about error:**
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-31-66966a6f055f> in <module>
      1 iterative_process = tff.learning.build_federated_averaging_process(
----> 2     MnistTrainableModel)

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/learning/federated_averaging.py in build_federated_averaging_process(model_fn, server_optimizer_fn, client_weight_fn, stateful_delta_aggregate_fn, stateful_model_broadcast_fn)
    162   return optimizer_utils.build_model_delta_optimizer_process(
    163       model_fn, client_fed_avg, server_optimizer_fn,
--> 164       stateful_delta_aggregate_fn, stateful_model_broadcast_fn)

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py in build_model_delta_optimizer_process(model_fn, model_to_client_delta_fn, server_optimizer_fn, stateful_delta_aggregate_fn, stateful_model_broadcast_fn)
    350   # TensorFlow Computations
    351 
--> 352   @tff.tf_computation
    353   def tf_init_fn():
    354     return server_init(model_fn, server_optimizer_fn,

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/api/computations.py in tf_computation(*args)
    154     patterns and examples of usage above.
    155   """"""
--> 156   return computation_wrapper_instances.tensorflow_wrapper(*args)
    157 
    158 

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py in __call__(self, *args)
    408           args[0],
    409           computation_types.to_type(args[1]) if len(args) > 1 else None,
--> 410           self._wrapper_fn)
    411     else:
    412       if len(args) > 1:

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py in _wrap(fn, parameter_type, wrapper_fn)
    101 
    102   # Either we have a concrete parameter type, or this is no-arg function.
--> 103   concrete_fn = wrapper_fn(fn, parameter_type, unpack=None)
    104   py_typecheck.check_type(concrete_fn, function_utils.ConcreteFunction,
    105                           'value returned by the wrapper')

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/computation_wrapper_instances.py in _tf_wrapper_fn(***failed resolving arguments***)
     42   ctx_stack = context_stack_impl.context_stack
     43   comp_pb, extra_type_spec = tensorflow_serialization.serialize_py_fn_as_tf_computation(
---> 44       target_fn, parameter_type, ctx_stack)
     45   return computation_impl.ComputationImpl(comp_pb, ctx_stack, extra_type_spec)
     46 

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/tensorflow_serialization.py in serialize_py_fn_as_tf_computation(target, parameter_type, context_stack)
    266     context = tf_computation_context.TensorFlowComputationContext(graph)
    267     with context_stack.install(context):
--> 268       result = target(*args)
    269 
    270       # TODO(b/122081673): This needs to change for TF 2.0. We may also

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in <lambda>()
    519       # Deliberate wrapping to isolate the caller from `fn`, e.g., to prevent
    520       # the caller from mistakenly specifying args that match fn's defaults.
--> 521       return lambda: fn()  # pylint: disable=unnecessary-lambda
    522     else:
    523       raise TypeError(

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py in tf_init_fn()
    354     return server_init(model_fn, server_optimizer_fn,
    355                        stateful_delta_aggregate_fn.initialize(),
--> 356                        stateful_model_broadcast_fn.initialize())
    357 
    358   tf_dataset_type = tff.SequenceType(dummy_model_for_metadata.input_spec)

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py in server_init(model_fn, optimizer_fn, delta_aggregate_state, model_broadcast_state)
    219   model = model_utils.enhance(model_fn())
    220   optimizer = optimizer_fn()
--> 221   _, optimizer_vars = _build_server_optimizer(model, optimizer)
    222   return ServerState(
    223       model=model.weights,

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py in _build_server_optimizer(model, optimizer)
    120   # Create a dummy input and trace apply_delta so that
    121   # we can determine the optimizer's variables.
--> 122   weights_delta = tf.nest.map_structure(tf.zeros_like, model.weights.trainable)
    123 
    124   # TODO(b/109733734): We would like to call get_concrete_function,

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/learning/model_utils.py in weights(self)
    146   def weights(self):
    147     """"""Returns a `tff.learning.ModelWeights`.""""""
--> 148     return ModelWeights.from_model(self)
    149 
    150   #

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/learning/model_utils.py in from_model(cls, model)
     76     # keras_model.set_weights
     77     return cls(
---> 78         tensor_utils.to_var_dict(model.trainable_variables),
     79         tensor_utils.to_var_dict(model.non_trainable_variables))
     80 

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/learning/model_utils.py in trainable_variables(self)
    154   @property
    155   def trainable_variables(self):
--> 156     return _check_iterable_of_variables(self._model.trainable_variables)
    157 
    158   @property

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/learning/model_utils.py in _check_iterable_of_variables(variables)
    122 
    123 def _check_iterable_of_variables(variables):
--> 124   py_typecheck.check_type(variables, collections.Iterable)
    125   for v in variables:
    126     py_typecheck.check_type(v, tf.Variable)

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/common_libs/py_typecheck.py in check_type(target, type_spec, label)
     46     raise TypeError('Expected {}{}, found {}.'.format(
     47         '{} to be of type '.format(label) if label is not None else '',
---> 48         type_string(type_spec), type_string(type(target))))
     49   return target
     50 

TypeError: Expected collections.abc.Iterable, found method.
```

Thanks a lot !Hi @phalangee,

I am guessing this may be a problem with side-by-side install of TF 1.14.0 and TF-nightly; this is generally not a recommended practice for us.

Can you try creating a fresh `virtualenv`, and installing TFF directly to this environment?

Another option I can see is the possibility that there is an override of the `trainable_variables` property; before you build federated_averaging process, can you construct an instance of `MnistTrainableModel`, and call `type` on its `trainable_variables` property? This *should* be a list, but from my parsing of the stacktrace above, it may not be.

Thanks for your interest in TFF!
KeithThank you very much !@jkr26   
I found out that the code  `@property` in front of this code `def trainable_variables(self)` was accidentally deleted by me. Because you inmentioned `trainable_variables` make me find this point! I should figure out the @ Decorator as soon as possible.
Sorry to bother you !
Best wishes to you!Sweet! Yes, the `@property` decorator will allow you to treat the thing it decorates as simply another attribute, although you may even be doing some computation on the fly.

Hope you enjoy TFF!",3,2019-09-11 11:27:32,2019-09-12 23:42:23,2019-09-12 23:42:23
https://github.com/tensorflow/federated/issues/759,[],Problem with 'tf-nightly version',"Problem with 'tf-nightly version' 
   File ""/Users/haiqw/anaconda3/envs/py3/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py"", line 2106, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/haiqw/anaconda3/envs/py3/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py"", line 1997, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/haiqw/anaconda3/envs/py3/lib/python3.5/site-packages/tensorflow_core/python/framework/func_graph.py"", line 884, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/haiqw/anaconda3/envs/py3/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py"", line 325, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/haiqw/anaconda3/envs/py3/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py"", line 2614, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""/Users/haiqw/anaconda3/envs/py3/lib/python3.5/site-packages/tensorflow_core/python/framework/func_graph.py"", line 874, in wrapper
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in converted code:
    relative to /Users/haiqw/anaconda3/envs/py3/lib/python3.5/site-packages:

    tensorflow_federated/python/tensorflow_libs/tensor_utils.py:115 zero_all_if_any_non_finite
        if all_finite:
    tensorflow_core/python/framework/ops.py:762 __bool__
        self._disallow_bool_casting()
    tensorflow_core/python/framework/ops.py:531 _disallow_bool_casting
        ""using a `tf.Tensor` as a Python `bool`"")
    tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled
        "" decorating it directly with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
Thanks for the report!

Could you share a few more pieces of information? 

- output of `pip freeze`
- output of `python --version`
- operating system
- the code snippet being run that leads to the stack trace

Thanks!> Thanks for the report!
> 
> Could you share a few more pieces of information?
> 
> * output of `pip freeze`
> * output of `python --version`
> * operating system
> * the code snippet being run that leads to the stack trace
> 
> Thanks!
pip freeze:
absl-py==0.8.0
astor==0.8.0
attrs==18.2.0
cachetools==3.1.1
certifi==2019.6.16
enum34==1.1.6
gast==0.3.1
google-pasta==0.1.7
grpcio==1.22.1
h5py==2.10.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
Markdown==3.1.1
numpy==1.17.2
opt-einsum==3.0.1
portpicker==1.3.1
protobuf==3.9.1
six==1.12.0
tb-nightly==1.15.0a20190908
tensorflow-federated==0.8.0
tensorflow-model-optimization==0.1.3
termcolor==1.1.0
tf-estimator-nightly==1.14.0.dev2019090801
tf-nightly==1.15.0.dev20190805
Werkzeug==0.15.6
wrapt==1.11.2

python --version:
Python 3.6.9 :: Anaconda, Inc.

OS: macOS mojave 10.14.2

@ZacharyGarrett I also encountered a similar problem as HaiQW. So I directly post it here for your reference. Thanks. 

- OS
System: macOS 10.14.6, Kernel: Darwin 18.7.0

- python --version
Python 3.7.4

- pip freeze 
absl-py==0.8.0
astor==0.8.0
attrs==18.2.0
cachetools==3.1.1
enum34==1.1.6
gast==0.3.1
google-pasta==0.1.7
grpcio==1.22.1
h5py==2.10.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
Markdown==3.1.1
numpy==1.17.2
opt-einsum==3.0.1
portpicker==1.3.1
protobuf==3.9.1
six==1.12.0
tb-nightly==1.15.0a20190908
tensorboard==1.14.0
tensorflow==1.14.0
tensorflow-estimator==1.14.0
tensorflow-federated==0.8.0
tensorflow-model-optimization==0.1.3
termcolor==1.1.0
tf-estimator-nightly==1.14.0.dev2019090801
tf-nightly==1.15.0.dev20190805
Werkzeug==0.15.6
wrapt==1.11.2

- code snippet
A very simple example from the tutorial 

```
 from six.moves import range
 import tensorflow as tf
 import tensorflow_federated as tff
 from tensorflow_federated.python.examples import mnist
 tf.compat.v1.enable_v2_behavior()

 # Load simulation data.
 source, _ = tff.simulation.datasets.emnist.load_data()
 def client_data(n):
   dataset = source.create_tf_dataset_for_client(source.client_ids[n])
   return mnist.keras_dataset_from_emnist(dataset).repeat(10).batch(20)

 # Pick a subset of client devices to participate in training.
 train_data = [client_data(n) for n in range(3)]

 # Grab a single batch of data so that TFF knows what data looks like.
 sample_batch = tf.nest.map_structure(
     lambda x: x.numpy(), iter(train_data[0]).next())

 # Wrap a Keras model for use with TFF.
 def model_fn():
   return tff.learning.from_compiled_keras_model(
       mnist.create_simple_keras_model(), sample_batch)

 # Simulate a few rounds of training with the selected client devices.
 trainer = tff.learning.build_federated_averaging_process(model_fn)
 state = trainer.initialize()
 for _ in range(5):
   state, metrics = trainer.next(state, train_data)
   print (metrics.loss)
```
> @ZacharyGarrett I also encountered a similar problem as HaiQW. So I directly post it here for your reference. Thanks.
> 
> * OS
>   System: macOS 10.14.6, Kernel: Darwin 18.7.0
> * python --version
>   Python 3.7.4
> * pip freeze
>   absl-py==0.8.0
>   astor==0.8.0
>   attrs==18.2.0
>   cachetools==3.1.1
>   enum34==1.1.6
>   gast==0.3.1
>   google-pasta==0.1.7
>   grpcio==1.22.1
>   h5py==2.10.0
>   Keras-Applications==1.0.8
>   Keras-Preprocessing==1.1.0
>   Markdown==3.1.1
>   numpy==1.17.2
>   opt-einsum==3.0.1
>   portpicker==1.3.1
>   protobuf==3.9.1
>   six==1.12.0
>   tb-nightly==1.15.0a20190908
>   tensorboard==1.14.0
>   tensorflow==1.14.0
>   tensorflow-estimator==1.14.0
>   tensorflow-federated==0.8.0
>   tensorflow-model-optimization==0.1.3
>   termcolor==1.1.0
>   tf-estimator-nightly==1.14.0.dev2019090801
>   tf-nightly==1.15.0.dev20190805
>   Werkzeug==0.15.6
>   wrapt==1.11.2
> * code snippet
>   A very simple example from the tutorial
> 
> ```
>  from six.moves import range
>  import tensorflow as tf
>  import tensorflow_federated as tff
>  from tensorflow_federated.python.examples import mnist
>  tf.compat.v1.enable_v2_behavior()
> 
>  # Load simulation data.
>  source, _ = tff.simulation.datasets.emnist.load_data()
>  def client_data(n):
>    dataset = source.create_tf_dataset_for_client(source.client_ids[n])
>    return mnist.keras_dataset_from_emnist(dataset).repeat(10).batch(20)
> 
>  # Pick a subset of client devices to participate in training.
>  train_data = [client_data(n) for n in range(3)]
> 
>  # Grab a single batch of data so that TFF knows what data looks like.
>  sample_batch = tf.nest.map_structure(
>      lambda x: x.numpy(), iter(train_data[0]).next())
> 
>  # Wrap a Keras model for use with TFF.
>  def model_fn():
>    return tff.learning.from_compiled_keras_model(
>        mnist.create_simple_keras_model(), sample_batch)
> 
>  # Simulate a few rounds of training with the selected client devices.
>  trainer = tff.learning.build_federated_averaging_process(model_fn)
>  state = trainer.initialize()
>  for _ in range(5):
>    state, metrics = trainer.next(state, train_data)
>    print (metrics.loss)
> ```

Thanks.  The problem is now solved after upgrading TFF to the master version. @HaiQW I'm having similar issues -- would you kindly post the working output from `pip freeze`? > @HaiQW I'm having similar issues -- would you kindly post the working output from `pip freeze`?

The output of pip freeze is attached below.

```
absl-py==0.8.0
appnope==0.1.0
asn1crypto==0.24.0
astor==0.8.0
atomicwrites==1.3.0
attrs==19.1.0
backcall==0.1.0
bleach==3.1.0
cachetools==3.1.1
certifi==2019.9.11
cffi==1.12.3
chardet==3.0.4
cryptography==2.7
cycler==0.10.0
decorator==4.4.0
defusedxml==0.6.0
dill==0.3.0
entrypoints==0.3
enum34==1.1.6
fuel==0.2.0
future==0.17.1
gast==0.2.2
google-pasta==0.1.7
googleapis-common-protos==1.6.0
grpcio==1.22.1
h5py==2.10.0
idna==2.8
importlib-metadata==0.19
ipykernel==5.1.2
ipython==7.8.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
jedi==0.15.1
Jinja2==2.10.1
joblib==0.13.2
jsonschema==3.0.2
jupyter==1.0.0
jupyter-client==5.3.3
jupyter-console==6.0.0
jupyter-core==4.5.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
kiwisolver==1.1.0
Markdown==3.1.1
MarkupSafe==1.1.1
matplotlib==3.1.1
mistune==0.8.4
mkl-fft==1.0.14
mkl-random==1.0.2
mkl-service==2.3.0
mock==3.0.5
more-itertools==7.2.0
mpmath==1.1.0
nbconvert==5.6.0
nbformat==4.4.0
notebook==6.0.1
numexpr==2.7.0
numpy==1.17.2
olefile==0.46
opt-einsum==3.0.1
packaging==19.2
pandas==0.24.2
pandocfilters==1.4.2
parso==0.5.1
pexpect==4.7.0
picklable-itertools==0.1.1
pickleshare==0.7.5
Pillow==6.1.0
pluggy==0.13.0
portpicker==1.3.1
progressbar2==3.46.1
prometheus-client==0.7.1
promise==2.2.1
prompt-toolkit==2.0.9
protobuf==3.9.1
psutil==5.6.3
ptyprocess==0.6.0
py==1.8.0
pycparser==2.19
Pygments==2.4.2
pyOpenSSL==19.0.0
pyparsing==2.4.2
pyrsistent==0.14.11
PySocks==1.7.1
pytest==5.1.2
pytest-runner==5.1
python-dateutil==2.8.0
python-utils==2.3.0
pytz==2019.2
PyYAML==5.1.2
pyzmq==18.1.0
qtconsole==4.5.5
requests==2.22.0
scikit-learn==0.21.3
scipy==1.3.1
Send2Trash==1.5.0
six==1.12.0
tables==3.5.2
tb-nightly==1.15.0a20190911
tensorflow-datasets==1.2.0
tensorflow-federated==0.8.0
tensorflow-metadata==0.14.0
tensorflow-model-optimization==0.1.3
tensorflow-privacy==0.0.1
termcolor==1.1.0
terminado==0.8.2
testpath==0.4.2
tf-estimator-nightly==1.14.0.dev2019091601
tf-nightly==1.15.0.dev20190805
tornado==6.0.3
tqdm==4.36.1
traitlets==4.3.2
typing==3.7.4.1
urllib3==1.25.5
wcwidth==0.1.7
webencodings==0.5.1
Werkzeug==0.15.6
widgetsnbextension==3.5.1
wrapt==1.11.2
yapf==0.28.0
zipp==0.5.2
```

A walkaround solution here. 
First, download the newest  version of tensorflow federated. 
Then,  run ""pip install -r requirements.txt"" in federated folder you pulled from github.
Last, pip install --upgrade tensorflow_federated 

It works for me.",6,2019-09-09 08:50:35,2019-10-08 02:15:37,2019-09-10 13:19:21
https://github.com/tensorflow/federated/issues/758,[],How to manually reset the model weights of federated averaging algo？,"How to manually reset the model weights of federated averaging algo？Hi there,

What I want to do now is reset the model using the `model.set_weights()`. But how should I do that, should I recreate both `iterative_process` (by `tff.learning.build_federated_averaging_process(model_fn)`) and `state` (by `iterative_process.initialize()`)? 

UPDATE 6/9/2019:
Well, I just found that there is a method called `tff.utils.update_state()`. Can I use this method to change the model weights without doing anything to the `iterative_process`? Also, for the pass-in parameter `**kwargs`, should I pass key-value pairs into it? And how should I name this key if I wanna change the weights?

UPDATE 7/9/2019:
The `tff.utils.update_state()` seems not to support the AnonymousTuple at the moment. I am a bot confused what the state should be hereHi @zzhu2019, could you help me understand more clearly the context around ""reset the module using `model.set_weights()`? If you could share the code you're writing it would help clarify the intent. 

If you want to reset the model weights within the `state` object to the original initialization, calling `iterative_process.initialize()` will produce a new, reset model. Did this not accomplish want you wanted?


> Hi @zzhu2019, could you help me understand more clearly the context around ""reset the module using `model.set_weights()`? If you could share the code you're writing it would help clarify the intent.
> 
> If you want to reset the model weights within the `state` object to the original initialization, calling `iterative_process.initialize()` will produce a new, reset model. Did this not accomplish want you wanted?

Hi Zachary. 

Sorry for the late reply as I cannot log in my account today, I have to reply with my friend's account. Apologize for not being clear, as the `model.set_weights()` is what I found in `tf.keras.layers.Layer.set_weights()` [(tf source code)](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/engine/base_layer.py#L1090-L1121). To be specific, the reset I mean here is kinda changing the model weights into a set of weights calculated beforehand but not reset to original initialization. The `model.set_weights()` works fine by itself, but when I 

1. initialize a model with `model = tf.keras.models.Sequential([tf.keras.layers.Dense(10, activation=tf.nn.softmax, kernel_initializer='zeros', input_shape=(784,))])`, and 

2. use the `model.set_weights()`, 
3. then `model.compile()`, 
4. after that, use this generated model in `tff.learning.from_compiled_keras_model()` 
5. and then `tff.learning.build_federated_averaging_process()` to produce a new `iterative_process` 
6. and `iterative_process.initialize()` to produce a new state. 
It seems like the model weight is not resetted to the target weights but the initialization weights instead.

Generally, is my understanding of the code correct? And is there a more elegant way to do it? > Hi @zzhu2019, could you help me understand more clearly the context around ""reset the module using `model.set_weights()`? If you could share the code you're writing it would help clarify the intent.
> 
> If you want to reset the model weights within the `state` object to the original initialization, calling `iterative_process.initialize()` will produce a new, reset model. Did this not accomplish want you wanted?

And, can I use the code below to only update the `state` in order to change the weights without regenerate the `iterative_process`?
`value_dict = {}`
`value_dict['desne/kernal'] = states[edge_id][0]`
`value_dict['desne/bias'] = states[edge_id][1]`
`state = tff.utils.update_state(state=state, **value_dict)`Could you share the code you are using the do steps 1 through 5 in https://github.com/tensorflow/federated/issues/758#issuecomment-530302262? Setting a Keras model's weights with pre-trained weights and then using that in TFF is expected to work (the [Federated Learning for Text Generation tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation) does this, see the sections on _Load a pre-trained model_ and _Fine-tune the model with Federated Learning_). Lets see what differs from those examples.

> Could you share the code you are using the do steps 1 through 5 in [#758 (comment)](https://github.com/tensorflow/federated/issues/758#issuecomment-530302262)? Setting a Keras model's weights with pre-trained weights and then using that in TFF is expected to work (the [Federated Learning for Text Generation tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation) does this, see the sections on _Load a pre-trained model_ and _Fine-tune the model with Federated Learning_). Lets see what differs from those examples.

Basically, I changed a bit about the `create_compiled_keras_model()` as follow:
![image](https://user-images.githubusercontent.com/37321309/64748468-9c233400-d555-11e9-8fe7-5cf723d794b5.png) The `state_spec` basically is a list contains two numpy array, which is extracted from the previous calculated `state`. (state[0][0][0] and state[0][0][1])
And the `model_fn()`:
![image](https://user-images.githubusercontent.com/37321309/64745836-b86ea300-d54c-11e9-8e5d-f472382d731a.png)
And the `model_fn()` is used in:
![image](https://user-images.githubusercontent.com/37321309/64745875-e48a2400-d54c-11e9-8bfc-9add2dd68e72.png)
which produce a new  `iterative_process` and `state`. The problem is the keras_model in the `model_fn` has the target weights, but when I check the state generated by `iterative_process.initialize()` it still has the original initialization weights.

> Could you share the code you are using the do steps 1 through 5 in [#758 (comment)](https://github.com/tensorflow/federated/issues/758#issuecomment-530302262)? Setting a Keras model's weights with pre-trained weights and then using that in TFF is expected to work (the [Federated Learning for Text Generation tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation) does this, see the sections on _Load a pre-trained model_ and _Fine-tune the model with Federated Learning_). Lets see what differs from those examples.

It seems like there is something wrong with the way I use `model.reset_weights()` cuz the weights inside the state generated by `iterative_process.initialize()` just change correspondingly to the `kernel_initializer` in `model = tf.keras.models.Sequential([
            tf.keras.layers.Dense(
                10, activation=tf.nn.softmax, kernel_initializer='random_uniform', input_shape=(784,))])`

The `model.set_weights()` wroks well and it can be supported by `model.get_weights()`. But somehow when the created `iterative_process` doesn't recognize that..> Could you share the code you are using the do steps 1 through 5 in [#758 (comment)](https://github.com/tensorflow/federated/issues/758#issuecomment-530302262)? Setting a Keras model's weights with pre-trained weights and then using that in TFF is expected to work (the [Federated Learning for Text Generation tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation) does this, see the sections on _Load a pre-trained model_ and _Fine-tune the model with Federated Learning_). Lets see what differs from those examples.

Hi Zachary, I am pretty sure that the weight of the model before `model.compile()` inside the `created_compiled_keras_model()` has been changed which can be confirmed by both (`model.weights` and `model.get_weights()`).Its hard to diagnose the issue without a full picture of the situation, the code snippets are helpful but without the complete code I'm still having a hard time fully grasping what the goal is and how it is being attempted.

From what I gather so far this doesn't seem like an issue that needs fixing in TFF, but a question about how to use TFF (or TF) to achieve a particular behavior? If you agree, these kind of questions are better asked at https://stackoverflow.com/tags/tensorflow_federated, which preserves easily searchable database of questions and answers for the community to benefit from.

I'd recommend posting a question there and adding the `keras`, `tensorflow`, and `tensorflow_federated` tags. When posting a question, please try to create a minimal example (from https://github.com/tensorflow/federated/issues/758#issuecomment-531138660 it sounds like only the code inside `def create_compiled_keras_model()` may be necessary?), and post _all_ the code related to that example.> Its hard to diagnose the issue without a full picture of the situation, the code snippets are helpful but without the complete code I'm still having a hard time fully grasping what the goal is and how it is being attempted.
> 
> From what I gather so far this doesn't seem like an issue that needs fixing in TFF, but a question about how to use TFF (or TF) to achieve a particular behavior? If you agree, these kind of questions are better asked at https://stackoverflow.com/tags/tensorflow_federated, which preserves easily searchable database of questions and answers for the community to benefit from.
> 
> I'd recommend posting a question there and adding the `keras`, `tensorflow`, and `tensorflow_federated` tags. When posting a question, please try to create a minimal example (from [#758 (comment)](https://github.com/tensorflow/federated/issues/758#issuecomment-531138660) it sounds like only the code inside `def create_compiled_keras_model()` may be necessary?), and post _all_ the code related to that example.

Hi Zachary, thank you for your reply to my question which is not really a bug of so of tff. I have given up changing the model before return the compiled Keras model to generate the new state. But I just found the `tff.learning.state_with_new_model_weights` which can fix my problem in a way more convenient way. I will close this issue soon. : )@ZacharyGarrett Thanks! Please close this issue since I'm not be able to log in my own GIthub account recently.",10,2019-09-06 00:16:58,2019-09-19 15:40:13,2019-09-19 15:40:13
https://github.com/tensorflow/federated/issues/755,[],TypeError: `dataset_or_iterator` must be a Dataset or Iterator object,"TypeError: `dataset_or_iterator` must be a Dataset or Iterator object```
@tff.tf_computation(tff.SequenceType(tf.float32))
def get_local_temperature_average(local_temperatures):
  sum_and_count = (
      local_temperatures.reduce((0.0, 0), lambda x, y: (x[0] + y, x[1] + 1)))
  return sum_and_count[0] / tf.cast(sum_and_count[1], tf.float32)
```

**This code from custom_federated_algorithms_1.ipynb** 
    **python: 3.7.2** 
   **tensorflow: 1.14.0**
   **tensorflow-federated : 0.8.0**
   **tf-nightly: 1.15.0.dev20190805**  
**and I got this error:** 

`TypeError: ""dataset_or_iterator"" must be a Dataset or Iterator object, but got <class 'tensorflow.python.data.ops.dataset_ops._VariantDataset'>.`
**more information about error:**
```
TypeError                                 Traceback (most recent call last)
<ipython-input-35-3c145a7e5ece> in <module>
----> 1 @tff.tf_computation(tff.SequenceType(tf.float32))
      2 
      3 def get_local_temperature_average(local_temperatures):
      4   sum_and_count = (
      5       local_temperatures.reduce((0.0, 0), lambda x, y: (x[0] + y, x[1] + 1)))

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py in <lambda>(fn)
    413         args = (args,)
    414       arg_type = computation_types.to_type(args[0])
--> 415       return lambda fn: _wrap(fn, arg_type, self._wrapper_fn)

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py in _wrap(fn, parameter_type, wrapper_fn)
    101 
    102   # Either we have a concrete parameter type, or this is no-arg function.
--> 103   concrete_fn = wrapper_fn(fn, parameter_type, unpack=None)
    104   py_typecheck.check_type(concrete_fn, function_utils.ConcreteFunction,
    105                           'value returned by the wrapper')

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/computation_wrapper_instances.py in _tf_wrapper_fn(***failed resolving arguments***)
     42   ctx_stack = context_stack_impl.context_stack
     43   comp_pb, extra_type_spec = tensorflow_serialization.serialize_py_fn_as_tf_computation(
---> 44       target_fn, parameter_type, ctx_stack)
     45   return computation_impl.ComputationImpl(comp_pb, ctx_stack, extra_type_spec)
     46 

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/tensorflow_serialization.py in serialize_py_fn_as_tf_computation(target, parameter_type, context_stack)
    266     context = tf_computation_context.TensorFlowComputationContext(graph)
    267     with context_stack.install(context):
--> 268       result = target(*args)
    269 
    270       # TODO(b/122081673): This needs to change for TF 2.0. We may also

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in <lambda>(arg)
    600       except NameError:
    601         raise AssertionError('Args to be bound must be in scope.')
--> 602       return lambda arg: _call(fn, parameter_type, arg)
    603 
    604 

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py in _call(fn, parameter_type, arg)
    587       # forwards the call as a last-minute check.
    588       def _call(fn, parameter_type, arg):
--> 589         arg_type = type_utils.infer_type(arg)
    590         if not type_utils.is_assignable_from(parameter_type, arg_type):
    591           raise TypeError('Expected an argument of type {}, found {}.'.format(

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/type_utils.py in infer_type(arg)
     63     return computation_types.SequenceType(
     64         tf_dtypes_and_shapes_to_type(
---> 65             tf.compat.v1.data.get_output_types(arg),
     66             tf.compat.v1.data.get_output_shapes(arg)))
     67   elif isinstance(arg, anonymous_tuple.AnonymousTuple):

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in get_legacy_output_types(dataset_or_iterator)
   2037     of an element of this dataset.
   2038   """"""
-> 2039   return get_structure(dataset_or_iterator)._to_legacy_output_types()  # pylint: disable=protected-access
   2040 
   2041 

~/Python-3.7.2/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in get_structure(dataset_or_iterator)
   2001     pass
   2002   raise TypeError(""`dataset_or_iterator` must be a Dataset or Iterator object, ""
-> 2003                   ""but got %s."" % type(dataset_or_iterator))
   2004 
   2005 

TypeError: `dataset_or_iterator` must be a Dataset or Iterator object, but got <class 'tensorflow.python.data.ops.dataset_ops._VariantDataset'>.
```
 **if you need any information please tell me,i'd like to add it.  Thanks a lot !**@phalangee could you try adding `tf.compat.v1.enable_v2_behavior()` just after the code imports tensorflow?@ZacharyGarrett  > @phalangee could you try adding `tf.compat.v1.enable_v2_behavior()` just after the code imports tensorflow?
I met the same problem. I tried this method you suggested, but the same problem arose. Is there any other solution? Look forward to your reply, Thanks.
> @ZacharyGarrett > @phalangee could you try adding `tf.compat.v1.enable_v2_behavior()` just after the code imports tensorflow?
> I met the same problem. I tried this method you suggested, but the same problem arose. Is there any other solution? Look forward to your reply, Thanks.

Me too I tried this suggestion but didn't work for me as well. Is there any suggestion please. Thank YouSorry I didn't notice earlier: it looks like both `tensorflow==1.14.0` and  `tf-nightly==1.15.0.dev20190805` pip packages are installed, which might be causing problems. @nakibmehdi is this true for your system also?

TFF requires the newer nightly package, and the system maybe pulling the libraries from the `tensorflow=1.14.0` package. Could you please try uninstalling the `tensorflow` pip package and only installing the `tf-nightly` package?  I'd strongly recommend doing this inside of a virtual environment (see more at https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)> Sorry I didn't notice earlier: it looks like both `tensorflow==1.14.0` and `tf-nightly==1.15.0.dev20190805` pip packages are installed, which might be causing problems. @nakibmehdi is this true for your system also?
> 
> TFF requires the newer nightly package, and the system maybe pulling the libraries from the `tensorflow=1.14.0` package. Could you please try uninstalling the `tensorflow` pip package and only installing the `tf-nightly` package? I'd strongly recommend doing this inside of a virtual environment (see more at https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)

thank you,I tried the two methods that you mentioned but it didn't work.
`tensorflow=1.14.0`,`tf-nightly` ,`tensorflow-federated` are all installed in my virtual enviroment ,I noticed that  most code in tutorials need` import tensorflow` ,if  `pip uninstall tensorflow` , it will report an error: `""No module named 'tensorflow'""` . Is there any suggestion please or you need any information about error please tell me I met the same error today when I run the mnist demo:

os: Ubuntu 18.04
python: 3.6.8
IDE: Pycharm Runtime version: 11.0.3+12-b304.39 amd64
tensorflow: 1.14.0
tensorflow-federated : 0.8.0
tf-nightly: 1.15.0.dev20190805

**Codes:**

`

    from __future__ import absolute_import, division, print_function

    import collections

    import warnings

    from six.moves import range

    import numpy as np

    import six

    import tensorflow as tf

    warnings.simplefilter('ignore')

    tf.compat.v1.enable_v2_behavior()

    import tensorflow_federated as tff

    np.random.seed(0)

    NUM_CLIENTS = 10

    emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()

    example_dataset = emnist_train.create_tf_dataset_for_client(emnist_train.client_ids[0])

    example_element = iter(example_dataset).next()

    NUM_EPOCHS = 10

    BATCH_SIZE = 20

    SHUFFLE_BUFFER = 500

    def preprocess(dataset):
      def element_fn(element):
        return collections.OrderedDict([
            ('x', tf.reshape(element['pixels'], [-1])),
            ('y', tf.reshape(element['label'], [1])),
        ])
      return dataset.repeat(NUM_EPOCHS).map(element_fn).shuffle(
          SHUFFLE_BUFFER).batch(BATCH_SIZE)

    preprocessed_example_dataset = preprocess(example_dataset)

    sample_batch = tf.nest.map_structure(lambda x: x.numpy(), iter(preprocessed_example_dataset).next())

    def make_federated_data(client_data, client_ids):
      return [preprocess(client_data.create_tf_dataset_for_client(x)) for x in client_ids]

    sample_clients = emnist_train.client_ids[0:NUM_CLIENTS]

    federated_train_data = make_federated_data(emnist_train, sample_clients)

    def create_compiled_keras_model():
        model = tf.keras.models.Sequential([
            tf.keras.layers.Dense(
                10, activation=tf.nn.softmax, kernel_initializer='zeros', input_shape=(784,))])

    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.02),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
    return model

    def model_fn():
      keras_model = create_compiled_keras_model()
      return tff.learning.from_compiled_keras_model(keras_model, sample_batch)

    iterative_process = tff.learning.build_federated_averaging_process(model_fn)

    state = iterative_process.initialize()

    for round_num in range(2, 11):
      state, metrics = iterative_process.next(state, federated_train_data)
      print('round {:2d}, metrics={}'.format(round_num, metrics))
`


**Error details:**

`

    Traceback (most recent call last):

      File ""/home/hans/WorkSpace/FederatedLearning/main.py"", line 56, in <module>

       iterative_process = tff.learning.build_federated_averaging_process(model_fn)

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow_federated/python/learning/federated_averaging.py"", line 164, in build_federated_averaging_process

        stateful_delta_aggregate_fn, stateful_model_broadcast_fn)

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py"", line 361, in build_model_delta_optimizer_process

        @tff.tf_computation(tf_dataset_type, server_state_type.model)

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py"", line 415, in <lambda>

        return lambda fn: _wrap(fn, arg_type, self._wrapper_fn)

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py"", line 103, in _wrap

        concrete_fn = wrapper_fn(fn, parameter_type, unpack=None)

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/computation_wrapper_instances.py"", line 44, in _tf_wrapper_fn

        target_fn, parameter_type, ctx_stack)

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/tensorflow_serialization.py"", line 268, in serialize_py_fn_as_tf_computation

        result = target(*args)

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py"", line 584, in <lambda>

        return lambda arg: _unpack_and_call(fn, arg_types, kwarg_types, arg)

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py"", line 555, in _unpack_and_call

        actual_type = type_utils.infer_type(element_value)

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/type_utils.py"", line 65, in infer_type

        tf.compat.v1.data.get_output_types(arg),

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2039, in get_legacy_output_types

        return get_structure(dataset_or_iterator)._to_legacy_output_types()  # pylint: disable=protected-access

      File ""/home/hans/WorkSpace/venv/FederatedLearning/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2003, in get_structure

        ""but got %s."" % type(dataset_or_iterator))

    TypeError: `dataset_or_iterator` must be a Dataset or Iterator object, but got <class 'tensorflow.python.data.ops.dataset_ops._VariantDataset'>.

`

Thank you @ZacharyGarrett for the two methotds, but neither of them worked. @ZacharyGarrett @phalangee 

I solved this problem by uninstall tensorflow and tf-nightly and then reinstall tf-nightly.Thanks @HandsomeHans!

@phalangee If `tensorflow` was not uninstalled before installing `tensorflow-federated`, likely `tf-nightly` needs to be installed again the steps form @HandsomeHans should achieve this. Alternatively, when creating a new virtual environment avoid pip installing `tensorflow` and only install `tensorflow-federated` which will also install the necessary dependency `tf-nightly` automatically.Thanks @ZacharyGarrett  @HandsomeHans  I have solved this problem as you suggested. Thank you very much ! I am still getting same error.. I have tried to uninstall tensorflow , and installing tensorflow- federated",10,2019-09-02 07:13:08,2019-10-02 08:52:26,2019-09-09 03:03:38
https://github.com/tensorflow/federated/issues/752,[],Fail to test the tff in macos,"Fail to test the tff in macosHi there,

I have been playing with tff in colab for a while, but whenever I tried to install tff in  my local Pycharm and run the test code `tff.federated_computation(lambda: 'Hello, World!')()`. There is just nothing to print, and the process finished normally. I have double-checked that I have the same pre-requisite as in the doc.
![image](https://user-images.githubusercontent.com/38281790/63576904-8a8ee200-c5d0-11e9-9764-f393b7a4a0bf.png)
Could you try wrapping with a `print()` call, similar to the change made in #738

```python
print(tff.federated_computation(lambda: 'Hello, World!')())
```> Could you try wrapping with a `print()` call, similar to the change made in #738
> 
> ```python
> print(tff.federated_computation(lambda: 'Hello, World!')())
> ```

Oh, I should have found that. Thx for your help.",2,2019-08-23 08:05:13,2019-08-24 04:58:41,2019-08-24 04:58:41
https://github.com/tensorflow/federated/issues/751,[],Memory blows up using CNN model with multiple clients,"Memory blows up using CNN model with multiple clientsI tried to implement the tutorial of federated image classification with a CNN model, but it turns out that while I set the client number to 100, each round of training takes up about 16 Gb of memory and it stacks on after each round.  I am not sure that is there any issue with my code or is it because that after each round the weight tensors are not deleted.  I would appreciate it so much if anyone could help.  Below is my python code:
  
`from __future__ import absolute_import
from six.moves import range
import tensorflow as tf
from tensorflow.python.keras.optimizer_v2 import gradient_descent
import tensorflow_federated as tff
import collections
import time
import random

from tensorflow_federated.python.examples import mnist
tf.compat.v1.enable_v2_behavior()

NUM_CLIENTS = 100
NUM_EPOCHS = 5
BATCH_SIZE = 10
ROUNDS = 100
LEARNING_RATE = 0.02

tff.framework.set_default_executor(
    tff.framework.create_local_executor(NUM_CLIENTS))

source, _ = tff.simulation.datasets.emnist.load_data()

def client_data(n):
  dataset = source.create_tf_dataset_for_client(source.client_ids[n])
  return mnist.keras_dataset_from_emnist(dataset).repeat(NUM_EPOCHS).batch(BATCH_SIZE)

train_data = [client_data(n) for n in range(NUM_CLIENTS)]

sample_batch = tf.nest.map_structure(
    lambda x: x.numpy(), iter(train_data[0]).next())

def CNN_compiled():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Reshape((28, 28, 1), input_shape=(784,)),
        tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation=""relu"", padding=""same"", strides=1),
        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding='valid'),
        tf.keras.layers.Conv2D(64, kernel_size=(5, 5), activation=""relu"", padding=""same"", strides=1),
        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding='valid'),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation=""relu""),
        tf.keras.layers.Dense(10, activation=""softmax""),
    ])
    def loss_fn(y_true, y_pred):
        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(
            y_true, y_pred))
    model.compile(
        loss=loss_fn,
        optimizer=gradient_descent.SGD(LEARNING_RATE),  # decay
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
    return model

def model_fn():
  keras_model = CNN_compiled()
  return tff.learning.from_compiled_keras_model(keras_model, sample_batch)

trainer = tff.learning.build_federated_averaging_process(model_fn)
state = trainer.initialize()
for _ in range(ROUNDS):
    now = time.time()
    state, metrics = trainer.next(state, train_data)
    time_spent = time.time() - now
    print (metrics, time_spent)`Hi @JJJJJamie 
We are looking at some related issues. Wanted to let you know we haven't forgotten about this!

KeithAfter trying the code for a few days, I'm able to run your code @JJJJJamie successfully with GPU and with no memory leak. My problem was that my path was pointed at CUDA 9.0. After I reset the path pointing to CUDA10.0 and update my cudnn to 7.6, I'm able to run your code with no memory leak and still use GPU. You might first want to try the following code to see if you can run tensorflow code with GPU.

```
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python import layers
from tensorflow.python import nn

mnist = input_data.read_data_sets(""./MNIST_data/"", one_hot=True)

sess = tf.InteractiveSession()

x = tf.placeholder(tf.float32, [None, 784])
y_ = tf.placeholder(tf.float32, [None, 10])
x_image = tf.reshape(x, [-1, 28, 28, 1])

conv1 = layers.conv2d(x_image, 32, 5, padding='same', name='conv1')
relu1 = nn.relu(conv1, name='relu1')
maxppool1 = layers.max_pooling2d(relu1, 2, 2, name='maxpool1')

conv2 = layers.conv2d(maxppool1, 64, 5, padding='same', name='conv2')
relu2 = nn.relu(conv2, name='relu2')
maxppool2 = layers.max_pooling2d(relu2, 2, 2, name='maxpool2')

flattern = layers.flatten(maxppool2, name='flattern')
fc1 = layers.dense(flattern, 1024, activation=tf.nn.relu, name='fc1')
fc1_dropout = layers.dropout(fc1, 0.8, name='fc1_dropout')
fc2 = layers.dense(fc1_dropout, 10, activation=tf.nn.softmax, name='fc2')

cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(fc2), reduction_indices=[1]))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
correct_prediciton = tf.equal(tf.argmax(fc2, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediciton, tf.float32))

tf.global_variables_initializer().run()
for i in range(1, 1001):
    batch = mnist.train.next_batch(64)
    if i % 10 == 0:
        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})
        print(""step %d, examples %d, training accuracy %g"" % (i, i * 64, train_accuracy))
        print(""test accuracy %g"" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
    train_step.run(feed_dict={x: batch[0], y_: batch[1]})

print(""test accuracy %g"" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
```Given [this recent commit](https://github.com/tensorflow/federated/commit/5299226ee7c736c46d1d3def8c6726214f6f9cb0), this issue should be mitigated. Can those seeing this problem confirm?Closing since this should be no longer an issue.",4,2019-08-22 20:52:07,2019-12-01 19:58:34,2019-12-01 19:58:34
https://github.com/tensorflow/federated/issues/748,[],The official document should be updated,"The official document should be updatedHi there,

I got a bug that the `tff.utlis.get_variables()` function cannot be found under [python/core/utils/tf_computation_utils.py](http://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/core/utils/tf_computation_utils.py) which confused me a lot. After I accessed the directory to check the tf_computation_utlis.py file, I found that `tff.utlis.get_variables()` has been changed into `tff.utlis.create_variables()`. I suppose both the API document and tutorials should be updated correspondingly to prevent further confusions.And the version of TFF should be updated too. The document said the version is 0.7.0, but the current version for demo is 0.8.0 these days.Apologies for the delay, the documentation should now be updated. The text generation and image classification tutorials still need some work, but the API docs and the custom federated algorithms tutorials (part1 and part2) should be good to go.",2,2019-08-22 05:59:56,2019-09-02 13:25:41,2019-09-02 13:25:40
https://github.com/tensorflow/federated/issues/747,[],"Need help for ""federated_learning_for_text_generation""","Need help for ""federated_learning_for_text_generation""when run the line ""state, metrics = fed_avg.next(state, [example_dataset.take(1)])""

I got this error:
TypeError: The tensor type int64[8,100] of the value representation does not match the type spec int32[?,100].

How to solve it?

Thx first~~~ Hi @JKbasara,

I think our best bet here is some version mismatch--so can we get some more detail on a few things?

First, how are you running this code? Is it in a hosted colab runtime (e.g. in a browser on colab.sandbox.google.com), a local jupyter notebook, or as a standalone python script?

Second, can you give the versions of all the python packages in the runtime you are using? E.g. if you're on colab, the output of running `!pip freeze` in a cell, or if you are running as a script in a Conda environment, the output of `conda list -n myenv`? However you're running this, I think we will need the package versions.

Finally, could we get a deeper stacktrace? The error is from TFF, but it's not exactly clear to me where in the stack this is coming from.

Happy to help! Thanks for your interest in TFF!

Keith> Hi @JKbasara,
> 
> I think our best bet here is some version mismatch--so can we get some more detail on a few things?
> 
> First, how are you running this code? Is it in a hosted colab runtime (e.g. in a browser on colab.sandbox.google.com), a local jupyter notebook, or as a standalone python script?
> 
> Second, can you give the versions of all the python packages in the runtime you are using? E.g. if you're on colab, the output of running `!pip freeze` in a cell, or if you are running as a script in a Conda environment, the output of `conda list -n myenv`? However you're running this, I think we will need the package versions.
> 
> Finally, could we get a deeper stacktrace? The error is from TFF, but it's not exactly clear to me where in the stack this is coming from.
> 
> Happy to help! Thanks for your interest in TFF!
> 
> Keith

Thank you! Keith
I got your idea~~~

Im a graduate student and our team are trying to use TFF for a project research.
All my codes run above the environment of **Win10 and Visual Studio 2019**.
These are my python packages:
 
![image](https://user-images.githubusercontent.com/4019752/63480094-33e7b200-c4c3-11e9-9b52-4e6d337ec409.png)

all packages list:
![image](https://user-images.githubusercontent.com/4019752/63480143-642f5080-c4c3-11e9-9f70-3e33f95687f3.png)
![image](https://user-images.githubusercontent.com/4019752/63480180-89bc5a00-c4c3-11e9-954c-882e9e950391.png)

By the way, **I can run it well at Google Colab**, so I think that maybe I miss some packages.

The log for error:

**Apply a constraint manually following the optimizer update step.
E0822 10:09:35.617088 20464 federated_learning_for_text_generation.py:173] The tensor type int64[8,100] of the value representation does not match the type spec int32[?,100].
Traceback (most recent call last):
  File ""D:\XXX\Python\XXX\TFF_tutorials\federated_learning_for_text_generation.py"", line 171, in <module>
    state, metrics = fed_avg.next(state, [example_dataset.take(1)])
  File ""C:\Users\XXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_federated\python\core\impl\utils\function_utils.py"", line 629, in __call__
    arg = pack_args(self._type_signature.parameter, args, kwargs, context)
  File ""C:\Users\XXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_federated\python\core\impl\utils\function_utils.py"", line 387, in pack_args
    context)
  File ""C:\Users\XXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_federated\python\core\impl\utils\function_utils.py"", line 309, in pack_args_into_anonymous_tuple
    result_elements.append((name, context.ingest(arg_value, elem_type)))
  File ""C:\Users\XXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py"", line 670, in ingest
    return to_representation_for_type(arg, type_spec, _handle_callable)
  File ""C:\Users\XXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py"", line 240, in to_representation_for_type
    for v in value
  File ""C:\Users\XXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py"", line 240, in <listcomp>
    for v in value
  File ""C:\Users\XXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py"", line 199, in to_representation_for_type
    for v in value
  File ""C:\Users\XXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py"", line 199, in <listcomp>
    for v in value
  File ""C:\Users\XXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py"", line 191, in to_representation_for_type
    callable_handler)
  File ""C:\Users\XXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py"", line 164, in to_representation_for_type
    'the type spec {}.'.format(inferred_type_spec, type_spec))
TypeError: The tensor type int64[8,100] of the value representation does not match the type spec int32[?,100].**

For more details about my code (I am a beginner for python and TFF), I am very glad to offer more info.Interesting. I suspect that this has something to do with running on Windows, as we generally build and run on Linux. What seems to be *not* happening correctly is the following code snippet from the tutorial:
```
def create_tff_model():
  x = tf.constant(np.random.randint(1, len(vocab), size=[BATCH_SIZE, SEQ_LENGTH]))
  dummy_batch = collections.OrderedDict([('x', x), ('y', x)]) 
  keras_model_clone = compile(tf.keras.models.clone_model(keras_model))
  return tff.learning.from_compiled_keras_model(
      keras_model_clone, dummy_batch=dummy_batch)
```

TFF is strongly-typed (with its own type system), so functions generally need type declarations, or some way to infer the types they should expect to accept and return. This *should* be taken care of by this code snippet, as part of the under-the-hood integration between TFF and TF, in particular keras.

What seems to be happening here is: TFF performs type inference on the models it constructs by using a dummy batch of data. It looks to me that the data generated by the `tf.constant` call in the above snippet is actually coming out as `tf.int32`, and thus this is the type expected by TFF for its input data. However, the dataset clearly generates int64 entries, and thus TFF raises.

It is possible that there are some subtle differences in the Windows distributions, although one would think `pip` would abstract this away. Perhaps the lack of a standalone Keras package is causing some misdirection (when I `pip freeze` after installing TFF to a colab, I see a Keras 2.2.5 install, which comes pre-installed in Colab runtimes).

I think if you don't want to worry about these issues, however, there *should* be a simple fix. Try simply replacing 
```
x = tf.constant(np.random.randint(1, len(vocab), size=[BATCH_SIZE, SEQ_LENGTH]))
```
above, with:
```
x = tf.constant(np.random.randint(1, len(vocab), size=[BATCH_SIZE, SEQ_LENGTH]), dtype=tf.int64)
```
Unfortunately since we have no Windows machines easily available, we can't really repro your issue and make sure this works...> Interesting. I suspect that this has something to do with running on Windows, as we generally build and run on Linux. What seems to be _not_ happening correctly is the following code snippet from the tutorial:
> 
> ```
> def create_tff_model():
>   x = tf.constant(np.random.randint(1, len(vocab), size=[BATCH_SIZE, SEQ_LENGTH]))
>   dummy_batch = collections.OrderedDict([('x', x), ('y', x)]) 
>   keras_model_clone = compile(tf.keras.models.clone_model(keras_model))
>   return tff.learning.from_compiled_keras_model(
>       keras_model_clone, dummy_batch=dummy_batch)
> ```
> 
> TFF is strongly-typed (with its own type system), so functions generally need type declarations, or some way to infer the types they should expect to accept and return. This _should_ be taken care of by this code snippet, as part of the under-the-hood integration between TFF and TF, in particular keras.
> 
> What seems to be happening here is: TFF performs type inference on the models it constructs by using a dummy batch of data. It looks to me that the data generated by the `tf.constant` call in the above snippet is actually coming out as `tf.int32`, and thus this is the type expected by TFF for its input data. However, the dataset clearly generates int64 entries, and thus TFF raises.
> 
> It is possible that there are some subtle differences in the Windows distributions, although one would think `pip` would abstract this away. Perhaps the lack of a standalone Keras package is causing some misdirection (when I `pip freeze` after installing TFF to a colab, I see a Keras 2.2.5 install, which comes pre-installed in Colab runtimes).
> 
> I think if you don't want to worry about these issues, however, there _should_ be a simple fix. Try simply replacing
> 
> ```
> x = tf.constant(np.random.randint(1, len(vocab), size=[BATCH_SIZE, SEQ_LENGTH]))
> ```
> 
> above, with:
> 
> ```
> x = tf.constant(np.random.randint(1, len(vocab), size=[BATCH_SIZE, SEQ_LENGTH]), dtype=tf.int64)
> ```
> 
> Unfortunately since we have no Windows machines easily available, we can't really repro your issue and make sure this works...

@jkr26 Thank you! It worked.",4,2019-08-21 11:18:07,2019-09-06 04:12:04,2019-09-06 04:12:04
https://github.com/tensorflow/federated/issues/744,[],Cannot use with tf-nightly-gpu,"Cannot use with tf-nightly-gpuHere are my installation steps

machine:
cuda 10.0
cuDNN v7.5

1. activate virtualenv with python 3.5.2, 
2. install tensorflow_federated from pip command
    -> successfully finished with learning on cpu
3. remove tf-nightly and install tensorflow-gpu
    -> got an error; dataset_or_iterator must be a Dataset or Iterator object.
4. install tf-nightly-gpu
    -> Things seems to work well, but it tells me i need cuDNN v7.6.

But mine is cuDNN v7.5. Do i need to upgrade the library ? 
Or I got something wrong with the installation steps ?
Hi @klize,

It does seem that upgrading cuDNN is your best bet. TFF develops close to tf-nightly internally, and this can cause issues when using a released version of tensorflow (the `dataset_or_iterator` error is one such issue; TFF uses variant tensors to represent `tf.data.Datasets`, as in `tf.data.experimental.to/from_variant`).

This means that the best bet for a TFF power user is to always ensure they are using the appropriate `tf-nightly` package, and it seems that `tf-nightly-gpu` is dropping support for cuDNN 7.5 and moving to 7.6.",1,2019-08-12 01:55:13,2019-08-12 16:00:23,2019-08-12 16:00:23
https://github.com/tensorflow/federated/issues/743,[],Unable to run Image classification example,"Unable to run Image classification exampleI am running the image classification example on my local computer with Python3.5.1, tensorflow 1.4.1 and tensorflow-federated 0.7

I used the exact same codes and the following error occurs at `iterative_process = tff.learning.build_federated_averaging_process(model_fn)`

the following the Error trace:
`TypeError                                 Traceback (most recent call last)
<ipython-input-11-fe6de5c64986> in <module>
      1 #@test {""output"": ""ignore""}
----> 2 iterative_process = tff.learning.build_federated_averaging_process(model_fn)

~/.local/lib/python3.5/site-packages/tensorflow_federated/python/learning/federated_averaging.py in build_federated_averaging_process(model_fn, server_optimizer_fn, client_weight_fn, stateful_delta_aggregate_fn, stateful_model_broadcast_fn)
    162   return optimizer_utils.build_model_delta_optimizer_process(
    163       model_fn, client_fed_avg, server_optimizer_fn,
--> 164       stateful_delta_aggregate_fn, stateful_model_broadcast_fn)

~/.local/lib/python3.5/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py in build_model_delta_optimizer_process(model_fn, model_to_client_delta_fn, server_optimizer_fn, stateful_delta_aggregate_fn, stateful_model_broadcast_fn)
    359   server_state_type = tf_init_fn.type_signature.result
    360 
--> 361   @tff.tf_computation(tf_dataset_type, server_state_type.model)
    362   def tf_client_delta(tf_dataset, initial_model_weights):
    363     """"""Performs client local model optimization.

~/.local/lib/python3.5/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py in <lambda>(fn)
    413         args = (args,)
    414       arg_type = computation_types.to_type(args[0])
--> 415       return lambda fn: _wrap(fn, arg_type, self._wrapper_fn)

~/.local/lib/python3.5/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py in _wrap(fn, parameter_type, wrapper_fn)
    101 
    102   # Either we have a concrete parameter type, or this is no-arg function.
--> 103   concrete_fn = wrapper_fn(fn, parameter_type, unpack=None)
    104   py_typecheck.check_type(concrete_fn, function_utils.ConcreteFunction,
    105                           'value returned by the wrapper')

~/.local/lib/python3.5/site-packages/tensorflow_federated/python/core/impl/computation_wrapper_instances.py in _tf_wrapper_fn(***failed resolving arguments***)
     42   ctx_stack = context_stack_impl.context_stack
     43   comp_pb, extra_type_spec = tensorflow_serialization.serialize_py_fn_as_tf_computation(
---> 44       target_fn, parameter_type, ctx_stack)
     45   return computation_impl.ComputationImpl(comp_pb, ctx_stack, extra_type_spec)
     46 

~/.local/lib/python3.5/site-packages/tensorflow_federated/python/core/impl/tensorflow_serialization.py in serialize_py_fn_as_tf_computation(target, parameter_type, context_stack)
    266     context = tf_computation_context.TensorFlowComputationContext(graph)
    267     with context_stack.install(context):
--> 268       result = target(*args)
    269 
    270       # TODO(b/122081673): This needs to change for TF 2.0. We may also

~/.local/lib/python3.5/site-packages/tensorflow_federated/python/core/impl/function_utils.py in <lambda>(arg)
    583       except NameError:
    584         raise AssertionError('Args to be bound must be in scope.')
--> 585       return lambda arg: _unpack_and_call(fn, arg_types, kwarg_types, arg)
    586     else:
    587       # An interceptor function that verifies the actual parameter before it

~/.local/lib/python3.5/site-packages/tensorflow_federated/python/core/impl/function_utils.py in _unpack_and_call(fn, arg_types, kwarg_types, arg)
    554         for idx, expected_type in enumerate(arg_types):
    555           element_value = arg[idx]
--> 556           actual_type = type_utils.infer_type(element_value)
    557           if not type_utils.is_assignable_from(expected_type, actual_type):
    558             raise TypeError('Expected element at position {} to be '

~/.local/lib/python3.5/site-packages/tensorflow_federated/python/core/impl/type_utils.py in infer_type(arg)
     64     return computation_types.SequenceType(
     65         tf_dtypes_and_shapes_to_type(
---> 66             tf.compat.v1.data.get_output_types(arg),
     67             tf.compat.v1.data.get_output_shapes(arg)))
     68   elif isinstance(arg, anonymous_tuple.AnonymousTuple):

~/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py in get_legacy_output_types(dataset_or_iterator)
   2037     of an element of this dataset.
   2038   """"""
-> 2039   return get_structure(dataset_or_iterator)._to_legacy_output_types()  # pylint: disable=protected-access
   2040 
   2041 

~/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py in get_structure(dataset_or_iterator)
   2001     pass
   2002   raise TypeError(""`dataset_or_iterator` must be a Dataset or Iterator object, ""
-> 2003                   ""but got %s."" % type(dataset_or_iterator))
   2004 
   2005 

TypeError: `dataset_or_iterator` must be a Dataset or Iterator object, but got <class 'tensorflow.python.data.ops.dataset_ops._VariantDataset'>.
``tensorflow-federated v0.7.0` may not be compatible with `tensorflow v1.14.1`. The table of TFF and TF compatibility is here: https://github.com/tensorflow/federated#compatibility

Could you try uninstalling the `tensorflow` pip package and install `tf-nightly` with version `1.15.0.dev20190711` or newer?

> `tensorflow-federated v0.7.0` may not be compatible with `tensorflow v1.14.1`. The table of TFF and TF compatibility is here: https://github.com/tensorflow/federated#compatibility
> 
> Could you try uninstalling the `tensorflow` pip package and install `tf-nightly` with version `1.15.0.dev20190711` or newer?

Thanks! I solved tihs by using python3.7.",2,2019-08-08 06:25:13,2019-08-09 06:10:54,2019-08-09 06:10:54
https://github.com/tensorflow/federated/issues/741,[],TF 2.0 migration,"TF 2.0 migrationAny plans to migrate to support 2.0?

From contrib side, I can see package failing on `tf.contrib.image`, where some functionality like `angles_to_projective_transforms` a well as `translations_to_projective_transforms ` and `compose_transforms` are already supported by [tf/addons](https://github.com/tensorflow/addons/tree/master/tensorflow_addons/image)

So if you are interested, I can migrate this to use addons directly.

Additional visible contrib dependency is `contrib.framework.nest` that looks like is available within `tensorflow.python.data.util` with example would be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L540

cc @dynamicwebpaige@lc0,

We are tracking this internally--thanks for the offer! But there are some extra complications that come with being a TensorFlow family open-source project, and this is unfortunately one of them. We are planning to be fully 2.0-compatible *soon*, but unfortunately exactly when ""soon"" is is a little up in the air right now.

Thanks for your interest!  We are likewise excited for TF2.

Keith@jkr26 sounds great. Lemme know if you have a beta to test. Happy to test on my side@lc0 the latest version of TFF (`0.9.0`) is fully migrated to TF 2.0. Please take a look!",3,2019-07-31 22:35:02,2019-10-24 14:16:26,2019-10-24 14:16:25
https://github.com/tensorflow/federated/issues/724,[],"Can't run the result of ""hello world""","Can't run the result of ""hello world""I have installed all the dependent function packages, and I have not reported an error, but I have been running for no results.
/home/liuyi/venv/bin/python3.6 /home/liuyi/PycharmProjects/FL_images/fl_minst.py
WARNING: Logging before flag parsing goes to stderr.
W0723 15:41:17.035517 140592739182400 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

2019-07-23 15:41:17.663317: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-23 15:41:17.690462: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2601000000 Hz
2019-07-23 15:41:17.690809: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fde20149ef0 executing computations on platform Host. Devices:
2019-07-23 15:41:17.690836: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>


This situation has remained unchanged for 15 minutes.Hi @hdliuyi,

Could you give some more detail? In particular, what command you are running, the output of pip freeze in your environment, and the code you are trying to execute? Also, are you building TFF from source or installing the pip package?Just tested on fresh env in conda and it works fine. Output is a bit noisy but the last line is actually `Hello world`

@hdliuyi are you sure, that you do not get the last line as expected?> Hi @hdliuyi,
> 
> Could you give some more detail? In particular, what command you are running, the output of pip freeze in your environment, and the code you are trying to execute? Also, are you building TFF from source or installing the pip package?

I am building TFF by installing the pip package. Actually, I can not run the code to output ""hello world"", but I can run other codes, for example, TFF_CNN. Very strange!!!> Just tested on fresh env in conda and it works fine. Output is a bit noisy but the last line is actually `Hello world`
> 
> @hdliuyi are you sure, that you do not get the last line as expected?

Yes, sure. I can not run the code to output 'hello world'.My code as follow:
`from __future__ import absolute_import, division, print_function

import collections
import warnings
from six.moves import range
import numpy as np
import six
import tensorflow as tf

warnings.simplefilter('ignore')

tf.compat.v1.enable_v2_behavior()

import tensorflow_federated as tff

np.random.seed(0)

NUM_CLIENTS = 10

if six.PY3:
  tff.framework.set_default_executor(
      tff.framework.create_local_executor(NUM_CLIENTS))

tff.federated_computation(lambda: 'Hello, World!')()`@niklausliu can you print your pip freeze or even directly in notebook

```py
try:
    from pip._internal.operations import freeze
except ImportError:  # pip < 10.0
    from pip.operations import freeze

for module in freeze.freeze():
    print(module)
```From looking at the snippet above, are you running this in a ipython notebook, or as a Python script? If as a Python script, is it possible that the string `'Hello, World!'` is being returned inside the script, but not printed? If so, simply inserting a print statement on the last line should resolve this issue.Closing, assuming this was lack of a print statement. Feel free to reopen if that wasn't the case.",9,2019-07-23 06:40:09,2019-08-13 16:04:51,2019-08-13 16:04:51
https://github.com/tensorflow/federated/issues/717,[],Can't be installed in Macos,"Can't be installed in MacosFail in ""pip install tensorflow_federated"".
The system is Macos 10.14.5 .
Get the:
running setup.py clean for grpcio
Failed to build grpcio
Installing collected packages: grpcio, tb-nightly, tf-nightly, tensorflow-federated
  Found existing installation: grpcio 1.22.0
    Uninstalling grpcio-1.22.0:
      Successfully uninstalled grpcio-1.22.0
  Running setup.py install for grpcio ... error
    ERROR: Complete output from command /anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/private/tmp/pip-install-_0lu2taf/grpcio/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/tmp/pip-record-asdxp84g/install-record.txt --single-version-externally-managed --compile:
    ERROR: Found cython-generated files...
    running install

Have the same problem here. Failed to install with Anaconda on MacOS.grpc/grpc#19646 indicates we need to update the `grpcio` version, I'll take a look at this.Could one of your try building TFF with the new requirements and seeing if this has fixed your issue?

https://github.com/tensorflow/federated/blob/master/docs/install.md#build-the-tensorflow-federated-pip-packageClosing due to inactivity (2 months). Please let us know if this issue persists.",4,2019-07-22 11:06:43,2019-10-24 14:17:11,2019-10-24 14:17:10
https://github.com/tensorflow/federated/issues/709,[],"Unsuccessfully running tensorflow federated ""Hello world"" example","Unsuccessfully running tensorflow federated ""Hello world"" exampleHello, 

I am trying to set up tensorflow federated environment on python 3.6, but can't manage to run the following recommended instruction from https://www.tensorflow.org/federated/install :
python3 -c ""import tensorflow_federated as tff; tff.federated_computation(lambda: 'Hello World')()"" 

The return message that I get is: 
WARNING: Logging before flag parsing goes to stderr.
W0719 10:34:39.394229 140431673673536 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

2019-07-19 10:34:39.827082: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-19 10:34:39.859038: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494080000 Hz
2019-07-19 10:34:39.859594: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49125d0 executing computations on platform Host. Devices:
2019-07-19 10:34:39.859632: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
Exception ignored in: <bound method _TensorCacheDeleter.__del__ of <tensorflow.python.eager.context._TensorCacheDeleter object at 0x7fb8a6fdd908>>
Traceback (most recent call last):
  File ""/home/katarina/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py"", line 316, in __del__
TypeError: argument of type 'NoneType' is not iterable


After a while I've noticed that if I re-run the command ""pip3 install tensorflow"" the error changes. When I then re-run ""pip3 install tensorflow_federated"" again, the same error comes back. I've checked with ""pip3 freeze"" if any packages changed versions in between running the two commands, but all the packages stayed the same. 

In testing the behavior of the error, I was using the following two commands to tryout tensorflow and tensorflow_federated: 
1) python3 -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
2) python3 -c ""import tensorflow_federated as tff; tff.federated_computation(lambda: 'Hello World')()""

#AFTER RUNNING ""pip3 install tensorflow""
1)  python3 -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
Tensor(""Sum:0"", shape=(), dtype=float32)
(No error)

2) python3 -c ""import tensorflow_federated as tff; tff.federated_computation(lambda: 'Hello World')()""

WARNING: Logging before flag parsing goes to stderr.
W0719 10:36:51.846138 140628469716800 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

2019-07-19 10:36:51.854925: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-19 10:36:51.878986: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494080000 Hz
2019-07-19 10:36:51.879500: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x39654d0 executing computations on platform Host. Devices:
2019-07-19 10:36:51.879536: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>

#AFTER RUNNING ""pip3 install tensorflow_federated""
1) python3 -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""

Tensor(""Sum:0"", shape=(), dtype=float32)
Exception ignored in: <bound method _TensorCacheDeleter.__del__ of <tensorflow.python.eager.context._TensorCacheDeleter object at 0x7f8709ca2eb8>>
Traceback (most recent call last):
  File ""/home/katarina/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py"", line 316, in __del__
TypeError: argument of type 'NoneType' is not iterable

2) python3 -c ""import tensorflow_federated as tff; tff.federated_computation(lambda: 'Hello World')()""

WARNING: Logging before flag parsing goes to stderr.
W0719 10:34:39.394229 140431673673536 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

2019-07-19 10:34:39.827082: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-19 10:34:39.859038: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494080000 Hz
2019-07-19 10:34:39.859594: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49125d0 executing computations on platform Host. Devices:
2019-07-19 10:34:39.859632: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
Exception ignored in: <bound method _TensorCacheDeleter.__del__ of <tensorflow.python.eager.context._TensorCacheDeleter object at 0x7fb8a6fdd908>>
Traceback (most recent call last):
  File ""/home/katarina/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py"", line 316, in __del__
TypeError: argument of type 'NoneType' is not iterable


I'm also attaching the list of packages installed 
[TFF_packages.txt](https://github.com/tensorflow/federated/files/3411247/TFF_packages.txt)


Thank you in advance! :)

Oops sorry for the ugly issue, I didn't realize it will get this ugly. The TypeError is during TF cleanup, we expect it to be transient. 

On the other issue, the lack of print in 
```
import tensorflow_federated as tff; tff.federated_computation(lambda: 'Hello World')()
```
was causing 'hello world' to simple be returned as a string, and just not pushed to stdout. Given the merged PR above, `print` is added to the install documentation, and the current install instructions should read something like 
```
import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())
```

which should actually print this time.

Thanks for your interest!",2,2019-07-19 13:30:38,2019-08-27 16:00:15,2019-08-27 16:00:14
https://github.com/tensorflow/federated/issues/677,[],front page example does not run,"front page example does not runhi,

I've installed tensorflow-federated and tried to run the main short example in https://www.tensorflow.org/federated

it fails because from `tensorflow_federated.python.examples` does not seem to be included in the published version

(I think the example should be updated in a way that makes it immediately runnable after tff install)Sorry for that, please try this: https://github.com/tensorflow/federated/releases/tag/v0.7.0works with tff@0.7.0 (and python 3.7) 

(do note that the tutorials in the site still point to the 0.6.0 version)

thanks!",2,2019-07-11 10:34:09,2019-07-12 07:02:44,2019-07-12 06:58:52
https://github.com/tensorflow/federated/issues/611,[],AttributeError: module 'tensorflow._api.v1.data.experimental' has no attribute 'NestedStructure',"AttributeError: module 'tensorflow._api.v1.data.experimental' has no attribute 'NestedStructure'Hello, 
I was able to run my code just 2 days ago on colab, but now I can't because of this error AttributeError: module 'tensorflow._api.v1.data.experimental' has no attribute 'NestedStructure' . The error is related to tff.learning.build_federated_averaging_process(model_fn). 
 What is the source of this problem ? 
Thank you Duplicate of #609 

Thanks!
Keith",1,2019-06-25 16:24:34,2019-06-25 16:52:03,2019-06-25 16:52:03
https://github.com/tensorflow/federated/issues/610,[],"The error ""ImportError: cannot import name 'computation_pb2'"" still appears","The error ""ImportError: cannot import name 'computation_pb2'"" still appearsHi!

The error ""ImportError: cannot import name 'computation_pb2'"" still appears when running
python -c ""import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())""

even after installation of TF Federated via Bazel. At the same time running
bazel test tensorflow_federated/python/examples/mnist:models_test
performs well, with no error.

Specs:
MacOS Sierra 10.12
Python 3.6
PyCharm CE 2019 IDE
Conda envOne hypothesis is that this error is seen when running `python -c ""import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())""` in the root directory of the git repository. This might result in `import tensorflow_federated` importing from `./tensorflow_federated/__init__.py` instead of the pip package.

Could you verify whether the ""hello world"" test works from a directly that does not have a `tensorflow_federated` sub-directory?I had the same issue. When I changed the directory. The ""Hello world"" worked! ThanksThanks for confirming @bsabri!",3,2019-06-25 11:13:01,2019-07-10 22:43:03,2019-07-10 22:43:03
https://github.com/tensorflow/federated/issues/609,[],tff tutorial notebooks are broken,"tff tutorial notebooks are brokenFor text generation, it would stuck on 
```python
#@test {""output"": ""ignore""}
iterative_process = tff.learning.build_federated_averaging_process(model_fn)
```
with error message
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-15-0fdb188570d0> in <module>()
----> 1 iterative_process = tff.learning.build_federated_averaging_process(model_fn)

17 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation_wrapper.py in __getattr__(self, name)
    102     if name.startswith('_dw_'):
    103       raise AttributeError('Accessing local variables before they are created.')
--> 104     attr = getattr(self._dw_wrapped_module, name)
    105     if (self._dw_warning_count < _PER_MODULE_WARNING_LIMIT and
    106         name not in self._dw_deprecated_printed):

AttributeError: module 'tensorflow._api.v1.data.experimental' has no attribute 'NestedStructure'
```
However on my machine the error is different, I can build the averaging process, but would stuck on the next step:
```python
state = fed_avg.initialize()
state, metrics = fed_avg.next(state, [example_dataset.take(1)])
print(metrics)
```
with error message
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-20-aa5f31726a7a> in <module>
      1 state = fed_avg.initialize()
----> 2 state, metrics = fed_avg.next(state, [example_dataset.take(1)])
      3 print(metrics)

~\Anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\impl\function_utils.py in __call__(self, *args, **kwargs)
    628   def __call__(self, *args, **kwargs):
    629     context = self._context_stack.current
--> 630     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
    631     return context.invoke(self, arg)
    632 

~\Anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\impl\function_utils.py in pack_args(parameter_type, args, kwargs, context)
    385       else:
    386         arg = pack_args_into_anonymous_tuple(args, kwargs, parameter_type,
--> 387                                              context)
    388       return context.ingest(arg, parameter_type)
    389 

~\Anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\impl\function_utils.py in pack_args_into_anonymous_tuple(args, kwargs, type_spec, context)
    307           else:
    308             arg_value = args[index]
--> 309             result_elements.append((name, context.ingest(arg_value, elem_type)))
    310             positions_used.add(index)
    311         elif name is not None and name in kwargs:

~\Anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py in ingest(self, arg, type_spec)
    666       return fn
    667 
--> 668     return to_representation_for_type(arg, type_spec, _handle_callable)
    669 
    670   def invoke(self, fn, arg):

~\Anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py in to_representation_for_type(value, type_spec, callable_handler)
    240       return [
    241           to_representation_for_type(v, type_spec.member, callable_handler)
--> 242           for v in value
    243       ]
    244   else:

~\Anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py in <listcomp>(.0)
    240       return [
    241           to_representation_for_type(v, type_spec.member, callable_handler)
--> 242           for v in value
    243       ]
    244   else:

~\Anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py in to_representation_for_type(value, type_spec, callable_handler)
    199         return [
    200             to_representation_for_type(v, type_spec.element, callable_handler)
--> 201             for v in value
    202         ]
    203       else:

~\Anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py in <listcomp>(.0)
    199         return [
    200             to_representation_for_type(v, type_spec.element, callable_handler)
--> 201             for v in value
    202         ]
    203       else:

~\Anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py in to_representation_for_type(value, type_spec, callable_handler)
    191                 value_elem_name, type_elem_name, index, value, type_spec))
    192       converted_value_elem = to_representation_for_type(value_elem, type_elem,
--> 193                                                         callable_handler)
    194       result_elements.append((type_elem_name, converted_value_elem))
    195     return anonymous_tuple.AnonymousTuple(result_elements)

~\Anaconda3\envs\tff\lib\site-packages\tensorflow_federated\python\core\impl\reference_executor.py in to_representation_for_type(value, type_spec, callable_handler)
    163       raise TypeError(
    164           'The tensor type {} of the value representation does not match '
--> 165           'the type spec {}.'.format(str(inferred_type_spec), str(type_spec)))
    166     return value
    167   elif isinstance(type_spec, computation_types.NamedTupleType):

TypeError: The tensor type int64[8,100] of the value representation does not match the type spec int32[?,100].
```

Similarly, the notebook for image classification doesn't work as well.I did a type-casting in this function and it runs on my machine:
```python
def to_ids(x):
    s = tf.reshape(x['snippets'], shape=[1])
    chars = tf.string_split(s, delimiter='').values
    ids = tf.cast(table.lookup(chars), tf.int32) # this is changed
    return ids  
```Hi @le0tan 

Yes, you are definitely correct--[this](https://github.com/tensorflow/tensorflow/commit/1d29b5c344c63379a1a8587d52210267352c6911) change causes TFF's PIP package to be out of sync with TF-nightly.

TFF needs to do a release to sync these back up. We could alternatively pin to a fixed tf-nightly, but this has been decided against for the time being.

In the meantime, as you noticed, you should be able to build and run these locally via Bazel.

Thanks!!
Keith",2,2019-06-25 10:08:11,2019-06-25 16:51:11,2019-06-25 16:51:11
https://github.com/tensorflow/federated/issues/593,[],Federated with KerasRegressor?,"Federated with KerasRegressor?Hello

I'm trying to get my model of KerasRegressor working with TFF framework. But it seems that ""tff.learning.from_compiled_keras_model"" does not accept it, right?. My main aim is to differentiate between federated on both regression problem and classification problem.

This is my relevant part of the code:

```
def create_SK_model():
    modelF = create_SGD_model()
    modelF.compile(loss=tf.keras.losses.MSE,optimizer=tf.keras.optimizers.SGD(lr=learn_rate))
    return modelF
    
def create_Reg_model():
    modelF_Reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn = create_SK_model,nb_epoch=SNN_epoch, batch_size=SNN_batch_size)
        
    return modelF_Reg

def create_Class_model():
    modelF_Reg = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn = create_SK_model,nb_epoch=SNN_epoch, batch_size=SNN_batch_size)
    return modelF_Class

def create_Single_model():
    if Use_RegClas:
        if Use_Regressor:
            return create_Reg_model()
        elif Use_Classification:
            return create_Class_model()
    else:
        return create_SK_model()
def model_fn_Federated():
    if Use_RegClas:
        if Use_Regressor:
            return tff.learning.from_compiled_keras_model(create_Reg_model,sample_batch)
        elif Use_Classification:
            return tff.learning.from_compiled_keras_model(create_Class_model(),sample_batch)
    elif Use_FLAveraging:
        return tff.learning.from_compiled_keras_model(create_SK_model(),sample_batch)
    else:
        return tff.learning.from_keras_model(create_SGD_model(),sample_batch,loss=tf.keras.losses.MSE)


................... some other code ..................

if Use_FLAveraging:
    trainer_Itr_Process = tff.learning.build_federated_averaging_process(model_fn_Federated,server_optimizer_fn=(lambda : tf.keras.optimizers.SGD(learning_rate=learn_rate)),client_weight_fn=None)
else:
    trainer_Itr_Process = tff.learning.build_federated_sgd_process(model_fn_Federated,server_optimizer_fn=(lambda : tf.keras.optimizers.SGD(learning_rate=learn_rate)),client_weight_fn=None)
    

```

My main problem is how to incorporate the Regression/classfiication problem into Federated TF framework. I tried the above implementation, correct? wrong? please advice.

Based on the above implementation I get the following error:

```
.....
    py_typecheck.check_type(keras_model, tf.keras.Model)
  File ""/home/..../.local/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/py_typecheck.py"", line 48, in check_type
    type_string(type_spec), type_string(type(target))))
TypeError: Expected tensorflow.python.keras.engine.training.Model, found function.

```[`tff.learning.from_keras_model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/from_keras_model) and [`tff.learning.from_compiled_keras_model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/from_compiled_keras_model) support (and require) the [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=en) interface. Both regression and classification models can be implemented using the `tf.keras.Model` interface.

`tf.keras.wrappers.scikit_learn` provides wrapper classes that allow a `tf.keras.Model` to be used in the `scitkit_learn` module. They do not inherit from `tf.keras.Model` nor implement its interface, and hence are not supported in TFF. 

From the code it looks like passing the result of `create_SGD_model()` (assumed to be a `tf.kears.Model`) to `tff.learning.from_keras_model()` may be what is desired. Wrapping the `tf.keras.Model`  with `KerasClassifier` or `KerasRegressor` adds an additional layer of abstraction for using the model with `scikit_learn`, but not necessary for TFF.",1,2019-06-23 10:33:18,2019-06-23 14:00:35,2019-06-23 14:00:35
https://github.com/tensorflow/federated/issues/578,[],Cannot use GPU to train in 0.5.0,"Cannot use GPU to train in 0.5.0My enviroment : 
    RedHat Linux 7.2
    gcc 4.8.5
    nvidia-driver : 410.48
    cuda : 10.0
    cudnn: 7.5
    python2.7.5
    tf-nightly-gpu==1.14.1-dev20190604
    tensorflow_federated==0.5.0

When I use the example: federated_learning_for_image_classification.ipynb.
I change the model
    `model = tf.keras.models.Sequential([
           tf.keras.layers.Dense(
          10, activation=tf.nn.softmax, kernel_initializer='zeros', input_shape=(784,))])
`
 to like this below:
`model = tf.keras.models.Sequential([
              tf.keras.layers.Reshape(target_shape=[28, 28, 1], input_shape=(28 * 28,)),
              tf.keras.layers.Conv2D(
                 32, 5, padding='same', activation=tf.nn.relu),
              tf.keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same'),
              tf.keras.layers.Conv2D(
                 64, 5, padding='same', activation=tf.nn.relu),
              tf.keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same'),
              tf.keras.layers.Flatten(),
              tf.keras.layers.Dense(
                  10, activation=tf.nn.softmax, kernel_initializer='zeros')
          ])`
It show 
    **MaxPoolingOp only support NHWC on device type cpu.**
When I comment the all MaxPooling2D layer,
it also show the similar but more confuse problem:
    **Conv2DCustomBackprofFilterOp only supports NHWC.**

I only change the model.
And I run this code in tf-nightly==1.14.1-dev20190604 (cpu versoin) it is fine.

Can anyone help?
Thanks


Could you try training the Keras model without an TFF and see if the same error is raised? i.e. call `model.fit()` without using any of the `tff.learning` modules.I run `model.fit()` without TFF, it is OK in GPU Training.

And I also run the mnist example in tf-gpu:

`import tensorflow as tf`
`from tensorflow.examples.tutorials.mnist import input_data`
`from tensorflow.python import layers`
`from tensorflow.python import nn`

`mnist = input_data.read_data_sets(""./MNIST_data/"", one_hot=True)`

`sess = tf.InteractiveSession()`

`x = tf.placeholder(tf.float32, [None, 784])`
`y_ = tf.placeholder(tf.float32, [None, 10])`
`x_image = tf.reshape(x, [-1, 28, 28, 1])`

`conv1 = layers.conv2d(x_image, 32, 5, padding='same', name='conv1')`
`relu1 = nn.relu(conv1, name='relu1')`
`maxppool1 = layers.max_pooling2d(relu1, 2, 2, name='maxpool1')`

`conv2 = layers.conv2d(maxppool1, 64, 5, padding='same', name='conv2')`
`relu2 = nn.relu(conv2, name='relu2')`
`maxppool2 = layers.max_pooling2d(relu2, 2, 2, name='maxpool2')`

`flattern = layers.flatten(maxppool2, name='flattern')`
`fc1 = layers.dense(flattern, 1024, activation=tf.nn.relu, name='fc1')`
`fc1_dropout = layers.dropout(fc1, 0.8, name='fc1_dropout')`
`fc2 = layers.dense(fc1_dropout, 10, activation=tf.nn.softmax, name='fc2')`

`cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(fc2), reduction_indices=[1]))`
`train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)`
`correct_prediciton = tf.equal(tf.argmax(fc2, 1), tf.argmax(y_, 1))`
`accuracy = tf.reduce_mean(tf.cast(correct_prediciton, tf.float32))`

`tf.global_variables_initializer().run()`
`for i in range(1, 10001):`
     `batch = mnist.train.next_batch(64)`
    `if i % 10 == 0:`
         `train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})`
         `print(""step %d, examples %d, training accuracy %g"" % (i, i * 64, train_accuracy))`
         `print(""test accuracy %g"" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))`
     `train_step.run(feed_dict={x: batch[0], y_: batch[1]})`

`print(""test accuracy %g"" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))`

It also work.
@ZacharyGarrett Hello!

I am experiencing the same issue with version 0.7.0 and tf-nightly-gpu (gpu)

And it works just fine with tf-nightly (cpu)

@ZacharyGarrett if we use model.fit instead of tff.learning, is that a workaround for now?

Or do we have to wait for some sort of fix here?


I installed with version 0.8.0 and am able to run codes with GPU.I tried it with 0.8.0 and I am still getting this error. 

@jiachangliu  is your code open source? Can you point me to it?

My code is open source [1]. Run federated.py.

1. https://github.com/anupamme/CheXpert-Keras@anupamme I'm actually just running the image classification tutorial from federated learning. https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classificationTraining on GPU with the local executor stack. That is, calling `tff.framework.set_default_executor(tff.framework.create_local_executor())` before execution, should allow you to utilize the GPU.

In particular, if you run the image classification colab with a GPU-backed cloud-hosted runtime, you will see a roughly 2X speedup compared with the CPU-backed runtime.

We've seen local training working with our recent versions as well, utilizing GPU resources; closing this issue.",7,2019-06-17 07:43:44,2019-12-06 20:27:22,2019-12-06 20:27:21
https://github.com/tensorflow/federated/issues/534,[],Why the evaluation on test using the sample_clients from training in the EMNIST tutorial?,"Why the evaluation on test using the sample_clients from training in the EMNIST tutorial?I went through the EMNIST tutorial https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification
, but found the problem in the evaluation section:

> __federated_test_data__ = make_federated_data(emnist_test, sample_clients)
> len(federated_test_data), federated_test_data[0]

The `federated_test_data` is generated from `sample_clients`, while the `sample_clients` comes from the training set
>#@test {""output"": ""ignore""}
NUM_CLIENTS = 3
__sample_clients__ = __emnist_train__.client_ids[0:NUM_CLIENTS]
federated_train_data = make_federated_data(emnist_train, sample_clients)
len(federated_train_data), federated_train_data[0]

Why not take it as 
```python
sample_clients_test = emnist_test.client_ids[0:NUM_CLIENTS]
federated_test_data = make_federated_data(emnist_test, sample_clients_test)
evaluation(state.model, federated_test_data)
```

Is that an error or can someone explain it?This is by design; from the [documentation](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/emnist/load_data) for `tff.simulation.datasets.emnist.load_data()`:

> Rather than holding out specific users, each user's examples are split across train and test so that all users have at least one example in train and one example in test. Writers that had less than 2 examples are excluded from the data set.

So the while the evaluation is processing the same clients, it is not processing the same examples.

As you suggest, holding out entire *clients* (instead of *examples*) definitely seems reasonable and is an alternative way to evaluate the quality of a model.",1,2019-06-04 00:32:53,2019-06-05 03:04:37,2019-06-05 03:04:34
https://github.com/tensorflow/federated/issues/522,[],Follow up on #258 TFF with BatchNormalization,"Follow up on #258 TFF with BatchNormalizationHello
I followed the discussion on #258 about keras models with batch_normalization, this is the relevant part of the code:

```
for round_num in range(2, 70): 
    FLstate, FLoutputs = trainer_Itr_Process.next(FLstate, federated_train_data)   
    FLlosses_arr.append(FLoutputs.loss)
    tff_weights= tff.learning.keras_weights_from_tff_weights(FLstate.model)
    tff_weights.assign_weights_to(tff_weights, Local_model_Fed)
```

I appreciate the help with the following error:

```
Traceback (most recent call last):
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 2225, in <module>
    main()
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 2218, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 1560, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 1567, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\_pydev_imps\_pydev_execfile.py"", line 25, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:\Users\ezalaab\Documents\eclipse-workspace\MANA-FederatedLearning\location-based-federated-learning\Test\FederatedLearningTFv3.py"", line 299, in <module>
    tff_weights.assign_weights_to(tff_weights, Local_model_Fed)
AttributeError: 'list' object has no attribute 'assign_weights_to
```
I believe the code should skip the call to `tff_weights.assign_weights_to()`. Instead, just call [`tff.learning.assign_weights_to_keras_model()`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/assign_weights_to_keras_model). Something like:

```python
for round_num in range(2, 70): 
    FLstate, FLoutputs = trainer_Itr_Process.next(FLstate, federated_train_data)   
    FLlosses_arr.append(FLoutputs.loss)
    tff.learning.assign_weights_to_keras_model(Local_model_Fed, FLstate.model)
```

This seems like it might be a better question for StackOverflow using the [tensorflow-federated] tag?Thanks for the response.

1. I have no idea how to differentiate between a question for StackOverflow or TFF forum.
2. At first I had problem with the installation, that is why the above error occurred. Then after fixing that, now I get another error as below:

Traceback (most recent call last):
```
  File ""/home/abbasi/.eclipse/360744294_linux_gtk_x86_64/plugins/org.python.pydev.core_7.2.1.201904261721/pysrc/pydevd.py"", line 2316, in <module>
    main()
  File ""/home/abbasi/.eclipse/360744294_linux_gtk_x86_64/plugins/org.python.pydev.core_7.2.1.201904261721/pysrc/pydevd.py"", line 2309, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/home/abbasi/.eclipse/360744294_linux_gtk_x86_64/plugins/org.python.pydev.core_7.2.1.201904261721/pysrc/pydevd.py"", line 1642, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File ""/home/abbasi/.eclipse/360744294_linux_gtk_x86_64/plugins/org.python.pydev.core_7.2.1.201904261721/pysrc/pydevd.py"", line 1649, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/home/abbasi/.eclipse/360744294_linux_gtk_x86_64/plugins/org.python.pydev.core_7.2.1.201904261721/pysrc/_pydev_imps/_pydev_execfile.py"", line 25, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/abbasi/eclipse-workspace/MANA/location-based-federated-learning/Test/FederatedLearningTFv3.py"", line 298, in <module>
    FLstate.model.assign_weights_to(Local_model_Fed)
  File ""/home/abbasi/.local/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/anonymous_tuple.py"", line 135, in __getattr__
    list(self._name_to_index.keys())[:10]))
AttributeError: The tuple of length 2 does not have named field ""assign_weights_to"".Names (only first 10): ['trainable', 'non_trainable']
```
Thanks for the supportFor (1): its not always clear to us initially either, but its totally okay to move a question/issue from one to the other. Generally, questions about *how to use* TFF might better go on StackOverflow, especially when someone else may have the same question and would benefit from finding previous answers (preserved by StackOverflow). When a GitHub issue is closed, its not always easy to rediscover a solution. Otherwise, *issues with* TFF (e.g. running the vanilla colabs produces errors, feature requests, etc) probably are better as GitHub issues.

For (2): It looks like the code is still using `assign_weights_to()`. Could you try using `tff.learning.assign_weights_to_keras_model()`, as mentioned in the previous comment?Apologies for the lengthy discussion, now I'm using : 

```
# training/fitting with TF federated learning
trainer_Itr_Process = tff.learning.build_federated_averaging_process(model_fn_Federated,server_optimizer_fn=(lambda : gradient_descent.SGD(learning_rate=learn_rate)),client_weight_fn=None)
FLstate = trainer_Itr_Process.initialize()
FLlosses_arr  = []
Fed_eval_arr = []
# Track loss of different ...... of federated iteration
FLstate, FLoutputs = trainer_Itr_Process.next(FLstate, federated_train_data)   
# Track the loss.
FLlosses_arr.append(FLoutputs.loss)
# Setting federated weights on copied Object of local model
tff.learning.assign_weights_to_keras_model(FLstate.model,Local_model)
```

and got this error now:

```
Traceback (most recent call last):
  File ""/home/abbasi/.eclipse/360744294_linux_gtk_x86_64/plugins/org.python.pydev.core_7.2.1.201904261721/pysrc/pydevd.py"", line 2316, in <module>
    main()
  File ""/home/abbasi/.eclipse/360744294_linux_gtk_x86_64/plugins/org.python.pydev.core_7.2.1.201904261721/pysrc/pydevd.py"", line 2309, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/home/abbasi/.eclipse/360744294_linux_gtk_x86_64/plugins/org.python.pydev.core_7.2.1.201904261721/pysrc/pydevd.py"", line 1642, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File ""/home/abbasi/.eclipse/360744294_linux_gtk_x86_64/plugins/org.python.pydev.core_7.2.1.201904261721/pysrc/pydevd.py"", line 1649, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/home/abbasi/.eclipse/360744294_linux_gtk_x86_64/plugins/org.python.pydev.core_7.2.1.201904261721/pysrc/_pydev_imps/_pydev_execfile.py"", line 25, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/abbasi/eclipse-workspace/MANA/location-based-federated-learning/Test/FederatedLearningTFv3.py"", line 299, in <module>
    tff.learning.assign_weights_to_keras_model(FLstate.model,Local_model)
  File ""/home/abbasi/.local/lib/python3.6/site-packages/tensorflow_federated/python/learning/keras_utils.py"", line 66, in assign_weights_to_keras_model
    tff_weights, (anonymous_tuple.AnonymousTuple, model_utils.ModelWeights))
  File ""/home/abbasi/.local/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/py_typecheck.py"", line 48, in check_type
    type_string(type_spec), type_string(type(target))))
TypeError: Expected tensorflow_federated.python.common_libs.anonymous_tuple.AnonymousTuple or tensorflow_federated.python.learning.model_utils.ModelWeights, found tensorflow.python.keras.engine.sequential.Sequential.
```

I tracked the error it seems that in keras_utils have a problem with the tff.weights type, in the below function:

```
py_typecheck.check_type(
      tff_weights, (anonymous_tuple.AnonymousTuple, model_utils.ModelWeights))
```

I checked the type of tff_weights and model_utils.ModelWeights: 

```
>>> tff_weights
<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f5438501eb8>
>>> model_utils.ModelWeights
<class 'tensorflow_federated.python.learning.model_utils.ModelWeights'>
```

Do you think its related to the definition of the model?

```
def create_SK_model():
        modelF = tf.keras.models.Sequential([tf.keras.layers.Dense(SNN_Layers[0],activation=tf.nn.relu,input_shape=(Act_Inputs.shape[1],), kernel_initializer='RandomNormal'),
                                      #tf.keras.layers.BatchNormalization(),
                                      tf.keras.layers.Dense(SNN_Layers[1], activation=tf.nn.relu, kernel_initializer='RandomNormal'),
                                      tf.keras.layers.Dropout(0.2),
                                      tf.keras.layers.Dense(1, activation=tf.nn.relu, kernel_initializer='RandomNormal'),
                                      ])
```It looks like the code has the argument order flipped. `tff.learning.assign_weights_to_keras_model(keras_model, tff_weights)`  takes the local Keras model as the first argument, similar to the first code snippet I posted. The API docs are [here](https://www.tensorflow.org/federated/api_docs/python/tff/learning/assign_weights_to_keras_model).

Could you try flipping the order?Thanks a lot Zachary, I feel a shame for not identifying such basic error.",6,2019-06-01 18:50:08,2019-06-06 21:00:57,2019-06-06 21:00:57
https://github.com/tensorflow/federated/issues/494,[],cannot install v0.5.0,"cannot install v0.5.0that version is not in pypi and trying to install via `pip install -e git://github.com/tensorflow/federated.git@v0.5.0#egg=tensorflow_federated`

(seems to be missing root setup.py) (using windows)

produces 
```
Obtaining tensorflow_federated from git+git://github.com/tensorflow/federated.git@v0.5.0#egg=tensorflow_federated
  Cloning git://github.com/tensorflow/federated.git (to revision v0.5.0) to my-project\venv\src\tensorflow-federated
  Running command git clone -q git://github.com/tensorflow/federated.git 'my-project\venv\src\tensorflow-federated'
    ERROR: Complete output from command python setup.py egg_info:
    ERROR: Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""C:\Users\my-user\AppData\Local\Programs\Python\Python37\lib\tokenize.py"", line 447, in open
        buffer = _builtin_open(filename, 'rb')
    FileNotFoundError: [Errno 2] No such file or directory: my-project\\venv\\src\\tensorflow-federated\\setup.py'
    ----------------------------------------
ERROR: Command ""python setup.py egg_info"" failed with error code 1 in my-project\venv\src\tensorflow-federated\
```closing this as 0.5.0 was just published to pypi",1,2019-05-29 06:51:36,2019-05-29 07:01:21,2019-05-29 07:01:21
https://github.com/tensorflow/federated/issues/493,[],Results cannot improve by rounding (.next): Federated TF ,"Results cannot improve by rounding (.next): Federated TF Hello

I'm trying to compare federated learning TF with keras fitting, as we see below. However, the results of TFF does not improve by rounding. Also, when I set weight a local model based on the TFF model, I get worse results than normally training the model on keras. The Loss results are mentioned after the code.

Thanks for the help.

Regards


# Code:
tf.compat.v1.enable_v2_behavior()

TobeRemoved_Array = ['feature1','feature2','feature3','feature4','feature5','feature6','feature7', 'is_training']

train_perc = 0.8
Norm_Input  = True
Norm_Output = False
Input_str  = ['feature1', 'feature2']
if Norm_Output:
    Output_str = ['feature3']
else: 
    Output_str = ['feature4']

Final_Tag = 'Tag3' # here just decide which tagging method do you want to use
Num_Clients = 10 # cannot be less than one
Num_Cons_Sample_Client = 5 # cannot be less than one

# Load simulation data.
##############################################
dir_name = 'pickle-data/'
file_name = 'logs_april_2019.pickle'
files = os.listdir('pickle-data/')
dataframe = Import_Pickle.Import_v1(dir_name,file_name,False) # choose False to use 2019 data
# Just to reduce the processing
ave = dataframe.core.min() + 1000000000
df2 = dataframe[dataframe.core < ave]
df = Import_Pickle.PreProcessing_v2019(df2,Norm_Input)
train_df,test_df,X_traindf,X_testdf,Y_traindf,Y_testdf,XY_traindf,XY_testdf = Import_Pickle.Splitting_Train_Test(df,train_perc,Norm_Output,TobeRemoved_Array)
########## splitting for clients ############  
def Tag_per_day(train_df_loc,TagNum):
    train_df_loc['log2'] =  train_df_loc['log'].apply(lambda x: x.replace(""_"",""""))
    tag_Index = train_df_loc.log2.apply(lambda x: x.index(""201""))
    tag_Index2 = tag_Index.values[1]
    tag_date =train_df_loc.log2.apply(lambda x: x[tag_Index2:tag_Index2+8])
    train_df_loc.loc[:,'Tag'+str(TagNum)] = pd.Series(tag_date.to_list(),index=train_df.index)  # to be fixed
    return train_df_loc

# Introduce time as input
X_traindf['feature1'] = train_df['feature1']
# introduce first tag per day
TagNum=1
train_df = Tag_per_day(train_df,TagNum)
#examples on groupby
Unq_tag1_grps = list(train_df.groupby(train_df.Tag1).groups.keys())
train_df.groupby(train_df.Tag1).first()
train_df.groupby(train_df.Tag1)['feature1'].count()
X_traindf['Tag'+str(TagNum)] =  train_df['Tag'+str(TagNum)]
#############################
# introduce epoch as tag
#############################
TagNum=2
train_df['Tag'+str(TagNum)] = train_df.epoch
X_traindf['Tag'+str(TagNum)] =  train_df['Tag'+str(TagNum)]
#############################
# introduce core as tag
#############################
TagNum=3
train_df['Tag'+str(TagNum)] = train_df.core
X_traindf['Tag'+str(TagNum)] =  train_df['Tag'+str(TagNum)]
#############################
# introduce day as tag per client
#############################
TagNum = 4
RepNum = np.ceil(train_df.shape[0]/(Num_Cons_Sample_Client*Num_Clients))
Part_Tag_Array=[]
for i in np.arange(Num_Clients):
    Part_Tag_Tmp = list(map(lambda _: i+1,range(Num_Cons_Sample_Client)))
    Part_Tag_Array.extend(Part_Tag_Tmp)

Full_Tag_Array2 = Part_Tag_Array * int(RepNum)
extra_tags = np.abs(len(Full_Tag_Array2) - train_df.shape[0])
Full_Tag_Array = Full_Tag_Array2[:-extra_tags]

train_df.loc[:,'Tag'+str(TagNum)] = pd.Series(Full_Tag_Array,index=train_df.index)
X_traindf.loc[:,'Tag'+str(TagNum)] = train_df['Tag'+str(TagNum)]
#############################
# END day as tag per client
#############################
######### Introduce gpsTime and Tag to the input
Input_str.extend(['feature1',Final_Tag])

###### Adding StandardSalarization:
scaler = StandardScaler()
removed_column = Input_str.pop()
X_train_ScaledTmp = scaler.fit_transform(X_traindf[Input_str],Y_traindf[Output_str])
# Adding Int tag per client without scalarization
X_train_Scaled = np.c_[X_train_ScaledTmp, train_df[removed_column].values.reshape(train_df.shape[0],1)]


# All In/Out data Numpy
Act_Inputs_Int_Tag  = X_train_Scaled
Act_Outputs_Int = Y_traindf[Output_str].values
# Remove Tags
Act_Inputs_Int = np.delete(Act_Inputs_Int_Tag,-1,axis=1) 

# prepare In/Out per Client
All_Act_Inputs_Int_Tag  = [Act_Inputs_Int_Tag[np.where(Act_Inputs_Int_Tag[:,-1]== x)] for x in np.arange(1,Num_Clients+1)]
All_Act_Outputs_Int = [Act_Outputs_Int[np.where(Act_Inputs_Int_Tag[:,-1]== x)] for x in np.arange(1,Num_Clients+1)]
# Remove Tags
All_Act_Inputs_Int = [np.delete(All_Act_Inputs_Int_Tag[x],-1,axis=1)  for x in np.arange(0,Num_Clients) ]


# a need conversion to float32
Act_Inputs = np.float32(Act_Inputs_Int)
Act_Outputs = np.float32(Act_Outputs_Int)
# convert dataset to client based dataset
All_Act_Inputs = [np.float32(All_Act_Inputs_Int[x]) for x in np.arange(0,Num_Clients)]
All_Act_Outputs = [np.float32(All_Act_Outputs_Int[x]) for x in np.arange(0,Num_Clients)]
# convert to OrderedDict
new_batch = collections.OrderedDict([('In', Act_Inputs),('Out', Act_Outputs)])
All_new_batch = [collections.OrderedDict([('In', All_Act_Inputs[x]),('Out', All_Act_Outputs[x])]) for x in np.arange(0,Num_Clients)]
# Convert to tensor
dataset_input = tf.data.Dataset.from_tensor_slices(new_batch)#,,maxval=100, dtype=tf.float32)
# All_new_batch has different item per In / Out
All_dataset_input = [tf.data.Dataset.from_tensor_slices(All_new_batch[x]) for x in np.arange(0,Num_Clients)]
# Select among the datasets
Used_dataset= dataset_input
All_Used_dataset= All_dataset_input


#with eager_mode():
learning_rate = 1
Val_Train_Split = 0.8
SNN_epoch = 50
SNN_batch_size = 1100
shuffle_buffer = 200

def preprocess(new_dataset):
    #return Used_dataset.repeat(2).batch(2)
    def map_fn(elem):
        return collections.OrderedDict([('x', tf.reshape(elem['In'], [-1])),('y', tf.reshape(elem['Out'],[1]))])
     
    DS2= new_dataset.map(map_fn)
    return DS2.repeat(SNN_epoch).batch(SNN_batch_size)

train_data = [preprocess(Used_dataset)]

#######changes###############33
def make_federated_data(client_data, client_ids):
    return [preprocess(client_data[x]) for x in client_ids]

federated_train_data =  make_federated_data(All_Used_dataset, np.arange(0,Num_Clients))




sample_batch = tf.contrib.framework.nest.map_structure(lambda x: x.numpy(), next(iter(train_data[0])))
    
########## END Changes ############            

def create_SK_model():
    modelF = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(Act_Inputs.shape[1],)),
                                  tf.keras.layers.Dense(10, activation=tf.nn.relu, kernel_initializer='zeros'),
                                  tf.keras.layers.Dense(100, activation=tf.nn.relu, kernel_initializer='zeros'),
                                  tf.keras.layers.Dropout(0.5),
                                  tf.keras.layers.Dense(100, activation=tf.nn.relu, kernel_initializer='zeros'),
                                  tf.keras.layers.Dropout(0.5),
                                  tf.keras.layers.Dense(1, activation=tf.nn.relu, kernel_initializer='zeros'),
                                  ])
    return modelF

def loss_fn(y_true, y_pred):
    return tf.reduce_mean(tf.keras.losses.MSE(y_true, y_pred))

def model_fn():
    return tff.learning.from_keras_model(create_SK_model(), 
                                         sample_batch, loss=loss_fn, 
                                         optimizer=gradient_descent.SGD(0.1))

YTrain = Act_Outputs #np.random.rand(50,1)
XTrain = Act_Inputs  #np.random.rand(50,100)
# locally compile the model
Local_model = create_SK_model()
Local_model.compile(loss=loss_fn,optimizer=gradient_descent.SGD(learning_rate))

# training/fitting with TF federated learning
trainer_Itr_Process = tff.learning.build_federated_averaging_process(model_fn,server_optimizer_fn=(lambda : gradient_descent.SGD(learning_rate=1.0)),client_weight_fn=None)
FLstate = trainer_Itr_Process.initialize()
FLlosses  = []
# Track loss of different ...... of federated iteration
for round_num in range(2, 5): 
    """"""
    The second of the pair of federated computations, next, represents a single round of Federated Averaging, which consists of pushing the server state (including the model parameters) to the clients, on-device training on their local data, collecting and averaging model updates, and producing a new updated model at the server.
    """""" 
    FLstate, FLoutputs = trainer_Itr_Process.next(FLstate, federated_train_data)
    print('round {:2d}, metrics={}'.format(round_num, FLoutputs.loss))
    # Track the loss.
    FLlosses.append(FLoutputs.loss)

# fitting without federated learning
trained_local_Model = Local_model.fit(XTrain,YTrain, validation_split=Val_Train_Split, epochs=SNN_epoch, batch_size=SNN_batch_size) #tbuc
# Loss of local model
local_Loss = trained_local_Model.history['loss'] # tbuc
print(""Local Model Loss: ""+str(local_Loss))
# Copy local model for comparison purposes
Local_model_Fed = Local_model
# Setting federated weights on copied Object of local model
Local_model_Fed.set_weights(tff.learning.keras_weights_from_tff_weights(FLstate.model))
# Evaluate loss of the copied federated weights on local model
Fed_predicted = Local_model_Fed.predict(XTrain)
Fed_eval = Local_model_Fed.evaluate(XTrain,YTrain)

print(""Local Model with Federated Weights Loss: ""+str(Fed_eval ))
print(""New: "" + str(Local_model_Fed.loss))
print('End')



# Output:
round  2, metrics=6838.99658203125
round  3, metrics=6838.99658203125
round  4, metrics=6838.99658203125
round  5, metrics=6838.99658203125
round  6, metrics=6838.99658203125
Train on 733 samples, validate on 2936 samples
Epoch 1/50

733/733 [==============================] - 0s 14us/sample - loss: 6449.8022 - val_loss: 6936.1622
Epoch 4/50
.........................................................
733/733 [==============================] - 0s 18us/sample - loss: 6449.8022 - val_loss: 6936.1622

Local Model Loss: [6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375]

  32/3669 [..............................] - ETA: 0s - loss: 10104.2188
1632/3669 [============>.................] - ETA: 0s - loss: 6897.5380 
3232/3669 [=========================>....] - ETA: 0s - loss: 6860.1061
3669/3669 [==============================] - 0s 31us/sample - loss: 6838.9962
Local Model with Federated Weights Loss: 6838.996183714057
It appears this model is not training even without federation, I believe the following lines show the loss during non-federated training,:

```
local_Loss = trained_local_Model.history['loss'] # tbuc
print(""Local Model Loss: ""+str(local_Loss))
```

The loss does not appear to be decreasing:

```
Local Model Loss: [6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375]
```

TFF only extends local training to the federated setting; we would be very surprised if a model that will not train locally was trainable in the federated setting. Closing this as not a TFF issue.

I'd recommend first getting local training to work. StackOverflow with the [tensorflow] and [keras] tags can be good resources. Once local training is working, try adding back TFF for federated optimization.",1,2019-05-29 01:30:22,2019-06-05 03:05:13,2019-05-29 18:46:30
https://github.com/tensorflow/federated/issues/448,[],Issue of AssignAddVariableOp,"Issue of AssignAddVariableOpHello,

I'm using TFF, and I've been many errors. I tried to fix, but this error is a bit difficult. Your help is very much appreciated. I posted the code Traceback below, please let me know if I need to post other things.

Your help is most appreciated

Code:
======================================================
Outputs = tf.convert_to_tensor(np.random.rand(50,1))
        input1  = tf.convert_to_tensor(np.random.rand(50,100))
        Inputs  = input1
        
        Outputs = tf.cast(Outputs,tf.float32)
        Inputs = tf.cast(Inputs,tf.float32)
        new_batchInput = collections.OrderedDict([('In', Inputs),('Out', Outputs)])

        def generate_one_new_batch():
            yield new_batchInput
        
        dataset_input = tf.data.Dataset.from_tensor_slices(new_batchInput)#,outputs,maxval=100, dtype=tf.float32)
        dataset2 = dataset_input
        Used_dataset= dataset2
        
        def client_data(self,new_dataset):
            def map_fn(elem):
                #return collections.OrderedDict([('x', tf.reshape(elem['In'], [-1])),('y', tf.reshape(elem['Out'],[1]))])
                return collections.OrderedDict([('x', tf.reshape(elem['In'], [-1])),('y', tf.reshape(elem['Out'],[1]))])
            
            DS2= new_dataset.map(map_fn,num_parallel_calls=None)
            
            return DS2.repeat(10).batch(10)
        
        train_data = [client_data(self,Used_dataset)]

        sample_batch = tf.contrib.framework.nest.map_structure(lambda x: x.numpy(), next(iter(train_data[0])))

        def create_SK_model(learning_rate=0.1):
            
            modelF = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(100,)),
                                         tf.keras.layers.Dense(10, activation=tf.nn.relu, kernel_initializer='zeros'),
                                         tf.keras.layers.Dense(1, activation=tf.nn.relu, kernel_initializer='zeros'),
                                         ])
            def loss_fn(y_true, y_pred):
                return tf.reduce_mean(tf.keras.losses.mse(y_true, y_pred))

            modelF.compile(loss=tf.keras.losses.mse,optimizer=gradient_descent.SGD(learning_rate))
            return modelF
        
           
        def model_fn():
            return tff.learning.from_compiled_keras_model(create_SK_model(0.1), sample_batch)
        
        YTrain = np.random.rand(50,1)
        XTrain = np.random.rand(50,100)
        
        create_SK_model(0.1).fit(XTrain,YTrain)
        
        trainer = tff.learning.build_federated_averaging_process(model_fn)
        state = trainer.initialize()
        losses = []
        for _ in range(2):
            state, outputs = trainer.next(state, train_data)
            # Track the loss.
            losses.append(outputs.loss)
                
        self.assertLess(losses[1], losses[0])

======================================================
Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1659, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 0 and 1 for 'AssignAddVariableOp' (op: 'AssignAddVariableOp') with input shapes: [], [?].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 2225, in <module>
    main()
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 2218, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 1560, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 1567, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\_pydev_imps\_pydev_execfile.py"", line 25, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:\Users\ezalaab\Documents\eclipse-workspace\MANA-FederatedLearning\location-based-federated-learning\Test\FLTF_models_test2.py"", line 149, in <module>
    FLTest() 
  File ""C:\Users\ezalaab\Documents\eclipse-workspace\MANA-FederatedLearning\location-based-federated-learning\Test\FLTF_models_test2.py"", line 143, in __init__
    self.test_self_contained_example()
  File ""C:\Users\ezalaab\Documents\eclipse-workspace\MANA-FederatedLearning\location-based-federated-learning\Test\FLTF_models_test2.py"", line 125, in test_self_contained_example
    trainer = tff.learning.build_federated_averaging_process(model_fn)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\learning\federated_averaging.py"", line 148, in build_federated_averaging_process
    model_fn, client_fed_avg, server_optimizer_fn)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\learning\framework\optimizer_utils.py"", line 309, in build_model_delta_optimizer_process
    federated_dataset_type)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\computation_wrapper.py"", line 408, in <lambda>
    return lambda fn: _wrap(fn, arg_type, self._wrapper_fn)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\computation_wrapper.py"", line 96, in _wrap
    name=fn_name)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\computation_wrapper_instances.py"", line 54, in _federated_computation_wrapper_fn
    suggested_name=name))
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\federated_computation_utils.py"", line 75, in zero_or_one_arg_fn_to_building_block
    parameter_name, parameter_type), context_stack))
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\function_utils.py"", line 553, in <lambda>
    return (lambda fn, at, kt: lambda arg: _unpack_and_call(fn, at, kt, arg))(
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\function_utils.py"", line 547, in _unpack_and_call
    return fn(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\learning\framework\optimizer_utils.py"", line 323, in run_one_round_tff
    @tff.tf_computation(tf_dataset_type, model_weights_type)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\computation_wrapper.py"", line 408, in <lambda>
    return lambda fn: _wrap(fn, arg_type, self._wrapper_fn)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\computation_wrapper.py"", line 96, in _wrap
    name=fn_name)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\computation_wrapper_instances.py"", line 38, in _tf_wrapper_fn
    target_fn, parameter_type, ctx_stack)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\tensorflow_serialization.py"", line 92, in serialize_py_fn_as_tf_computation
    result = target(*args)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\function_utils.py"", line 553, in <lambda>
    return (lambda fn, at, kt: lambda arg: _unpack_and_call(fn, at, kt, arg))(
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\core\impl\function_utils.py"", line 547, in _unpack_and_call
    return fn(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\learning\framework\optimizer_utils.py"", line 344, in client_delta_tf
    client_output = client_delta_fn(tf_dataset, initial_model_weights)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\learning\federated_averaging.py"", line 97, in __call__
    initial_state=tf.constant(0.0), reduce_func=reduce_fn)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1224, in reduce
    add_to_graph=False)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2129, in __init__
    self._function._create_definition_if_needed()  # pylint: disable=protected-access
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 341, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 355, in _create_definition_if_needed_impl
    whitelisted_stateful_ops=self._whitelisted_stateful_ops)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\function.py"", line 883, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2099, in tf_data_structured_function_wrapper
    ret = func(*nested_args)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\learning\federated_averaging.py"", line 91, in reduce_fn
    output = model.train_on_batch(batch)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\learning\model_utils.py"", line 544, in train_on_batch
    self._model.train_on_batch(batch_input), model_lib.BatchOutput)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\eager\def_function.py"", line 312, in __call__
    canon_args, canon_kwds = self._initialize(args, kwds)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\eager\def_function.py"", line 280, in _initialize
    self._stateful_fn._get_concrete_function_internal(*args, **kwds))  # pylint: disable=protected-access
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 876, in _get_concrete_function_internal
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 1176, in _maybe_define_function
    arg_names=arg_names),
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 448, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\eager\def_function.py"", line 255, in wrapped_fn
    return wrapped_fn.__wrapped__(*args, **kwds)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 1652, in bound_method_wrapper
    return wrapped_fn(weak_instance(), *args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\learning\model_utils.py"", line 451, in train_on_batch
    batch_output = self._forward_pass(batch_input)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\learning\model_utils.py"", line 382, in _forward_pass
    metric.update_state(y_true=y_true, y_pred=predictions)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\metrics.py"", line 98, in decorated
    update_op = update_state_fn(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_federated\python\learning\model_utils.py"", line 321, in update_state
    op = self._total_loss.assign_add(batch_total_loss)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 881, in assign_add
    name=name)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\gen_resource_variable_ops.py"", line 66, in assign_add_variable_op
    ""AssignAddVariableOp"", resource=resource, value=value, name=name)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 281, in create_op
    compute_device=compute_device)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1823, in __init__
    control_input_ops)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1662, in _create_c_op
    raise ValueError(str(e))
ValueError: Shapes must be equal rank, but are 0 and 1 for 'AssignAddVariableOp' (op: 'AssignAddVariableOp') with input shapes: [], [?].@AbbasiAYE 

It looks like when compiling the Keras model, the `tf.keras.losses.mse` function provided is not reducing the batch dimension ([API docs](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/mean_squared_error?hl=en)). Just before compilation, the code defines a new method `loss_fn` that does the reduce. Could you try using this in the compile call?

When writing federated learning code, the recommendation is to start by writing only TF code (e.g. the Keras model) and ensuring the model can be trained and evaluated without any TFF.  Once that is working, wrap it with `tff.learning.from_compiled_keras_model()` and use it to build a federated learning process. Removing the TFF wrapping can also help in debugging where an error is occurring.Thanks @ZacharyGarrett 

I think you were right. If I remove the loss function, and NOT compile the model, then use tff.learning.from_keras_model (instead of from_compiled_keras_model) as a federated model in build_federated_averaging_process the process works fine. However, if I add any loss function and compile before passing to federated, then the same error occurs. BTW, as you suggested above, If i use the same loss function and compile/fit locally everything works fine. Any advice?

As mentioned previously, `tf.keras.losses.mse` is not reducing the batch dimension, which is necessary to compute a scalar loss (this is what `loss_fn` is doing). If I remember correctly, Keras will do this for you if you use `loss='mse'` or `loss=tf.keras.losses.MeanSquaredError()` when calling `compile()`.

It appears this is not a TFF issue, but a Keras usage issue so I'm going to close. If you're still having trouble, I'd recommended consulting the [tensorflow] and/or [keras] tags on StackOverflow.",3,2019-05-17 08:13:43,2019-05-20 15:35:25,2019-05-20 15:35:25
https://github.com/tensorflow/federated/issues/418,[],Error tf.keras.losses,"Error tf.keras.lossesHello, i am trying with the tutorial of image classification when I met the error which said ""keras.losses' has no attribute 'SparseCategoricalCrossentropy'"". How to fix it? Is that a problem of version of my tensorflow?
Another question is that how many kinds of loss functions does tff support?
Thank you in advance!BTW, my tf version is 1.13.1 and my keras is 2.2.4The tutorials are versioned and must be paired with the appropriate TFF release.

The current release is `0.4.0`; the associated tutorial for image classification is viewable in Colab at:
https://colab.research.google.com/github/tensorflow/federated/blob/v0.4.0/docs/tutorials/federated_learning_for_text_generation.ipynb

It sounds like the notebook you're using is from the `master` branch (the changes to use `SparseCategoricalCrossentropy` were merged recently). This will require building the pip package from source until the next release. Instructions to do so are at: https://github.com/tensorflow/federated/blob/master/docs/install.md#build-the-tensorflow-federated-pip-package Note that the `master` branch depends on the `tf-nightly` (not `tensorflow`) pip package.

TFF should support any Keras loss function, as well as any custom function that Keras can understand. Please let us know if this is not the case.I tried with the pip package from source but failed with the Bazel which seems hard to solve in another issue. Is there any other loss functions to replace with? Such as ""keras.losses.CategoricalCrossentropy()""? Another question is that when I tried with ""optimizer=tf.keras.optimizers.SGD(lr=0.02)"", an error occur saying ""'SGD' object has no attribute 'variables'"". Then I wrote ""optimizer=''sgd"" and it works well. What's the reason about it and I still need to test for different learning rate so could you give me some advice based on my current tf and keras version(1.13.1/ 2.2.4) to compile the model correctly?@wangyixu14 it wasn't clear if you were able to use the released version of the notebooks? If you don't want to troubleshoot building the pip package from source, the only other supported route is to use the released version of the notebook (currently at git tag `v0.4.0`). Link to colab:
https://colab.research.google.com/github/tensorflow/federated/blob/v0.4.0/docs/tutorials/federated_learning_for_text_generation.ipynb

The problem with `'SGD' object has no attribute 'variables'` is the same the previous error: TFF is expecting a newer version of TensorFlow (the `tf-nightly` pip package) than the system has available. To continue to use the older TF API, the code will need to do a deep import into the TensorFlow modules (the notebook linked above does this).

```python
from tensorflow.python.keras.optimizer_v2 import gradient_descent
...
model.compile(optimizer=gradient_descent.SGD(lr=0.02, loss=...)
```Thank you. Your suggestion works well. What should I do if I want to try the tff with other image datasets for the classification? Also, when I try build the pip package from source with bazel(0.25.2, system: ubuntu 16.04), an error occured as follows:
ERROR: error loading package 'tensorflow_federated/proto/v0': Encountered error while reading extension file 'protobuf.bzl': no such package '@com_google_protobuf//': Traceback (most recent call last):
	File ""/home/SENSETIME/wangyixuan2/.cache/bazel/_bazel_SENSETIME\wangyixuan2/302ddd240f6e4d6449f8fd791c1cce01/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 234
		_clone_or_update(ctx)
	File ""/home/SENSETIME/wangyixuan2/.cache/bazel/_bazel_SENSETIME\wangyixuan2/302ddd240f6e4d6449f8fd791c1cce01/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 74, in _clone_or_update
		fail((""error cloning %s:\n%s"" % (ctx....)))
error cloning com_google_protobuf:
bash: line 1: cd: /home/SENSETIME/wangyixuan2/.cache/bazel/_bazel_SENSETIMEwangyixuan2/302ddd240f6e4d6449f8fd791c1cce01/external: No such file or directory
+ cd /home/SENSETIME/wangyixuan2/.cache/bazel/_bazel_SENSETIMEwangyixuan2/302ddd240f6e4d6449f8fd791c1cce01/external
bash: line 3: cd: /home/SENSETIME/wangyixuan2/.cache/bazel/_bazel_SENSETIMEwangyixuan2/302ddd240f6e4d6449f8fd791c1cce01/external: No such file or directory
+ git -C '/home/SENSETIME/wangyixuan2/.cache/bazel/_bazel_SENSETIME\wangyixuan2/302ddd240f6e4d6449f8fd791c1cce01/external/com_google_protobuf' reset --hard 5902e759108d14ee8e6b0b07653dac2f4e70ac73
fatal: Not a git repository (or any of the parent directories): .git
+ git -C '/home/SENSETIME/wangyixuan2/.cache/bazel/_bazel_SENSETIME\wangyixuan2/302ddd240f6e4d6449f8fd791c1cce01/external/com_google_protobuf' fetch --shallow-since=2019-04-01 origin 5902e759108d14ee8e6b0b07653dac2f4e70ac73:5902e759108d14ee8e6b0b07653dac2f4e70ac73
fatal: Not a git repository (or any of the parent directories): .git
+ git -C '/home/SENSETIME/wangyixuan2/.cache/bazel/_bazel_SENSETIME\wangyixuan2/302ddd240f6e4d6449f8fd791c1cce01/external/com_google_protobuf' fetch origin 5902e759108d14ee8e6b0b07653dac2f4e70ac73:5902e759108d14ee8e6b0b07653dac2f4e70ac73
fatal: Not a git repository (or any of the parent directories): .git
INFO: Elapsed time: 2.576s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (20 packages loaded)
FAILED: Build did NOT complete successfully (20 packages loaded)
    currently loading: tensorflow_federated/proto/v0
Could you help with this error, please?How to create a custom federated dataset is discussed in this stack overflow question:
https://stackoverflow.com/questions/55434004/create-a-custom-federated-data-set-in-tensorflow-federated

Sounds like the original issue of keras loss objects not being found has been resolved. I'm closing this issue, but  could you please open a new issue for building the pip package and include (the first two you've already provided, thanks!):
- the full error log
- the bazel version
- the full commandline used to build
- the git commit hash being built from",7,2019-05-13 01:34:46,2019-05-15 14:22:19,2019-05-15 14:22:19
https://github.com/tensorflow/federated/issues/416,[],Multiple inputs with keras model?,"Multiple inputs with keras model?I'm trying to apply federated learning to an existing keras model that takes two inputs. When I call `tff.learning.from_compiled_keras_model` and include a dummy batch, I get this error: `ValueError: Layer model_1 expects 2 inputs, but it received 1 input tensors. Inputs received: [<tf.Tensor 'packed:0' shape=(2, 20) dtype=int64>]`.

The model accepts two numpy arrays as inputs, so I defined my dummy_batch as:
```
x = tf.constant(np.random.randint(1,100, size=[20]))
collections.OrderedDict([('x', [x, x]), ('y', x)])
```

I dug around a little bit and saw that eventually, `tf.convert_to_tensor_or_sparse_tensor` gets called on the input list (in the `__init__` for `_KerasModel`), and that returns a single tensor of shape (2,20), instead of two separate arrays or tensors. Is there some other way I can represent the list of inputs to avoid this issue?@pgoldberg,

This is definitely something we can take a look at. Attaching a colab with a repro and some quick investigation, and assigning myself.

Colab: 

https://colab.research.google.com/drive/1E7VS3AXICnwmcHXwYum5avh_8K_oLuFyPeter, see [this commit](https://github.com/tensorflow/federated/pull/431/commits/1a87f07eb65962ce300ddfc0f5b96e54468cca24); it should contain the bugfix that you need.Thank you!",3,2019-05-12 01:49:41,2019-05-15 17:09:54,2019-05-15 17:09:54
https://github.com/tensorflow/federated/issues/399,[],How adpot other datasets,"How adpot other datasetsHi there, does the current version support other dataset customized by ourselves. The data shown in the tutorials are in HDF5 format and I have a hard time adapting mine to the pipeline.This is probably a good question for StackOverflow, where there is a `tensorflow-federated` tag. A similar question has already been asked in [Create a custom federated data set in TensorFlow Federated](https://stackoverflow.com/questions/55434004/create-a-custom-federated-data-set-in-tensorflow-federated), which might give some of the information you are looking for. 

The `tff.simulation.ClientData` interface supports any file format. The `0.4.0` release contains two implementations: [tff.simulation.HDF5ClientData](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/HDF5ClientData) and [tff.simulation.FilePerUserClientData](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/FilePerUserClientData).

If you have further questions about how to use those APIs, the TFF team would encourage using StackOverflow so the community can benefit from the recorded answers in the future.",1,2019-05-07 04:49:33,2019-05-07 15:48:54,2019-05-07 15:48:54
https://github.com/tensorflow/federated/issues/395,[],AttributeError: module 'tensorflow' has no attribute 'function',"AttributeError: module 'tensorflow' has no attribute 'function'Hello,When i execute import tensorflow_federataed as tff,there exist a error:
AttributeError: module 'tensorflow' has no attribute 'function'.
tensorflow_federated vision:  0.4.0;
python vision: 3.5;
tensorflow vision :1.13.1;
Anyone can help me solve this question?
@mlchuxueing this looks like you're building TFF from source on the `master` branch; usage of `tf.function` was not part of the 0.4.0 release (was merged in f149d83399d7dc615a2efcfd5d728f4c1f706785). Is this correct?

If you'd like to continue using the source version before the next release, I believe you'll need to use `tf-nightly` TensorFlow pip package. On `master` we've moved to depend on the `tf-nightly` pip package (in 7cd281e6916638c865eb43dece888235f75cad94). The `requirements.txt` file has been updated to reflect this. Since you already have the `tensorflow` pip package installed, you may need to first uninstall it before installing `tf-nightly`. Something like:

  ```shell
  pip uninstall tensorflow
  pip install --upgrade -r tensorflow_federated/requirements.txt
  ```> @mlchuxueing this looks like you're building TFF from source on the `master` branch; usage of `tf.function` was not part of the 0.4.0 release (was merged in [f149d83](https://github.com/tensorflow/federated/commit/f149d83399d7dc615a2efcfd5d728f4c1f706785)). Is this correct?
> 
> If you'd like to continue using the source version before the next release, I believe you'll need to use `tf-nightly` TensorFlow pip package. On `master` we've moved to depend on the `tf-nightly` pip package (in [7cd281e](https://github.com/tensorflow/federated/commit/7cd281e6916638c865eb43dece888235f75cad94)). The `requirements.txt` file has been updated to reflect this. Since you already have the `tensorflow` pip package installed, you may need to first uninstall it before installing `tf-nightly`. Something like:
> 
> ```shell
> pip uninstall tensorflow
> pip install --upgrade -r tensorflow_federated/requirements.txt
> ```

Okay， thank you!",2,2019-05-06 08:57:42,2019-05-16 16:11:09,2019-05-16 16:11:09
https://github.com/tensorflow/federated/issues/373,[],Why TFF runs slowly even with only one client?,"Why TFF runs slowly even with only one client?I found TFF is extremely slow even if I only simulated with one client. In addition, when I insert tf.print() in function ""def __call__(self, dataset, initial_weights)"" in the class ClientFedAvg in federated_averaging.py, running the script in federated_learning_for_image_classification.ipynb will print several times in each round with only one client. But this function should be executed once in each round with each client. I don't know whether it is a bugHi. The runtime we initially included is minimal - it was originally intended primarily for testing and small-scale experiments. Support for higher-performance simulations is coming. This is not to say we cannot make the reference runtime faster. We have not invested much in trying to memoize or cache things to avoid repeated calls like those you described. Perhaps you can help by posting here stack traces from these repeated calls, so that we can see exactly when they happen and diagnose this (I do not remember off the top of my head). Thanks!
Thanks for the reply!
For federated_learning_for_image_classification tutorial, set NUM_CLIENTS = 1 and insert tf.print(""call ClientFedAvg"") at the beginning of function ""def call(self, dataset, initial_weights)"" in the class ClientFedAvg in federated_averaging.py. Then I run the tutorial and get the following results:
call ClientFedAvg
call ClientFedAvg
call ClientFedAvg
round  0, loss=<loss=3.1112683>
call ClientFedAvg
call ClientFedAvg
call ClientFedAvg
round  1, loss=<loss=2.9389212>
call ClientFedAvg
call ClientFedAvg
call ClientFedAvg
round  2, loss=<loss=2.666009>
call ClientFedAvg
call ClientFedAvg
call ClientFedAvg
round  3, loss=<loss=2.1996822>
I think with NUM_CLIENTS = 1, it should print ""call ClientFedAvg"" only once. But I got three each round. It means that it trains repeatly from same initial weights in vain for each client in each round. For another case I wrote, with NUM_CLIENTS = 1 it printed more than 10 times. Thanks!After dividing into this problem, `client_outputs.__getattr__` was called by three times in `run_one_round_tff` in optimizer_utils.py: `client_outputs.weights_delta_weight`, `client_outputs.weights_delta`, and `client_outputs.model_output`, so `ClientFedAvg` was evaluated by three times.This is to be expected in the interpreted runtime, as it is naively interpreting the syntax trees TFF generates.

Have you tried using `tff.framework.set_default_executor(tff.framework.create_local_executor())`?please what is version of cuda that requires TFF?
I would like to execute this tutorial but I find incompatibility in cuda versionI think we can consider this issue closable.",6,2019-04-29 11:44:21,2020-03-09 04:34:53,2020-03-09 04:34:52
https://github.com/tensorflow/federated/issues/361,[],Using GPU produces error after upgrading to 0.4.0,"Using GPU produces error after upgrading to 0.4.0I updated TFF to 0.4.0 yesterday and found that the image classification tutorial cannot be run correctly now (which worked in previous TFF versions). After executing 

> #@test {""timeout"": 600, ""output"": ""ignore""}
state, metrics = iterative_process.next(state, federated_train_data)
print('round  1, metrics={}'.format(metrics))

The following error message occurs. I believe this problem is also partly mentioned [here](https://github.com/tensorflow/federated/issues/345). Plus, I am interested in knowing of optimizing GPU usage for TFF possibly by looping on the clients in parallel (as the previous [closed issue](https://github.com/tensorflow/federated/issues/278)). Or is there any advice on speeding up the federated training? Now it takes really long time when having a large neural network. Thank you!

> ---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1318       return self._call_tf_sessionrun(
-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)
   1320 

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1406         self._session, options, feed_dict, fetch_list, target_list,
-> 1407         run_metadata)
   1408 

InvalidArgumentError: Could not colocate node with its resource and reference inputs; devices /job:localhost/replica:0/task:0/device:CPU:0 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.
	 [[{{node ReduceDataset}}]]
	 [[{{node subcomputation/StatefulPartitionedCall_1}}]]
	 [[{{node subcomputation/StatefulPartitionedCall_1}}]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-16-8cee327e1bd3> in <module>()
      1 #@test {""timeout"": 600, ""output"": ""ignore""}
----> 2 state, metrics = iterative_process.next(state, federated_train_data)
      3 print('round  1, metrics={}'.format(metrics))

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/function_utils.py in __call__(self, *args, **kwargs)
    598     context = self._context_stack.current
    599     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 600     return context.invoke(self, arg)
    601 
    602 

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in invoke(self, fn, arg)
    698       else:
    699         computed_arg = None
--> 700       result = computed_comp.value(computed_arg)
    701       py_typecheck.check_type(result, ComputedValue)
    702       type_utils.check_assignable_from(comp.type_signature.result,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in <lambda>(x)
    841       return ComputationContext(context, {comp.parameter_name: arg})
    842 
--> 843     return ComputedValue(lambda x: self._compute(comp.result, _wrap(x)),
    844                          comp.type_signature)
    845 

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    789     else:
    790       computed_arg = None
--> 791     result = computed_fn.value(computed_arg)
    792     py_typecheck.check_type(result, ComputedValue)
    793     type_utils.check_assignable_from(computed_fn.type_signature.result,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in <lambda>(x)
    869         arg_type = comp.type_signature.parameter
    870         return ComputedValue(
--> 871             lambda x: my_method(fit_argument(x, arg_type, context)),
    872             comp.type_signature)
    873       else:

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _federated_map(self, arg)
    912     fn = arg.value[0]
    913     result_val = [
--> 914         fn(ComputedValue(x, mapping_type.parameter)).value for x in arg.value[1]
    915     ]
    916     result_type = computation_types.FederatedType(mapping_type.result,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in <listcomp>(.0)
    912     fn = arg.value[0]
    913     result_val = [
--> 914         fn(ComputedValue(x, mapping_type.parameter)).value for x in arg.value[1]
    915     ]
    916     result_type = computation_types.FederatedType(mapping_type.result,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in <lambda>(x)
    773           'but found \'{}\' instead.'.format(computation_oneof))
    774     else:
--> 775       return ComputedValue(lambda x: run_tensorflow(comp, x),
    776                            comp.type_signature)
    777 

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in run_tensorflow(comp, arg)
    342     if init_op:
    343       sess.run(init_op)
--> 344     result_val = graph_utils.fetch_value_in_session(sess, result)
    345   return capture_computed_value_from_graph(result_val,
    346                                            comp.type_signature.result)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/graph_utils.py in fetch_value_in_session(sess, value)
    759       if not tf.contrib.framework.is_tensor(v):
    760         raise ValueError('Unsupported value type {}.'.format(str(v)))
--> 761     flattened_results = sess.run(flattened_value)
    762 
    763     def _to_unicode(v):

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-> 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

InvalidArgumentError: Could not colocate node with its resource and reference inputs; devices /job:localhost/replica:0/task:0/device:CPU:0 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.
	 [[{{node ReduceDataset}}]]
	 [[node subcomputation/StatefulPartitionedCall_1 (defined at /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/tensorflow_deserialization.py:122) ]]
	 [[node subcomputation/StatefulPartitionedCall_1 (defined at /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/tensorflow_deserialization.py:122) ]]

We believe GPU support may be working again in release `0.5.0`, would you try upgrading?Closing as this seems to be handled with relatively few issues by the `local_executor`.",2,2019-04-24 23:51:44,2019-12-06 20:02:33,2019-12-06 20:02:33
https://github.com/tensorflow/federated/issues/345,[],Can we offer nightly pip packages? ,"Can we offer nightly pip packages? Recently I encounter a problem with the evaluation function in tensorflow federated,  as described [here](https://stackoverflow.com/questions/55510877/problem-with-evaluation-function-in-tensorflow-federated).

The answer below says it has been fixed in #309 and will be included in the next release.
I cannot wait for it and thus try to build the pip package from source. 
The package built by me works fine on CPU but fails on GPU, and I have no idea what went wrong.

So I hope you can offer official nightly built pip packages.
Thanks.@yaox12 Thanks for the feedback, publishing a nightly pip package is on our minds as well. It's good to know there would be demand. It also sounds like you did try and build the GPU pip package? Can you share your error message? Maybe I can help unblock you there short term.@michaelreneer It is exactly the same error as described in #361.
I have run the TFF tutorial code in [Colab](https://colab.research.google.com/drive/1joRyTabr9Euvj7eDJmTWE67jnpJmk6nM) (It is editable for convenience).
It works well when the runtime is set to CPU but occurs the above error using GPU.@yaox12 the GPU build is failing right now.

Duplicate of #361",3,2019-04-19 06:41:59,2019-05-31 15:37:49,2019-05-31 15:37:49
https://github.com/tensorflow/federated/issues/333,[],How to train on multiple GPUs?,"How to train on multiple GPUs?Hello,
The model is created with keras, following code is tried to train on 2 gpus:

> from tensorflow.keras.utils import multi_gpu_model
> model = multi_gpu_model(model, gpus=2)

but get an error:

> Traceback (most recent call last):
>   File ""/aiml/code/fl_CNN_train_v0.5.py"", line 336, in <module>
>     iterative_process = tff.learning.build_federated_averaging_process(model_fn) 
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_federated/python/learning/federated_averaging.py"", line 149, in build_federated_averaging_process
>     model_fn, client_fed_avg, server_optimizer_fn)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py"", line 286, in build_model_delta_optimizer_process
>     dummy_model_for_metadata = model_utils.enhance(model_fn())
>   File ""/aiml/code/fl_CNN_train_v0.5.py"", line 325, in model_fn
>     return tff.learning.from_compiled_keras_model(keras_model, sample_batch)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_federated/python/learning/model_utils.py"", line 193, in from_compiled_keras_model
>     return enhance(_TrainableKerasModel(keras_model, dummy_batch))
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_federated/python/learning/model_utils.py"", line 437, in __init__
>     inner_model.test_on_batch(**dummy_batch)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py"", line 1253, in test_on_batch
>     outputs = self.test_function(inputs)  # pylint: disable=not-callable
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 3073, in __call__
>     self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 3019, in _make_callable
>     callable_fn = session._make_callable_from_options(callable_opts)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1471, in _make_callable_from_options
>     return BaseSession._Callable(self, callable_options)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1425, in __init__
>     session._session, options_ptr, status)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Tensor conv2d_input:0, specified in either feed_devices or fetch_devices was not found in the Graph
> Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f53902a8898>>
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1455, in __del__
>     self._session._session, self._handle, status)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 444335424

How to train on multiple GPUs using TFF?Could I know if you have succeed in using on single GPU to train the TFF project? It seems that TFF doesn't support GPU training right now?> Could I know if you have succeed in using on single GPU to train the TFF project? It seems that TFF doesn't support GPU training right now?

@wangyixu14  I can train TFF model with one single GPU with TFF 0.2.0. However, the GPU utilization is not high, it will be idle periodically. 
I also tried it with 0.4.0 but failed.Currently the GPU architecture is failing to build. @krzys-ostrowski might be able to provide more insight.Duplicate of #361",4,2019-04-12 02:39:41,2019-05-31 15:38:35,2019-05-31 15:38:35
https://github.com/tensorflow/federated/issues/326,[],is it work in win10,"is it work in win10Hi,

We haven't tested anything on Windows machines, so I think we would say this is unsupported.

However, since we publish a pip package under tensorflow-federated, in theory at least this should be cross-platform.

Feel free to submit fixes for anything you may encounter deploying on Windows!",1,2019-04-10 12:43:33,2019-05-03 17:57:51,2019-05-03 17:57:50
https://github.com/tensorflow/federated/issues/304,[],Memory blow up when using when not flattened image array,"Memory blow up when using when not flattened image arrayHello,

I am trying to use federated learning for my use case where I already have a densenet121 weights which were built on non-flattened image array as input.

So I tried following the mnist image classification example with some modifications for my use case. But when I do not flatten the image array I run into memory blow up.

Even with 2 training examples per client, my memory of 25GB runs out. The model weights are ~ 28 MB. The dimension of input and output are 240, 240, 3 (for image) and 6 for the label (it is a 6-dimensional vector).

So I am wondering whether there is some memory leak issue when running it without flattening of images in the input.

Thanks.
Hi @anupamme,

Would you be able to provide a code snippet? Perhaps a link to a notebook on [Google Colab](https://colab.research.google.com)?

Reproducing this issue on our end should make it much easier to diagnose.

Thanks!Hello!

Follow these steps:

1. Clone https://github.com/anupamme/CheXpert-Keras
2. Install requirements.txt (some of the packages are not required for the federated code but it is easier to just install all)
3. python3.7 federated.py

Within few mins (10-15 mins) federated.py will consume all the memory. In my last execute my RAM was ~ 12G. However, I have earlier tested it on higher RAM (25G) as well.

If you are not able to run please ask.Here is a colab repro of the essentials; we'll look into the internals this week:

https://colab.research.google.com/drive/1_CxFUFinhX8jLWCreS642aophu6nRWJw

@jkr26 any update on this?Hi @anupamme 

Haven't had the time to look into this yet, but we haven't forgotten.

Thanks for your interest!Finally got around to looking at this. It seems that it no longer repros; there's no problem training these models now. This may have been an issue in the dependencies that has been solved in the meantime. Closing, everything should work now.",6,2019-04-04 06:07:24,2019-06-21 20:42:46,2019-06-21 20:42:46
https://github.com/tensorflow/federated/issues/281,[],"Tutorial ""Federated Learning for Image Classification"" does not work.","Tutorial ""Federated Learning for Image Classification"" does not work.The code here 

[federated/docs/tutorials/federated_learning_for_image_classification.ipynb](https://github.com/tensorflow/federated/blob/v0.2.0/docs/tutorials/federated_learning_for_image_classification.ipynb)

does not work, since this import does not work:
`from tensorflow.python.keras.optimizer_v2 import gradient_descent`

My conda env has the following packages:

![image](https://user-images.githubusercontent.com/49069128/55237792-3eaf5b00-5233-11e9-8019-cdecf62e6796.png)

      Hi,

Could you please advise how you are trying to execute this notebook? If it is through a local Jupyter backend, note that this is only a supported path when TFF is built from source with Bazel; the pip package may become out of sync with the master version of the notebooks.I am trying to interpret the code by a python installation in a conda environment on windows.
I could get the tutorial working now by:
- creating a NEW conda environment &
- explicitly using pip in that environment for the installation of tensorflow_federatedIt is possible then that there was a version conflict. If possible, could you attach the output of `conda list` in your new env?The output of ""conda list"" in the conda env where tff is working looks like:
![image](https://user-images.githubusercontent.com/49069128/55545023-a569c400-56cc-11e9-8d35-77808d82a972.png)
I just tried to repro this issue locally, was unsuccessful. Closing, since this seems to be resolved.",5,2019-03-29 13:59:47,2019-04-04 20:25:15,2019-04-04 20:25:15
https://github.com/tensorflow/federated/issues/262,[],AttributeError: module 'tensorflow_federated.python' has no attribute 'federated_mean',"AttributeError: module 'tensorflow_federated.python' has no attribute 'federated_mean'When i execute image classification example(Customizing the model implementation) on PyCharm in the link below:
https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb

wrong:
Traceback (most recent call last):
  File ""/data/PycharmProjects/federated_learning/test.py"", line 120, in <module>
    MnistTrainableModel)
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/learning/federated_averaging.py"", line 149, in build_federated_averaging_process
    model_fn, client_fed_avg, server_optimizer_fn)
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py"", line 303, in build_model_delta_optimizer_process
    federated_dataset_type)
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py"", line 408, in <lambda>
    return lambda fn: _wrap(fn, arg_type, self._wrapper_fn)
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py"", line 96, in _wrap
    name=func_name)
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/computation_wrapper_instances.py"", line 54, in _federated_computation_wrapper_fn
    suggested_name=name))
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/federated_computation_utils.py"", line 75, in zero_or_one_arg_func_to_building_block
    parameter_name, parameter_type), context_stack))
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/func_utils.py"", line 553, in <lambda>
    return (lambda fn, at, kt: lambda arg: _unpack_and_call(fn, at, kt, arg))(
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/func_utils.py"", line 547, in _unpack_and_call
    return func(*args, **kwargs)
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py"", line 394, in run_one_round_tff
    client_outputs.model_output)
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/func_utils.py"", line 642, in __call__
    concrete_fn = self._concrete_function_factory(arg_type)
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py"", line 85, in <lambda>
    lambda pt: _wrap_polymorphic(wrapper_fn, func, pt))
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/computation_wrapper.py"", line 82, in _wrap_polymorphic
    name=name)
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/computation_wrapper_instances.py"", line 54, in _federated_computation_wrapper_fn
    suggested_name=name))
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/federated_computation_utils.py"", line 75, in zero_or_one_arg_func_to_building_block
    parameter_name, parameter_type), context_stack))
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/func_utils.py"", line 553, in <lambda>
    return (lambda fn, at, kt: lambda arg: _unpack_and_call(fn, at, kt, arg))(
  File ""/home/BH/fyn123456/.conda/envs/py36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/func_utils.py"", line 547, in _unpack_and_call
    return func(*args, **kwargs)
  File ""/data/PycharmProjects/federated_learning/test.py"", line 60, in aggregate_mnist_metrics_across_clients
    'loss': tff.federated_mean(metrics.loss, metrics.num_examples),
AttributeError: module 'tensorflow_federated.python' has no attribute 'federated_mean'


my code:
import collections
import numpy as np
import tensorflow as tf
from tensorflow_federated import python as tff


tf.compat.v1.enable_v2_behavior()


#Defining model variables,forward pass,and metrics
MnistVariables = collections.namedtuple(
    'MnistVariables', 'weights bias num_examples loss_sum accuracy_sum')

def create_mnist_variables():
  return MnistVariables(
      weights = tf.Variable(
          lambda: tf.zeros(dtype=tf.float32, shape=(784, 10)),
          name='weights',
          trainable=True),
      bias = tf.Variable(
          lambda: tf.zeros(dtype=tf.float32, shape=(10)),
          name='bias',
          trainable=True),
      num_examples = tf.Variable(0.0, name='num_examples', trainable=False),
      loss_sum = tf.Variable(0.0, name='loss_sum', trainable=False),
      accuracy_sum = tf.Variable(0.0, name='accuracy_sum', trainable=False))

def mnist_forward_pass(variables, batch):
  y = tf.nn.softmax(tf.matmul(batch['x'], variables.weights) + variables.bias)
  predictions = tf.cast(tf.argmax(y, 1), tf.int32)

  flat_labels = tf.reshape(batch['y'], [-1])
  loss = -tf.reduce_mean(tf.reduce_sum(
      tf.one_hot(flat_labels, 10) * tf.log(y), reduction_indices=[1]))
  accuracy = tf.reduce_mean(
      tf.cast(tf.equal(predictions, flat_labels), tf.float32))

  num_examples = tf.to_float(tf.size(batch['y']))

  tf.assign_add(variables.num_examples, num_examples)
  tf.assign_add(variables.loss_sum, loss * num_examples)
  tf.assign_add(variables.accuracy_sum, accuracy * num_examples)

  return loss, predictions



def get_local_mnist_metrics(variables):
  return collections.OrderedDict([
      ('num_examples', variables.num_examples),
      ('loss', variables.loss_sum / variables.num_examples),
      ('accuracy', variables.accuracy_sum / variables.num_examples)
    ])


@tff.federated_computation
def aggregate_mnist_metrics_across_clients(metrics):
  return {
      'num_examples': tff.federated_sum(metrics.num_examples),
      'loss': tff.federated_mean(metrics.loss, metrics.num_examples),
      'accuracy': tff.federated_mean(metrics.accuracy, metrics.num_examples)
  }


#Constructing an instance of tff.learning.Model
class MnistModel(tff.learning.Model):

  def __init__(self):
    self._variables = create_mnist_variables()

  @property
  def trainable_variables(self):
    return [self._variables.weights, self._variables.bias]

  @property
  def non_trainable_variables(self):
    return []

  @property
  def local_variables(self):
    return [
        self._variables.num_examples, self._variables.loss_sum,
        self._variables.accuracy_sum
    ]

  @property
  def input_spec(self):
    return collections.OrderedDict([('x', tf.TensorSpec([None, 784],
                                                        tf.float32)),
                                    ('y', tf.TensorSpec([None, 1], tf.int32))])

  @tf.contrib.eager.function(autograph=False)
  def forward_pass(self, batch, training=True):
    del training
    loss, predictions = mnist_forward_pass(self._variables, batch)
    return tff.learning.BatchOutput(loss=loss, predictions=predictions)

  @tf.contrib.eager.function(autograph=False)
  def report_local_outputs(self):
    return get_local_mnist_metrics(self._variables)

  @property
  def federated_output_computation(self):
    return aggregate_mnist_metrics_across_clients


class MnistTrainableModel(MnistModel, tff.learning.TrainableModel):
.
  @tf.contrib.eager.defun(autograph=False)
  def train_on_batch(self, batch):
    output = self.forward_pass(batch)
    optimizer = tf.train.GradientDescentOptimizer(0.02)
    optimizer.minimize(output.loss, var_list=self.trainable_variables)
    return output

if __name__=='__main__':
    iterative_process = tff.learning.build_federated_averaging_process(
        MnistTrainableModel)



@INERKIDFYN You have two options here...
1. Run the colab that you linked from the latest release v0.2.0. https://github.com/tensorflow/federated/blob/v0.2.0/docs/tutorials/federated_learning_for_image_classification.ipynb
2. Build source at master, build a pip package, install the pip package, build a IPython kernel, and then run the notebook at master.

I would recommend option 1.@michaelreneer 
According to your suggestion 1, my problem has been solved. Thank you very much.> m
hi. i'm trying option 1 and it informed me that: ""module 'tensorflow_federated.python' has no attribute 'federated_omputation'""",3,2019-03-22 14:51:52,2021-06-18 03:02:59,2019-03-22 15:29:38
https://github.com/tensorflow/federated/issues/258,[],Keras set_weights function not working with batch normalization layer in the network,"Keras set_weights function not working with batch normalization layer in the network`model.set_weights()` function from Keras is not working if the network consists of batch normalization layer(s). I looked into the issue, I believe, it is due to having an incorrect order of array elements as returned by `tff.learning.keras_weights_from_tff_weights(state.model)` function which does not match the output of `model.get_weights()`. Also, the underlying reason could be that the `state.model` contains separate tuple for trainable and non-trainable weights.


Any updates on this? Thank you in advance. Thank you for the report @aqibsaeed, apologies for the delay.

I've been trying to reproduce this with little success. Might you be able to provide a minimal code that defines an example model and input data to reproduce this? Which TFF release you see this behavior?

Its also not entirely clear to us (yet) how the batch normalization parameters should be handled in the federated setting.

I used the [colab notebook](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) provided on tensorflow.org/federated website. 

```
def create_compiled_keras_model():
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(64, activation='linear', kernel_initializer='zeros', input_shape=(784,)),
      tf.keras.layers.BatchNormalization(),
      tf.keras.layers.Activation('relu'),
      tf.keras.layers.Dense(64, activation='linear', kernel_initializer='zeros'),
      tf.keras.layers.BatchNormalization(),
      tf.keras.layers.Activation('relu'),
      tf.keras.layers.Dense(10, activation=tf.nn.softmax, kernel_initializer='zeros')
  ])
  
  def loss_fn(y_true, y_pred):
    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(
        y_true, y_pred))
 
  model.compile(
      loss=loss_fn,
      optimizer=gradient_descent.SGD(learning_rate=0.02),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
  return model
```

After one round of training as described in the notebook, I tried to set the weights as follows and got the error pasted below: (Am I not setting the weights correctly?)
```
n_model = create_compiled_keras_model()
n_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))
```

`ValueError                                Traceback (most recent call last)
<ipython-input-31-06625dec9d3d> in <module>()
      1 nglobal_model = create_compiled_keras_model()
----> 2 nglobal_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in set_weights(self, weights)
    406         tuples.append((sw, w))
    407       weights = weights[num_param:]
--> 408     backend.batch_set_value(tuples)
    409 
    410   def compute_mask(self, inputs, mask):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in batch_set_value(tuples)
   2858   if ops.executing_eagerly_outside_functions():
   2859     for x, value in tuples:
-> 2860       x.assign(np.asarray(value, dtype=dtype(x)))
   2861   else:
   2862     with get_graph().as_default():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in assign(self, value, use_locking, name, read_value)
    913     with _handle_graph(self.handle):
    914       value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)
--> 915       self._shape.assert_is_compatible_with(value_tensor.shape)
    916       assign_op = gen_resource_variable_ops.assign_variable_op(
    917           self.handle, value_tensor, name=name)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)
   1021     """"""
   1022     if not self.is_compatible_with(other):
-> 1023       raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
   1024 
   1025   def most_specific_compatible_shape(self, other):

ValueError: Shapes (64,) and (64, 64) are incompatible`



To debug this issue I tried printing weight matrix length for each layer, it seems the order of weight matrix as returned by `tff.learning.keras_weights_from_tff_weights()` is different than `base_model.get_weights()`. 

```
base_model = create_compiled_keras_model()
random_weights = base_model.get_weights()
for rw in random_weights:
  print(len(rw))
```
> 784
64
64
64
64
64
64
64
64
64
64
64
64
10

```
federated_weights = tff.learning.keras_weights_from_tff_weights(state.model)
for fw in federated_weights:
  print(len(fw))
```
> 784
64
64
64
64
64
64
64
64
10
64
64
64
64@aqibsaeed thank you for sharing the model you used and the steps you have already taken, they were very helpful!

It looks like `tf.keras.Model.weights` and `tf.keras.Moldel.get_weights()` are returning the weights in different order. I suspect `set_weights()` is expecting the order returned by `get_weights()`, but TFF is pulling from `.weights` parameter.

I'll look into a fix for this.

```
federated_weights = tff.learning.keras_weights_from_tff_weights(state.model)
for f_w, k_w in zip(federated_weights, base_model.weights):
  print('{} == {}: {}'.format(f_w.shape, k_w.shape, f_w.shape == k_w.shape))
```

> (784, 64) == (784, 64): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64, 64) == (64, 64): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64, 10) == (64, 10): True
> (10,) == (10,): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64,): True

```
for f_w, k_w in zip(federated_weights, base_model.get_weights()):
  print('{} == {}: {}'.format(f_w.shape, k_w.shape, f_w.shape == k_w.shape))
```
> (784, 64) == (784, 64): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64, 64) == (64,): False
> (64,) == (64,): True
> (64,) == (64, 64): False
> (64,) == (64,): True
> (64, 10) == (64,): False
> (10,) == (64,): False
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64, 10): False
> (64,) == (10,): FalseThank you very much! @aqibsaeed 

(**Note:** this checked-in to source, but isn't in the pip package yet. The next release will be in a week or two)

`tff.learning.framework.ModelWeights` has a new interface method called `assign_weights_to` ([api_docs](https://github.com/tensorflow/federated/blob/master/docs/api_docs/python/tff/learning/framework/ModelWeights.md)) which should be a workaround in the short term.

The code  would look something like:

```
keras_model = tf.keras.models.Sequential(...)
tff_weights = tff.learning.keras_weights_from_tff_weights(state.model)
tff_weights.assign_weights_to(keras_model)
```

Please also be aware that BatchNorm has trainable variables (beta, gamma) and non-trainable variables (mean, variance). How these should be aggregated at the global model is still an open question, and likely the vanilla FedAvg algorithm in the FL API may not be doing what is desired.Sounds good! Thank you very much for the prompt action. I think this issue can be closed now. Hello
I followed the above discussion on keras models with batch_normalization, this is the relevant part of the code:
```
for round_num in range(2, 70): 
    FLstate, FLoutputs = trainer_Itr_Process.next(FLstate, federated_train_data)   
    FLlosses_arr.append(FLoutputs.loss)
    tff_weights= tff.learning.keras_weights_from_tff_weights(FLstate.model)
    tff_weights.assign_weights_to(tff_weights, Local_model_Fed)
```


I appreciate the help with the following error:
```
Traceback (most recent call last):
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 2225, in <module>
    main()
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 2218, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 1560, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 1567, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\_pydev_imps\_pydev_execfile.py"", line 25, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:\Users\ezalaab\Documents\eclipse-workspace\MANA-FederatedLearning\location-based-federated-learning\Test\FederatedLearningTFv3.py"", line 299, in <module>
    tff_weights.assign_weights_to(tff_weights, Local_model_Fed)
AttributeError: 'list' object has no attribute 'assign_weights_to
```",8,2019-03-20 20:58:09,2019-06-01 16:52:28,2019-04-01 17:30:47
https://github.com/tensorflow/federated/issues/250,[],"No output from tff.federated_computation(), image classification example","No output from tff.federated_computation(), image classification exampleWhen i execute image classification example in the link below:
https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb
Following the installation steps provided.
I am not getting ourput of 'Hello, World!'

System: Ubuntu 18.04
Output:
![image](https://user-images.githubusercontent.com/32511895/54412596-56f28680-4719-11e9-88c7-52a88567cbf2.png)


Please help to know the reason.I have met the same problem, as I tried using `jupyter notebook` and follow the linked example, it will be totally fine to run it.@ANSHUMAN87 It looks like you are using jupyter to convert the notebook to python, is that correct? Or are you possibly copying the code from the notebook into your own custom script? Have you tried to run the notebook in colab? One difference between running the code in a notebook and running the code in a script is that in a notebook the output is produced by the notebook not the python code. So when you bring the notebook into a python script you need to additionally `print` the output..

```
print(tff.federated_computation(lambda: 'Hello, World!')())
```@michaelreneer : Thank you very much for pointing out that. It is really the root-cause. Now it is solved, thanks for your help!",3,2019-03-15 06:26:42,2019-03-25 04:53:59,2019-03-22 15:32:16
https://github.com/tensorflow/federated/issues/247,[],examples/mnist problem,"examples/mnist problemI'm trying mnist example and something happens 

`During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""demo.py"", line 26, in <module>
trainer = tff.learning.build_federated_averaging_process(model_fn)`

![图片](https://user-images.githubusercontent.com/22471616/54344362-a2eeee00-467b-11e9-84cc-119ecefe5a84.png)
@drkingman Your issue mentions that you are trying to mnist example and your error and picture reference a `demo.py`.

Have you checked out the [install](https://www.tensorflow.org/federated/install) documentation? I would recommend first running the colabs if you are interesting in learning about federated learning. If you are interested in contributing please take a look at those install instructions and they try running the following command to run the mnist example.

```bash
bazel test //tensorflow_federated/python/examples/mnist:models_test
```@michaelreneer Thank a lot, the problem is solved.",2,2019-03-14 09:08:40,2019-03-20 01:11:45,2019-03-20 01:11:45
https://github.com/tensorflow/federated/issues/241,[],Error building the pip package with Bazel,"Error building the pip package with BazelI had a problem building the pip package with Bazel, as below.

------
```
(venv) heikos-air:federated hludwig$ mkdir ""/tmp/tensorflow_federated""
(venv) heikos-air:federated hludwig$ bazel build `//tensorflow_federated/tools:build_pip_package`
ERROR: /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/io_bazel_rules_closure/closure/webfiles/web_library.bzl:191:17: Traceback (most recent call last):
	File ""/private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/io_bazel_rules_closure/closure/webfiles/web_library.bzl"", line 183
		rule(implementation = _web_library, exe..., <2 more arguments>)
	File ""/private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/io_bazel_rules_closure/closure/webfiles/web_library.bzl"", line 191, in rule
		attr.label_list(cfg = ""data"", allow_files = True)
Using cfg = ""data"" on an attribute is a noop and no longer supported. Please remove it. You can use --incompatible_disallow_data_transition=false to temporarily disable this check.
ERROR: error loading package '': in /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/org_tensorflow/tensorflow/workspace.bzl: in /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/io_bazel_rules_closure/closure/defs.bzl: Extension file 'closure/webfiles/web_library.bzl' has errors
ERROR: error loading package '': in /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/org_tensorflow/tensorflow/workspace.bzl: in /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/io_bazel_rules_closure/closure/defs.bzl: Extension file 'closure/webfiles/web_library.bzl' has errors
INFO: Elapsed time: 0.330s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```
-----------

It's my first time using Bazel. I had installed it as per instructions but the test had errors. Any thoughts?

------------
(venv) heikos-air:federated hludwig$ bazel test //tensorflow_federated/...
Starting local Bazel server and connecting to it...
ERROR: /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/io_bazel_rules_closure/closure/webfiles/web_library.bzl:191:17: Traceback (most recent call last):
	File ""/private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/io_bazel_rules_closure/closure/webfiles/web_library.bzl"", line 183
		rule(implementation = _web_library, exe..., <2 more arguments>)
	File ""/private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/io_bazel_rules_closure/closure/webfiles/web_library.bzl"", line 191, in rule
		attr.label_list(cfg = ""data"", allow_files = True)
Using cfg = ""data"" on an attribute is a noop and no longer supported. Please remove it. You can use --incompatible_disallow_data_transition=false to temporarily disable this check.
ERROR: error loading package '': in /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/org_tensorflow/tensorflow/workspace.bzl: in /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/io_bazel_rules_closure/closure/defs.bzl: Extension file 'closure/webfiles/web_library.bzl' has errors
ERROR: error loading package '': in /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/org_tensorflow/tensorflow/workspace.bzl: in /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/io_bazel_rules_closure/closure/defs.bzl: Extension file 'closure/webfiles/web_library.bzl' has errors
INFO: Elapsed time: 41.430s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
FAILED: Build did NOT complete successfully (0 packages loaded)
Thanks for the report. Can you try adding the flag 
   `--incompatible_disallow_data_transition=false`
to your `bazel test` commnadline, as a temporary workaround while we dig into the issue?I tried that. It went really well for a long time but then it failed. 

````
ERROR: /private/var/tmp/_bazel_hludwig/b759d6cfdc3b8fefc6d5ace86fb89267/external/org_tensorflow/tensorflow/python/BUILD:5919:1: Linking of rule '@org_tensorflow//tensorflow/python:framework/fast_tensor_util.so' failed (Exit 1) cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -lc++ -fobjc-link-runtime -Wl,-S -shared -o bazel-out/host/bin/external/org_tensorflow/tensorflow/python/framework/fast_tensor_util.so ... (remaining 4 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
Undefined symbols for architecture x86_64:
  ""_PyBaseObject_Type"", referenced from:
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_5AppendFloat32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_7AppendFloat64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_9AppendInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_11AppendUInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_13AppendInt64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""_PyBuffer_Release"", referenced from:
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx__GetBufferAndValidate(bufferinfo*, _object*, __Pyx_TypeInfo*, int, int, int, __Pyx_BufFmt_StackElem*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_5AppendFloat32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_7AppendFloat64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_9AppendInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_11AppendUInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""_PyBytes_FromStringAndSize"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyCFunction_NewEx"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyCFunction_Type"", referenced from:
      __Pyx_PyObject_Append(_object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_CallOneArg(_object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_29AppendObjectArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_31AppendBoolArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyCode_New"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
  ""_PyDict_GetItemString"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyDict_New"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyDict_Next"", referenced from:
      __Pyx_ParseOptionalKeywords(_object*, _object***, _object*, _object**, long, char const*) in fast_tensor_util.o
  ""_PyDict_SetItem"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyDict_SetItemString"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyDict_Size"", referenced from:
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_5AppendFloat32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_7AppendFloat64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_9AppendInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_11AppendUInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_13AppendInt64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""_PyErr_Clear"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
  ""_PyErr_ExceptionMatches"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyErr_Format"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
      __Pyx_ImportType(char const*, char const*, unsigned long, int) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_ParseOptionalKeywords(_object*, _object***, _object*, _object**, long, char const*) in fast_tensor_util.o
      __Pyx__GetBufferAndValidate(bufferinfo*, _object*, __Pyx_TypeInfo*, int, int, int, __Pyx_BufFmt_StackElem*) in fast_tensor_util.o
      __Pyx_PyInt_As_long(_object*) in fast_tensor_util.o
      __Pyx_BufFmt_CheckString(__Pyx_BufFmt_Context*, char const*) in fast_tensor_util.o
      ...
  ""_PyErr_Occurred"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_ParseOptionalKeywords(_object*, _object***, _object*, _object**, long, char const*) in fast_tensor_util.o
      __Pyx_PyInt_As_long(_object*) in fast_tensor_util.o
      __Pyx_PyObject_Call(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_CallOneArg(_object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""_PyErr_SetString"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_PyInt_As_long(_object*) in fast_tensor_util.o
      __Pyx_BufFmt_CheckString(__Pyx_BufFmt_Context*, char const*) in fast_tensor_util.o
      __Pyx_BufFmt_ProcessTypeChunk(__Pyx_BufFmt_Context*) in fast_tensor_util.o
      __Pyx_PyObject_Call(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_CallOneArg(_object*, _object*) in fast_tensor_util.o
      ...
  ""_PyErr_WarnEx"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
      __Pyx_ImportType(char const*, char const*, unsigned long, int) in fast_tensor_util.o
  ""_PyErr_WarnFormat"", referenced from:
      __Pyx_PyInt_As_long(_object*) in fast_tensor_util.o
  ""_PyEval_EvalCodeEx"", referenced from:
      __Pyx_PyFunction_FastCallDict(_object*, _object**, int, _object*) in fast_tensor_util.o
  ""_PyEval_EvalFrameEx"", referenced from:
      __Pyx_PyFunction_FastCallDict(_object*, _object**, int, _object*) in fast_tensor_util.o
  ""_PyExc_AttributeError"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyExc_DeprecationWarning"", referenced from:
      __Pyx_PyInt_As_long(_object*) in fast_tensor_util.o
  ""_PyExc_ImportError"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyExc_NameError"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_29AppendObjectArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_31AppendBoolArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyExc_SystemError"", referenced from:
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_Call(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_CallOneArg(_object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_5AppendFloat32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_7AppendFloat64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_9AppendInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""_PyExc_TypeError"", referenced from:
      __Pyx_ImportType(char const*, char const*, unsigned long, int) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_ParseOptionalKeywords(_object*, _object***, _object*, _object**, long, char const*) in fast_tensor_util.o
      __Pyx_PyInt_As_long(_object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_5AppendFloat32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_7AppendFloat64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""_PyExc_ValueError"", referenced from:
      __Pyx_ImportType(char const*, char const*, unsigned long, int) in fast_tensor_util.o
      __Pyx__GetBufferAndValidate(bufferinfo*, _object*, __Pyx_TypeInfo*, int, int, int, __Pyx_BufFmt_StackElem*) in fast_tensor_util.o
      __Pyx_BufFmt_CheckString(__Pyx_BufFmt_Context*, char const*) in fast_tensor_util.o
      __Pyx_BufFmt_RaiseExpected(__Pyx_BufFmt_Context*) in fast_tensor_util.o
      __Pyx_BufFmt_ProcessTypeChunk(__Pyx_BufFmt_Context*) in fast_tensor_util.o
  ""_PyFloat_FromDouble"", referenced from:
      __pyx_pw_16fast_tensor_util_5AppendFloat32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_7AppendFloat64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_25AppendComplex64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_27AppendComplex128ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyFrame_New"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
      __Pyx_PyFunction_FastCallDict(_object*, _object**, int, _object*) in fast_tensor_util.o
  ""_PyFunction_Type"", referenced from:
      __Pyx_PyObject_Append(_object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_CallOneArg(_object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_29AppendObjectArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_31AppendBoolArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyImport_AddModule"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyImport_GetModuleDict"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyImport_Import"", referenced from:
      __Pyx_ImportType(char const*, char const*, unsigned long, int) in fast_tensor_util.o
  ""_PyImport_ImportModuleLevelObject"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyList_Append"", referenced from:
      __Pyx_PyObject_Append(_object*, _object*) in fast_tensor_util.o
  ""_PyList_New"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyList_Type"", referenced from:
      __Pyx_PyObject_Append(_object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_29AppendObjectArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_31AppendBoolArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyLong_AsLong"", referenced from:
      __Pyx_PyInt_As_long(_object*) in fast_tensor_util.o
  ""_PyLong_FromLong"", referenced from:
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_9AppendInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_11AppendUInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_13AppendInt64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_17AppendUInt8ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_19AppendUInt16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""_PyLong_FromSsize_t"", referenced from:
      __pyx_pw_16fast_tensor_util_29AppendObjectArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_31AppendBoolArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyLong_FromUnsignedLong"", referenced from:
      __pyx_pw_16fast_tensor_util_15AppendUInt64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyLong_Type"", referenced from:
      __Pyx_PyInt_As_long(_object*) in fast_tensor_util.o
  ""_PyMem_Malloc"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
  ""_PyMem_Realloc"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
  ""_PyMethod_Type"", referenced from:
      __Pyx_PyObject_Append(_object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_29AppendObjectArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_31AppendBoolArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyModule_Create2"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyModule_GetDict"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyOS_snprintf"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
      __Pyx_ImportType(char const*, char const*, unsigned long, int) in fast_tensor_util.o
  ""_PyObject_Call"", referenced from:
      __Pyx_PyObject_Call(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyObject_GetAttr"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
      __Pyx_ImportType(char const*, char const*, unsigned long, int) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_Append(_object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_5AppendFloat32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""_PyObject_GetBuffer"", referenced from:
      __Pyx__GetBufferAndValidate(bufferinfo*, _object*, __Pyx_TypeInfo*, int, int, int, __Pyx_BufFmt_StackElem*) in fast_tensor_util.o
  ""_PyObject_GetItem"", referenced from:
      __pyx_pw_16fast_tensor_util_29AppendObjectArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_31AppendBoolArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyObject_Hash"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyObject_Not"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
  ""_PyObject_SetAttr"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
  ""_PyObject_SetAttrString"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyThreadState_Get"", referenced from:
      __Pyx_PyFunction_FastCallDict(_object*, _object**, int, _object*) in fast_tensor_util.o
      __Pyx_PyObject_Call(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_CallOneArg(_object*, _object*) in fast_tensor_util.o
  ""_PyTraceBack_Here"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
  ""_PyTuple_New"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
      __Pyx_PyObject_Append(_object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_CallOneArg(_object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_29AppendObjectArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_31AppendBoolArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyTuple_Pack"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyTuple_Type"", referenced from:
      __pyx_pw_16fast_tensor_util_29AppendObjectArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_31AppendBoolArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
  ""_PyUnicode_AsUnicode"", referenced from:
      __Pyx_ParseOptionalKeywords(_object*, _object***, _object*, _object**, long, char const*) in fast_tensor_util.o
  ""_PyUnicode_Compare"", referenced from:
      __Pyx_ParseOptionalKeywords(_object*, _object***, _object*, _object**, long, char const*) in fast_tensor_util.o
  ""_PyUnicode_Decode"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyUnicode_FromFormat"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
  ""_PyUnicode_FromString"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
      __Pyx_ImportType(char const*, char const*, unsigned long, int) in fast_tensor_util.o
  ""_PyUnicode_FromStringAndSize"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_PyUnicode_InternFromString"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""_Py_GetVersion"", referenced from:
      _PyInit_fast_tensor_util in fast_tensor_util.o
  ""__PyDict_GetItem_KnownHash"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_5AppendFloat32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_7AppendFloat64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_9AppendInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_11AppendUInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""__PyObject_GetDictPtr"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
  ""__PyThreadState_UncheckedGet"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_PyFunction_FastCallDict(_object*, _object**, int, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_5AppendFloat32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_7AppendFloat64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_9AppendInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""__Py_CheckRecursionLimit"", referenced from:
      __Pyx_PyFunction_FastCallDict(_object*, _object**, int, _object*) in fast_tensor_util.o
      __Pyx_PyObject_Call(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_CallOneArg(_object*, _object*) in fast_tensor_util.o
  ""__Py_CheckRecursiveCall"", referenced from:
      __Pyx_PyFunction_FastCallDict(_object*, _object**, int, _object*) in fast_tensor_util.o
      __Pyx_PyObject_Call(_object*, _object*, _object*) in fast_tensor_util.o
      __Pyx_PyObject_CallOneArg(_object*, _object*) in fast_tensor_util.o
  ""__Py_FalseStruct"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
  ""__Py_NoneStruct"", referenced from:
      __pyx_pw_16fast_tensor_util_1AppendBFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_3AppendFloat16ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_5AppendFloat32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_7AppendFloat64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_9AppendInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_11AppendUInt32ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      __pyx_pw_16fast_tensor_util_13AppendInt64ArrayToTensorProto(_object*, _object*, _object*) in fast_tensor_util.o
      ...
  ""__Py_TrueStruct"", referenced from:
      __Pyx_AddTraceback(char const*, int, int, char const*) in fast_tensor_util.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
INFO: Elapsed time: 7092.364s, Critical Path: 443.38s
INFO: 3115 processes: 3114 darwin-sandbox, 1 local.
FAILED: Build did NOT complete successfully
//tensorflow_federated/python/common_libs:anonymous_tuple_test        NO STATUS
//tensorflow_federated/python/common_libs:py_typecheck_test           NO STATUS
//tensorflow_federated/python/common_libs:test_test                   NO STATUS
//tensorflow_federated/python/core/api:computation_types_test         NO STATUS
//tensorflow_federated/python/core/api:computations_test              NO STATUS
//tensorflow_federated/python/core/api:intrinsics_test                NO STATUS
//tensorflow_federated/python/core/api:values_test                    NO STATUS
//tensorflow_federated/python/core/impl:compiler_pipeline_test        NO STATUS
//tensorflow_federated/python/core/impl:computation_building_blocks_test NO STATUS
//tensorflow_federated/python/core/impl:computation_constructing_utils_test NO STATUS
//tensorflow_federated/python/core/impl:computation_impl_test         NO STATUS
//tensorflow_federated/python/core/impl:computation_wrapper_instances_test NO STATUS
//tensorflow_federated/python/core/impl:computation_wrapper_test      NO STATUS
//tensorflow_federated/python/core/impl:context_stack_test            NO STATUS
//tensorflow_federated/python/core/impl:federated_computation_context_test NO STATUS
//tensorflow_federated/python/core/impl:federated_computation_utils_test NO STATUS
//tensorflow_federated/python/core/impl:func_utils_test               NO STATUS
//tensorflow_federated/python/core/impl:graph_utils_test              NO STATUS
//tensorflow_federated/python/core/impl:intrinsic_bodies_test         NO STATUS
//tensorflow_federated/python/core/impl:intrinsic_defs_test           NO STATUS
//tensorflow_federated/python/core/impl:intrinsic_utils_test          NO STATUS
//tensorflow_federated/python/core/impl:placement_literals_test       NO STATUS
//tensorflow_federated/python/core/impl:reference_executor_test       NO STATUS
//tensorflow_federated/python/core/impl:tensorflow_deserialization_test NO STATUS
//tensorflow_federated/python/core/impl:tensorflow_serialization_test NO STATUS
//tensorflow_federated/python/core/impl:tf_computation_context_test   NO STATUS
//tensorflow_federated/python/core/impl:transformations_test          NO STATUS
//tensorflow_federated/python/core/impl:type_constructors_test        NO STATUS
//tensorflow_federated/python/core/impl:type_serialization_test       NO STATUS
//tensorflow_federated/python/core/impl:type_utils_test               NO STATUS
//tensorflow_federated/python/core/impl:value_impl_test               NO STATUS
//tensorflow_federated/python/core/impl:value_utils_test              NO STATUS
//tensorflow_federated/python/core/utils:computation_utils_test       NO STATUS
//tensorflow_federated/python/core/utils:tf_computation_utils_test    NO STATUS
//tensorflow_federated/python/examples/mnist:models_test              NO STATUS
//tensorflow_federated/python/learning:federated_averaging_benchmark  NO STATUS
//tensorflow_federated/python/learning:federated_averaging_test       NO STATUS
//tensorflow_federated/python/learning:federated_evaluation_test      NO STATUS
//tensorflow_federated/python/learning:federated_sgd_test             NO STATUS
//tensorflow_federated/python/learning:model_examples_test            NO STATUS
//tensorflow_federated/python/learning:model_utils_test               NO STATUS
//tensorflow_federated/python/learning/framework:optimizer_utils_test NO STATUS
//tensorflow_federated/python/simulation:file_per_user_client_data_test NO STATUS
//tensorflow_federated/python/simulation:hdf5_client_data_test        NO STATUS
//tensorflow_federated/python/simulation:transforming_client_data_test NO STATUS
//tensorflow_federated/python/tensorflow_libs:tensor_utils_test       NO STATUS

FAILED: Build did NOT complete successfully

```I probably should have added this at the outset: I built on MacOS 10.14.3 on a MacBook Air. If you have any suggestions for a less problematic platform to try it out let me know.@heikoludwig 

Could you provide more information on your setup? (e.g. what version of Bazel?)

This may be bazelbuild/bazel#7607. If you're confident enough, you might try the workaround mentioned in that issue.My Bazel version is 

```
(venv) heikos-air:federated hludwig$ bazel version
Starting local Bazel server and connecting to it...
Build label: 0.23.1
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Mon Mar 4 10:40:32 2019 (1551696032)
Build timestamp: 1551696032
Build timestamp as int: 1551696032
```

I also use the same MacOS version as the poster of  [#bazelbuild/bazel#7607]. So, it appears that the error is along the same lines although the error message is slightly different.You have to use the bazel version corresponding to the tensor flow version listed on this page

https://www.tensorflow.org/install/source#tested_build_configurationsAfter f765a2387c65a843371fa4c85f90df5d21b7cd8f TFF no longer builds TensorFlow from source, instead relies on the system to provide TensorFlow. @heikoludwig could you see if this is still an issue?I'm going to close this issue @heikoludwig please reopen if you discover this is still an issue.",8,2019-03-12 20:22:24,2019-05-31 15:34:12,2019-05-31 15:34:12
https://github.com/tensorflow/federated/issues/236,[],Tensor is not an element of this graph,"Tensor is not an element of this graphI've tried to reproduce a tutorial ""Federated Learning for Image Classification"" in Colab [1] and encountered a `ValueError` during execution

    iterative_process = tff.learning.build_federated_averaging_process(model_fn)

The error is following:

     ---------------------------------------------------------------------------
     ValueError                                Traceback (most recent call last)
     /usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
        1091             subfeed_t = self.graph.as_graph_element(
     -> 1092                 subfeed, allow_tensor=True, allow_operation=False)
        1093           except Exception as e:

     /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)
        3477     with self._lock:
     -> 3478       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
        3479 

     /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)
        3556       if obj.graph is not self:
     -> 3557         raise ValueError(""Tensor %s is not an element of this graph."" % obj)
        3558       return obj

     ValueError: Tensor Tensor(""Placeholder:0"", shape=(), dtype=float32) is not an element of this graph.

How it can be solved?


[1] https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynbThanks for the report @osanwe. We're are actively working on the next release, which will fix this.

The following link should work for the next few days, until the new release:
https://colab.research.google.com/github/tensorflow/federated/blob/test_237819542/docs/tutorials/federated_learning_for_image_classification.ipynbHi @osanwe, we updated to v0.2.0 earlier today; this notebook now works. Thanks for the vigilance!Hi, the above link is not working. I also faced the same issue.",3,2019-03-12 09:56:59,2022-07-01 00:29:20,2019-03-13 04:36:48
https://github.com/tensorflow/federated/issues/235,[],install and import problem,"install and import problemI have followed the install steps
`virtualenv --python python3 ""venv""
source ./venv/bin/activate
pip install --upgrade pip
(veno) pip install --upgrade tensorflow_federated
`
and I have installed `bazel 0.23.2`
when testing `python -c ""import tensorflow_federated as tff; tff.federated_computation(lambda: 'Hello, World!')()""` it returned `bash: !': event not found`

when testing the demo, it shows error `import tensorflow_federated as tff`
![图片](https://user-images.githubusercontent.com/22471616/54172868-8a83a580-44ba-11e9-839c-1f9759bd1038.png)
You may check the version of tensorflow.
Tensorflow does not have a contrib module for 2.0 version.
#224 Duplicate of #224> Duplicate of #224

actually since I updated to 0.2.0 version,  it still returned `bash: !': event not found`Sorry I should have been more clear previously. The error that I initially noticed actually had nothing to do with `bash: !': event not found` and instead was the line in your screenshot reading `AttributeError: module 'tensorflow' has not attribute 'contrib'`. This error was described in #224.

I suspect that you tried to re-run these commands using your old virtual environment? I am not sure if `pip install --upgrade tensorflow_federated` will downgrade the version of TF. I would recommend running `pip freeze` and replying with the output from that command. I suspect that you would find tensorflow `2.0.0a` in that list. If this is the case, the fix is to simply delete your virtual environment and create a new one.",4,2019-03-12 03:34:25,2019-03-22 15:31:34,2019-03-22 15:31:34
https://github.com/tensorflow/federated/issues/231,[],ImportError: cannot import name 'computation_pb2',"ImportError: cannot import name 'computation_pb2'When running the code ''tensorflow_federated/python/examples/mnist/**models_test.py**'', I got an error

File ""/home/wzy/Desktop/github_project/federated-master/tensorflow_federated/python/core/impl/computation_impl.py"", line 20, in <module>
    from tensorflow_federated.proto.v0 import computation_pb2 as pb
**ImportError: cannot import name 'computation_pb2'**

Then I viewed the folder  **tensorflow_federated/proto/v0**, I find there are no file named **computation_pb2**, could you please tell me what should I do? @michaelreneer @jkr26 
Thank you!
@wuzhiyu666 Can you go into more detail by what you mean when you say ""when running""? Is it possible that you are possibly not using Bazel to run the `models_test` target?

```
bazel test tensorflow_federated/python/examples/mnist:models_test
```

You can see that there is a protobuf [computation.proto](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/proto/v0/computation.proto) located at the path that you referenced. Bazel will resolve these dependencies and eventually generate the python that you are looking for `computation_pb2` from the `computation.proto`.

Take a look at our [installation instructions](https://github.com/tensorflow/federated/blob/master/docs/install.md) for how to install Bazel and test TFF.thank you so much! I will try it.Sorry, when I used bazel to compile the code, it took 2 hours and failed in the end. when I say ""when running"", I am running the code ""`models_test`"" in pycharm. I have question that can I run the code in pycharm locally without bazel and docker?
Thank you so much! @michaelreneer 
So there are a few things here:

* It's possible that it would take 2 hours to build. TFF has a source-level dependency on TensorFlow so you end up building TFF from source. This entirely depends on your machine.

* We have a hard dependency on protobuf and converting the `computation.proto` in to the missing `computation_pb2`. Which practically means that we also have a dependency on Bazel. We don't have a dependency on Docker, you could use `virtualenv` to create a local environment. Docker or `virtaulenv` are used here to keep the pip package dependencies separated from the system Python.

* You can use pycharm as an IDE if you want, but you still need to compile the protobuf. You would also need to install the packages that we do not build from source, you can find those in the `requirements.txt`.

* I am curious about the error that you got after it took 2 hours to build. Did you happen to save the log? What command ended up failing?

Keep in mind you don't need to build everything from source to **use** TFF, you can instead pip install the pip package. You would not be able to run the unit tests by only installing the pip package, but you could use and play with the API.Thank you so much! @michaelreneer 
Sorry for the late reply, I have reinstalled my operating system and run bazel again. But I got a new problem. The bazel have used up all my RAM and crash my computer. 
Could you please tell me how to sovle the problem?
Thank you!I have a new question that if using bazel is not mandatory, is there another demo can I run without bazel?
I am new to federated-leaning and just want to run a demo to see how it works.
Thank you! @michaelreneer Hi @wuzhiyu666,

If you are looking for a demo, the place I would start is the Colab notebooks linked from tensorflow.org/federated, under tutorials, e.g.:

https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification

If you open these notebooks, you will see a button for ""open in Google Colab""; this will connect you to a runtime which allows you to step through the code and text, executing code block-by-block.

Hopefully this is what you are looking for! We are happy to hear of your interest in federated learning!@wuzhiyu666 

As I mentioned before you can simply install the pip package to use TFF or you could do as @jkr26 suggested and run one of the TFF tutorials directly in your browser in Colab. **I would strongly recommend one of these approachs, the easiest is simply running the notebook in your browser.***

Bazel is required to build TFF source. If you are having issues with Bazel RAM usage, you can check out this [bazel build options](https://www.tensorflow.org/install/source#bazel_build_options) documentation from TensorFlow or the Bazel documentation on the [local_resources](https://docs.bazel.build/versions/master/user-manual.html#flag--local_resources) flag.@wuzhiyu666 As a data point for us, would you mind sharing the specs of your machine?Hi!

The error **""ImportError: cannot import name 'computation_pb2'""** still appears when running 
`python -c ""import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())""`

even after installation of TF Fedefrated via Bazel. At the same time running
` bazel test tensorflow_federated/python/examples/mnist:models_test`
performs well, with no error.

Specs:
MacOS Sierra 10.12
Python 3.6
PyCharm CE 2019 IDE
Conda env",10,2019-03-10 05:20:29,2019-06-25 11:11:59,2019-03-22 15:31:18
https://github.com/tensorflow/federated/issues/225,[],No 'loss' attribute,"No 'loss' attributeI was able to run the Federated Image classification tutorial last week. But now this error is popping:
![tff_eror](https://user-images.githubusercontent.com/8556809/53893611-6e00ec80-4026-11e9-9a05-e03eed235a5d.PNG)
Testing #226, will merge once all tests pass#226 is merged, this error should be fixed.Thanks!",3,2019-03-06 15:42:24,2019-03-06 17:21:07,2019-03-06 17:21:07
https://github.com/tensorflow/federated/issues/224,[],Installing with pip breaks because TF2.0 doesn't have contrib,"Installing with pip breaks because TF2.0 doesn't have contribCurrently you cannot install by:
`pip install --upgrade tensorflow-federated`
because it will download tf 2.0 which doesn't have a contrib module. You have to pip install a fixed version<2.0 (it works with 1.13.1 for instance).@jeandut Thanks for bringing this to our attention. This will be fixed in the next release. I have assigned this issue to myself and I will close it with that release. 

As you mentioned you can pip install a fixed version < 2.0 to work around this issue until then.This is fixed in [v0.2.0](https://github.com/tensorflow/federated/tree/v0.2.0).",2,2019-03-06 14:01:46,2019-03-13 01:01:13,2019-03-13 01:01:12
https://github.com/tensorflow/federated/issues/220,[],Issues with enabling eager execution.,"Issues with enabling eager execution.I was following the tutorials included with the repo, The one on https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb to be specific, and on executing 
`example_element = iter(example_dataset).next()`

I got the following error 
`RuntimeError: dataset.__iter__() is only supported when eager execution is enabled.`

While following the tutorial I did run `tf.enable_eager_execution()`, which didn't return any unusual output, but `tf.executing_eagerly()` still returns False. 

TF Version 1.13.1 not sure, what was wrong. Works on a different VM.",1,2019-03-05 07:33:07,2019-03-05 09:17:28,2019-03-05 09:15:43
https://github.com/tensorflow/federated/issues/103,[],Error in pip installation in venv: No matching distribution found,"Error in pip installation in venv: No matching distribution foundWhen following the [install from package](https://github.com/tensorflow/federated/blob/master/docs/install.md#install-from-package) instructions, I receive an error when attempting `pip` installation: `Could not find a version that satisfies the requirement tensorflow_federated (from versions: )
No matching distribution found for tensorflow_federated`

## Steps to reproduce:

```
$ /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""
$ export PATH=""/usr/local/bin:/usr/local/sbin:$PATH""
$ brew update
$ brew install python  # Python 3
$ sudo pip3 install --upgrade virtualenv  # system-wide install
$ virtualenv --python python3 ""venv""
$ source ""venv/bin/activate""
(venv) $ pip install --upgrade pip
(venv) $ pip install tensorflow_federated
Collecting tensorflow_federated
*Could not find a version that satisfies the requirement tensorflow_federated (from versions: )
No matching distribution found for tensorflow_federated*
```

## System info:
`pip 10.0.1`
`Homebrew 2.0.1`
`Python 3.6.7`
`macOS High Sierra 10.13.6`
@jpgard Thanks for the feedback. The pip package has not been published yet, so you will need to follow the instructions for installing from source. Sorry I forgot to mention that.Makes sense, I should have thought about that when posting.@michaelreneer I assume the same applies to the tensorflow_federated docker container?@jpgard yeah I am changing that now :).Has the docker issue been solved@drkingman We have not published a Docker image yet. See these instructions for how [build a TFF docker image](https://github.com/tensorflow/federated/blob/master/docs/install.md#3-build-a-docker-image).",6,2019-02-07 19:31:35,2019-03-07 04:20:09,2019-02-07 19:47:09