url	label	title	all_text	comments	created_time	updated_time	closed_time
https://github.com/tensorflow/federated/issues/3035	['bug']	AttributeError: 'LearningAlgorithmState' object has no attribute 'model'	"AttributeError: 'LearningAlgorithmState' object has no attribute 'model'Hello, 

I am using tensorflow_federated version 0.28.

When I do `state.model.assign_weights_to(eval_model)` I get the following error:

`AttributeError: 'LearningAlgorithmState' object has no attribute 'model'`

Any help would be appreciated. 

**Additional context**
Add any other context about the problem here.
Hi @shifdz. Can you post the code snippet you used to get this?

It looks like you're using something like
```
fed_avg = tff.learning.algorithms.build_weighted_fed_avg(...)`
```
If so, the state of `fed_avg` (eg. as returned by `fed_avg.initialize()`) does not have a `.model` attribute in its state. You can get the model weights by instead calling `fed_avg.get_model_weights(state)`.Hi @zcharles8 ,

Thank you. That worked."	2	2022-07-05 19:32:24	2022-07-08 19:59:36	2022-07-08 19:59:36
https://github.com/tensorflow/federated/issues/2888	['bug']	tf.print not outputting anything in federated computation in multi-machine setup	"tf.print not outputting anything in federated computation in multi-machine setupHi, is there a way to output to std.out or a file from a client directly when running in a multi-machine setup?

For example:
```
@computations.federated_computation(FederatedType(tf.string, placements.CLIENTS))
def fed_print(x):
  tf.print(x)

federated_map(fed_print, client_messages)
```

Doesn't printout anything on the head or worker nodes.Doing something like this *is possible*, but the `tf.print` op must be captured in a TF computation. That is, something like the following should work:

```python
@tff.tf_computation(tf.string)
@tf.function
def print_and_return(x):
  # we get program order semantics here due to the tf.function,
  # which is why we use that as a second-decorator. Otherwise
  # we'd need to control-dep.
  tf.print(x)
  return x

@tff.federated_computation(tff.type_at_clients(tf.string))
def map_and_print(x):
  return tff.federated_map(print_and_return, x)
```

I'd share a colab, but I think they're all broken at the moment due to version mismatch (unless Colab has updated to Python 3.9). In general, TFF does not support the 'mixing' of TensorFlow and federated code; all TensorFlow code should be captured in TFF's `tf_computation` decorator. This was an early design choice in TFF, in order to help separate concerns--`federated_computation` decorators should usually contain logic which 'moves' tensors around--broadcasting, aggregating, etc.--or transforms them 'in groups'--mapping functions across the client or the server, for instance. Local logic should usually be expressed in one of TFF's local computation decorators, like `tf_computation` or the recently introduced `jax_computation`."	1	2022-05-25 16:01:50	2022-07-16 18:47:03	2022-07-16 18:47:02
https://github.com/tensorflow/federated/issues/2768	['bug']	Accuracy getting stucked at around 0.1 in some models	"Accuracy getting stucked at around 0.1 in some modelsI'm trying train a federated model for the mnist dataset. I am using the code avaible at https://www.tensorflow.org/federated/tutorials/simulations for the setup. 
The dataset version being used is the the one from keras (not the federated version from leaf that is used in tff). I'm making a partition of it, saving it on a dictionary and implementing my ClientData instance with `tff.simulation.datasets.TestClientData`.  
Applying this change works just fine. However, if I change the model from the simulation, every round gives me a ~0.1 accuracy.  

The model in the tutorial is as simple as it can get, an input layer of 28*28=784 neurons stacked over an output layer of dim 10 with Softmax activation: 

    model = tf.keras.models.Sequential([
      tf.keras.layers.InputLayer(input_shape=(784,)), 
      tf.keras.layers.Dense(units=10, kernel_initializer='zeros'),
      tf.keras.layers.Softmax(),
    ])

And the new model is a cnn:

```
 model = tf.keras.Sequential(
        [
            tf.keras.layers.Conv2D(
                16,
                8,
                strides=2,
                padding=""same"",
                activation=""relu"",
                input_shape=(28, 28, 1),
            ),
            tf.keras.layers.MaxPool2D(2, 1),
            tf.keras.layers.Conv2D(
                32, 4, strides=2, padding=""valid"", activation=""relu""
            ),
            tf.keras.layers.MaxPool2D(2, 1),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(32, activation=""relu""),
            tf.keras.layers.Dense(10),
        ]
    )
```

Accuracy changed from round to round on the first case, increasing, reaching 0.94 quite fast. 
On the second case I ran it for about 240 rounds with 3 fixed clients, 20k elements each, 10 epochs, batch size 32. Still couldn't get out of the ~0.1 accuracy and loss of ~2.3 

The model works fine for this dataset. I already tested it on a centrilized version and a federated version using Flower framework reaching 0.99 accuracy. But for some reason I can't make it work on tff.

Environment:
MacOs BigSur
tensorflow==2.8.0
tensorflow-federated==0.22.0

I expect the metrics and loss to change more. Could it be that there is a problem with using other Models?

Full code:

```
from tensorflow.keras.datasets import cifar10, mnist
import numpy as np

EPOCHS = 10
BATCH_SIZE = 32

# ROUND_CLIENTS <= NUM_CLIENTS
ROUND_CLIENTS = 3
NUM_CLIENTS = 3

NUM_ROUNDS = 400

    
def make_client(num_clients,X, y):
    total_image_count = len(X)
    image_per_set = int(np.floor(total_image_count/num_clients))

    client_train_dataset = collections.OrderedDict()
    for i in range(1, num_clients+1):
        client_name = i-1
        start = image_per_set * (i-1)
        end = image_per_set * i

        print(f""Adding data from {start} to {end} for client : {client_name}"")
        data = collections.OrderedDict((('label', y[start:end]), ('pixels', X[start:end])))
        client_train_dataset[client_name] = data
    
    train_dataset = tff.simulation.datasets.TestClientData(client_train_dataset)
    
    return train_dataset


def preprocess(X: np.ndarray, y: np.ndarray):
    """"""Basic preprocessing for MNIST dataset.""""""
    X = np.array(X, dtype=np.float32) / 255
    X = X.reshape((X.shape[0], 28, 28, 1))

    y = np.array(y, dtype=np.int32)
    y = tf.keras.utils.to_categorical(y, num_classes=10)

    return X, y


(X_train, y_train), (X_test, y_test) = mnist.load_data()
(X_train, y_train) = preprocess(X_train, y_train)
(X_test, y_test) = preprocess(X_test, y_test)

mnistFedTrain = make_client(NUM_CLIENTS,X_train,y_train)

def map_fn(example):
    return collections.OrderedDict(
      x=example['pixels'], 
        y=example['label'])


def client_data(client_id):
    ds = mnistFedTrain.create_tf_dataset_for_client(mnistFedTrain.client_ids[client_id])
    return ds.repeat(EPOCHS).shuffle(500).batch(BATCH_SIZE).map(map_fn)


train_data = [client_data(n) for n in range(ROUND_CLIENTS)]
element_spec = train_data[0].element_spec

def create_cnn_model() -> tf.keras.Model:
    """"""Returns a sequential keras CNN Model.""""""
    return tf.keras.Sequential(
        [
            tf.keras.layers.Conv2D(
                16,
                8,
                strides=2,
                padding=""same"",
                activation=""relu"",
                input_shape=(28, 28, 1),
            ),
            tf.keras.layers.MaxPool2D(2, 1),
            tf.keras.layers.Conv2D(
                32, 4, strides=2, padding=""valid"", activation=""relu""
            ),
            tf.keras.layers.MaxPool2D(2, 1),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(32, activation=""relu""),
            tf.keras.layers.Dense(10),
        ]
    )

def model_fn():
    model = create_cnn_model()
    return tff.learning.from_keras_model(
      model,
      input_spec=element_spec,
      loss=tf.keras.losses.CategoricalCrossentropy(
                from_logits=True, reduction=tf.losses.Reduction.NONE
            ),
      metrics=[tf.keras.metrics.CategoricalAccuracy()]
    )


trainer = tff.learning.build_federated_averaging_process(
    model_fn, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.02))


def evaluate(num_rounds=NUM_ROUNDS):
    state = trainer.initialize()
    for i in range(num_rounds):
        t1 = time.time()
        state, metrics = trainer.next(state, train_data)
        t2 = time.time()
        print('\n Round {r}: metrics {m}, round time {t:.2f} seconds'.format(
            m=metrics['train'], r=i, t=t2 - t1))

t1 = time.time()
evaluate(NUM_ROUNDS)
t2 = time.time()

print('Seconds:',t2 - t1,' = Minutes:', (t2 - t1)/60)
```


I've had a similar problem with other models as well, e.g.  MobileNetV2 implemented in tf for cifar10:
`model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)Were you able to find the solution?I tried to test the same models in a centralized setup and I had similar results. The problem was that the loss, output type and accuracy have to match. What i mean is:

When using one-hot encoding for the output the loss has to be of the type SparseCategorical, the accuracy as well.
So I changed 
```
loss=tf.keras.losses.CategoricalCrossentropy(
          from_logits=True, reduction=tf.losses.Reduction.NONE
      ),
metrics=[tf.keras.metrics.CategoricalAccuracy()]
)
```

into 

```
  loss=tf.keras.losses.SparseCategoricalCrossentropy(),
  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
```

also dropping the parameters in the loss, `from_logits=True` makes the loss believe you are NOT gonna be outputing probabilities, which is what my models where doing.

Also careful with `kernel_initializer='zeros'`. Its ok in the output layer  but using it on a hidden layer makes the gradient vanish, see [this](https://datascience.stackexchange.com/questions/37378/what-are-kernel-initializers-and-what-is-their-significance), which was another reason why my models where learning bananas. 

The problem wasn't in the TFF side of things. If there is anything to improve on their side is to just make it simple with the models in the tutorials. Like leave as much default parameters in the keras models as you can."	2	2022-04-29 03:02:22	2022-06-03 03:47:55	2022-06-03 03:47:55
https://github.com/tensorflow/federated/issues/2685	['bug']	Multiple output and multiple losses with tensorflow_federated	"Multiple output and multiple losses with tensorflow_federatedI have the following model architecture:

    def create_keras_model():
        input = keras.Input(shape=(24, ), name=""original_inpt"")
        x = tf.keras.layers.Dense(512,activation='relu')(input)
        x = tf.keras.layers.Dense(256,activation='relu')(x)
        x = tf.keras.layers.Dense(64,activation='relu')(x)

        out1 = keras.layers.Dense(1,name = 'loss1')(x)
        out2 = keras.layers.Dense(1,name = 'loss2')(x)


        encoder = keras.Model(inputs = input, outputs = [out1, out2], name=""encoder"")
        return encoder

How do I pass the model_fn function:

    def model_fn():
        keras_model = create_keras_model()
        return tff.learning.from_keras_model(
          keras_model,
          input_spec=preprocessed_sample_dataset.element_spec,
           loss = [tf.keras.losses.MeanSquaredError(),tf.keras.losses.AbsoluteError()],,
          metrics = {
               ""loss1"": [tf.keras.metrics.MeanSquaredError()],
               ""loss2"": [tf.keras.metrics.MeanSquaredError()]
           })

It gives the following error:
InvalidArgumentError: Graph execution error:

Incompatible shapes: [500,1] vs. [2,500]
	 [[{{node SquaredDifference}}]]
	 [[StatefulPartitionedCall_1/StatefulPartitionedCall/ReduceDataset]] [Op:__inference_pruned_97396]

Does `keras.Input(shape=(24, ), name=""original_inpt"")` match the shape of your input dataset? Also, metrics should be a list, not a dictionary.@wennanzhu Thank you for your response. Yes. Also, I was able to fix the error by changing code to this:

  def create_keras_model():

      input = keras.Input(shape=(24, 1), name=""original_inpt"")
      x = tf.keras.layers.Conv1D(filters=4,kernel_size=4,padding = 'same',input_shape=(24,1))(input)
      x = tf.keras.layers.MaxPool1D(pool_size=2)(x)
      x = tf.keras.layers.Conv1D(filters=8,kernel_size=4,padding = 'same')(x)
      x = tf.keras.layers.MaxPool1D(pool_size=2)(x)
      x = tf.keras.layers.LSTM(16, return_sequences=False)(x)
      
      
      x1_2 = tf.keras.layers.Dense(4,activation='relu')(x)
      x2_2 = tf.keras.layers.Dense(4,activation='relu')(x)

      out1 = keras.layers.Dense(1,name = 'hypo_loss')(x1_2)
      out2 = keras.layers.Dense(1,name = 'hyper_loss')(x2_2)

       encoder = keras.Model(inputs = input, outputs = [out1, out2], name=""encoder"")
       return encoder

  def model_fn():

      keras_model = create_keras_model()
      return tff.learning.from_keras_model(
        keras_model,
        input_spec=preprocessed_sample_dataset.element_spec,
        loss = [custom_loss_hyper(),custom_loss_hypo()],
        metrics = [tf.keras.metrics.MeanSquaredError(),tf.keras.metrics.MeanSquaredError()])


But my model is performing terribly compared to a central model. The training-loss in a central model reduces to about 600-700. In the FL setup, it reduces until 6000 but starts to increase after that.
I checked all my local models individually to see if there are outlier issues but they all seem to work fine. In addition, I got access to a manual code that averages weights across local models and seems to work as expected giving loss in the range near to the central model. Not sure, why it is not performing the same with 'tensorflow_federated'. Can you please help@darpitdavetamu It's unfortunately difficult to debug this without the full code. That being said, loss diverging is often symptomatic of a learning rate (either client or server) being too large. I'd recommend trying a couple. Note that the learning rate needed may be slightly different than the ""manual code"" you refer to depending on how certain normalizations occur when taking averages.@zcharles8 Thank you for your reply. That's an interesting point. I did try to play with a few but not that rigorously. Give me a day or two to rigorously go through multiple combinations and check if there is change in model behavior. I will get back here for the same. 

In the manual code and tensorflow_federated:
Client optimizer: Adam and lr = 0.001
Server optimizer: SGD and lr = 1.0

However, I suspect that in the manual codes weights (\theta) are directly being averaged as opposed to differences between \theta_t and \theta_(t-1) as in 'tensorflow_federated'. Any suggestions on the changes I should focus on to match the performance?

Also, here is the link to the code I am using to implement federated learning: https://github.com/darpitdavetamu/Federated-Learning/blob/main/FL_MultiOutput(Manual_Code).ipynb"	4	2022-03-28 23:26:33	2022-04-29 17:38:04	2022-04-29 17:38:04
https://github.com/tensorflow/federated/issues/2659	['bug']	dataset_computation not implemented error	"dataset_computation not implemented errorWhile trying to implement the python program working_with_client_data.ipynb provided in githhub, I am getting the below error as he function is not implemented. Can anyone help me when this will be available for public use? I was trying to check the program if it helps in the model training on the client device data.

def dataset_computation(self):
    raise NotImplementedError(""b/162106885"")




Looking in our code, I think this implementation should be finished (that bug is [no longer referenced in TFF](https://github.com/tensorflow/federated/search?q=b%2F162106885)). This jibes with my recollection as well.

It might be that you're running an old version of TFF? Can you check you're on the 0.20.0 release, as I *believe* this should contain the appropriate implementations?Apologies!! Updated the version to 0.20.0 and it works fine!!"	2	2022-03-18 14:49:09	2022-04-04 15:13:34	2022-04-04 15:13:33
https://github.com/tensorflow/federated/issues/2561	['bug']	Docs site: Tutorials link straight to ipynb files instead of colab	"Docs site: Tutorials link straight to ipynb files instead of colab**Describe the bug**
Links from [this tutorials page](https://www.tensorflow.org/federated/tutorials/tutorials_overview) are just pointing to downloadable iPython notebooks. I would've expected them to open in colab, since the tutorial page specifically calls the tutorials ""colab-based"".

**Environment (please complete the following information):**
Brave Browser (don't think this matters though)

**Expected behavior**
Tutorials should either open up in colab or link to the GitHub-hosted version of the rendered ipynb. Alternatively, the tutorials page should just say that the tutorials are ipynb instead of ""colab-based""@jvmncs Thanks for reporting this. I saw that this was indeed happening yesterday, but can't seem to reproduce it today. Can you confirm whether or not this is still an issue for you? A nightly TF doc change might have fixed it.Yes, looks like the tutorials are redirecting to the tensorflow.org docs site on my end again. Thanks for prompt response 👍 "	2	2022-02-22 16:31:58	2022-02-24 22:23:09	2022-02-24 22:23:09
https://github.com/tensorflow/federated/issues/2211	[]	What is the role of 'clipped_count_stddev' in differentially private gaussian_adaptive aggregator?	"What is the role of 'clipped_count_stddev' in differentially private gaussian_adaptive aggregator?I was going through the documentation of [gaussian_adaptive aggregator](https://www.tensorflow.org/federated/api_docs/python/tff/aggregators/DifferentiallyPrivateFactory?version=nightly#gaussian_adaptive), and one of the function arguments is `clipped_count_stddev`. Description says that it is 
```
The stddev of the noise added to the clipped counts in the adaptive clipping algorithm
```
But I thought the standard deviation of Gaussian noise added is calculated as **'clipping_threshold * noise_multiplier'**, where clipping threshold is found adaptively and the noise multiplier is fixed. 

What am I missing? Thank you!`clipped_count_stddev` is the standard deviation added to the count of clipped updates for private estimation of the quantile for adaptive clipping. It corresponds to \sigma_b in the paper https://arxiv.org/pdf/1905.03871.pdf. If you do not set this argument in `gaussian_adaptive`, it will default to the recommended settings from the paper as implemented in `adaptive_clip_noise_params`. This should work very well for almost all users.Thank you @galenmandrew! So if I understand it right, `sigma_b` is the standard deviation of noise used in estimating the _clipping threshold_ and noise multiplier `z_delta` is used to add the noise to the updates themselves. 

I have another question then. If `clipped_count_stddev` is by default = _**0.05 * clients_per_round**_, then won't this noise be higher for more clients_per_round? Unless it is later normalized with respect to number of clients. 

I actually wanted to see the effect of number of clients participating in a round on training.Your first point is almost correct. The noise multiplier that you specify is the *effective noise multiplier* for DP accounting (z in the paper). Due to the need to account for privacy when estimating the clipping threshold, the actual amount of noise added to the updates (z_\Delta in the paper) will be very slightly (even negligibly) higher.

On the second point: yes the noise added to the sum of clipped counts will be higher for more clients per round, but note that it will be normalized by the `clients_per_round` so the noise on the average remains the same: 0.05. We find that 0.05 gives a reasonable amount of error in the estimation process while using a negligible fraction of the privacy budget.Thank you for clarifying! I have understood it now."	4	2021-12-02 16:52:45	2021-12-04 02:08:03	2021-12-04 02:08:03
https://github.com/tensorflow/federated/issues/2190	[]	Using CIFAR-100 datased with VGG19 model in simple_fedavg example	"Using CIFAR-100 datased with VGG19 model in simple_fedavg exampleI'm actually trying to change the dataset and the model, but i can't get any positive feedback, the accuracy is always at 1%.

```
# Copyright 2020, The TensorFlow Federated Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Simple FedAvg to train EMNIST.

This is intended to be a minimal stand-alone experiment script built on top of
core TFF.
""""""

import collections
import functools
from absl import app
from absl import flags
import numpy as np
import tensorflow as tf
import tensorflow_federated as tff
import random

import simple_fedavg_tf
import simple_fedavg_tff
import os

np.set_printoptions(precision=None, suppress=None)

# Training hyperparameters
flags.DEFINE_integer('total_rounds', 50, 'Number of total training rounds.')
flags.DEFINE_integer('rounds_per_eval', 1, 'How often to evaluate')
flags.DEFINE_integer('train_clients_per_round', 1,
                     'How many clients to sample per round.')
flags.DEFINE_integer('client_epochs_per_round', 1,
                     'Number of epochs in the client to take per round.')
flags.DEFINE_integer('batch_size', 16, 'Batch size used on the client.')
flags.DEFINE_integer('test_batch_size', 128, 'Minibatch size of test data.')

# Optimizer configuration (this defines one or more flags per optimizer).
flags.DEFINE_float('server_learning_rate', 0.0005, 'Server learning rate.')
flags.DEFINE_float('client_learning_rate', 0.0005, 'Client learning rate.')

FLAGS = flags.FLAGS


def create_vgg19_model():
    model = tf.keras.applications.VGG19(include_top=True,
                                        weights=None,
                                        input_shape=(32, 32, 3),
                                        classes=100)
    return model


def get_cifar100_dataset():
    cifar100_train, cifar100_test = tff.simulation.datasets.cifar100.load_data()

    def element_fn(element):
        return collections.OrderedDict(
            x=tf.expand_dims(element['image'], -1), y=element['label'])

    def preprocess_train_dataset(dataset):
        # Use buffer_size same as the maximum client dataset size,
        # 418 for Federated EMNIST
        return dataset.map(element_fn).shuffle(buffer_size=418).repeat(
            count=FLAGS.client_epochs_per_round)  # .batch(
        # FLAGS.batch_size, drop_remainder=False)

    def preprocess_test_dataset(dataset):
        return dataset.map(element_fn).batch(
            FLAGS.test_batch_size, drop_remainder=False)

    cifar100_train = cifar100_train.preprocess(preprocess_train_dataset)
    cifar100_test = preprocess_test_dataset(
        cifar100_test.create_tf_dataset_from_all_clients())
    return cifar100_train, cifar100_test

def server_optimizer_fn():
    return tf.keras.optimizers.SGD(learning_rate=FLAGS.server_learning_rate)

def client_optimizer_fn():
    return tf.keras.optimizers.Adam(learning_rate=FLAGS.client_learning_rate)


def main(argv):
    if len(argv) > 1:
        raise app.UsageError('Too many command-line arguments.')
    client_devices = tf.config.list_logical_devices('GPU')
    print(client_devices)
    server_device = tf.config.list_logical_devices('CPU')[0]
    tff.backends.native.set_local_execution_context(
        server_tf_device=server_device, client_tf_devices=client_devices)

    train_data, test_data = get_cifar100_dataset()

    def tff_model_fn():
        """"""Constructs a fully initialized model for use in federated averaging.""""""
        # keras_model = create_original_fedavg_cnn_model(only_digits=False)
        keras_model = create_vgg19_model()
        # keras_model.summary()
        loss = tf.keras.losses.SparseCategoricalCrossentropy()
        return simple_fedavg_tf.KerasModelWrapper(keras_model,
                                                  test_data.element_spec, loss)

    iterative_process = simple_fedavg_tff.build_federated_averaging_process(
        tff_model_fn, tff_model_fn, tff_model_fn, tff_model_fn, server_optimizer_fn, client_optimizer_fn)
    server_state = iterative_process.initialize()

    metric = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')
    model = tff_model_fn()

    for round_num in range(FLAGS.total_rounds):
        sampled_clients = np.random.choice( train_data.client_ids,  size=FLAGS.train_clients_per_round,   replace=False)
        sampled_train_data = [ train_data.create_tf_dataset_for_client(client).batch( FLAGS.batch_size, drop_remainder=False)
            for client in sampled_clients
        ]

        server_state, train_metrics = iterative_process.next(
            server_state, sampled_train_data)

        print(f'Round {round_num} training loss: {train_metrics}')
        if round_num % FLAGS.rounds_per_eval == 0:
            model.from_weights(server_state.model_weights)
            accuracy = simple_fedavg_tf.keras_evaluate(model.keras_model, test_data,
                                                       metric)
            print(f'Round {round_num} validation accuracy: {accuracy * 100.0}')
if __name__ == '__main__':
    app.run(main)

```

This is the code, i just changed the model part and the dataset from the simple_fedavg example. Any idea? I tried with different optimizers, but still no luck.Hi @fanto88. Just to clarify, the code above runs without error, you just aren't getting good results on it?

This is generally out of scope for the issues tab here, I'd encourage you to instead post on Stack Overflow. That being said, one thing I'd encourage you to do is to first train a centralized model on the dataset to make sure that you can select a good optimizer, and that the model works.Closing this issue due to a lack of activity."	2	2021-11-29 19:57:15	2022-02-23 23:13:04	2022-02-23 23:13:03
https://github.com/tensorflow/federated/issues/1958	['bug']	TFF simulations on GCP	"TFF simulations on GCPHello, Community:

I am trying to replicate the GCP setup steps.

https://www.tensorflow.org/federated/gcp_setup

But it returns an error that I don't understand:

`I1006 10:02:36.246272 139901428516608 remote_executor.py:84] Received retryable gRPC error: <_InactiveRpcError of RPC that terminated with:
        status = StatusCode.UNKNOWN
        details = ""Exception calling application: The executor service has not yet been configured with cardinalities and cannot execute any concrete requests.""
        debug_error_string = ""{""created"":""@1633514556.245906275"",""description"":""Error received from peer ipv4:XX.XXX.XX.X:8000"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1062,""grpc_message"":""Exception calling application: The executor service has not yet been configured with cardinalities and cannot execute any concrete requests."",""grpc_status"":2}""`

From client docker, I can connect from docker's client to the server instance without any problem.

`root@322de86816b2:/simulation# telnet XX.XXX.XX.X 8000
Trying XX.XXX.XX.X...
Connected to XX.XXX.XX.X.
Escape character is '^]'.`

I have seen that by default Docker's server deploys the container using IPv6:
![image](https://user-images.githubusercontent.com/54352971/136185868-aa9f148a-d69a-41d7-a304-2ecb76c13346.png)


Can anybody help me, please?
Thanks in advance.
See you!

**Environment**:
- Use Tensorflow Federated master version.
- Client and Server are two Compute Engine ""e2-micro"" into two different VPC.
- Allow HTTP traffic and Allow HTTPS traffic in Firewall.
- In both VPC networks, allow firewall rules for port 8000
![image](https://user-images.githubusercontent.com/54352971/136185464-b372a067-176b-464a-8b64-58dc6fc0f66d.png)


Glancing at the example, it looks like it was written long ago and hasn't been updated to reflect changes in the TFF runtime underneath.

In particular, the TFF runtime now attempts to infer the number of clients that need to be used to execute a given computation. Previously this number was hardcoded or flag-parameterized in various places; in this example I believe it was set by a flag at the worker.

However, this value cannot be inferred from the arguments to all computations--in particular, computations which do not accept CLIENTS-placed data. This computation is one such example. For this purpose, the higher-level executor stack constructors TFF exposes usually accept a `default_num_clients` arguments, which will be used in the case that this value cannot be inferred.

Opening a PR now to use our higher-level libraries and thus use this codepath. However, looking at the example indicates that this may *not* be such a great test of the GCP TFF runtime. In particular, since no logic is running at the `CLIENTS` placement, the TFF runtime reserves the right to (and in this case, in fact will) run all this logic on your local (client) machine. (Yes, the overloading of clients/server is painful in this context).

If you want to to more 'fully' exercise this setup, executing something like this should do it:
```python
@tff.federated_computation(tff.type_at_clients(tf.int32))
def sum_client_values(clients_int):
  return tff.federated_sum(clients_int)

assert sum_client_values(list(range(100)) == sum(range(100))
```
If this is a change you'd be interested in making to the `test.py` binary, I think we'd be happy to accept it.


@dvdgnzlz-maths with the commit above, this should be fixed. Can you verify?Hello @jkr26, 

Thanks for your help! I'm going to check and I will back with any comment. 
Best,Hello again, 

it works! Great!

> root@ae7b51a83b90:/simulation# python3 test.py --host 10.132.0.4
2021-10-18 13:40:08.799928: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
I1018 13:40:12.203198 139735513319168 executor_stacks.py:943] 1 TFF workers available out of a total of 1.
2021-10-18 13:40:12.214330: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2021-10-18 13:40:12.214455: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
b'Hello World'

Thanks @jkr26"	4	2021-10-06 10:27:07	2021-10-18 13:45:17	2021-10-18 13:45:17
https://github.com/tensorflow/federated/issues/1811	['bug']	Minimal Stand-Alone Implementation of Federated Averaging doesnt work after the last commit	"Minimal Stand-Alone Implementation of Federated Averaging doesnt work after the last commitwith the last commit, ID'd 44d012f690005ecf9217e3be970a4f8a356e88ed, KerasModelWrapper is removed from [simple fedavg
](https://github.com/tensorflow/federated/tree/main/tensorflow_federated/python/examples/simple_fedavg) yet some parts arent updated properly. these are 

1) tff_model_fn still returns simple_fedavg_tf.KerasModelWrapper which no longer exists. should have returned tff.learning.from_keras_model according to my understanding of commit notes.

2) while loading server_weights for validation we use model.from_weights(server_state.model_weights) which no longer exists. I have used tf.nest.map_structure(lambda v,t: v.assign(t), model.weights, server_state.model_weights) instead.

3) for validation we use simple_fedavg_tf.keras_evaluate which exists but needs to be updated since it uses preds = model(batch['x'], training=False) and its input is model.keras_model we got errors. I guess this can be handled in two ways:

3.a) instead of creating model = tff_model_fn() we can create keras_model. use       
tf.nest.map_structure(lambda v,t: v.assign(t), model.trainable_weights, server_state.model_weights.trainable)
tf.nest.map_structure(lambda v,t: v.assign(t), model.non_trainable_weights, server_state.model_weights.non_trainable)
to load server weights and get accuracy with
accuracy = simple_fedavg_tf.keras_evaluate(model, test_data, metric)
when done like that we dont need to update keras_evaluate

3.b) keep using model = tff_model_fn() and use this model as the input of keras_evaluate. but this means we cant use 
preds = model(batch['x], training=False) in keras evaluate. I have used 
preds = model.forward_pass(batch, training=False).predictions
here things get a bit tricky since metric.update_state(y_true=batch['y'], y_pred=preds) returns 
AttributeError: 'NoneType' object has no attribute '_ unconditional update' for some reason. so if the metric of keras_evaluate and the metric we defined in the tff.learning.from_keras_model are same then I guess we can multiply the loss returned by forward_pass for each batch with the batch size and calculate loss_sum and total_size for all the batches and then divide it at the endI have used tff 0.17 with tf 2.3 on windows. the only code changes I have made except the ones listed above are had to change the return ServerUpdate in tf file to

  return ServerState(
      model_weights=model_weights,
      optimizer_state=server_optimizer.variables(),
      round_num=server_state.round_num + 1)

since the original uses tff.structure.update_struct which is not available in tff 0.17. and i also commented out 

client_devices = tf.config.list_logical_devices('GPU')
server_device = tf.config.list_logical_devices('CPU')[0]
tff.backends.native.set_local_python_execution_context(
server_tf_device=server_device, client_tf_devices=client_devices)It looks like the problem is solved with commit 8b769d83a57602507f622c4b973f07271f1d4ba6 so I am closing the issue."	2	2021-09-13 13:41:28	2021-09-14 18:38:09	2021-09-14 18:38:09
https://github.com/tensorflow/federated/issues/1784	[]	How to deal with disconnection issues.	"How to deal with disconnection issues.How does TFF deal with disconnection issues. For example, during a certain aggregation process, most of the users suddenly dropped.  Thanks a lot.@veryhannibal Can you post more detail about this? In particular, what code you are attempting to run, what output (or error) you encountered. We have an issues template that you can follow that would be helpful for debugging this.@zcharles8    Thanks for your feedback. I just began to learn TFF.  My question is about how TFF can deal with straggle. For instance, I have one server to run FedAvg and several mobile devices to train model locally, but if some of the mobile devices dropped, will this task be failed? This sounds like a question about 1) the algorithm being employed and 2) the TFF executor being used. I don't think there's a one-size-fits-all answer. 

If you'd like to start a more general conversation about straggler tolerance in the context of federated learning and TFF, I would encourage you to a question on stack overflow using the `tensorflow-federated` tag ([link](https://stackoverflow.com/questions/tagged/tensorflow-federated)). However, given the more open-ended nature of this question I am closing this Github issue (which is primarily reserved for things such as bugs and feature requests)."	3	2021-09-07 07:29:02	2021-09-08 17:18:17	2021-09-08 17:18:17
https://github.com/tensorflow/federated/issues/1739	['bug']	TensorFlow Federated (TFF) TypeError in tff.templates.IterativeProcess.next() when clients_per_round exceed 99	"TensorFlow Federated (TFF) TypeError in tff.templates.IterativeProcess.next() when clients_per_round exceed 99**Describe the bug**
I implemented a custom federated learning GAN training loop with TFF similar to [this code by Google Research](https://github.com/google-research/federated/blob/master/gans/tff_gans.py).

The client data for a particular training round is found using the following code snippet:

``` python
def client_dataset_fn():
    # Sample clients and data
    sampled_clients = np.random.choice(train_data.client_ids, size=cfg.clients_per_round, replace=False)
    datasets = [(next(client_gen_inputs_iterator),
                 train_data.create_tf_dataset_for_client(client_id).take(cfg.n_critic))
                for client_id in sampled_clients]
    return datasets

client_noise_inputs, client_real_data = zip(*client_dataset_fn())
```

This works perfectly up until `cfg.clients_per_round` is set to 99. When it is set to 100 or a larger value (with the total number of clients being larger of course), I receive the following error:

```
Traceback (most recent call last):
  File ""main.py"", line 109, in main
    metrics = run_single_trial(train_data, test_data, cfg)
  File ""/mnt/workspace/tff/GAN/federated/fedgan_main.py"", line 73, in run_single_trial
    metrics = train_loop(iterative_process, server_dataset_fn, client_dataset_fn, model, eval_hook_fn, cfg)
  File ""/mnt/workspace/tff/GAN/federated/fedgan_main.py"", line 124, in train_loop
    client_real_data)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/computation/function_utils.py"", line 525, in __call__
    return context.invoke(self, arg)
  File ""/usr/local/lib/python3.6/dist-packages/retrying.py"", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
  File ""/usr/local/lib/python3.6/dist-packages/retrying.py"", line 206, in call
    return attempt.get(self._wrap_exception)
  File ""/usr/local/lib/python3.6/dist-packages/retrying.py"", line 247, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 703, in reraise
    raise value
  File ""/usr/local/lib/python3.6/dist-packages/retrying.py"", line 200, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 226, in invoke
    _ingest(executor, unwrapped_arg, arg.type_signature)))
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete
    return future.result()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/tracing.py"", line 396, in _wrapped
    return await coro
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 111, in _ingest
    ingested = await asyncio.gather(*ingested)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 116, in _ingest
    return await executor.create_value(val, type_spec)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 294, in create_value
    value, type_spec))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 111, in create_value
    self._target_executor.create_value(value, type_spec))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 105, in _delegate
    result_value = await _delegate_with_trace_ctx(coro, self._event_loop)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/tracing.py"", line 396, in _wrapped
    return await coro
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/federating_executor.py"", line 394, in create_value
    return await self._strategy.compute_federated_value(value, type_spec)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/federated_composing_strategy.py"", line 279, in compute_federated_value
    py_typecheck.check_type(value, list)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/py_typecheck.py"", line 41, in check_type
    type_string(type_spec), type_string(type(target))))
TypeError: Expected list, found tuple.
```

During debugging, I looked at the `target` variable in the final line of the traceback and found it to be the abovementioned `client_real_data` and `client_noise_inputs`. Their types are in fact tuples, not lists, however, this does not change with different numbers of cfg.clients_per_round. The only usage of `cfg.clients_per_round` is shown above in the random choice. I really cannot explain why this is happening, maybe somebody out there has experienced something similar and can help me out.

As a workaround I now manually change the data type of `client_noise_inputs` and `client_real_data` using `list(tuple_var)`, but I am still curious as to why the list is required somehow.

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GNU/Linux 5.4.0-74-generic x86_64
* Python package versions (e.g., TensorFlow Federated, TensorFlow): TFF 0.19.0, TF 2.5.1
* Python version: 3.6.9
* CUDA/cuDNN version: CUDA 11.0, cuDNN 8
This seems to me to be an implementation distinction between the [`federated_composing_strategy`](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/impl/executors/federated_composing_strategy.py) and the [`federated_resolving_strategy`](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/impl/executors/federated_resolving_strategy.py). IIRC, by default we don't inject a composing executor into your stack until you hit 100 clients--which would be the source of this exciting mystery.

In particular, the composing strategy is programmed against the assumption that the incoming clients-placed value is represented [as a list](https://github.com/tensorflow/federated/blob/153f90562bdf0d9c7ee9240c9c1b1b1d8469737a/tensorflow_federated/python/core/impl/executors/federated_composing_strategy.py#L279), whereas the resolving strategy codes against a [much more flexible set](https://github.com/tensorflow/federated/blob/153f90562bdf0d9c7ee9240c9c1b1b1d8469737a/tensorflow_federated/python/core/impl/executors/federated_resolving_strategy.py#L255) of containers.

It's not wild to coerce your clients-placed value to a list--we also could extend the permitted representation of clients-placed values in the composing executor to match that in the resolving one, possibly pulling the appropriate logic to a shared place [like here](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/impl/executors/executor_utils.py). I think its a contribution wed be very happy to accept if youre up for it!Thanks a lot for your quick reply and for clearing up this issue. 

I am happy to put the contribution on my to-do list and will hopefully get to it soon."	2	2021-08-23 10:46:06	2021-08-25 08:50:10	2021-08-25 08:50:09
https://github.com/tensorflow/federated/issues/1570	['bug']	"Got ""RuntimeError: Event loop is closed"" when change the optimizer from SGD to DPGradientDescentGaussianOptimizer in emnist_fedavg tensorflow federated"	"Got ""RuntimeError: Event loop is closed"" when change the optimizer from SGD to DPGradientDescentGaussianOptimizer in emnist_fedavg tensorflow federatedI want to train the client model with dp-sgd, so I changed the client optimizer from tf.compat.v1.train.GradientDescentOptimizer to dp_optimizer.DPGradientDescentGaussianOptimizer
```
def client_update(model, dataset, server_weights, client_optimizer):
  client_weights = model.trainable_variables
  tf.nest.map_structure(lambda x, y: x.assign(y), client_weights, server_weights)
  for batch in dataset:
    outputs = model.forward_pass(batch)
    grads_and_vars = client_optimizer.compute_gradients(outputs.loss, client_weights)
    client_optimizer.apply_gradients(grads_and_vars)
  return client_weights
```

```
@tff.tf_computation(tf_dataset_type, model_weights_type)
def client_update_fn(tf_dataset, server_weights):
  model = model_fn()
  # client_optimizer=tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)
  client_optimizer = dp_optimizer.DPGradientDescentGaussianOptimizer(
          l2_norm_clip=1.0,
          noise_multiplier=1.0,
          num_microbatches=BATCH_SIZE,
          learning_rate=0.01)
  return client_update(model, tf_dataset, server_weights, client_optimizer)
```
But I got an error ""RuntimeError: Event loop is closed"". when I run
`server_state = federated_algorithm.next(server_state, federated_train_data)`

However, If I set the batchsize of tf_dataset as 1, the code runs well.

How could I solve this problem?


**Environment (please complete the following information):**
*  Linux Ubuntu 16.04
* TensorFlow Federated 0.19.0, TensorFlow 2.5.0, Tensorflow Privacy 0.5.2

Hi @wangdan269. Can you post the full stack trace? It's tough to say what's going on in your example, as it involves things such as `federated_train_data` that you haven't specified here.> Hi @wangdan269. Can you post the full stack trace? It's tough to say what's going on in your example, as it involves things such as `federated_train_data` that you haven't specified here.

Thanks for your reply! @zcharles8 .

Below is my example code.  TensorFlow Federated 0.19.0, TensorFlow 2.5.0, Tensorflow Privacy 0.5.2
It runs when the Batch_size =1, but has errors when Batch_size >1.   


ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c326ba8 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c3d53c8 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c3266a0 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c3265f8 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb0405d320 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c326cf8 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c3264a8 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb2c3269e8 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7fdb0406df28 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed

Process finished with exit code 1



```
import collections
import attr
import functools
import numpy as np
import tensorflow as tf
import tensorflow_federated as tff
from tensorflow_privacy.privacy.optimizers import dp_optimizer

# nest_asyncio.apply()

emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()

NUM_CLIENTS = 10
BATCH_SIZE = 1
#if batch size >1, the code cannot work properly

def preprocess(dataset):

  def batch_format_fn(element):
    """"""Flatten a batch of EMNIST data and return a (features, label) tuple.""""""
    return (tf.reshape(element['pixels'], [-1, 784]),
            tf.reshape(element['label'], [-1, 1]))

  return dataset.batch(BATCH_SIZE).map(batch_format_fn)

""""""sample a small number of clients, and apply the preprocessing above to their datasets.""""""
client_ids = np.random.choice(emnist_train.client_ids, size=NUM_CLIENTS, replace=False)
federated_train_data = [preprocess(emnist_train.create_tf_dataset_for_client(x))
  for x in client_ids]

def create_keras_model():
  return tf.keras.models.Sequential([
      tf.keras.layers.Input(shape=(784,)),
      tf.keras.layers.Dense(10, kernel_initializer='zeros'),
      tf.keras.layers.Softmax(),
  ])


def model_fn():
  keras_model = create_keras_model()
  return tff.learning.from_keras_model(
      keras_model,
      input_spec=federated_train_data[0].element_spec,
      # loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

def initialize_fn():
  model = model_fn()
  return model.trainable_variables


@tf.function
def client_update(model, dataset, server_weights, client_optimizer):
  """"""Performs training (using the server model weights) on the client's dataset.""""""
  # Initialize the client model with the current server weights.
  client_weights = model.trainable_variables
  # Assign the server weights to the client model.
  tf.nest.map_structure(lambda x, y: x.assign(y), client_weights, server_weights)
  # Use the client_optimizer to update the local model.
  for batch in dataset:
    with tf.GradientTape() as tape:
      # Compute a forward pass on the batch of data
      outputs = model.forward_pass(batch)
    # Compute the corresponding gradient
    # grads = tape.gradient(outputs.loss, client_weights)
    # grads_and_vars = zip(grads, client_weights)
    grads_and_vars = client_optimizer.compute_gradients(outputs.loss, client_weights)
    # Apply the gradient using a client optimizer.
    client_optimizer.apply_gradients(grads_and_vars)
  return client_weights


@tf.function
def server_update(model, mean_client_weights):
  """"""Updates the server model weights as the average of the client model weights.""""""
  model_weights = model.trainable_variables
  # Assign the mean client weights to the server model.
  tf.nest.map_structure(lambda x, y: x.assign(y),
                        model_weights, mean_client_weights)
  return model_weights

@tff.tf_computation
def server_init():
  model = model_fn()
  print(""server init"")
  return model.trainable_variables

@tff.federated_computation
def initialize_fn():
  return tff.federated_value(server_init(), tff.SERVER)

whimsy_model = model_fn()
tf_dataset_type = tff.SequenceType(whimsy_model.input_spec)
model_weights_type = server_init.type_signature.result

@tff.tf_computation(tf_dataset_type, model_weights_type)
def client_update_fn(tf_dataset, server_weights):
  model = model_fn()
  # client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
  client_optimizer = dp_optimizer.DPGradientDescentGaussianOptimizer(
          l2_norm_clip=1.0,
          noise_multiplier=1.0,
          num_microbatches=BATCH_SIZE,
          learning_rate=0.01)
  return client_update(model, tf_dataset, server_weights, client_optimizer)

@tff.tf_computation(model_weights_type)
def server_update_fn(mean_client_weights):
  model = model_fn()
  return server_update(model, mean_client_weights)

federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)
federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)


@tff.federated_computation(federated_server_type, federated_dataset_type)
def next_fn(server_weights, federated_dataset):
    # Broadcast the server weights to the clients.
    server_weights_at_client = tff.federated_broadcast(server_weights)
    # Each client computes their updated weights.
    client_weights = tff.federated_map(
        client_update_fn, (federated_dataset, server_weights_at_client))
    # The server averages these updates.
    mean_client_weights = tff.federated_mean(client_weights)
    # The server updates its model.
    server_weights = tff.federated_map(server_update_fn, mean_client_weights)
    return server_weights

federated_algorithm = tff.templates.IterativeProcess(
    initialize_fn=initialize_fn,
    next_fn=next_fn
)
print(""Instantiation"")

central_emnist_test = emnist_test.create_tf_dataset_from_all_clients().take(1000)
central_emnist_test = preprocess(central_emnist_test)


def evaluate(server_state):
  keras_model = create_keras_model()
  keras_model.compile(
      loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
  )
  keras_model.set_weights(server_state)
  keras_model.evaluate(central_emnist_test)

server_state = federated_algorithm.initialize()
evaluate(server_state)



for round in range(100):
  print(""Ronud:"", round)
  server_state = federated_algorithm.next(server_state, federated_train_data)

evaluate(server_state)

```Are you running this in a Jupyter notebook? Regardless, the errors in the stack trace seem to be caused by `asyncio`, not by `tensorflow_federated`. This [stack overflow post](https://stackoverflow.com/questions/60966874/running-the-event-loop-in-tf-federated/60975831#60975831) may be relevant.> Are you running this in a Jupyter notebook? Regardless, the errors in the stack trace seem to be caused by `asyncio`, not by `tensorflow_federated`. This [stack overflow post](https://stackoverflow.com/questions/60966874/running-the-event-loop-in-tf-federated/60975831#60975831) may be relevant.

I run it in Linux Ubuntu 16.04. I have tried this method,
```
import nest_asyncio
nest_asyncio.apply()
```
but it doesn't work.Does the training code work with a `tf.keras.optimizer`? Additionally, is there more of the stack trace that we could look at? I don't see any reference to which line of the code you posted that it's failing at.> Does the training code work with a `tf.keras.optimizer`? Additionally, is there more of the stack trace that we could look at? I don't see any reference to which line of the code you posted that it's failing at.

It works with `tf.keras.optimizer`, but has errors after I change the optimizer into `dp_optimizer.DPGradientDescentGaussianOptimizer`. The full stack trace is shown below:

Traceback (most recent call last):
  File ""/root/userfolder/PPFL/test/MNIST-LDP-FL.py"", line 157, in <module>
    server_state = federated_algorithm.next(server_state, federated_train_data)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/computation/function_utils.py"", line 525, in __call__
    return context.invoke(self, arg)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/retrying.py"", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/retrying.py"", line 206, in call
    return attempt.get(self._wrap_exception)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/retrying.py"", line 247, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/six.py"", line 703, in reraise
    raise value
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/retrying.py"", line 200, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 230, in invoke
    _invoke(executor, comp, arg, result_type)))
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete
    return future.result()
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 396, in _wrapped
    return await coro
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/execution_context.py"", line 135, in _invoke
    result = await executor.create_call(comp, arg)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 341, in create_call
    return await comp_repr.invoke(self, arg)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 161, in invoke
    return await executor._evaluate(comp_lambda.result, new_scope)  # pylint: disable=protected-access
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 511, in _evaluate
    return await self._evaluate_block(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 475, in _evaluate_block
    return await self._evaluate(comp.block.result, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 503, in _evaluate
    return await self._evaluate_reference(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 427, in _evaluate_reference
    return await scope.resolve_reference(comp.reference.name)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 110, in resolve_reference
    return await value
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 505, in _evaluate
    return await self._evaluate_call(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 443, in _evaluate_call
    func, arg = await asyncio.gather(func, get_arg())
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 439, in get_arg
    return await self._evaluate(comp.call.argument, scope=scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 509, in _evaluate
    return await self._evaluate_struct(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 463, in _evaluate_struct
    values = await asyncio.gather(*values)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 503, in _evaluate
    return await self._evaluate_reference(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 427, in _evaluate_reference
    return await scope.resolve_reference(comp.reference.name)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 110, in resolve_reference
    return await value
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 505, in _evaluate
    return await self._evaluate_call(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 443, in _evaluate_call
    func, arg = await asyncio.gather(func, get_arg())
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 439, in get_arg
    return await self._evaluate(comp.call.argument, scope=scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 503, in _evaluate
    return await self._evaluate_reference(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 427, in _evaluate_reference
    return await scope.resolve_reference(comp.reference.name)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 110, in resolve_reference
    return await value
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 505, in _evaluate
    return await self._evaluate_call(comp, scope)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 444, in _evaluate_call
    return await self.create_call(func, arg=arg)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py"", line 339, in create_call
    comp_repr, delegated_arg))
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 120, in create_call
    return await self._delegate(self._target_executor.create_call(comp, arg))
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 105, in _delegate
    result_value = await _delegate_with_trace_ctx(coro, self._event_loop)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 396, in _wrapped
    return await coro
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/federating_executor.py"", line 460, in create_call
    comp.internal_representation.uri, arg)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/federating_executor.py"", line 138, in compute_federated_intrinsic
    return await fn(arg)  # pylint: disable=not-callable
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/federated_resolving_strategy.py"", line 465, in compute_federated_map
    return await self._map(arg, all_equal=False)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/federated_resolving_strategy.py"", line 342, in _map
    for (value, child) in zip(val, children)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/federated_resolving_strategy.py"", line 338, in _map_child
    return await child.create_call(fn_at_child, value)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 120, in create_call
    return await self._delegate(self._target_executor.create_call(comp, arg))
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/thread_delegating_executor.py"", line 105, in _delegate
    result_value = await _delegate_with_trace_ctx(coro, self._event_loop)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 396, in _wrapped
    return await coro
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/tracing.py"", line 201, in async_trace
    result = await fn(*fn_args, **fn_kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 683, in create_call
    comp.internal_representation(arg.internal_representation),
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 365, in <lambda>
    return lambda arg: fn_to_return(arg)  # pylint: disable=unnecessary-lambda
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 346, in fn_to_return
    destroy_after_invocation=destroy_after)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py"", line 203, in _call_embedded_tf
    result_parts = wrapped_fn(*param_elements)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1711, in __call__
    return self._call_impl(args, kwargs)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py"", line 247, in _call_impl
    args, kwargs, cancellation_manager)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1729, in _call_impl
    return self._call_with_flat_signature(args, kwargs, cancellation_manager)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1778, in _call_with_flat_signature
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1961, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 596, in call
    ctx=ctx)
  File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input to reshape is a tensor with 1 values, but the requested shape requires a multiple of 2
	 [[{{node Reshape}}]]
	 [[StatefulPartitionedCall_1/ReduceDataset]] [Op:__inference_pruned_3604]

Function call stack:
pruned

ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc25fe48 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc5072b0 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc277940 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc25fd68 state=finished returned EagerValue>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc25f160 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
ERROR:concurrent.futures:exception calling callback for <Future at 0x7f24cc25fda0 state=finished raised InvalidArgumentError>
Traceback (most recent call last):
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  File ""/usr/lib/python3.6/asyncio/futures.py"", line 417, in _call_set_state
    dest_loop.call_soon_threadsafe(_set_state, destination, source)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 637, in call_soon_threadsafe
    self._check_closed()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 377, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed

Process finished with exit code 1

This looks like the important part to me:

```
File ""/root/userfolder/PPFL/env/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 1 values, but the requested shape requires a multiple of 2
[[{{node Reshape}}]]
[[StatefulPartitionedCall_1/ReduceDataset]] [Op:__inference_pruned_3604]
```Hi @wangdan269. Thanks to @cramertj's comment, I realized where the issue lies. You call at some point:

```
client_optimizer = dp_optimizer.DPGradientDescentGaussianOptimizer(
        l2_norm_clip=1.0,
        noise_multiplier=1.0,
        num_microbatches=BATCH_SIZE,
        learning_rate=0.01)
```

If you instead set `num_microbatches = None`, everything succeeds. I believe this is due to having batches at the end of a dataset that are smaller than `BATCH_SIZE`. An alternative would be to call `.batch(..., drop_remainder=True)` in your data pipeline.While this is not technically a bug on TFF, it is an unfortunate stack trace that you found. One suggestion I might have in the future is to try simply training on a single client before wrapping everything into an iterative process. I believe this will catch the error above, in a way that will have many fewer layers between you and the error.Thanks! @zcharles8 and @cramertj This problem is solved!!"	10	2021-07-08 02:04:12	2021-07-22 01:14:13	2021-07-22 01:14:13
https://github.com/tensorflow/federated/issues/1458	['bug']	How do I modify this code?	"How do I modify this code?tf.keras.layers(body = concatenate([upbody1, downbody1],axis=-1)),
tf.keras.layers(body1 = LSTM(8, activation='tanh'))(body),

tf.keras.laayers(out = Dense(60, activation='softmax'))(body1),

tf.keras.layers(model = Model([bdbd, rhrh, lhlh, rfrf, lflf]), out),
])

File ""<ipython-input-367-2708f0ec1ea2>"", line 105
      ])
      ^
  SyntaxError: invalid syntaxClosed, as it is a duplicate of #1471."	1	2021-06-10 05:17:44	2021-06-18 16:30:53	2021-06-18 16:30:53
https://github.com/tensorflow/federated/issues/1434	['bug']	Jax serialization of tff.types.StructType	"Jax serialization of tff.types.StructTypeI am trying to implement the client update step in JAX while making use of the `jax.experimental.stax` module for the definition of my model. Initialized model weights are given as Jax TracedArray objects. However, serializing a function that takes model weights as a struct of jax TracedArray objects gives the error

`AttributeError: The `Struct` of length 2 does not have named field ""tensor_index"". Fields (up to first 10): []`

A short reconstruction of the error can be found [here](https://colab.research.google.com/drive/1NAsV7yfVQZv-izRTer2vBstoaEZsUh8H?usp=sharing)

It seems that `get_model_weights.type_signature.result` returns a nested `tff.StructType` object. The arrays at the bottom of this nested structure are correctly identified as `tff.TensorType`. However, the error I get keeps throwing `AttributeError: The `Struct` of length 2 does not have named field ""tensor_index"". Fields (up to first 10): []`. 

After looking into `tensorflow_federated/python//core/impl/jax_context/jax_serialization.py`, I found that the problem lies in line 147-148:

```
flattened_obj, _ = jax.tree_util.tree_flatten((args, kwargs))  
tensor_indexes = list(np.argsort([x.tensor_index for x in flattened_obj]))
```

`flattened_obj` does not seem to be flattened, there is still a `StructType` object nested in the list, which causes `x.tensor_index` to fail in the next line. 

Any suggestions on how this can be solved?Looking at this code, I think there is a bug here with nested structures. Writing a failing test now, will tag this issueShould be resolved by [9e1edc7](https://github.com/tensorflow/federated/commit/9e1edc7fc86b014642a1bfde263d89f035f94ba3); when TFF-Nightly is released overnight the fix should be usable.Problem still seems to be there after the update @jkr26. [See the same colab file again](https://colab.research.google.com/drive/1NAsV7yfVQZv-izRTer2vBstoaEZsUh8H?usp=sharing). Tff-nightly is up to date as can be seen.

`flattened_obj, _ = jax.tree_util.tree_flatten((args, kwargs))` still returns a nested Struct just as before. It seems that the tensor values at the leaf nodes are correctly serialized into `_XlaSerializerTensorArg` but the nested `Struct`objects fail to become `_XlaSerializerStructArg`

Flattening the `StructType` with `structure.flatten()` **before** feeding it into the jax serialization is a workaround but requires properly nesting it again within the jax function.



assigning @zcharles8 to this, he is taking a look at fleshing out TFF Jax relationship in Python, I think in particular what is failing at the moment."	4	2021-06-01 09:39:29	2022-06-30 19:31:28	2021-06-02 17:07:26
https://github.com/tensorflow/federated/issues/1381	['bug']	DPKerasSGDOptimizer throws AssertionError	"DPKerasSGDOptimizer throws AssertionError**Describe the bug**
[colab](https://colab.research.google.com/drive/1A1xaXL8-DPuVPIGG2Z2LX8aKzWqrTpbo?usp=sharing) notebook reproducing the error.

I changed the client optimizer to `DPKerasSGDOptimizer` from `tensorflow_privacy.privacy.optimizers.dp_optimizer_keras` in the [mnist tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) to play around. It throws the following AssertionError.  I used the [mnist_dpsgd_tutorial_keras.py])https://github.com/tensorflow/privacy/blob/master/tutorials/mnist_dpsgd_tutorial_keras.py) to set up the params, loss func, etc. I am not sure but, this is perhaps because TFF does not use the optimizer to calculate gradients and only uses the apply_gradient method from the optimizer. How can I fix/workaround this ?
```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-55-74bf67640887> in <module>()
      2     model_fn,
      3     client_optimizer_fn= get_optimizer,
----> 4     server_optimizer_fn= lambda: tf.keras.optimizers.SGD(learning_rate=1.0),
      5 )

13 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    984           except Exception as e:  # pylint:disable=broad-except
    985             if hasattr(e, ""ag_error_metadata""):
--> 986               raise e.ag_error_metadata.to_exception(e)
    987             else:
    988               raise

AssertionError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py:395 _compute_local_training_and_client_delta  *
        client_output = client_delta_fn(dataset, initial_model_weights)
    /usr/local/lib/python3.7/dist-packages/tensorflow_federated/python/learning/federated_averaging.py:96 reduce_fn  *
        optimizer.apply_gradients(zip(gradients, model.weights.trainable))
    /usr/local/lib/python3.7/dist-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py:28 _dataset_reduce_fn  *
        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:2178 reduce  **
        add_to_graph=False)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3898 __init__
        self._function = fn_factory()
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3144 get_concrete_function
        *args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3109 _get_concrete_function_garbage_collected
        graph_function, _ = self._maybe_define_function(args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3456 _maybe_define_function
        graph_function = self._create_graph_function(args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3301 _create_graph_function
        capture_by_value=self._capture_by_value),
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:999 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3873 wrapped_fn
        ret = wrapper_helper(*args)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3803 wrapper_helper
        ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
    /tmp/tmp3y46nwy4.py:23 reduce_fn
        ag__.converted_call(ag__.ld(optimizer).apply_gradients, (ag__.converted_call(ag__.ld(zip), (ag__.ld(gradients), ag__.ld(model).weights.trainable), None, fscope_1),), None, fscope_1)
    /usr/local/lib/python3.7/dist-packages/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras_vectorized.py:190 apply_gradients  **
        'Neither _compute_gradients() or get_gradients() on the '

    AssertionError: Neither _compute_gradients() or get_gradients() on the differentially private optimizer was called. This means the training is not differentially private. It may be the case that you need to upgrade to TF 2.4 or higher to use this particular optimizer.
```

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 'Ubuntu', '18.04', 'bionic' at Google Colab
* Python package versions (e.g., TensorFlow Federated, TensorFlow): TFF, TFP
* Python version: 3.7.10
* What TensorFlow Federated execution stack are you using? TFF-nightly

Output of  [script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).
```
numpy                         1.19.5             
protobuf                      3.12.4             
tensorflow-datasets           4.0.1              
tensorflow-estimator          2.4.0              
tensorflow-federated-nightly  0.18.0.dev20210501 
tensorflow-gcs-config         2.4.0              
tensorflow-hub                0.12.0             
tensorflow-metadata           0.29.0             
tensorflow-model-optimization 0.5.0              
tensorflow-privacy            0.5.2              
tensorflow-probability        0.12.1  
```

**Expected behavior**
The training should proceed as usual.


**Additional Context**
Using a DPSequential model from TFP with the default SGD optimizer works. I trust it is doing the same thing?
Is using dp_aggregator as follows equivalent to using dpsgd optimizer?
```
iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),
    model_update_aggregation_factory = tff.learning.dp_aggregator(0.1, 10)
)
```This error is from TFP https://github.com/tensorflow/privacy/blob/755ed26671f5567ba1519a4e80078eded7a6299b/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py#L180. 

TFP has some known compatibility issue with TF2 Keras. Please consider following https://github.com/tensorflow/privacy/issues/134. 

One workaround is to explicitly call `compute_gradients` of TFP optimizers. The default of TFF used keras `tape.gradient` at https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/learning/federated_averaging.py#L95. You probably want to write your customized TFF iterative process by following https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/examples/simple_fedavgHello @clippedgrad did you solve the problem? ThanksI have the same problem as you. Did you solve the problem? Thanks! @clippedgrad"	3	2021-05-04 10:41:35	2021-07-06 05:20:47	2021-05-05 00:51:47
https://github.com/tensorflow/federated/issues/1265	['bug']	DataLossError upon loading dataset using tf.data.experimental.load	"DataLossError upon loading dataset using tf.data.experimental.loadI would like to train two independent ```TFF``` models using ```emnist dataset```. Each model should train on a ```1000``` distinct participants randomly drawn from the dataset.

Code below
```python
emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()

participants_ids = np.random.choice(a=emnist_train.client_ids, 
                                    size=1000,
                                    replace=False)

federated_dataset = 
        [data_train.create_tf_dataset_for_client(i) for i in participants_ids]

nested_dataset = tf.data.Dataset.from_tensor_slices(federated_dataset)
```
Trying to save the dataset using the function ```tf.data.experimental.save```

```python
tf.data.experimental.save(nested_dataset, 'model_dataset')
```
the warning below is generated. However, the save is completed.

```python
E tensorflow/core/framework/dataset.cc:89] The Encode() method is not implemented for DatasetVariantWrapper objects.
```

**The problem occurs upon loading the dataset and trying to inspect its contents**
```python
dataset = tf.data.experimental.load('model_dataset', 
                      element_spec= 
                      DatasetSpec(collections.OrderedDict([
                         ('label', TensorSpec(shape=(), dtype=tf.int32)),
                         ('pixels', TensorSpec(shape=(28, 28), dtype=tf.float32))]), 
                      TensorShape([])

# verifying elements
for example in dataset:
        print(example)
```
Error below
```python
tensorflow.python.framework.errors_impl.DataLossError: Unable to parse tensor from stored proto.
```
Trying other methods such as ```pickle.dump``` and ``` np.save```, all resulted in error below even after converting the dataset into ```numpy.ndarray``` using ```np.asarray()```
```python
pickle.dump(np.asarray(nested_dataset), open('model_dataset', 'wb'))

tensorflow.python.framework.errors_impl.InternalError: Tensorflow type 21 not convertible to numpy dtype.
```

Env details
```python
python -V 3.8.5
tensorflow==2.4.1
tensorflow-datasets==4.2.0
tensorflow-federated==0.18.0
Keras==2.4.3
numpy==1.19.2
```
Is this a ```bug```, or is there any good way to slice the dataset into clients and save the newly created datasets ?Hi @AHabes. Generally, `tf.data.experimental.save` is not suitable for saving federated datasets, since there is no way to disambiguate which examples are associated to each client.

If you're interested in saving federated datasets, you can use `tff.simulation.FilePerUserClientData`, see https://www.tensorflow.org/federated/api_docs/python/tff/simulation/FilePerUserClientData. Unless I'm mistaken, this seems to meet your exact use case.

Note: If you're using the tensorflow-federated-nightly, then you would instead use `tff.simulation.datasets.FilePerUserClientDatea`.Also note that `FilePerUserClientData` would be used to load the federated datasets, once you've saved the datasets to disk. How you want to do this is up to you, though typically a `tf.io.TFRecordWriter` is a good way to proceed in such cases (see https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/simulation/datasets/file_per_user_client_data_test.py for an example of how to use this).@zcharles8,Thanks for the follow up, and for the useful info indeed.

Actually, the main idea is to use this ```federated dataset``` to train a ```federated model```,  then transform this same dataset into a ```flatten``` one and save it in order to process and use it later to train a traditional ```centralized model```. 

The final intension is to establish a comparison between the ```federated model``` and the ```centralized model``` when trained on the same dataset. 

That is reason behind the attempt to convert the federated dataset into a flatten format usable by a traditional centralized model and save it.@AHabes is there a good reason to save the centralized dataset? It sounds like `ClientData.create_tf_dataset_from_all_clients` is exactly the functionality you want (amalgamating a dataset across clients).

From the code snippet above, it looks like you're trying to use the EMNIST dataset, but with only 1000 of the clients. Can you elaborate on this? For training purposes, you only need to sample a small number per round to get decent convergence on most models, and for testing, I don't see the benefit of throwing away client data. @zcharles8, Thanks for the follow up, much appreciated

Actually, I would run several experiments, each with different expected number of participants (ranging from ~10 to ~1000). 

In general, the process goes like this:

1. Experiment with various sampling techniques (Poisson, uniform, Binomial etc ..) to sample an expected number of participants per round (say 1000).
2. Train a federated model for x rounds based on the clients sampled at each round.
3. Collect the ids of clients participated in the federated model
4. Prepare a centralized dataset from the participants' datasets (and ideally save it for future usage and referencing).
4. Use the centralized dataset to train a traditional centralized model for x epochs.
5. Compare the performance of federated vs traditional models trained on the same dataset (to investigate the effects of sampling techniques and number of participants).

Using ```ClientData.create_tf_dataset_from_all_clients``` I would assume I need to first slice the ```ClientData``` to contain only the data of the sampled participants (step 1 above) before calling ```create_tf_dataset_from_all_clients```. Otherwise, it will create the centralized dataset out of all 3400 participants ?@AHabes I believe that https://www.tensorflow.org/federated/api_docs/python/tff/simulation/ClientData#from_clients_and_fn will do exactly what you want. Basically, you can use this to create a new `ClientData` that only uses the smaller number of clients, in which case `create_tf_dataset_from_all_clients` would give exactly the desired outcome.

Note that to make this more performant, you may want to pass in `emnist_train.dataset_computation` instead, so you would end up doing something like

```
emnist_train, _ = tff.simulation.datasets.emnist.load_data()
subsampled_clients = emnist_train.client_ids[:10]
subsampled_train = tff.simulation.datasets.ClientData.from_clients_and_fn(
  subsampled_clients, emnist_train.dataset_computation)
```
Hi @AHabes. I'm closing this issue due to a lack of activity. If the suggestion above does not work, feel free to re-open this."	7	2021-03-24 19:25:15	2021-06-30 20:06:42	2021-06-30 20:06:42
https://github.com/tensorflow/federated/issues/1245	['bug']	Memory explodes	"Memory explodesWhile running the following command:
bazel run main:federated_trainer -- --task=cifar100 --total_rounds=1500 --client_optimizer=sgd --client_learning_rate=0.1 --client_batch_size=20 --server_optimizer=sgd --server_learning_rate=1 --clients_per_round=10 --client_epochs_per_round=1 --experiment_name=cifar100_fedavg
the VRAM usage explodes.
It quickly reaches 8.9 GB in the first round. It remains stable, until around the 125. round, when it doubles to 16.9 GB. This phenomenon repeats itself until the training crashes.Given that this library functionally wraps each tensorflow native function for the conjugate training components, note that there may be a problem with tensorflow federated and the GPU memory allocation during training, which is especially something to keep in mind of if you are training on a single GPU with multiple instances of the networks e.g. the client models training in federated environment. This could be incorrect, but I would recommend looking into your system metadata to narrow down your problem. Hi @raghavchugh21. It seems like this might be related to https://github.com/google-research/federated/issues/29. Additionally, this issue seems to be specifically related to the `federated_research` repository, so it might be better to post your issue there.@raghavchugh21 I believe this issue is fixed by de97c52ca835b5557c0c1c9dbd768f742a2739f5. If possible, can you confirm this is the case by running the same command using the latest nightly version of TFF?Based on feedback from google-research/federated#29, I believe that this has been resolved. Feel free to re-open it if the issue persists."	4	2021-03-17 16:12:28	2021-04-09 17:25:36	2021-04-09 17:25:36
https://github.com/tensorflow/federated/issues/1232	[]	TFF GCP Setup Tutorial/Guide syntax error	"TFF GCP Setup Tutorial/Guide syntax error**Issue**
Hi while I was going through the GCP Setup tutorial for TFF, I found that after running step
>6. Run the client container interactively.
> The string ""Hello World"" should print to the terminal.
`$ docker run \
    --interactive \
    --tty \
    --name=tff-client \
    --volume ~:/simulation \
    --workdir /simulation \
    <registry>/tff-client \
    bash`

of [4. Run a simulation on a client container.](https://www.tensorflow.org/federated/gcp_setup#4_run_a_simulation_on_a_client_container)
I got the following error regarding the `--volume` flag: 
![MicrosoftTeams-image](https://user-images.githubusercontent.com/33724568/111151235-17b8d380-8587-11eb-94b0-98aac1995b40.png)
**Potential Solution**
It looks like `--volume ~/:/simulation` solves the problem, but I'm still not getting ""Hello World"", could you pls fix that?Thank you for the report!

Would you be up for submitting a PR to update the guide to the correct docker `--volume` flag? The source can be found in this markdown file https://github.com/tensorflow/federated/blob/master/docs/gcp_setup.md

Its hard to tell without more information why `""Hello World!""` still isn't being printed. Any chance of attaching the logs of what happens after the `--volume` flag is changed?> **Issue**
> Hi while I was going through the GCP Setup tutorial for TFF, I found that after running step
> 
> > 1. Run the client container interactively.
> >    The string ""Hello World"" should print to the terminal.
> >    `$ docker run \     --interactive \     --tty \     --name=tff-client \     --volume ~:/simulation \     --workdir /simulation \     <registry>/tff-client \     bash`
> 
> of [4. Run a simulation on a client container.](https://www.tensorflow.org/federated/gcp_setup#4_run_a_simulation_on_a_client_container)
> I got the following error regarding the `--volume` flag:
> ![MicrosoftTeams-image](https://user-images.githubusercontent.com/33724568/111151235-17b8d380-8587-11eb-94b0-98aac1995b40.png)
> **Potential Solution**
> It looks like `--volume ~/:/simulation` solves the problem, but I'm still not getting ""Hello World"", could you pls fix that?"	2	2021-03-15 12:11:03	2021-03-16 01:50:29	2021-03-16 01:50:29
https://github.com/tensorflow/federated/issues/1212	[]	"Problem of ""no attribute 'from_compiled_keras_model'"", any alternative?"	"Problem of ""no attribute 'from_compiled_keras_model'"", any alternative?I have tensorflow version 2.2.0 and tensorflow_federated version 0.14.0. In the code, I have the following line:

`return tff.learning.from_compiled_keras_model(keras_model, sample_batch)`

and it generates the following error:

`AttributeError: module 'tensorflow_federated.python.learning' has no attribute 'from_compiled_keras_model'`

I don't know what can I use as an alternative/equivalent to it (because `from_compiled_keras_model` is removed from the current tensorflow_federated). 

Thank you. Hi @sumeyyegsu 

This should be addressable simply by using `tff.learning.from_keras_model` instead, and just not compiling the Keras model before passing to TFF."	1	2021-03-07 20:33:08	2021-03-10 05:31:19	2021-03-10 05:31:19
https://github.com/tensorflow/federated/issues/1029	['bug']	Fed Avg on single machine multi GPU -- only ONE GPU is actually used 	"Fed Avg on single machine multi GPU -- only ONE GPU is actually used **Describe the bug**
Simulation is not using all GPUs. According to monitoring, it's using only 1 of 8.

**Environment (please complete the following information):**
* OS Platform and Distribution: Linux Ubuntu 16.04
* Python package versions: tff nightly
* Python version: 3.8.0
* CUDA/cuDNN version: cuDNN 8000
* What TensorFlow Federated execution stack are you using? `build_federated_averaging_process`

Note: You can collect the Python package information by running `pip3 freeze`
from the command line and most of the other information can be collected using
TensorFlows environment capture
[script](https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh).

**Expected behavior**
Each client optimizer should run on a different GPU in parallel, improving the overall speed.

**Additional context**
According to https://www.tensorflow.org/federated/tutorials/simulations, I wonder if parallelization across GPUs is currently supported. 
Could the issue be updated with more details about how the simulation is being run?

If using the `tff.learning` package, could you try using the `use_experimental_simulation_loop=True` argument? https://www.tensorflow.org/federated/api_docs/python/tff/learning/build_federated_averaging_process?version=nightly#args

@nightldj may have tips on running with multiple GPUsLike in TF and other frameworks, multi-GPU is not on by default. Could you config the TFF executor like this https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/examples/simple_fedavg/emnist_fedavg_main.py#L140? And use `use_experimental_simulation_loop=True` as Zach suggested. 

A more detailed instruction is on my TODO list, but let me know if these works. @ZacharyGarrett Thanks so much for the tip! 

We are using `tff` for research on a custom compression algorithm. In detail, it's very much alike the tutorial at https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification. The main difference is that we created our own aggregation factory where we simulate our compression algorithm and passed that into `build_fed_avg_process`. 

@nightldj Thanks for the help! 

After setting `use_experimental_simulation_loop=True` and setting up devices like the following immediately at the beginning of the `main` function

```python
    client_devices = tf.config.list_logical_devices('GPU')
    server_device = tf.config.list_logical_devices('CPU')[0]
    tff.backends.native.set_local_execution_context(
        server_tf_device=server_device, client_tf_devices=client_devices
    )
```

According to `gpustat` still only one of the eight GPU's is being used. @zehao-sean-huang What is the output of `tf.config.list_logical_devices('GPU')`? Did you see speedup moving from one GPU to multi-GPU? I would double check if the application is IO bounded. We do not use `gpustat`. Would you mind trying TF profiler?@nightldj The output for `tf.config.list_logical_devices('GPU')` is the following. Some logs are also attached. If it's no use, the actual list of devices is at the bottom of the block.

```
2021-01-12 17:09:19.542496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-12 17:09:20.042851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.054410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.071153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 2 with properties: 
pciBusID: 0000:06:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.082475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 3 with properties: 
pciBusID: 0000:07:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.085875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 4 with properties: 
pciBusID: 0000:08:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.089990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 5 with properties: 
pciBusID: 0000:0b:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.095699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 6 with properties: 
pciBusID: 0000:0c:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.115366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 7 with properties: 
pciBusID: 0000:0d:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:20.115478: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-12 17:09:20.120351: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-01-12 17:09:20.120437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-01-12 17:09:20.123474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-12 17:09:20.123925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-12 17:09:20.127652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-01-12 17:09:20.128875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-01-12 17:09:20.129122: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-01-12 17:09:20.344744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-01-12 17:09:20.345421: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-12 17:09:29.743279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: 
pciBusID: 0000:04:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.747043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.756720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 2 with properties: 
pciBusID: 0000:06:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.760139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 3 with properties: 
pciBusID: 0000:07:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.763145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 4 with properties: 
pciBusID: 0000:08:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.765249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 5 with properties: 
pciBusID: 0000:0b:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.767875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 6 with properties: 
pciBusID: 0000:0c:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.775241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 7 with properties: 
pciBusID: 0000:0d:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s
2021-01-12 17:09:29.775399: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-12 17:09:29.775487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-01-12 17:09:29.775555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-01-12 17:09:29.775600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-12 17:09:29.775628: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-12 17:09:29.775659: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-01-12 17:09:29.775697: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-01-12 17:09:29.775730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-01-12 17:09:29.845646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-01-12 17:09:29.845773: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-12 17:09:36.161719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-12 17:09:36.161820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 1 2 3 4 5 6 7 
2021-01-12 17:09:36.161842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N Y Y Y Y Y Y Y 
2021-01-12 17:09:36.161855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 1:   Y N Y Y Y Y Y Y 
2021-01-12 17:09:36.161869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 2:   Y Y N Y Y Y Y Y 
2021-01-12 17:09:36.161880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 3:   Y Y Y N Y Y Y Y 
2021-01-12 17:09:36.161891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 4:   Y Y Y Y N Y Y Y 
2021-01-12 17:09:36.161907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 5:   Y Y Y Y Y N Y Y 
2021-01-12 17:09:36.161922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 6:   Y Y Y Y Y Y N Y 
2021-01-12 17:09:36.161935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 7:   Y Y Y Y Y Y Y N 
2021-01-12 17:09:36.202960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10910 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:04:00.0, compute capability: 7.0)
2021-01-12 17:09:36.209779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10910 MB memory) -> physical GPU (device: 1, name: TITAN V, pci bus id: 0000:05:00.0, compute capability: 7.0)
2021-01-12 17:09:36.224329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10910 MB memory) -> physical GPU (device: 2, name: TITAN V, pci bus id: 0000:06:00.0, compute capability: 7.0)
2021-01-12 17:09:36.233402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10910 MB memory) -> physical GPU (device: 3, name: TITAN V, pci bus id: 0000:07:00.0, compute capability: 7.0)
2021-01-12 17:09:36.247370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10910 MB memory) -> physical GPU (device: 4, name: TITAN V, pci bus id: 0000:08:00.0, compute capability: 7.0)
2021-01-12 17:09:36.257711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10910 MB memory) -> physical GPU (device: 5, name: TITAN V, pci bus id: 0000:0b:00.0, compute capability: 7.0)
2021-01-12 17:09:36.269125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10910 MB memory) -> physical GPU (device: 6, name: TITAN V, pci bus id: 0000:0c:00.0, compute capability: 7.0)
2021-01-12 17:09:36.280726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10910 MB memory) -> physical GPU (device: 7, name: TITAN V, pci bus id: 0000:0d:00.0, compute capability: 7.0)
[LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU'), LogicalDevice(name='/device:GPU:2', device_type='GPU'), LogicalDevice(name='/device:GPU:3', device_type='GPU'), LogicalDevice(name='/device:GPU:4', device_type='GPU'), LogicalDevice(name='/device:GPU:5', device_type='GPU'), LogicalDevice(name='/device:GPU:6', device_type='GPU'), LogicalDevice(name='/device:GPU:7', device_type='GPU')]
```

We didn't really observe much speedups. The application shouldn't be IO bounded cuz we're training CIFAR10. The whole dataset is stored in memory. 

And yes, I'll go ahead and try TF profiler and get back here. Thanks for the help. @nightldj @ZacharyGarrett Thanks guys! After trying on a few machines it works on one of them configuring devices like this. @nightldj @ZacharyGarrett Is there a way to use multiple GPUs for the server right now? Our research code might benefit from that since the majority of our compression bottleneck is happening on the server. If we could use multiple GPUs it would definite be great. @zehao-sean-huang You can pin server computation on *one* GPU by the `server_tf_device` argument.  There is no native TFF way to do it. One possibility I can think of is to write `tf.distribute` code for server computation, but we have never tested the usage before. Hi, I haven't read the entire thread, so I may be missing some context, but just to make sure all bases are covered, I want to mention that typically, TFF orchestration loop looks something along the lines of the following, with the ""distributed"" part of each round modeled as a computation in TFF (first line), and potentially ""potential_other_server_work"" (the second line) being arbitrary Python code that could e.g., use tf.distribute (or any other TF or non-TF APIs). If there's a heavyweight server-side training to be done in-between computation rounds on lots of server-side data, it would make sense to utilize whatever existing non-TFF infrastructure already exists for this purpose, and just invoke it directly from Python.

for ... in ...:
  round_output, ... = computation_round_modeled_in_tff(server_state, ....)
  server_state, ... = potential_other_server_work(round_output, ....)

Speeding up server-side parts of TFF computation proper is definitely on the table, but we'd need the programmer to be able to somehow communicate to TFF the information about what to parallelize (or make it TF runtime's responsibility to parallelize on the fly - but that could be hard without some additional information about the programmer's intent). It would probably be most actionable to unpack the motivation for this in the context of some specific use cases, workloads, and computation patterns.

Natively incorporating all of tf.distribute into TFF is problematic for architectural reasons. TFF wants the programmer to declare logically what to run in a manner that affords some deployment flexibility, and treat deployment details (specific physical devices, distribution strategies, etc.) as a separate concern, handled at a later stage (typically by the runtime). Not all commonly used APIs are compatible with this out of the box.
Please take a look at our colab tutorial for GPU usage https://www.tensorflow.org/federated/tutorials/simulations_with_accelerators. I will close this issue, but feel free to reopen if it is not resolved."	10	2021-01-11 07:22:32	2021-04-17 03:52:17	2021-04-17 03:52:17
https://github.com/tensorflow/federated/issues/972	[]	a problem of parameter	a problem of parameterIn TFF, which parameter defines how many clients are selected for training in each round? TFF version is 0.15GitHub Issues is more of a forum for discussing bugs, feature requests, and other issues with the code/software. The [`tensorflow-federated`](https://stackoverflow.com/questions/tagged/tensorflow-federated) tag on StackOverflow is a better places for questions about how to _use_ TFF, would you mind re-asking your question there?	1	2020-12-21 07:53:26	2020-12-21 15:41:15	2020-12-21 15:41:14
https://github.com/tensorflow/federated/issues/928	['bug']	a problem of import	"a problem of importcannot import name 'computation_pb2
Could we add additional information to the issue? It will greatly help debugging.

- What command-line was used that surfaced this error?
- What versions of TFF was being used?
- What environment was this being run in? (OS, any virtual enviroments, Python version, etc)同问题This is likely a duplicate of https://github.com/tensorflow/federated/issues/770.

Please see https://github.com/tensorflow/federated/issues/770#issuecomment-540058686 and re-open if this does not fix the issue."	3	2020-11-28 07:56:02	2021-03-31 15:18:56	2021-03-31 15:18:56
https://github.com/tensorflow/federated/issues/924	[]	Custom Keras Model with TFF and Optimizers	"Custom Keras Model with TFF and OptimizersHi,

I wanted to know is it possible to use custom keras model with TFF? I am not sure if the training behavior will be ok. I have this model:

```
 class CustomModel(tf.keras.Model):
  def train_step(self, data):
      x, y = data

      with tf.GradientTape() as tape:
          y_pred = self(x, training=True)
          loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)

      trainable_vars = self.trainable_variables
      gradients = tape.gradient(loss, trainable_vars)
      self.optimizer.apply_gradients(zip(gradients, trainable_vars))
      self.compiled_metrics.update_state(y, y_pred)
      return {m.name: m.result() for m in self.metrics}
```

```
def create_keras_model():
  inputs = tf.keras.Input(shape=(784,))
  outputs = tf.keras.layers.Dense(10, activation=""softmax"")(inputs)
  model = CustomModel(inputs, outputs)
  return model
```

```
def model_fn():
  keras_model = create_keras_model()
  return tff.learning.from_keras_model(
      keras_model,
      input_spec=preprocessed_example_dataset.element_spec,
      loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
```

Another question is there are 2 optimizers:
```
iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))
```
In the earlier versions of TFF there used to only one optimizer (on client). I read the Federated Learning paper, but did not understand why do we need to have an optimizer on server side. Could you please share some paper or relevant doc which provides information on this? It looks like the code wants to a custom training loop for the ""inner"" (on client) optimizer. Rather than implement this inside the Model, implementing this inside the training `tff.templates.IterativeProcess` maybe more natural?

There is a simplified, flat view of a Federatd Averaging implementation in 
https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/examples/simple_fedavg

The client training loop is here: https://github.com/tensorflow/federated/blob/826b66f3f0964bc3dec14e30286b5412bfeffb64/tensorflow_federated/python/examples/simple_fedavg/simple_fedavg_tf.py#L192
It might be worth trying to fork that code and modify?

Side note on subclassed `tf.keras.Model`: they are not supported. This is because TFF requires being able to serialize all computation logic, as the code is intended to run on edge devices. Unfortunately, Keras' subclass models do not support this, from https://keras.io/guides/model_subclassing (which now seems to be 404) use to say: 

> However, in subclassed models, the model's topology is defined as Python code (rather than as a static graph of layers). That means the model's topology cannot be inspected or serialized. As a result, the following methods and attributes are not available for subclassed models:
>
> model.inputs and model.outputs. model.to_yaml() and model.to_json() model.get_config() and model.save().

https://keras.io/guides/making_new_layers_and_models_via_subclassing/

In general we recommend Keras' Functional API for models: https://keras.io/guides/functional_api/Thanks @ZacharyGarrett  for the quick response. 

> ""It looks like the code wants to a custom training loop for the ""inner"" (on client) optimizer. Rather than implement this inside the Model, implementing this inside the training tff.templates.IterativeProcess maybe more natural?""

It seems like TFF is going in the opposite direction of TF2.0 where it is highly marketed to override train_step function. I tested the code I posted earlier with [this TFF tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) and it seems to be working fine and indeed I'm using a Keras' Functional API. 

Do you still recommend using [simple_fedavg](https://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/examples/simple_fedavg)?"	2	2020-11-10 12:35:56	2020-11-16 18:47:01	2020-11-16 18:47:01
https://github.com/tensorflow/federated/issues/922	[]	Non-IID data distribution for a custom dataset	"Non-IID data distribution for a custom datasetCurrent built in API's support only few datasets in a Non-IID setting. How can we create a Non-IID data distribution for a custom text dataset?@avinashsai I have the same problem.@CSJDeveloper, @avinashsai 

There are a few different ways that this can be done, generally depending on the data you have now. [This StackOverflow post](https://stackoverflow.com/questions/58965488/how-to-create-federated-dataset-from-a-csv-file) may be useful.

One thing to note--if your existing dataset does not contain a natural user-keyed structure, you will have to first determine a strategy for creating synthetic non-iid-ness. TFF distributes a [synthetic partitioning of CIFAR100](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/cifar100/load_data) which may be helpful as inspiration.

Generally, we prefer using StackOverflow for usage questions and federated computing questions in general, and reserving GitHub issues for bugs. This is an excellent question! Closing, however, for this reason."	2	2020-11-09 10:57:19	2020-12-07 05:05:35	2020-12-07 05:05:35
https://github.com/tensorflow/federated/issues/920	[]	ValueError: Expected a TensorFlow computation, got lambda.	"ValueError: Expected a TensorFlow computation, got lambda.I run this line

```{python}
# Test the TFF is working:
tff.federated_computation(lambda: 'Hello, World!')()
```

and this error occurs: 

```---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-68-a116efda9633> in <module>()
     11 
     12 # Test the TFF is working:
---> 13 tff.federated_computation(lambda: 'Hello, World!')()

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/tensorflow_deserialization.py in deserialize_and_call_tf_computation(computation_proto, arg, graph)
     65   if computation_oneof != 'tensorflow':
     66     raise ValueError(
---> 67         'Expected a TensorFlow computation, got {}.'.format(computation_oneof))
     68   py_typecheck.check_type(graph, tf.Graph)
     69   with graph.as_default():

ValueError: Expected a TensorFlow computation, got lambda.
```

It occurs when I run [Federated_Learning_for_Image_Classification.ipynb](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb#scrollTo=-gm-yx2Mr_bl) in Google Colab today. I think there's something wrong with Google Colab or tff yesterday. 
This line can run properly today. I had the same issue. I used `del interactive_process` to delete all the results of `tff.learning.build_federated_averaging_process` from my notebook, and I didn't run into the issue again."	2	2020-11-03 08:49:02	2020-12-28 22:03:27	2020-11-04 13:21:02
https://github.com/tensorflow/federated/issues/903	['bug']	Memory explosion	"Memory explosionWhile running the following command:
`bazel run main:federated_trainer -- --task=cifar100 --total_rounds=1500 --client_optimizer=sgd --client_learning_rate=0.1 --client_batch_size=20 --server_optimizer=sgd --server_learning_rate=1 --clients_per_round=10 --client_epochs_per_round=1 --experiment_name=cifar100_fedavg`
the VRAM usage explodes.
It quickly reaches 8.9 GB in the first round. It remains stable, until around the 125. round, when it doubles to 16.9 GB. This phenomenon repeats itself until the training crashes.Hi @matech96. Can you verify that you are using the latest version of TFF? If so, can you see what happens when you set `clients_per_round=1`?Hi, @zcharles8! I'm using version 0.16.1, which is the latest as I can tell. It is reproducible with `clients_per_round=1` as well. It starts off with 4.9 GB, then around the 70. round, jumps to 8.9 GB.I also got into this memory explosion issue when running TFF 0.16.1 on GPU (a single GPU). I would be happy to provide more details if neededI am marking this as resolved (see google-research/federated#29 for context)."	4	2020-09-08 06:26:24	2021-04-09 17:26:28	2021-04-09 17:26:28
https://github.com/tensorflow/federated/issues/902	['bug']	Can't run 'federated-learning-for-text-generation.ipynb' colab	"Can't run 'federated-learning-for-text-generation.ipynb' colabI tried to run the federated_learning_for_text_generation.ipynb but I am getting an error when running the cell 

```
import collections
import functools
import os
import time

import numpy as np
import tensorflow as tf
import tensorflow_federated as tff

tf.compat.v1.enable_v2_behavior()

np.random.seed(0)

# Test the TFF is working:
tff.federated_computation(lambda: 'Hello, World!')()
```

```
RuntimeError                              Traceback (most recent call last)
<ipython-input-1-8e1fcc795c0f> in <module>()
     13 
     14 # Test the TFF is working:
---> 15 tff.federated_computation(lambda: 'Hello, World!')()

8 frames
/usr/lib/python3.6/asyncio/base_events.py in run_forever(self)
    426         if events._get_running_loop() is not None:
    427             raise RuntimeError(
--> 428                 'Cannot run the event loop while another loop is running')
    429         self._set_coroutine_wrapper(self._debug)
    430         self._thread_id = threading.get_ident()

RuntimeError: Cannot run the event loop while another loop is running
```Try #842 ?Great, thanks. I will try it out.Closing this; thank you @hongliny for referencing #842!"	3	2020-09-07 13:11:28	2020-10-29 05:08:07	2020-10-29 05:08:07
https://github.com/tensorflow/federated/issues/892	['bug']	Got error when run personalization experiment	"Got error when run personalization experiment**Describe the bug**
A clear and concise description of what the bug is. It is often helpful to
provide a link to a [colab](https://colab.research.google.com/) notebook that
reproduces the bug.

**Environment (please complete the following information):**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
* Python package versions (e.g., TensorFlow Federated, TensorFlow): Tensoflow-gpu 2.2, federated 0.16.1(From pypi)
* CUDA/cuDNN version: 10.1
I tried to run personalization experiment unittest but I got some errors. [https://github.com/tensorflow/federated/blob/993493122ea908173eba68525482d5d522db236c/tensorflow_federated/python/research/personalization/p13n_utils_test.py](https://github.com/tensorflow/federated/blob/993493122ea908173eba68525482d5d522db236c/tensorflow_federated/python/research/personalization/p13n_utils_test.py#L1)

```
Testing started at 0:33 ...
ssh://liuyuan@122.207.82.54:14000/home/liuyuan/programs/miniconda3/envs/tf2/bin/python -u /home/liuyuan/.pycharm_helpers/pycharm/_jb_unittest_runner.py --target p13n_utils_test.P13NUtilsTest.test_build_personalize_fn_succeeds_with_valid_args
Launching unittests with arguments python -m unittest p13n_utils_test.P13NUtilsTest.test_build_personalize_fn_succeeds_with_valid_args in /home/liuyuan/shu_codes/federated_learning_tff/personalization

2020-08-13 00:33:25.201227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-08-13 00:33:33.437511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-13 00:33:33.437864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-13 00:33:33.440622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-13 00:33:33.443281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-13 00:33:33.443706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-13 00:33:33.446540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-13 00:33:33.448197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-13 00:33:33.454645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-13 00:33:33.456487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-13 00:33:33.458319: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-08-13 00:33:33.505750: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2100000000 Hz
2020-08-13 00:33:33.510571: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d7f3df9770 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-13 00:33:33.510589: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-13 00:33:33.511533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-13 00:33:33.511581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-13 00:33:33.511598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-13 00:33:33.511614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-13 00:33:33.511631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-13 00:33:33.511647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-13 00:33:33.511663: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-13 00:33:33.511680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-13 00:33:33.513154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-13 00:33:33.513194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-13 00:33:33.624360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-13 00:33:33.624391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-08-13 00:33:33.624397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-08-13 00:33:33.626161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10203 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5)
2020-08-13 00:33:33.628552: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d7f7d467b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-13 00:33:33.628570: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:718: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  if not isinstance(wrapped_dict, collections.Mapping):
WARNING:tensorflow:From /home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.

Error
Traceback (most recent call last):
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/unittest/case.py"", line 59, in testPartExecutor
    yield
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/unittest/case.py"", line 628, in run
    testMethod()
  File ""/home/liuyuan/shu_codes/federated_learning_tff/personalization/p13n_utils_test.py"", line 71, in test_build_personalize_fn_succeeds_with_valid_args
    p13n_metrics = p13n_fn(model=model, train_data=dataset, test_data=dataset)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 644, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1665, in _filtered_call
    self.captured_inputs)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1746, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 598, in call
    ctx=ctx)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node test_data/_32}}]]
	 [[Func/while/body/_11/input/_134/_52]]
  (1) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node test_data/_32}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_personalize_fn_1025]

Function call stack:
personalize_fn -> personalize_fn
```@ddayzzz Could you update TF to 2.3 to see if the error still exists? Thanks!> @ddayzzz Could you update TF to 2.3 to see if the error still exists? Thanks!

I upgrade tensorflow(gpu version) to 2.3 from pypi, but i still got the same errors.
```
Launching unittests with arguments python -m unittest p13n_utils_test.P13NUtilsTest.test_build_personalize_fn_succeeds_with_valid_args in /home/liuyuan/shu_codes/federated_learning_tff/personalization

2020-08-15 18:30:48.587939: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). 
 The versions of TensorFlow you are currently using is 2.3.0 and is not supported. 
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. 
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  UserWarning,
2020-08-15 18:30:49.779733: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-08-15 18:30:57.406770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:b2:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-15 18:30:57.406905: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-15 18:30:57.412322: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-15 18:30:57.415537: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-15 18:30:57.415976: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-15 18:30:57.418986: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-15 18:30:57.420530: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-15 18:30:57.426628: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-15 18:30:57.428847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-15 18:30:57.430272: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-15 18:30:57.478005: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz
2020-08-15 18:30:57.486456: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56144dbf0470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-15 18:30:57.486533: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-15 18:30:57.637296: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56144fcc6590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-15 18:30:57.637361: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-08-15 18:30:57.640469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:b2:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-15 18:30:57.640593: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-15 18:30:57.640646: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-15 18:30:57.640677: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-15 18:30:57.640707: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-15 18:30:57.640737: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-15 18:30:57.640766: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-15 18:30:57.640797: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-15 18:30:57.644716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-15 18:30:57.644789: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-15 18:30:58.383153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-15 18:30:58.383202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-08-15 18:30:58.383211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-08-15 18:30:58.384784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10066 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:b2:00.0, compute capability: 7.5)

Error
Traceback (most recent call last):
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/unittest/case.py"", line 59, in testPartExecutor
    yield
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/unittest/case.py"", line 628, in run
    testMethod()
  File ""/home/liuyuan/shu_codes/federated_learning_tff/personalization/p13n_utils_test.py"", line 75, in test_build_personalize_fn_succeeds_with_valid_args
    loop()
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/home/liuyuan/programs/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node BatchDatasetV2/_18}}]]
  (1) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper
	 [[{{node BatchDatasetV2/_18}}]]
	 [[Func/while/body/_1/input/_51/_46]]
0 successful operations.
0 derived errors ignored. [Op:__inference_loop_85]

Function call stack:
loop -> loop



Assertion failed


Ran 1 test in 9.051s

FAILED (errors=1)

Process finished with exit code 1

Assertion failed

Assertion failed

```@ddayzzz Sorry that you still hit the issue. Could you try tf-nightly if possible? We cannot currently reproduce the issue, but I think I have seen it before. It is a bug inherited from TF on GPUs, and the bug has been fixed in TF. I am not sure if the fix has been in the released version yet, but TF-nightly should be good. > @ddayzzz Sorry that you still hit the issue. Could you try tf-nightly if possible? We cannot currently reproduce the issue, but I think I have seen it before. It is a bug inherited from TF on GPUs, and the bug has been fixed in TF. I am not sure if the fix has been in the released version yet, but TF-nightly should be good.

Install `tf-nightly` or `tf-nightly-gpu` ?> @ddayzzz Sorry that you still hit the issue. Could you try tf-nightly if possible? We cannot currently reproduce the issue, but I think I have seen it before. It is a bug inherited from TF on GPUs, and the bug has been fixed in TF. I am not sure if the fix has been in the released version yet, but TF-nightly should be good.

Thank you. tensorflow-federated works properly under tf 2.3.0 and tf-nightly."	5	2020-08-13 01:53:39	2020-08-16 08:36:49	2020-08-16 08:36:49
https://github.com/tensorflow/federated/issues/884	['bug']	asyncio loop error in Google Colab	"asyncio loop error in Google ColabSo as of today I've been getting the following asyncio error when running in Google Colab:

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-6-a23308ec3f7c> in <module>()
      7 np.random.seed(0)
      8 
----> 9 tff.federated_computation(lambda: 'Hello, World!')()

8 frames
/usr/lib/python3.6/asyncio/base_events.py in run_forever(self)
    426         if events._get_running_loop() is not None:
    427             raise RuntimeError(
--> 428                 'Cannot run the event loop while another loop is running')
    429         self._set_coroutine_wrapper(self._debug)
    430         self._thread_id = threading.get_ident()

RuntimeError: Cannot run the event loop while another loop is running
```

It suddenly showed up in a project which has been working fine before. 
When running the tutorial `Federated_Learning_for_Image_Classification.ipynb` notebook, it also shows up in the first 'Hello world' federated test computation. I tried several types of runtimes and factory reset of the runtime. 

Since it can easily be replicated; anyone have an idea why this suddenly shows up?Colab has recently updated their runtime to use a new version of various dependencies, including tornado. This has hopefully been fixed in `master` as of #1f9daf6

Does this still occur when visiting https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynbThis new notebook w/ the asyncio patch seems to work! 
Might want to change the 'run in colab' link at https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification cause it still directs to the v0.15.0 notebook without the patch.

Thanks anyway!Thanks for confirming. Yup, we'll look at updating the tensorflow.org links.Please check out v0.16.0, or master in GitHub. Also the published tutorials should be pointing at master now."	4	2020-07-27 15:55:54	2020-07-30 03:42:15	2020-07-30 03:42:14
https://github.com/tensorflow/federated/issues/881	['bug']	Docs of traced functions link to wrong source code location	"Docs of traced functions link to wrong source code location**Describe the bug**
Traced methods/functions have broken links to source code on tensorflow.org/federated/api_docs. For example, in [this method](https://www.tensorflow.org/federated/api_docs/python/tff/framework/EagerTFExecutor#create_call), clicking the `View Source` button redirects me to a [wrapper function](url) `async_trace` local to the `tracing.trace` decorator. The same is true of all other traced functions I tried this with.

**Expected behavior**
I would expect the above to direct me to [this place](https://github.com/tensorflow/federated/blob/v0.15.0/tensorflow_federated/python/core/impl/executors/eager_tf_executor.py#L467-L497) in the GH source code
I am working on a fix for this."	1	2020-07-21 15:45:16	2020-08-03 20:22:19	2020-08-03 20:22:19
https://github.com/tensorflow/federated/issues/860	['bug']	tf_computation returns garbage when used with `**` operator	"tf_computation returns garbage when used with `**` operator**Describe the bug**
```python
@tff.tf_computation(tf.int32) # or tf.float32
def pow10(n):
    return n ** 10

pow10(10) # returns garbage (different wrong values)
```
The code above does not throw any error, but quietly returns garbage
(even if `**` is not supported, it should throw an error)

**Environment (please complete the following information):**
* OS Platform and Distribution: windows 10
* Python package versions (e.g., TensorFlow Federated, TensorFlow):
tensorflow@2.2 tff@0.14
* Python version: 3.7
This seems to be integer overflow unrelated to TFF -- using `tf.int64` works as I would expect in this case.🤦‍♂️ sorry"	2	2020-06-01 09:42:02	2020-06-03 09:08:19	2020-06-03 07:52:40
https://github.com/tensorflow/federated/issues/854	[]	A problem of evaluation	"A problem of evaluationI'd like to use my own model and dataset to try TFF, when evaluating ,it taks a long time and I can't see the result. Is there any log file or something I can track the problem?
My dataset is the smart information of hard-drive.


![image](https://user-images.githubusercontent.com/65224044/82646930-f8dc0000-9c47-11ea-8dfa-842c71bfeeee.png)

It seems to me that this question might be best suited on StackOverflow, as it seems more like a usage question than a bug report or feature request. Would you mind posting there and tagging with tensorflow-federated?I have solved this problem.  When I run my code  by using "" python   ***.py"" ,it's ok"	2	2020-05-22 08:42:25	2020-05-25 09:55:19	2020-05-25 09:55:19
https://github.com/tensorflow/federated/issues/851	['bug']	Performance drops significantly with 2 federated code	"Performance drops significantly with 2 federated codeHi there, 
I am using federated setting for my customize dataset. I try the same data, same model (load pretrained model). However, results are very different between two code as follow:
1. 41.9% accuracy with `https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb`
2. 17% accuracy with `https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/optimization/shakespeare/run_federated.py`
I was trying different configurations for each model and the accuracy reported here is the best for each model.
I wonder whether the way of aggregating and assigning weights between clients and server are different between these code that makes the performances different.  
Have anyone got the same issues and is there any reason ? ThanksHi @PhungLai728. Can you describe what hyperparameters you were using with the two approaches? The choice of hyperparameters can make a huge difference, especially the choice of learning rates.

As you note, the tutorial uses a pre-trained RNN. However, currently `shakespeare/run_federated.py` trains from scratch. Did you also use the pre-trained model in `shakespeare/run_federated.py`?

As for differences in the code, the tutorial uses `tff.learning.build_federated_averaging_process` while `shakespeare/run_federated.py` uses `https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/research/optimization/shared/fed_avg_schedule.py`. While there are a few subtle differences between them, the primary difference is that the latter allows for learning rate scheduling. In the absence of using that, the two should perform comparably, barring randomness due to which clients are sampled in each round.Hi there, 
For both, I used pretrained model. And I mostly changed the learning rate for each model and choose the best one. The learning rate I tried is [0.01, 0.05, 0.1] for both. I also try learning rate schedule for the `tutorials/federated_learning_for_text_generation.ipynb`. My dataset has 208 users, and I used 50 clients for each training step when using `run_federated.py`. 
Thanks for the follow-up. It's difficult to tell what's happening without seeing code snippets, but `shakespeare/run_federated.py` has a whole suite of flags with specific default values, so it could be that there's still some difference in configuration between the two.

That being said, `shakespeare/run_federated.py` was designed for a specific dataset and model. If you're finding success in using the tutorial's setup, I would encourage you to continue using that  to train your model.

Currently, I don't see any indication of a bug in the `shakespeare/run_federated.py`, but please let me know if you still believe there is one.Hi @PhungLai728. I'm closing the issue for now, but if you see evidence that there is a bug in `shakespeare/run_federated.py`, please feel free to reopen this issue."	4	2020-05-19 12:49:36	2020-05-26 18:41:32	2020-05-26 18:41:31
https://github.com/tensorflow/federated/issues/846	[]	Possible bug in cifar100 data loading module	"Possible bug in cifar100 data loading modulehttps://github.com/tensorflow/federated/blob/269e70d9a0ecc5b73f38cab7403aae9676dbc9e2/tensorflow_federated/python/research/optimization/cifar100/dataset.py#L37

Given that 'crop_shape' is set as
`crop_shape = CIFAR_SHAPE = (32, 32, 3)`

The references line seems to crop the image to shape [32, 3] instead of [32, 32]. Perhaps the code should instead be

`image, target_height=crop_shape[0], target_width=crop_shape[1]) `
Hi Praneeth! Thanks for the close investigation. Further down, you can see that the batch size is put at the beginning of the train image shape: 

https://github.com/tensorflow/federated/blob/269e70d9a0ecc5b73f38cab7403aae9676dbc9e2/tensorflow_federated/python/research/optimization/cifar100/dataset.py#L77-L82

We also have some tests that make sure we have the right crop size: https://github.com/tensorflow/federated/blob/269e70d9a0ecc5b73f38cab7403aae9676dbc9e2/tensorflow_federated/python/research/optimization/cifar100/dataset_test.py#L38-L46

We appreciate extra eyes on this code, so please let us know if you do think there's any other issues."	1	2020-05-13 12:41:43	2020-05-14 16:04:43	2020-05-14 16:04:43
https://github.com/tensorflow/federated/issues/837	['bug']	User-level DP with emnist and stackoverflow not running!	"User-level DP with emnist and stackoverflow not running!I'd like to reproduce the results of User-level DP in the Learning Differentially Private Recurrent Language Models paper [H. Brendan McMahan et al. ICLR 2018]. But the Reddit dataset is not publicly available. So I try with emnist and stackoverflow that are provided in the implementation.

But I am not able to run the model with these datasets. (I am using the latest version tff 0.13.1 &	tensorflow 2.1.0).
1. With stackoverflow, I try to run with `bazel run //tensorflow_federated/..../stackoverflow:run_federated`. But after downloading the data, it stucks there for a while, then it shows an error `RuntimeError: generator raised StopIteration`. 
+ I am not sure how to make it run. Could you please help?
+ How can I adapt the code with other datasets and models (not dataset provided in the code) ? Is there a function to preprocess the data so that we obtain the same format with dataset in `simulation` ?

2. With emnist, I try to run directly with Python, and it 'seems to work'. But when it runs to `training_loop`, when it called `py_typecheck`, it raises an error `TypeError: Expected anonymous_tuple.AnonymousTuple, found tensorflow_federated.python.common_libs.anonymous_tuple.AnonymousTuple.`
+ How can I pass this error? 

3. I searched around and found this github `https://github.com/TalwalkarLab/leaf`. Is the Reddit data here as same as Reddit data used in the [H. Brendan McMahan et al. ICLR 2018] ? 

Thanks!Could you try the following for EMNIST/Stackoverflow for sanity check?

bazel run run_federated -- --clients_per_round 2 --uniform_weighting --noise_multiplier 0.1  --total_rounds 2 --client_optimizer sgd --client_learning_rate 0.1 --server_optimizer sgd --server_learning_rate 1.0 --root_output_dir /tmp/dp-opt/emnist --experiment_name debugI am able to make it run. Thanks."	2	2020-04-28 03:12:52	2020-05-04 03:14:42	2020-05-04 03:14:42
https://github.com/tensorflow/federated/issues/820	[]	Issue when running stackoverflow_lr	"Issue when running stackoverflow_lrWhen I tried to run the experiment of stackoverflow_lr:

> bazel run stackoverflow_lr:run_federated -- --total_rounds=1500 --client_optimizer=sgd --client_learning_rate=0.5 --server_optimizer=adagrad --server_learning_rate=10 --clients_per_round=10 --client_batch_size=100 --client_epochs_per_round=10 --experiment_name=stackoverflow_fedadagrad_experiment_05_10 --root_output_dir=/export/UserData/hzy/tmp/fed_opt/ --server_adagrad_initial_accumulator_value=0.1 --server_adagrad_epsilon=0.01


I have the following issue: 

> Traceback (most recent call last):                                                                                                                                                                          
>   File ""/home/hzy/.cache/bazel/_bazel_hzy/86f4822ac0d3e01f42858069fe654bbe/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_fed
> erated.runfiles/org_tensorflow_federated/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_federated.py"", line 125, in <module>                                                        
>     app.run(main)                                                                                                                                                                                           
>   File ""/home/hzy/anaconda3/envs/PY3/lib/python3.6/site-packages/absl/app.py"", line 299, in run                                                                                                             
>     _run_main(main, args)                                                                                                                                                                                   
>   File ""/home/hzy/anaconda3/envs/PY3/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main                                                                                                       
>     sys.exit(main(argv))                                                                                                                                                                                    
>   File ""/home/hzy/.cache/bazel/_bazel_hzy/86f4822ac0d3e01f42858069fe654bbe/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_fed
> erated.runfiles/org_tensorflow_federated/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_federated.py"", line 75, in main                                                             
>     num_validation_examples=FLAGS.num_validation_examples)                                                                                                                                                  
>   File ""/home/hzy/.cache/bazel/_bazel_hzy/86f4822ac0d3e01f42858069fe654bbe/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_fed
> erated.runfiles/org_tensorflow_federated/tensorflow_federated/python/research/optimization/stackoverflow_lr/dataset.py"", line 117, in get_stackoverflow_datasets                                            
>     stackoverflow_train, _, stackoverflow_test = tff.simulation.datasets.stackoverflow.load_data(                                                                                                           
>   File ""/home/hzy/.cache/bazel/_bazel_hzy/86f4822ac0d3e01f42858069fe654bbe/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_fed
> erated.runfiles/org_tensorflow_federated/tensorflow_federated/python/simulation/datasets/stackoverflow.py"", line 109, in load_data                                                                          
>     os.path.join(dir_path, 'stackoverflow_train.h5'))                                                                                                                                                       
>   File ""/home/hzy/.cache/bazel/_bazel_hzy/86f4822ac0d3e01f42858069fe654bbe/execroot/org_tensorflow_federated/bazel-out/k8-opt/bin/tensorflow_federated/python/research/optimization/stackoverflow_lr/run_fed
> erated.runfiles/org_tensorflow_federated/tensorflow_federated/python/simulation/hdf5_client_data.py"", line 50, in __init__                                                                                  
>     self._h5_file = h5py.File(self._filepath, ""r"")                                                                                                                                                          
>   File ""/home/hzy/anaconda3/envs/PY3/lib/python3.6/site-packages/h5py/_hl/files.py"", line 312, in __init__                                                                                                  
>     fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)                                                                                                                                             
>   File ""/home/hzy/anaconda3/envs/PY3/lib/python3.6/site-packages/h5py/_hl/files.py"", line 142, in make_fid                                                                                                  
>     fid = h5f.open(name, flags, fapl=fapl)                                                                                                                                                                  
>   File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper                                                                                                                                     
>   File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper                                                                                                                                     
>   File ""h5py/h5f.pyx"", line 78, in h5py.h5f.open                                                                                                                                                            
> OSError: Unable to open file (unable to open file: name = '/home/hzy/.keras/stackoverflow_train.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)                        
> 

tf version: 2.1.0 
tff version: 0.13.1 

Is this a bug caused by tf and tff mismatch @ZacharyGarrett ?
I solved this issue by changing this line: https://github.com/tensorflow/federated/blob/d145e66ebb113423a3c921621e1b11957642ad24/tensorflow_federated/python/research/optimization/stackoverflow_lr/dataset.py#L53 to: words = tf.strings.split(tf.strings.substr(sentence, 0, MAX_SEQ_LEN), sep=' ')
and https://github.com/tensorflow/federated/blob/d145e66ebb113423a3c921621e1b11957642ad24/tensorflow_federated/python/research/optimization/stackoverflow_lr/dataset.py#L59 to: tags = tf.strings.split(tags, sep='|')

Those versions of TFF and TF should be compatible.

@zcharles8 @jkr26 have written a lot of the code in this directory, they might have ideas?@zcharles8 just pushed [this change](https://github.com/tensorflow/federated/commit/22c89fac4aa9e92e591a7de43e705773215986b2), which should have fixed this problem. @slowbull, could you confirm?Hi @slowbull, just wanted to ping this thread again. I've since attempted to make sure that using the code internally and externally produced the same results, and things look good as far as I can tell. Have your issues been resolved?Sorry for the late reply. It works.  @jkr26 @zcharles8"	4	2020-04-03 15:34:32	2020-04-25 04:29:19	2020-04-25 04:29:19
https://github.com/tensorflow/federated/issues/819	[]	AttributeError: type object 'DatasetV2' has no attribute 'TextLineDataset'	"AttributeError: type object 'DatasetV2' has no attribute 'TextLineDataset'Hi, I am following the tutorial 'Federated Learning for Image Classification', but using my own dataset. I got this error.

Each .csv has a 3 column dataset which I'm trying to simulate as a client data for testing federated learning.

here is my code:
```
tf.compat.v1.enable_v2_behavior()
import tensorflow_federated as tff
dataset_paths = {
  'client_0': '/content/drive/My Drive/tmp/a.csv',
  'client_1': '/content/drive/My Drive/tmp/b.csv',
  'client_2': '/content/drive/My Drive/tmp/c.csv',
}

def create_tf_dataset_for_client_fn(id):
   path = dataset_paths.get(id)
   if path is None:
     raise ValueError(f'No dataset for client {id}')
   return tf.data.Dataset.TextLineDataset(path)

source = tff.simulation.ClientData.from_clients_and_fn(
  ['client_0','client_1','client_2'], create_tf_dataset_for_client_fn)
```


I got the error

```
AttributeError                            Traceback (most recent call last)
<ipython-input-9-b3879bd82bf0> in <module>()
     14 
     15 source = tff.simulation.ClientData.from_clients_and_fn(
---> 16   ['client_0','client_1','client_2'], create_tf_dataset_for_client_fn)

2 frames
<ipython-input-9-b3879bd82bf0> in create_tf_dataset_for_client_fn(id)
     11    if path is None:
     12      raise ValueError(f'No dataset for client {id}')
---> 13    return tf.data.Dataset.TextLineDataset(path)
     14 
     15 source = tff.simulation.ClientData.from_clients_and_fn(

AttributeError: type object 'DatasetV2' has no attribute 'TextLineDataset'
```

my environment
tensorboard == '2.2.0'
tensorflow == '2.2.0-rc2'
tensorflow_federated == '0.13.1'
Just to be clear I also tried doing @ZacharyGarrett 's solution on [Stack Overflow](https://stackoverflow.com/questions/60265798/tff-how-define-tff-simulation-clientdata-from-clients-and-fn-function) 
i.e
```
dataset_paths = {
  'client_0': '/tmp/A.csv',
  'client_1': '/tmp/B.csv',
  'client_2': '/tmp/C.csv',
}

def create_tf_dataset_for_client_fn(id):
   path = dataset_paths.get(id)
   if path is None:
     raise ValueError(f'No dataset for client {id}')
   return tf.data.Dataset.TextLineDataset(path)

source = tff.simulation.ClientData.from_clients_and_fn(
  dataset_paths.keys(), create_tf_dataset_for_client_fn)
```


But I got the following error

```
TypeError                                 Traceback (most recent call last)
<ipython-input-13-3fa09c31a2b3> in <module>()
     12 
     13 source = tff.simulation.ClientData.from_clients_and_fn(
---> 14   dataset_paths.keys(), create_tf_dataset_for_client_fn)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/common_libs/py_typecheck.py in check_type(target, type_spec, label)
     42     raise TypeError('Expected {}{}, found {}.'.format(
     43         '{} to be of type '.format(label) if label is not None else '',
---> 44         type_string(type_spec), type_string(type(target))))
     45   return target
     46 

TypeError: Expected list, found dict_keys.
```> `TypeError: Expected list, found dict_keys.`

This is happening because `ClientData.from_clients_and_fn` takes a `list` of keys, and the code in your example passes `dataset_paths.keys()`. I'd try turning that into a `list`, like this: `list(dataset_paths.keys())`.> > `TypeError: Expected list, found dict_keys.`
> 
> This is happening because `ClientData.from_clients_and_fn` takes a `list` of keys, and the code in your example passes `dataset_paths.keys()`. I'd try turning that into a `list`, like this: `list(dataset_paths.keys())`.

Yes, I understand that! But it again goes to the main issue that I'm facing 

`AttributeError: type object 'DatasetV2' has no attribute 'TextLineDataset'` 
The code appears to use `tf.data.Dataset.TextLineDataset`, but the tensorflow.org documentation seems to say the path is [`tf.data.TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset). Does removing the extra `.Dataset` part help?@kinetickansra Please reopen this issue if @ZacharyGarrett suggestion did not fix your issue."	5	2020-04-01 15:01:53	2020-04-14 16:42:28	2020-04-14 16:42:28
https://github.com/tensorflow/federated/issues/815	[]	Preprocessing error: TypeError: <lambda>() takes 1 positional argument but 2 were given	"Preprocessing error: TypeError: <lambda>() takes 1 positional argument but 2 were givenHi,

I am attempting the duplicate the EMNIST tutorial with another dataset by using tf.data to read data from tf keras image data generators. The issue arises when mapping the batch_format_fn to each batch of data as the mapping method passes 2 arguments to the batch_format_fn rather than the 1 'element' argument. I inspected the 2 arguments that are getting passed to the batch_format_fn and they were: 

```python
Tensor(""args_0:0"", dtype=float32)
Tensor(""args_1:0"", dtype=float32)
```

But when the same arguments are inspected when running the EMNIST example, it is one argument:

```python
OrderedDict([('label', <tf.Tensor 'args_0:0' shape=() dtype=int32>), ('pixels', <tf.Tensor 'args_1:0' shape=(28, 28) dtype=float32>)])
```

Below is the error:
```python
Traceback (most recent call last):
  File ""src/federated_learning/federated_learning_model/model.py"", line 189, in <module>
    prefetch_buffer
  File ""src/federated_learning/federated_learning_model/model.py"", line 47, in __init__
    prefetch_buffer)
  File ""src/federated_learning/federated_learning_model/model.py"", line 157, in _get_sample_batch
    revert_one_hot=False)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/src/federated_learning/federated_learning_model/tff_utils.py"", line 61, in preprocess
    mapped_dataset = shuffled_dataset.map(batch_format_fn)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 1588, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 3888, in __init__
    use_legacy_function=use_legacy_function)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 3147, in __init__
    self._function = wrapper_fn._get_concrete_function_internal()
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2395, in _get_concrete_function_internal
    *args, **kwargs)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 3140, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 3082, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
  File ""/Users/cian.ohagan/Documents/federated_learning/project_repo/fl_env/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 237, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:


    TypeError: tf__batch_format_fn() takes 1 positional argument but 2 were given
```

The code below is extracted from the class that I am creating but has all the steps that are leading to the issue.

```python
    def read_client_data(path,
                         img_h,
                         img_w,
                        batch_size,
                        class_mode,
                        dataset='train'):
       path = os.path.join(path, dataset)


        def make_generator():
            train_datagen = image.ImageDataGenerator(rescale=1. / 255)
            train_generator = train_datagen.flow_from_directory(path,
                                                                target_size=[img_h, img_w],
                                                                class_mode=class_mode,
                                                                batch_size=batch_size)
            return train_generator

        train_dataset = tf.data.Dataset.from_generator(
            generator=make_generator,
            output_types=(tf.float32, tf.float32)
        )

        return train_dataset


    def batch_format_fn(element):
        """"""Flatten a batch `pixels` and return the features as an `OrderedDict`.""""""
        # batch = collections.OrderedDict(x=tf.reshape(element['pixels'], [-1, 784]),
        #                                 y=tf.reshape(element['label'], [-1, 1]))
        print('Running batch format ')
        print(element)
        batch = collections.OrderedDict(
            x=tf.reshape(element['pixels'], element['pixels'].shape),
            y=tf.reshape(element['label'], [-1, 1])
        )
        return batch


    def preprocess(dataset,
                   num_epochs,
                   shuffle_buffer,
                   batch_size,
                   prefetch_buffer,
                   revert_one_hot=False):
        # Repeating dataset count times
        repeated_dataset = dataset.repeat(count=num_epochs)
        # Randomly shuffling elements from dataset of size shuffle_buffer
        shuffled_dataset = repeated_dataset.shuffle(shuffle_buffer)
        # Combines consecutive elements of dataset into batches
        # Removed as the data is already in batches
        # batch = shuffled_dataset.batch(batch_size)
        # THE LINE BELOW THROWS THE ERROR
        mapped_dataset = shuffled_dataset.map(batch_format_fn)
        # mapped_dataset = shuffled_dataset.map(lambda self, x: batch_format_fn(x))
        if revert_one_hot:
            # Converting labels to ints
            mapped_dataset = mapped_dataset.map(lambda i, l: (i, tf.argmax(l, axis=1)))
        processed_dataset = mapped_dataset.prefetch(prefetch_buffer)
    return processed_dataset

    client_data = [read_client_data(os.path.join(path_to_data, client),
                                            self.img_h,
                                            self.img_w,
                                            self.batch_size,
                                            self.class_mode)
                              for client in clients]

    preprocessed_data = [preprocess(dataset,
                                            num_epochs,
                                            shuffle_buffer,
                                            batch_size,
                                            prefetch_buffer,
                                            revert_one_hot=False)
                                 for dataset in client_data]
```The code in question is only using `tensorflow` core APIs (specifically setting up a `tf.data` pipeline), no `tensorflow_federated` or `tff` API. Looks like a usage question, rather than a issue/bug with Tensorflow Federated.

The best place for such questions is StackOverflow using the `tensorflow` tag (https://stackoverflow.com/questions/tagged/tensorflow). Could you re-post this question there?"	1	2020-03-27 13:31:00	2020-03-27 13:54:08	2020-03-27 13:54:07
https://github.com/tensorflow/federated/issues/809	[]	How to aggregate only part of client model weights?	"How to aggregate only part of client model weights?Not sure if I should ask here again. I encounted similar issue as https://stackoverflow.com/questions/60501245/can-i-broadcast-different-models-for-each-of-the-clients-in-federated-tensorflow, but seems no one answered it. Wondering if tff supports aggregating only part of client model weights? I mean the client model contains two parts, and one part needs to aggregate, the other part needs to keep locally. Thanks!There isn't anything available yet for this that is plug-and-play-- it's possible to do by creating your own `IterativeProcess` and calling `tff.federated_aggregate`/`tff.federated_collect` manually. There are, however, some internal research examples that do this that we're looking to open-source soon. Stay tuned for those, otherwise feel free to ask questions if you decide to use `IterativeProcess` and hit any issues!Thanks!
I have implemented this by transfering weights deltas to python environment by tff.federated_collect, the patch is
[optimizer_utils.txt](https://github.com/tensorflow/federated/files/4455248/optimizer_utils.txt)
Do you think this implementation is suitable?
The following issue is that performance is lowered by about 10X after using tff.federated_collect. Do you have any advice?
The version of tensorflow_federated I used is 0.12.0 and 10 clients are used.
Thanks again!You're in luck! `federate_collect` had a severe performance issue on the simulation stack, and I have a patch locally that fixes it. I'll try to have it in some time this week-- thanks for your patience!

If you're curious, it boils down to the `EagerTFExecutor::create_value` call with values of `SequenceType` calling into [`make_dataset_from_elements`](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py#L928) and then [this numpy call](https://github.com/tensorflow/federated/blob/eb57cca1666670cf167a5468c761cd3724fe4a8f/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py#L902) which can take a *long* time. I believe we can replace that call in this case with a more efficient type conversion, but I'll update back here once I have a finished patch.I've just landed https://github.com/tensorflow/federated/commit/c7f90f11ca17224cdf130206dbd0736f9a8a7318 which gives me massive speedup on some cases of `federated_collect`. It'll be available in the next release. Let me know when you have a chance to try it and if it helps you!It works and I also get massive speedup! Thanks for your great help!Woooo!!! :)"	6	2020-03-19 11:24:32	2020-05-08 20:37:13	2020-05-08 20:37:13
https://github.com/tensorflow/federated/issues/804	[]	Bad Time Performance with  a lot of clients	"Bad Time Performance with  a lot of clientsI am using the TF Federated Framework to implement a fare price predictor on the NYC taxi dataset. 
Therefore i split the dataset into a federated clientdataset and use it to train a federated predictor like in the tutorials provided online. 
I noticed that the time needed to train increases rapedly when the amount of clients is increased. 
All data is used for the training and is split across the amount of clients evenly.
With the same network type and same epochs it takes : 

1000 Clients , 500 Per Round : 130 minutes
100 Clients , 50     Per Round : 13 minutes
10 Clinets , 5        Per Round :  5 minutes


Is this something that happens normaly or is this some kind of bug ? Or am i doing something wrong here ? 

![Code](https://user-images.githubusercontent.com/62025839/76345905-70b2b380-6304-11ea-94a0-9c05de4b989a.PNG)
Hi @steffenStra can I ask what version of TFF you are using?Hi @michaelreneer i am using TFF Version 0.12.0 on google colab.Hi @steffenStra I think this seems normal, lets assume that your setup can run 20 clients in parallel, which takes about 5 minutes

```markdown
10 clients: 1 batch (20 clients in parallel) = 5 minutes
50 clients: 2-3 batches of 20 clients in parallel * 5 minutes = 10-15 minutes
500 clients: ~25 batches of 20 clients in parallel * 5 minutes = 125 minutes
```

**You probably want to take a look at setting up a [multi-machine simulation](https://github.com/tensorflow/federated/blob/master/docs/tutorials/high_performance_simulation_with_kubernetes.ipynb).**

Also note that aggregation gets slower with more clients because this operation is serial, not parallel. *You might be able to take a look at creating a custom execution stack and adjusting the [max_fanout](https://www.tensorflow.org/federated/api_docs/python/tff/framework/local_executor_factory)*, which can be done with...

```python
executor_factory = tff.framework.local_executor_factory(
    num_clients=None,
    max_fanout=100,
    clients_per_thread=1)
tff.framework.set_default_executor(executor_factory)
```"	3	2020-03-10 18:25:46	2020-06-15 18:23:37	2020-06-15 18:23:37
https://github.com/tensorflow/federated/issues/803	[]	Does tff.tf_computation support tf.numpy_function?	"Does tff.tf_computation support tf.numpy_function?When I use tf.numpy_function in optimizer_utils.py(tensorflow_federated/python/learning/framework/optimizer_utils.py) like this

```
tensor_string_type = tff.TensorType(tf.string)
@tff.tf_computation(tensor_string_type)
    def cast_tensor_to_int(k):
        def _numpy_func(x):
            x = int(x) + 1
            return np.array(x, dtype='U100000')

        @tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.string)])
        def tf_function(input):
            y = tf.numpy_function(_numpy_func, [input], tf.string)
            y1 = tf.reshape(y, ())
            return y1
        return tf_function(k)
```

and I call this function in run_one_round_tff, It gives me the errors in client side

> ValueError: callback pyfunc_0 is not found
> 	 [[{{node import/StatefulPartitionedCall/PyFunc}}]] [Op:__inference_wrapped_function_13]
> Function call stack:

I don't know why this happened?
Hi @FancyXun 

I have been unable to reproduce this problem in our current setup (that is, with TFF 0.13.1). See the colab [here](https://colab.research.google.com/drive/15wfY6xv4LZa1oBf-mNjJXJfir1XIwLWy) for a working version of the above.

With a new version of TFF, is this fixed on your end?> Hi @FancyXun
> 
> I have been unable to reproduce this problem in our current setup (that is, with TFF 0.13.1). See the colab [here](https://colab.research.google.com/drive/15wfY6xv4LZa1oBf-mNjJXJfir1XIwLWy) for a working version of the above.
> 
> With a new version of TFF, is this fixed on your end?

Thanks!
Does this code in colab run in client side? Follow https://www.tensorflow.org/api_docs/python/tf/numpy_function , tf.numpy_function can not support  serializeTFF uses its own TensorFlow serialization mechanisms, and in fact the computations the TFF runtime invokes are essentially precisely backed by the same artifact that would run them on device (a TFF [computation.proto](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/proto/v0/computation.proto). If the code runs in the TFF runtime, it will run identically on device.

There are *likely* some things `numpy_function` could do that TFF's serialization mechanisms would fail to capture, but you would see this right away--the code would fail in the runtime. Likely any numpy arrays are baked into the graph as constants, for example."	3	2020-03-09 11:43:49	2020-07-08 04:22:17	2020-07-08 04:22:17
https://github.com/tensorflow/federated/issues/798	[]	Learning parameters of each simulated device	"Learning parameters of each simulated deviceDoes tensorflow-federated support assigning different hyper-parameters(like batch-size or learning rate) for different simulated devices?
This doesn't seem like a bug/feature request for the code, but rather usage question. This might be better asked at https://stackoverflow.com/questions/tagged/tensorflow-federated. Could you ask there?"	1	2020-02-18 16:24:15	2020-02-18 16:28:15	2020-02-18 16:28:14
https://github.com/tensorflow/federated/issues/795	[]	Text generation colab notebook throws error on keras evaluation	"Text generation colab notebook throws error on keras evaluationHello! I am running the Text generation colab notebook in Python 3 runtime with no hardware acceleration. Without making any modification to the notebook and simply running all cells, the notebook throws an error in the ""Fine-tune the model with Federated Learning"" section on line 21 where it calls the keras_evaluate function. 

https://colab.research.google.com/github/tensorflow/federated/blob/v0.11.0/docs/tutorials/federated_learning_for_text_generation.ipynb#scrollTo=vm_-PU8OFXpY

I have encountered a similar issue with my own work. I think tf keras, unlike TFF, does not accept preprocessed datasets where elements are OrderedDicts. The situation is fixed by switching the preprocess function to take a tuple as instead of an OrderedDict.

Full error is given below: 

Evaluating before training round 0
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-21-6e9921caacc8> in <module>()
     19 
     20 for round_num in range(NUM_ROUNDS):
---> 21   keras_evaluate(state, round_num)
     22   # N.B. The TFF runtime is currently fairly slow,
     23   # expect this to get significantly faster in future releases.

2 frames
<ipython-input-21-6e9921caacc8> in keras_evaluate(state, round_num)
     15   tff.learning.assign_weights_to_keras_model(keras_model, state.model)
     16   print('Evaluating before training round', round_num)
---> 17   keras_model.evaluate(example_dataset, steps=2)
     18 
     19 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v1.py in evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)
    883     self._check_call_args('evaluate')
    884 
--> 885     func = self._select_training_loop(x)
    886     return func.evaluate(
    887         self,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v1.py in _select_training_loop(self, inputs)
    563 
    564     # Experiment training loop with default DS path.
--> 565     if context.executing_eagerly() and self._experimental_run_tf_function:
    566       if self._in_multi_worker_mode():
    567         return training_distributed.DistributionMultiWorkerTrainingLoop(

AttributeError: 'Sequential' object has no attribute '_experimental_run_tf_function'Hi @r-o-s-h-a-n we just released version `v0.12.0`. The new version of the colab tutorial is [here](https://colab.research.google.com/github/tensorflow/federated/blob/v0.12.0/docs/tutorials/federated_learning_for_text_generation.ipynb). Our tests indicate this notebook does not raise errors, could you retry there and see if this issue still occurs?Hi @ZacharyGarrett , the notebook works now. Thanks!"	2	2020-02-07 18:17:46	2020-02-12 23:03:38	2020-02-12 23:03:38
https://github.com/tensorflow/federated/issues/793	[]	Use data generator to federated framework when train on large dataset 	"Use data generator to federated framework when train on large dataset Hi!

I was very glad to customize my own data and model to federated interfaces and the training converged!

Now I am confused about an issue that in an images classification task, the whole dataset is extreme large and it can't be stored in a single `federated_train_data ` nor be imported to memory for one time. So I need to load the dataset from the hard disk in batches to memory real-timely and use  Keras `model.fit_generator` instead of `model.fit` during training, the approach people use to deal with large data.

I suppose in `iterative_process` shown in image classification tutorial, the model is fitted on a fixed set of data. Is there any way to adjust the code to let it fit to a data generator?I have looked into the source codes but still quite confused. Would be incredibly grateful for any hints.Thanks for asking on SO! Dropping a [link here](https://stackoverflow.com/questions/59835749/implement-data-generator-in-federated-training/59865565#59865565), will close when there is an accepted answer.Hi! Thanks so much for your reply on SO! Unfortunately I still can't work it out. Do you know any way to change the model training process on local clients?@zm17943 could you take a look at the example in [this StackOverflow answer](https://stackoverflow.com/a/60266566/14692)? This does _not_ load all clients at once, only the clients in one round of competition are used at a time, and then the `tf.data.Dataset` also are only loading a subset of their entirely data, as-needed, for training.Thank you! I have looked into the StackOverflow answer, and adjusted my code to load each client at one time. However, I am still confused about the use of real-time data augmentation, for example, can I use tf.Data.Dataset.from_generator to load data into Federated?Hi, I tried to use tf.data.Dataset.from_generator to train federated model. But this step took forever. 
```
iterative_process.next
```
I tried to reduce the batch_size and trainable parameters to get it fast, but still. I was wondering how to diagnose the training process?I have exactly 

> Hi, I tried to use tf.data.Dataset.from_generator to train federated model. But this step took forever.
> 
> ```
> iterative_process.next
> ```
> 
> I tried to reduce the batch_size and trainable parameters to get it fast, but still. I was wondering how to diagnose the training process?

I have the exactly same issue!One thing that I might investigate here: try adding a `ds.take(1)` to your dataset constructors, or raise a `StopIteration` exception from your generator after yielding e.g. a single element.

If TFF is given an infinite `tf.data.Dataset`, it will likely reduce forever, keep pulling elements out of the dataset.

I am thinking this way because if your generator never raises `StopIteration`, I believe [`from_generator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) will treat the dataset as infinite. The docs just linked reference Python's [iterator protocol](https://docs.python.org/3/library/stdtypes.html#iterator-types), which states: 

>  If there are no further items, raise the StopIteration exception.

which therefore implies: if there is no `StopIteration`, there are further items.Yes, I am using `ImageDataGenerator` to create my generator, and it produces infinite batches. 
How to add `StopIteration`  in `ImageDataGenerator` ?  Or do I need to use another generator.We seem to be getting this question along multiple channel, so I think for ease of discoverability we would prefer to consolidate to stackoverflow. Please see the discussion [here](https://stackoverflow.com/questions/60153603/tensorflow-federated-why-my-iterative-process-unable-to-train-rounds/60517814#60517814), and open a question there if that does not suit your needs.

Thanks!"	9	2020-01-21 06:56:19	2020-03-06 04:23:17	2020-03-06 04:23:17
https://github.com/tensorflow/federated/issues/786	[]	Making federated data!	"Making federated data!Hi,

I have selected a dataset from kaggle which is the ""FIFA 19 complete player dataset"". I wanted to use this dataset as a federated dataset. However, when I was trying to convert the dataset into a federated one, it seems not working for me. I have imported the dataset as Dataframe. 

Does anyone have any suggestions? How do I make this dataset into federated one as they showed in the tutorial section?

Thanks@Krypton3 questions about _how to use_ TFF are often better addressed at StackOverflow with the [TensorFlow Federated tag](https://stackoverflow.com/questions/tagged/tensorflow-federated).

Perhaps the following questions are similar to yours and the answers might apply?

- [Create a custom federated data set in TensorFlow Federated](https://stackoverflow.com/questions/55434004/create-a-custom-federated-data-set-in-tensorflow-federated)
- [Is there a reasonable way to create tff clients datat sets?](https://stackoverflow.com/questions/58004272/is-there-a-reasonable-way-to-create-tff-clients-datat-sets)

@jpgard is currently working on a tutorial for structured data in #782, it might be good to follow that issue as well.+1 to @ZacharyGarrett 's comment. @Krypton3 if you do post this on StackOverflow, perhaps you can share a link and I will see if I can answer the question on StackOverflow.Thank you. I will post this on StackOverflow and use the proper tag. I am working on federated for a few days and hopefully will do my ms thesis on it. However, I need to solve this issue in which I am stuck right now. 

ThanksFYI I think this question has been posted on SO here: https://stackoverflow.com/questions/58965488/how-to-create-federated-dataset-from-a-csv-fileSeems like given the activity on SO, this issue can be closed."	5	2019-11-20 02:45:24	2020-01-16 18:05:07	2020-01-16 18:05:07
https://github.com/tensorflow/federated/issues/780	[]	FileCheckpointManager causes exception on windows	"FileCheckpointManager causes exception on windowsHi,

running FileCheckpointManager using a windows machine will cause an exception at line https://github.com/tensorflow/federated/blob/48d5e4860f3349086d018b9553f730c87def2429/tensorflow_federated/python/research/utils/checkpoint_manager.py#L88 :

`re.error: bad escape \c at position`

this happened because `os.path.join` creates a string that contains `\\ckpt_`

(I bypassed https://github.com/tensorflow/federated/issues/779 by commenting out emnist import, and am using the checkpoint_manager by copy-pasting the code. So I just hardcoded a solution for my project, but it is an issue with that code in this repo)@amitport Can you provide a few pieces of information...
* Python version
* Is `path` == '\\ckpt_'? If not can you provide the full value of `path`?
* python 3.6
* path has a prefix but the error happen because how `os.path.join` is adding the default `ckpt`

just run the following on windows to get an exception
```
path = os.path.join(anyPath, 'ckpt')
re.compile(r'{}([0-9]+)$'.format(path)) 
```@amitport Can you provide the value you are using for `anyPath` and what is `path` after you call join?

```python
path = os.path.join(anyPath, 'ckpt')
print(path)
```

Thanksit really does not matter since the exception happen on the `\\c`: `re.error: bad escape \c at position`

in any case mine is something like: `C:\\my_folder`

(the result of `path = os.path.abspath(__file__)` on windows)This issue is more subtle than you are making it out to be (or I am confused) and this is why I am asking you for specifics.

This is what we want:
```shell
>>> re.compile(r'c:\\my_folder')
re.compile('c:\\\\my_folder')
```

All the following generate the error you are experiencing:
```shell
>>> re.compile(r'c:\my_path')
...
re.error: bad escape \m at position 2
>>> re.compile('c:\\my_path')
...
re.error: bad escape \m at position 2
>>> re.compile('c:\my_path')
...
re.error: bad escape \m at position 2
```

Therefore I believe that `os.path.join` returning a string that has an escaped slash is correct. Instead I believe the issue is in combining Python raw strings with Python strings.

```shell
>>> re.compile(r'{}'.format(r'c:\\my_path'))
re.compile('c:\\\\my_path')
>>> re.compile(r'{}'.format('c:\\my_path'))
...
re.error: bad escape \m at position 2
```

I believe the solution is something like...
```shell
>>> re.compile(r'{}'.format('c:\\my_path'.encode('unicode-escape').decode()))
re.compile('c:\\\\my_path')
```

Note this is a Python 3 only fix. I'll follow up with a fix after a little more testing.The two lines I quoted in always cause the issue on windows and it's easy to reproduce. I didn't mean that it is easy to solve or that `path.join` directly causes the issue, but having `\\c` in the next line does.

anyway thanks for the effort"	6	2019-11-06 10:29:12	2020-06-15 18:23:05	2020-06-15 18:23:05
https://github.com/tensorflow/federated/issues/776	[]	How does tff allocate processes or threads?	"How does tff allocate processes or threads?How does tff allocate processes or threads in the code ?If it is parallel, then there will definitely be code to create a process or thread. I looked at the code but didn't see the content, so I was confused how it distributed the data to different clients and trained independently.
Thank you very muchThis question is may be better suited fro the `tensorflow-federated` tag on StackOverflow (https://stackoverflow.com/questions/tagged/tensorflow-federated); TFF uses GitHub issues for bugs and feature requests, and StackOverflow for questions & answers.

 The default simulation execution does not run multi-threaded; this executor is called the _reference executor_ ([code link](https://github.com/tensorflow/federated/blob/8c904235dbf00431a95b5b1c64017aecf14517e6/tensorflow_federated/python/core/impl/reference_executor.py#L552)) which is very straight forward and less performant by design; it is optimizing for _correctness_. This processes each client serially on the local machine.

There is also a _concurrent executor_ ([code link](https://github.com/tensorflow/federated/blob/8c904235dbf00431a95b5b1c64017aecf14517e6/tensorflow_federated/python/core/impl/concurrent_executor.py#L26)) which runs using Python's `asyncio` event loop. Most users will want to create an executor stack with `tff.framework.create_local_executor()`, there are example usages in [this tutorial](https://github.com/tensorflow/federated/blob/v0.10.1/docs/tutorials/simulations.ipynb). This can process multiple clients in parallel on a single machine.

Good progress has been made on a _remote executor_, which will allowing running simulations in a distributed fashion on multiple machine; stay tuned.

Note: this is all for _simulating_ federated learning."	1	2019-10-22 07:56:28	2019-11-07 13:01:03	2019-11-07 13:00:59
https://github.com/tensorflow/federated/issues/770	[]	cannot import name 'computation_pb2'	"cannot import name 'computation_pb2'I tried to connect pycharm to the remote server. The connection was successful. I used the virtual environment tff3. The jupyter remote connection to the virtual environment can make the code run successfully. However, the pycharm failed.
I created a new py file and entered the following statement. 
```
import tensorflow_federated as tff
print(tff.federateprint(tff.federated_computation(lambda: 'Hello World')()))
```
The error message is as follows:

```
ssh://liuxy@*.*.*.*:22/home/liuxy/anaconda3/envs/tff3/bin/python3 -u /home/liuxy/tmp/pycharm_project_567/docs/tutorials/image_classification.py
Traceback (most recent call last):
  File ""/home/liuxy/tmp/pycharm_project_567/docs/tutorials/image_classification.py"", line 1, in <module>
    import tensorflow_federated as tff
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/__init__.py"", line 23, in <module>
    from tensorflow_federated.python.core.api.computation_base import Computation
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/python/core/__init__.py"", line 21, in <module>
    from tensorflow_federated.python.core.api.computation_base import Computation
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/python/core/api/__init__.py"", line 29, in <module>
    from tensorflow_federated.python.core.api.computations import federated_computation
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/python/core/api/computations.py"", line 21, in <module>
    from tensorflow_federated.python.core.impl import computation_wrapper_instances
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/python/core/impl/computation_wrapper_instances.py"", line 22, in <module>
    from tensorflow_federated.python.core.impl import computation_impl
  File ""/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/python/core/impl/computation_impl.py"", line 21, in <module>
    from tensorflow_federated.proto.v0 import computation_pb2 as pb
ImportError: cannot import name 'computation_pb2' from 'tensorflow_federated.proto.v0' (/home/liuxy/tmp/pycharm_project_567/tensorflow_federated/proto/v0/__init__.py)
```
Thank you very muchHi @phalangee 

We are familiar with this error; this is likely due to trying to launch or run the interpreter from within the same directory in which you have a copy of the source. If you move out of this directory, it should all work. This is because the Python interpreter is looking into the raw source of our `proto` files, which generate Python when TFF is compiled or run with `bazel`, but are not themselves Python.@jkr26  Thank you very much！  it worked for me !Awesome! Happy to help!"	3	2019-10-09 14:13:10	2019-10-10 03:36:26	2019-10-10 03:36:26
https://github.com/tensorflow/federated/issues/769	[]	Unable to run tutorial code in Python 3	"Unable to run tutorial code in Python 3Hello, I'm receiving the error below when I attempt to run the code from the TFF front page. It's probably worth mentioning that it seems to run fine with python 2.7, but not 3.6.

Tutorial code:
```import tensorflow as tf
tf.compat.v1.enable_v2_behavior()
import tensorflow_federated as tff

source, _ = tff.simulation.datasets.emnist.load_data()
def client_data(n):
  return source.create_tf_dataset_for_client(source.client_ids[n]).map(
      lambda e: {
          'x': tf.reshape(e['pixels'], [-1]),
          'y': e['label'],
  }).repeat(10).batch(20)

train_data = [client_data(n) for n in range(3)]

sample_batch = tf.nest.map_structure(
    lambda x: x.numpy(), iter(train_data[0]).next())

def model_fn():
 model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, tf.nn.softmax, input_shape=(784,),
                          kernel_initializer='zeros')
 ])
 model.compile(
     loss=tf.keras.losses.SparseCategoricalCrossentropy(),
     optimizer=tf.keras.optimizers.SGD(0.1),
     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
 return tff.learning.from_compiled_keras_model(model, sample_batch)

trainer = tff.learning.build_federated_averaging_process(model_fn)
state = trainer.initialize()
for _ in range(5):
  state, metrics = trainer.next(state, train_data)
  print (metrics.loss)
```

Error:
```    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/thai/repos/ehr-project/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2614, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""/Users/thai/repos/ehr-project/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 874, in wrapper
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in converted code:
    relative to /Users/thai/repos/ehr-project/venv/lib/python3.6/site-packages:

    tensorflow_federated/python/tensorflow_libs/tensor_utils.py:115 zero_all_if_any_non_finite
        if all_finite:
    tensorflow_core/python/framework/ops.py:762 __bool__
        self._disallow_bool_casting()
    tensorflow_core/python/framework/ops.py:531 _disallow_bool_casting
        ""using a `tf.Tensor` as a Python `bool`"")
    tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled
        "" decorating it directly with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
```

requirements.txt:
```
tf-nightly==1.15.0.dev20190805
tensorflow_federated
tensorflow-estimator==1.15.1
setuptools
grpcio~=1.22.0
```

Pip freeze:
```
absl-py==0.8.0
astor==0.8.0
attrs==18.2.0
cachetools==3.1.1
enum34==1.1.6
gast==0.3.2
google-pasta==0.1.7
grpcio==1.22.1
h5py==2.10.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
Markdown==3.1.1
numpy==1.17.2
opt-einsum==3.1.0
portpicker==1.3.1
protobuf==3.10.0
six==1.12.0
tb-nightly==1.15.0a20190911
tensorflow-estimator==1.15.1
tensorflow-federated==0.8.0
tensorflow-model-optimization==0.1.3
termcolor==1.1.0
tf-estimator-nightly==2.0.0.dev2019100301
tf-nightly==1.15.0.dev20190805
Werkzeug==0.16.0
wrapt==1.11.2
```

Python version
```Python 3.6.5```Hi @pthai,

Can you advise when line from TFF threw this error? In particular, does it come from `build_federated_averaging_process` or invoking the returned function inside the loop? AutoGraph should be tracking this and it should be *somewhere* (perhaps non-obvious) in the stacktrace.

An alternative option here (probably the easier one) is to simply try and run the notebook code as it exists after [this commit](https://github.com/tensorflow/federated/commit/7f6cd06e0032e8afdc8584fde5b941761bd1e97d) instead; given the recent release of TF2 we just moved our pip package to pull in TF2 instead of 1.15, and converted our code to be fully TF2-compliant. The message about `tf.function` makes me think that this may be an artifact of TF1-style code in a TF2-style world.

Hope this helps!

KeithPinging, is this still an issue?Closing as unable to repro"	3	2019-10-04 04:47:22	2019-12-06 20:01:11	2019-12-06 20:01:10
https://github.com/tensorflow/federated/issues/764	[]	TypeError: Expected a TensorFlow computation, found intrinsic.	"TypeError: Expected a TensorFlow computation, found intrinsic.This is from Federated_learning_for_image_classification.ipynb
tensorflow_federated 0.8.0
tensorflow nightly 1.15.0-dev20190805

run as a local notebook. It works on colab!

state = iterative_process.initialize()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-18-0c0dcd96a774> in <module>
----> 1 state = iterative_process.initialize()

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\utils\function_utils.py in __call__(self, *args, **kwargs)
    628     context = self._context_stack.current
    629     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 630     return context.invoke(self, arg)
    631 
    632 

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\execution_context.py in invoke(self, comp, arg)
    118   def invoke(self, comp, arg):
    119     return asyncio.get_event_loop().run_until_complete(
--> 120         _invoke(self._executor, comp, arg))

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\base_events.py in run_until_complete(self, future)
    482             raise RuntimeError('Event loop stopped before Future completed.')
    483 
--> 484         return future.result()
    485 
    486     def stop(self):

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\tasks.py in _step(***failed resolving arguments***)
    180                 result = coro.send(None)
    181             else:
--> 182                 result = coro.throw(exc)
    183         except StopIteration as exc:
    184             if self._must_cancel:

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\execution_context.py in _invoke(executor, comp, arg)
     89     py_typecheck.check_type(arg, executor_value_base.ExecutorValue)
     90   comp = await executor.create_value(comp)
---> 91   result = await executor.create_call(comp, arg)
     92   py_typecheck.check_type(result, executor_value_base.ExecutorValue)
     93   result_val = _unwrap(await result.compute())

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\lambda_executor.py in _evaluate(self, comp, scope)
    376         arg = None
    377       return await self.create_call(
--> 378           LambdaExecutorValue(comp.call.function, scope=scope), arg=arg)
    379     elif which_computation == 'selection':
    380       which_selection = comp.selection.WhichOneof('selection')

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\lambda_executor.py in create_call(self, comp, arg)
    259         arg_type = type_utils.get_argument_type(arg.type_signature)
    260         type_utils.check_assignable_from(param_type, arg_type)
--> 261         return await self.create_call(comp, await self.create_call(arg))
    262     else:
    263       py_typecheck.check_none(arg)

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\lambda_executor.py in create_call(self, comp, arg)
    273       # so this is the only case left to handle.
    274       py_typecheck.check_type(comp_repr, pb.Computation)
--> 275       eval_result = await self._evaluate(comp_repr, comp.scope)
    276       py_typecheck.check_type(eval_result, LambdaExecutorValue)
    277       if arg is not None:

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\lambda_executor.py in _evaluate(self, comp, scope)
    352           comp,
    353           type_utils.get_function_type(
--> 354               type_serialization.deserialize_type(comp.type))))
    355     elif which_computation == 'lambda':
    356 

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\caching_executor.py in create_value(self, value, type_spec)
    240                                  target_future)
    241       self._cache[identifier] = cached_value
--> 242     target_value = await cached_value.target_future
    243     type_utils.check_assignable_from(type_spec, target_value.type_signature)
    244     return cached_value

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\tasks.py in _wakeup(self, future)
    248     def _wakeup(self, future):
    249         try:
--> 250             future.result()
    251         except Exception as exc:
    252             # This may also be a cancellation.

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\tasks.py in _step(***failed resolving arguments***)
    180                 result = coro.send(None)
    181             else:
--> 182                 result = coro.throw(exc)
    183         except StopIteration as exc:
    184             if self._must_cancel:

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\concurrent_executor.py in create_value(self, value, type_spec)
     66   async def create_value(self, value, type_spec=None):
     67     return await self._delegate(
---> 68         self._target_executor.create_value(value, type_spec))
     69 
     70   async def create_call(self, comp, arg=None):

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\futures.py in __iter__(self)
    325         if not self.done():
    326             self._asyncio_future_blocking = True
--> 327             yield self  # This tells Task to wait for completion.
    328         assert self.done(), ""yield from wasn't used with future""
    329         return self.result()  # May raise too.

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\tasks.py in _wakeup(self, future)
    248     def _wakeup(self, future):
    249         try:
--> 250             future.result()
    251         except Exception as exc:
    252             # This may also be a cancellation.

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\futures.py in result(self)
    241         self._log_traceback = False
    242         if self._exception is not None:
--> 243             raise self._exception
    244         return self._result
    245 

c:\users\XXX\appdata\local\continuum\anaconda3\envs\wz\lib\asyncio\tasks.py in _step(***failed resolving arguments***)
    178                 # We use the `send` method directly, because coroutines
    179                 # don't have `__iter__` and `__next__` methods.
--> 180                 result = coro.send(None)
    181             else:
    182                 result = coro.throw(exc)

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\eager_executor.py in create_value(self, value, type_spec)
    369     if not tf.executing_eagerly():
    370       raise RuntimeError('The eager executor may only be used in eager mode.')
--> 371     return EagerValue(value, type_spec, self._device)
    372 
    373   async def create_call(self, comp, arg=None):

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\eager_executor.py in __init__(self, value, type_spec, device)
    274       py_typecheck.check_type(type_spec, computation_types.Type)
    275     self._type_signature = type_spec
--> 276     self._value = to_representation_for_type(value, type_spec, device)
    277 
    278   @property

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\eager_executor.py in to_representation_for_type(value, type_spec, device)
    207         computation_impl.ComputationImpl.get_proto(value), type_spec, device)
    208   if isinstance(value, pb.Computation):
--> 209     return embed_tensorflow_computation(value, type_spec, device)
    210   if isinstance(type_spec, computation_types.TensorType):
    211     if not isinstance(value, tf.Tensor):

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_federated\python\core\impl\eager_executor.py in embed_tensorflow_computation(comp, type_spec, device)
     66   if which_computation != 'tensorflow':
     67     raise TypeError('Expected a TensorFlow computation, found {}.'.format(
---> 68         which_computation))
     69 
     70   if isinstance(type_spec, computation_types.FunctionType):

TypeError: Expected a TensorFlow computation, found intrinsic.
Hi @dsudzeng 

Thanks for pinging us on this!

We pushed the capability for the local executor to infer cardinalities from its arguments in August 29 in [this change](https://github.com/tensorflow/federated/commit/7107a4a8d474524341250b15da389614b478b1e8#diff-9a0820f4f746826fab7fd2b2f3612342), and released our last version of TFF (0.8.0) on August 20. This means that any ipython notebook currently at master which does not explicitly set its number of clients will behave as a non-federated executor (and e.g. won't know how to handle intrinsics); basically master is out of sync with the pip package.

If you are running these notebooks against the pip package, you will need to explicitly specify the number of clients in the executor stack. Here, `num_clients` should be 10 if I recall correctly; so if you call `tff.framework.set_default_executor(tff.framework.create_local_executor(10))` instead of `tff.framework.set_default_executor(tff.framework.create_local_executor())` at the top level of this notebook, this discrepancy should be resolved. you must add this lignes in notebook tff.framework.set_default_executor(tff.framework.create_local_executor(10)) tff.framework.set_default_executor(tff.framework.create_local_executor())

But 10 means the total number of clients ? or the fraction of clients C that we have to choose, because as you know, in Federated learning, not all the clients are going to participate in training."	2	2019-09-20 21:39:42	2019-12-20 12:01:02	2019-09-24 20:42:21
https://github.com/tensorflow/federated/issues/758	[]	How to manually reset the model weights of federated averaging algo？	"How to manually reset the model weights of federated averaging algo？Hi there,

What I want to do now is reset the model using the `model.set_weights()`. But how should I do that, should I recreate both `iterative_process` (by `tff.learning.build_federated_averaging_process(model_fn)`) and `state` (by `iterative_process.initialize()`)? 

UPDATE 6/9/2019:
Well, I just found that there is a method called `tff.utils.update_state()`. Can I use this method to change the model weights without doing anything to the `iterative_process`? Also, for the pass-in parameter `**kwargs`, should I pass key-value pairs into it? And how should I name this key if I wanna change the weights?

UPDATE 7/9/2019:
The `tff.utils.update_state()` seems not to support the AnonymousTuple at the moment. I am a bot confused what the state should be hereHi @zzhu2019, could you help me understand more clearly the context around ""reset the module using `model.set_weights()`? If you could share the code you're writing it would help clarify the intent. 

If you want to reset the model weights within the `state` object to the original initialization, calling `iterative_process.initialize()` will produce a new, reset model. Did this not accomplish want you wanted?


> Hi @zzhu2019, could you help me understand more clearly the context around ""reset the module using `model.set_weights()`? If you could share the code you're writing it would help clarify the intent.
> 
> If you want to reset the model weights within the `state` object to the original initialization, calling `iterative_process.initialize()` will produce a new, reset model. Did this not accomplish want you wanted?

Hi Zachary. 

Sorry for the late reply as I cannot log in my account today, I have to reply with my friend's account. Apologize for not being clear, as the `model.set_weights()` is what I found in `tf.keras.layers.Layer.set_weights()` [(tf source code)](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/engine/base_layer.py#L1090-L1121). To be specific, the reset I mean here is kinda changing the model weights into a set of weights calculated beforehand but not reset to original initialization. The `model.set_weights()` works fine by itself, but when I 

1. initialize a model with `model = tf.keras.models.Sequential([tf.keras.layers.Dense(10, activation=tf.nn.softmax, kernel_initializer='zeros', input_shape=(784,))])`, and 

2. use the `model.set_weights()`, 
3. then `model.compile()`, 
4. after that, use this generated model in `tff.learning.from_compiled_keras_model()` 
5. and then `tff.learning.build_federated_averaging_process()` to produce a new `iterative_process` 
6. and `iterative_process.initialize()` to produce a new state. 
It seems like the model weight is not resetted to the target weights but the initialization weights instead.

Generally, is my understanding of the code correct? And is there a more elegant way to do it? > Hi @zzhu2019, could you help me understand more clearly the context around ""reset the module using `model.set_weights()`? If you could share the code you're writing it would help clarify the intent.
> 
> If you want to reset the model weights within the `state` object to the original initialization, calling `iterative_process.initialize()` will produce a new, reset model. Did this not accomplish want you wanted?

And, can I use the code below to only update the `state` in order to change the weights without regenerate the `iterative_process`?
`value_dict = {}`
`value_dict['desne/kernal'] = states[edge_id][0]`
`value_dict['desne/bias'] = states[edge_id][1]`
`state = tff.utils.update_state(state=state, **value_dict)`Could you share the code you are using the do steps 1 through 5 in https://github.com/tensorflow/federated/issues/758#issuecomment-530302262? Setting a Keras model's weights with pre-trained weights and then using that in TFF is expected to work (the [Federated Learning for Text Generation tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation) does this, see the sections on _Load a pre-trained model_ and _Fine-tune the model with Federated Learning_). Lets see what differs from those examples.

> Could you share the code you are using the do steps 1 through 5 in [#758 (comment)](https://github.com/tensorflow/federated/issues/758#issuecomment-530302262)? Setting a Keras model's weights with pre-trained weights and then using that in TFF is expected to work (the [Federated Learning for Text Generation tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation) does this, see the sections on _Load a pre-trained model_ and _Fine-tune the model with Federated Learning_). Lets see what differs from those examples.

Basically, I changed a bit about the `create_compiled_keras_model()` as follow:
![image](https://user-images.githubusercontent.com/37321309/64748468-9c233400-d555-11e9-8fe7-5cf723d794b5.png) The `state_spec` basically is a list contains two numpy array, which is extracted from the previous calculated `state`. (state[0][0][0] and state[0][0][1])
And the `model_fn()`:
![image](https://user-images.githubusercontent.com/37321309/64745836-b86ea300-d54c-11e9-8e5d-f472382d731a.png)
And the `model_fn()` is used in:
![image](https://user-images.githubusercontent.com/37321309/64745875-e48a2400-d54c-11e9-8bfc-9add2dd68e72.png)
which produce a new  `iterative_process` and `state`. The problem is the keras_model in the `model_fn` has the target weights, but when I check the state generated by `iterative_process.initialize()` it still has the original initialization weights.

> Could you share the code you are using the do steps 1 through 5 in [#758 (comment)](https://github.com/tensorflow/federated/issues/758#issuecomment-530302262)? Setting a Keras model's weights with pre-trained weights and then using that in TFF is expected to work (the [Federated Learning for Text Generation tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation) does this, see the sections on _Load a pre-trained model_ and _Fine-tune the model with Federated Learning_). Lets see what differs from those examples.

It seems like there is something wrong with the way I use `model.reset_weights()` cuz the weights inside the state generated by `iterative_process.initialize()` just change correspondingly to the `kernel_initializer` in `model = tf.keras.models.Sequential([
            tf.keras.layers.Dense(
                10, activation=tf.nn.softmax, kernel_initializer='random_uniform', input_shape=(784,))])`

The `model.set_weights()` wroks well and it can be supported by `model.get_weights()`. But somehow when the created `iterative_process` doesn't recognize that..> Could you share the code you are using the do steps 1 through 5 in [#758 (comment)](https://github.com/tensorflow/federated/issues/758#issuecomment-530302262)? Setting a Keras model's weights with pre-trained weights and then using that in TFF is expected to work (the [Federated Learning for Text Generation tutorial](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation) does this, see the sections on _Load a pre-trained model_ and _Fine-tune the model with Federated Learning_). Lets see what differs from those examples.

Hi Zachary, I am pretty sure that the weight of the model before `model.compile()` inside the `created_compiled_keras_model()` has been changed which can be confirmed by both (`model.weights` and `model.get_weights()`).Its hard to diagnose the issue without a full picture of the situation, the code snippets are helpful but without the complete code I'm still having a hard time fully grasping what the goal is and how it is being attempted.

From what I gather so far this doesn't seem like an issue that needs fixing in TFF, but a question about how to use TFF (or TF) to achieve a particular behavior? If you agree, these kind of questions are better asked at https://stackoverflow.com/tags/tensorflow_federated, which preserves easily searchable database of questions and answers for the community to benefit from.

I'd recommend posting a question there and adding the `keras`, `tensorflow`, and `tensorflow_federated` tags. When posting a question, please try to create a minimal example (from https://github.com/tensorflow/federated/issues/758#issuecomment-531138660 it sounds like only the code inside `def create_compiled_keras_model()` may be necessary?), and post _all_ the code related to that example.> Its hard to diagnose the issue without a full picture of the situation, the code snippets are helpful but without the complete code I'm still having a hard time fully grasping what the goal is and how it is being attempted.
> 
> From what I gather so far this doesn't seem like an issue that needs fixing in TFF, but a question about how to use TFF (or TF) to achieve a particular behavior? If you agree, these kind of questions are better asked at https://stackoverflow.com/tags/tensorflow_federated, which preserves easily searchable database of questions and answers for the community to benefit from.
> 
> I'd recommend posting a question there and adding the `keras`, `tensorflow`, and `tensorflow_federated` tags. When posting a question, please try to create a minimal example (from [#758 (comment)](https://github.com/tensorflow/federated/issues/758#issuecomment-531138660) it sounds like only the code inside `def create_compiled_keras_model()` may be necessary?), and post _all_ the code related to that example.

Hi Zachary, thank you for your reply to my question which is not really a bug of so of tff. I have given up changing the model before return the compiled Keras model to generate the new state. But I just found the `tff.learning.state_with_new_model_weights` which can fix my problem in a way more convenient way. I will close this issue soon. : )@ZacharyGarrett Thanks! Please close this issue since I'm not be able to log in my own GIthub account recently."	10	2019-09-06 00:16:58	2019-09-19 15:40:13	2019-09-19 15:40:13
https://github.com/tensorflow/federated/issues/751	[]	Memory blows up using CNN model with multiple clients	"Memory blows up using CNN model with multiple clientsI tried to implement the tutorial of federated image classification with a CNN model, but it turns out that while I set the client number to 100, each round of training takes up about 16 Gb of memory and it stacks on after each round.  I am not sure that is there any issue with my code or is it because that after each round the weight tensors are not deleted.  I would appreciate it so much if anyone could help.  Below is my python code:
  
`from __future__ import absolute_import
from six.moves import range
import tensorflow as tf
from tensorflow.python.keras.optimizer_v2 import gradient_descent
import tensorflow_federated as tff
import collections
import time
import random

from tensorflow_federated.python.examples import mnist
tf.compat.v1.enable_v2_behavior()

NUM_CLIENTS = 100
NUM_EPOCHS = 5
BATCH_SIZE = 10
ROUNDS = 100
LEARNING_RATE = 0.02

tff.framework.set_default_executor(
    tff.framework.create_local_executor(NUM_CLIENTS))

source, _ = tff.simulation.datasets.emnist.load_data()

def client_data(n):
  dataset = source.create_tf_dataset_for_client(source.client_ids[n])
  return mnist.keras_dataset_from_emnist(dataset).repeat(NUM_EPOCHS).batch(BATCH_SIZE)

train_data = [client_data(n) for n in range(NUM_CLIENTS)]

sample_batch = tf.nest.map_structure(
    lambda x: x.numpy(), iter(train_data[0]).next())

def CNN_compiled():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Reshape((28, 28, 1), input_shape=(784,)),
        tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation=""relu"", padding=""same"", strides=1),
        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding='valid'),
        tf.keras.layers.Conv2D(64, kernel_size=(5, 5), activation=""relu"", padding=""same"", strides=1),
        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding='valid'),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation=""relu""),
        tf.keras.layers.Dense(10, activation=""softmax""),
    ])
    def loss_fn(y_true, y_pred):
        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(
            y_true, y_pred))
    model.compile(
        loss=loss_fn,
        optimizer=gradient_descent.SGD(LEARNING_RATE),  # decay
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
    return model

def model_fn():
  keras_model = CNN_compiled()
  return tff.learning.from_compiled_keras_model(keras_model, sample_batch)

trainer = tff.learning.build_federated_averaging_process(model_fn)
state = trainer.initialize()
for _ in range(ROUNDS):
    now = time.time()
    state, metrics = trainer.next(state, train_data)
    time_spent = time.time() - now
    print (metrics, time_spent)`Hi @JJJJJamie 
We are looking at some related issues. Wanted to let you know we haven't forgotten about this!

KeithAfter trying the code for a few days, I'm able to run your code @JJJJJamie successfully with GPU and with no memory leak. My problem was that my path was pointed at CUDA 9.0. After I reset the path pointing to CUDA10.0 and update my cudnn to 7.6, I'm able to run your code with no memory leak and still use GPU. You might first want to try the following code to see if you can run tensorflow code with GPU.

```
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python import layers
from tensorflow.python import nn

mnist = input_data.read_data_sets(""./MNIST_data/"", one_hot=True)

sess = tf.InteractiveSession()

x = tf.placeholder(tf.float32, [None, 784])
y_ = tf.placeholder(tf.float32, [None, 10])
x_image = tf.reshape(x, [-1, 28, 28, 1])

conv1 = layers.conv2d(x_image, 32, 5, padding='same', name='conv1')
relu1 = nn.relu(conv1, name='relu1')
maxppool1 = layers.max_pooling2d(relu1, 2, 2, name='maxpool1')

conv2 = layers.conv2d(maxppool1, 64, 5, padding='same', name='conv2')
relu2 = nn.relu(conv2, name='relu2')
maxppool2 = layers.max_pooling2d(relu2, 2, 2, name='maxpool2')

flattern = layers.flatten(maxppool2, name='flattern')
fc1 = layers.dense(flattern, 1024, activation=tf.nn.relu, name='fc1')
fc1_dropout = layers.dropout(fc1, 0.8, name='fc1_dropout')
fc2 = layers.dense(fc1_dropout, 10, activation=tf.nn.softmax, name='fc2')

cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(fc2), reduction_indices=[1]))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
correct_prediciton = tf.equal(tf.argmax(fc2, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediciton, tf.float32))

tf.global_variables_initializer().run()
for i in range(1, 1001):
    batch = mnist.train.next_batch(64)
    if i % 10 == 0:
        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})
        print(""step %d, examples %d, training accuracy %g"" % (i, i * 64, train_accuracy))
        print(""test accuracy %g"" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
    train_step.run(feed_dict={x: batch[0], y_: batch[1]})

print(""test accuracy %g"" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
```Given [this recent commit](https://github.com/tensorflow/federated/commit/5299226ee7c736c46d1d3def8c6726214f6f9cb0), this issue should be mitigated. Can those seeing this problem confirm?Closing since this should be no longer an issue."	4	2019-08-22 20:52:07	2019-12-01 19:58:34	2019-12-01 19:58:34
https://github.com/tensorflow/federated/issues/748	[]	The official document should be updated	"The official document should be updatedHi there,

I got a bug that the `tff.utlis.get_variables()` function cannot be found under [python/core/utils/tf_computation_utils.py](http://github.com/tensorflow/federated/tree/master/tensorflow_federated/python/core/utils/tf_computation_utils.py) which confused me a lot. After I accessed the directory to check the tf_computation_utlis.py file, I found that `tff.utlis.get_variables()` has been changed into `tff.utlis.create_variables()`. I suppose both the API document and tutorials should be updated correspondingly to prevent further confusions.And the version of TFF should be updated too. The document said the version is 0.7.0, but the current version for demo is 0.8.0 these days.Apologies for the delay, the documentation should now be updated. The text generation and image classification tutorials still need some work, but the API docs and the custom federated algorithms tutorials (part1 and part2) should be good to go."	2	2019-08-22 05:59:56	2019-09-02 13:25:41	2019-09-02 13:25:40
https://github.com/tensorflow/federated/issues/741	[]	TF 2.0 migration	"TF 2.0 migrationAny plans to migrate to support 2.0?

From contrib side, I can see package failing on `tf.contrib.image`, where some functionality like `angles_to_projective_transforms` a well as `translations_to_projective_transforms ` and `compose_transforms` are already supported by [tf/addons](https://github.com/tensorflow/addons/tree/master/tensorflow_addons/image)

So if you are interested, I can migrate this to use addons directly.

Additional visible contrib dependency is `contrib.framework.nest` that looks like is available within `tensorflow.python.data.util` with example would be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L540

cc @dynamicwebpaige@lc0,

We are tracking this internally--thanks for the offer! But there are some extra complications that come with being a TensorFlow family open-source project, and this is unfortunately one of them. We are planning to be fully 2.0-compatible *soon*, but unfortunately exactly when ""soon"" is is a little up in the air right now.

Thanks for your interest!  We are likewise excited for TF2.

Keith@jkr26 sounds great. Lemme know if you have a beta to test. Happy to test on my side@lc0 the latest version of TFF (`0.9.0`) is fully migrated to TF 2.0. Please take a look!"	3	2019-07-31 22:35:02	2019-10-24 14:16:26	2019-10-24 14:16:25
https://github.com/tensorflow/federated/issues/611	[]	AttributeError: module 'tensorflow._api.v1.data.experimental' has no attribute 'NestedStructure'	"AttributeError: module 'tensorflow._api.v1.data.experimental' has no attribute 'NestedStructure'Hello, 
I was able to run my code just 2 days ago on colab, but now I can't because of this error AttributeError: module 'tensorflow._api.v1.data.experimental' has no attribute 'NestedStructure' . The error is related to tff.learning.build_federated_averaging_process(model_fn). 
 What is the source of this problem ? 
Thank you Duplicate of #609 

Thanks!
Keith"	1	2019-06-25 16:24:34	2019-06-25 16:52:03	2019-06-25 16:52:03
https://github.com/tensorflow/federated/issues/593	[]	Federated with KerasRegressor?	"Federated with KerasRegressor?Hello

I'm trying to get my model of KerasRegressor working with TFF framework. But it seems that ""tff.learning.from_compiled_keras_model"" does not accept it, right?. My main aim is to differentiate between federated on both regression problem and classification problem.

This is my relevant part of the code:

```
def create_SK_model():
    modelF = create_SGD_model()
    modelF.compile(loss=tf.keras.losses.MSE,optimizer=tf.keras.optimizers.SGD(lr=learn_rate))
    return modelF
    
def create_Reg_model():
    modelF_Reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn = create_SK_model,nb_epoch=SNN_epoch, batch_size=SNN_batch_size)
        
    return modelF_Reg

def create_Class_model():
    modelF_Reg = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn = create_SK_model,nb_epoch=SNN_epoch, batch_size=SNN_batch_size)
    return modelF_Class

def create_Single_model():
    if Use_RegClas:
        if Use_Regressor:
            return create_Reg_model()
        elif Use_Classification:
            return create_Class_model()
    else:
        return create_SK_model()
def model_fn_Federated():
    if Use_RegClas:
        if Use_Regressor:
            return tff.learning.from_compiled_keras_model(create_Reg_model,sample_batch)
        elif Use_Classification:
            return tff.learning.from_compiled_keras_model(create_Class_model(),sample_batch)
    elif Use_FLAveraging:
        return tff.learning.from_compiled_keras_model(create_SK_model(),sample_batch)
    else:
        return tff.learning.from_keras_model(create_SGD_model(),sample_batch,loss=tf.keras.losses.MSE)


................... some other code ..................

if Use_FLAveraging:
    trainer_Itr_Process = tff.learning.build_federated_averaging_process(model_fn_Federated,server_optimizer_fn=(lambda : tf.keras.optimizers.SGD(learning_rate=learn_rate)),client_weight_fn=None)
else:
    trainer_Itr_Process = tff.learning.build_federated_sgd_process(model_fn_Federated,server_optimizer_fn=(lambda : tf.keras.optimizers.SGD(learning_rate=learn_rate)),client_weight_fn=None)
    

```

My main problem is how to incorporate the Regression/classfiication problem into Federated TF framework. I tried the above implementation, correct? wrong? please advice.

Based on the above implementation I get the following error:

```
.....
    py_typecheck.check_type(keras_model, tf.keras.Model)
  File ""/home/..../.local/lib/python3.6/site-packages/tensorflow_federated/python/common_libs/py_typecheck.py"", line 48, in check_type
    type_string(type_spec), type_string(type(target))))
TypeError: Expected tensorflow.python.keras.engine.training.Model, found function.

```[`tff.learning.from_keras_model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/from_keras_model) and [`tff.learning.from_compiled_keras_model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/from_compiled_keras_model) support (and require) the [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=en) interface. Both regression and classification models can be implemented using the `tf.keras.Model` interface.

`tf.keras.wrappers.scikit_learn` provides wrapper classes that allow a `tf.keras.Model` to be used in the `scitkit_learn` module. They do not inherit from `tf.keras.Model` nor implement its interface, and hence are not supported in TFF. 

From the code it looks like passing the result of `create_SGD_model()` (assumed to be a `tf.kears.Model`) to `tff.learning.from_keras_model()` may be what is desired. Wrapping the `tf.keras.Model`  with `KerasClassifier` or `KerasRegressor` adds an additional layer of abstraction for using the model with `scikit_learn`, but not necessary for TFF."	1	2019-06-23 10:33:18	2019-06-23 14:00:35	2019-06-23 14:00:35
https://github.com/tensorflow/federated/issues/534	[]	Why the evaluation on test using the sample_clients from training in the EMNIST tutorial?	"Why the evaluation on test using the sample_clients from training in the EMNIST tutorial?I went through the EMNIST tutorial https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification
, but found the problem in the evaluation section:

> __federated_test_data__ = make_federated_data(emnist_test, sample_clients)
> len(federated_test_data), federated_test_data[0]

The `federated_test_data` is generated from `sample_clients`, while the `sample_clients` comes from the training set
>#@test {""output"": ""ignore""}
NUM_CLIENTS = 3
__sample_clients__ = __emnist_train__.client_ids[0:NUM_CLIENTS]
federated_train_data = make_federated_data(emnist_train, sample_clients)
len(federated_train_data), federated_train_data[0]

Why not take it as 
```python
sample_clients_test = emnist_test.client_ids[0:NUM_CLIENTS]
federated_test_data = make_federated_data(emnist_test, sample_clients_test)
evaluation(state.model, federated_test_data)
```

Is that an error or can someone explain it?This is by design; from the [documentation](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/emnist/load_data) for `tff.simulation.datasets.emnist.load_data()`:

> Rather than holding out specific users, each user's examples are split across train and test so that all users have at least one example in train and one example in test. Writers that had less than 2 examples are excluded from the data set.

So the while the evaluation is processing the same clients, it is not processing the same examples.

As you suggest, holding out entire *clients* (instead of *examples*) definitely seems reasonable and is an alternative way to evaluate the quality of a model."	1	2019-06-04 00:32:53	2019-06-05 03:04:37	2019-06-05 03:04:34
https://github.com/tensorflow/federated/issues/493	[]	Results cannot improve by rounding (.next): Federated TF 	"Results cannot improve by rounding (.next): Federated TF Hello

I'm trying to compare federated learning TF with keras fitting, as we see below. However, the results of TFF does not improve by rounding. Also, when I set weight a local model based on the TFF model, I get worse results than normally training the model on keras. The Loss results are mentioned after the code.

Thanks for the help.

Regards


# Code:
tf.compat.v1.enable_v2_behavior()

TobeRemoved_Array = ['feature1','feature2','feature3','feature4','feature5','feature6','feature7', 'is_training']

train_perc = 0.8
Norm_Input  = True
Norm_Output = False
Input_str  = ['feature1', 'feature2']
if Norm_Output:
    Output_str = ['feature3']
else: 
    Output_str = ['feature4']

Final_Tag = 'Tag3' # here just decide which tagging method do you want to use
Num_Clients = 10 # cannot be less than one
Num_Cons_Sample_Client = 5 # cannot be less than one

# Load simulation data.
##############################################
dir_name = 'pickle-data/'
file_name = 'logs_april_2019.pickle'
files = os.listdir('pickle-data/')
dataframe = Import_Pickle.Import_v1(dir_name,file_name,False) # choose False to use 2019 data
# Just to reduce the processing
ave = dataframe.core.min() + 1000000000
df2 = dataframe[dataframe.core < ave]
df = Import_Pickle.PreProcessing_v2019(df2,Norm_Input)
train_df,test_df,X_traindf,X_testdf,Y_traindf,Y_testdf,XY_traindf,XY_testdf = Import_Pickle.Splitting_Train_Test(df,train_perc,Norm_Output,TobeRemoved_Array)
########## splitting for clients ############  
def Tag_per_day(train_df_loc,TagNum):
    train_df_loc['log2'] =  train_df_loc['log'].apply(lambda x: x.replace(""_"",""""))
    tag_Index = train_df_loc.log2.apply(lambda x: x.index(""201""))
    tag_Index2 = tag_Index.values[1]
    tag_date =train_df_loc.log2.apply(lambda x: x[tag_Index2:tag_Index2+8])
    train_df_loc.loc[:,'Tag'+str(TagNum)] = pd.Series(tag_date.to_list(),index=train_df.index)  # to be fixed
    return train_df_loc

# Introduce time as input
X_traindf['feature1'] = train_df['feature1']
# introduce first tag per day
TagNum=1
train_df = Tag_per_day(train_df,TagNum)
#examples on groupby
Unq_tag1_grps = list(train_df.groupby(train_df.Tag1).groups.keys())
train_df.groupby(train_df.Tag1).first()
train_df.groupby(train_df.Tag1)['feature1'].count()
X_traindf['Tag'+str(TagNum)] =  train_df['Tag'+str(TagNum)]
#############################
# introduce epoch as tag
#############################
TagNum=2
train_df['Tag'+str(TagNum)] = train_df.epoch
X_traindf['Tag'+str(TagNum)] =  train_df['Tag'+str(TagNum)]
#############################
# introduce core as tag
#############################
TagNum=3
train_df['Tag'+str(TagNum)] = train_df.core
X_traindf['Tag'+str(TagNum)] =  train_df['Tag'+str(TagNum)]
#############################
# introduce day as tag per client
#############################
TagNum = 4
RepNum = np.ceil(train_df.shape[0]/(Num_Cons_Sample_Client*Num_Clients))
Part_Tag_Array=[]
for i in np.arange(Num_Clients):
    Part_Tag_Tmp = list(map(lambda _: i+1,range(Num_Cons_Sample_Client)))
    Part_Tag_Array.extend(Part_Tag_Tmp)

Full_Tag_Array2 = Part_Tag_Array * int(RepNum)
extra_tags = np.abs(len(Full_Tag_Array2) - train_df.shape[0])
Full_Tag_Array = Full_Tag_Array2[:-extra_tags]

train_df.loc[:,'Tag'+str(TagNum)] = pd.Series(Full_Tag_Array,index=train_df.index)
X_traindf.loc[:,'Tag'+str(TagNum)] = train_df['Tag'+str(TagNum)]
#############################
# END day as tag per client
#############################
######### Introduce gpsTime and Tag to the input
Input_str.extend(['feature1',Final_Tag])

###### Adding StandardSalarization:
scaler = StandardScaler()
removed_column = Input_str.pop()
X_train_ScaledTmp = scaler.fit_transform(X_traindf[Input_str],Y_traindf[Output_str])
# Adding Int tag per client without scalarization
X_train_Scaled = np.c_[X_train_ScaledTmp, train_df[removed_column].values.reshape(train_df.shape[0],1)]


# All In/Out data Numpy
Act_Inputs_Int_Tag  = X_train_Scaled
Act_Outputs_Int = Y_traindf[Output_str].values
# Remove Tags
Act_Inputs_Int = np.delete(Act_Inputs_Int_Tag,-1,axis=1) 

# prepare In/Out per Client
All_Act_Inputs_Int_Tag  = [Act_Inputs_Int_Tag[np.where(Act_Inputs_Int_Tag[:,-1]== x)] for x in np.arange(1,Num_Clients+1)]
All_Act_Outputs_Int = [Act_Outputs_Int[np.where(Act_Inputs_Int_Tag[:,-1]== x)] for x in np.arange(1,Num_Clients+1)]
# Remove Tags
All_Act_Inputs_Int = [np.delete(All_Act_Inputs_Int_Tag[x],-1,axis=1)  for x in np.arange(0,Num_Clients) ]


# a need conversion to float32
Act_Inputs = np.float32(Act_Inputs_Int)
Act_Outputs = np.float32(Act_Outputs_Int)
# convert dataset to client based dataset
All_Act_Inputs = [np.float32(All_Act_Inputs_Int[x]) for x in np.arange(0,Num_Clients)]
All_Act_Outputs = [np.float32(All_Act_Outputs_Int[x]) for x in np.arange(0,Num_Clients)]
# convert to OrderedDict
new_batch = collections.OrderedDict([('In', Act_Inputs),('Out', Act_Outputs)])
All_new_batch = [collections.OrderedDict([('In', All_Act_Inputs[x]),('Out', All_Act_Outputs[x])]) for x in np.arange(0,Num_Clients)]
# Convert to tensor
dataset_input = tf.data.Dataset.from_tensor_slices(new_batch)#,,maxval=100, dtype=tf.float32)
# All_new_batch has different item per In / Out
All_dataset_input = [tf.data.Dataset.from_tensor_slices(All_new_batch[x]) for x in np.arange(0,Num_Clients)]
# Select among the datasets
Used_dataset= dataset_input
All_Used_dataset= All_dataset_input


#with eager_mode():
learning_rate = 1
Val_Train_Split = 0.8
SNN_epoch = 50
SNN_batch_size = 1100
shuffle_buffer = 200

def preprocess(new_dataset):
    #return Used_dataset.repeat(2).batch(2)
    def map_fn(elem):
        return collections.OrderedDict([('x', tf.reshape(elem['In'], [-1])),('y', tf.reshape(elem['Out'],[1]))])
     
    DS2= new_dataset.map(map_fn)
    return DS2.repeat(SNN_epoch).batch(SNN_batch_size)

train_data = [preprocess(Used_dataset)]

#######changes###############33
def make_federated_data(client_data, client_ids):
    return [preprocess(client_data[x]) for x in client_ids]

federated_train_data =  make_federated_data(All_Used_dataset, np.arange(0,Num_Clients))




sample_batch = tf.contrib.framework.nest.map_structure(lambda x: x.numpy(), next(iter(train_data[0])))
    
########## END Changes ############            

def create_SK_model():
    modelF = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(Act_Inputs.shape[1],)),
                                  tf.keras.layers.Dense(10, activation=tf.nn.relu, kernel_initializer='zeros'),
                                  tf.keras.layers.Dense(100, activation=tf.nn.relu, kernel_initializer='zeros'),
                                  tf.keras.layers.Dropout(0.5),
                                  tf.keras.layers.Dense(100, activation=tf.nn.relu, kernel_initializer='zeros'),
                                  tf.keras.layers.Dropout(0.5),
                                  tf.keras.layers.Dense(1, activation=tf.nn.relu, kernel_initializer='zeros'),
                                  ])
    return modelF

def loss_fn(y_true, y_pred):
    return tf.reduce_mean(tf.keras.losses.MSE(y_true, y_pred))

def model_fn():
    return tff.learning.from_keras_model(create_SK_model(), 
                                         sample_batch, loss=loss_fn, 
                                         optimizer=gradient_descent.SGD(0.1))

YTrain = Act_Outputs #np.random.rand(50,1)
XTrain = Act_Inputs  #np.random.rand(50,100)
# locally compile the model
Local_model = create_SK_model()
Local_model.compile(loss=loss_fn,optimizer=gradient_descent.SGD(learning_rate))

# training/fitting with TF federated learning
trainer_Itr_Process = tff.learning.build_federated_averaging_process(model_fn,server_optimizer_fn=(lambda : gradient_descent.SGD(learning_rate=1.0)),client_weight_fn=None)
FLstate = trainer_Itr_Process.initialize()
FLlosses  = []
# Track loss of different ...... of federated iteration
for round_num in range(2, 5): 
    """"""
    The second of the pair of federated computations, next, represents a single round of Federated Averaging, which consists of pushing the server state (including the model parameters) to the clients, on-device training on their local data, collecting and averaging model updates, and producing a new updated model at the server.
    """""" 
    FLstate, FLoutputs = trainer_Itr_Process.next(FLstate, federated_train_data)
    print('round {:2d}, metrics={}'.format(round_num, FLoutputs.loss))
    # Track the loss.
    FLlosses.append(FLoutputs.loss)

# fitting without federated learning
trained_local_Model = Local_model.fit(XTrain,YTrain, validation_split=Val_Train_Split, epochs=SNN_epoch, batch_size=SNN_batch_size) #tbuc
# Loss of local model
local_Loss = trained_local_Model.history['loss'] # tbuc
print(""Local Model Loss: ""+str(local_Loss))
# Copy local model for comparison purposes
Local_model_Fed = Local_model
# Setting federated weights on copied Object of local model
Local_model_Fed.set_weights(tff.learning.keras_weights_from_tff_weights(FLstate.model))
# Evaluate loss of the copied federated weights on local model
Fed_predicted = Local_model_Fed.predict(XTrain)
Fed_eval = Local_model_Fed.evaluate(XTrain,YTrain)

print(""Local Model with Federated Weights Loss: ""+str(Fed_eval ))
print(""New: "" + str(Local_model_Fed.loss))
print('End')



# Output:
round  2, metrics=6838.99658203125
round  3, metrics=6838.99658203125
round  4, metrics=6838.99658203125
round  5, metrics=6838.99658203125
round  6, metrics=6838.99658203125
Train on 733 samples, validate on 2936 samples
Epoch 1/50

733/733 [==============================] - 0s 14us/sample - loss: 6449.8022 - val_loss: 6936.1622
Epoch 4/50
.........................................................
733/733 [==============================] - 0s 18us/sample - loss: 6449.8022 - val_loss: 6936.1622

Local Model Loss: [6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375]

  32/3669 [..............................] - ETA: 0s - loss: 10104.2188
1632/3669 [============>.................] - ETA: 0s - loss: 6897.5380 
3232/3669 [=========================>....] - ETA: 0s - loss: 6860.1061
3669/3669 [==============================] - 0s 31us/sample - loss: 6838.9962
Local Model with Federated Weights Loss: 6838.996183714057
It appears this model is not training even without federation, I believe the following lines show the loss during non-federated training,:

```
local_Loss = trained_local_Model.history['loss'] # tbuc
print(""Local Model Loss: ""+str(local_Loss))
```

The loss does not appear to be decreasing:

```
Local Model Loss: [6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375, 6449.80224609375]
```

TFF only extends local training to the federated setting; we would be very surprised if a model that will not train locally was trainable in the federated setting. Closing this as not a TFF issue.

I'd recommend first getting local training to work. StackOverflow with the [tensorflow] and [keras] tags can be good resources. Once local training is working, try adding back TFF for federated optimization."	1	2019-05-29 01:30:22	2019-06-05 03:05:13	2019-05-29 18:46:30
https://github.com/tensorflow/federated/issues/416	[]	Multiple inputs with keras model?	"Multiple inputs with keras model?I'm trying to apply federated learning to an existing keras model that takes two inputs. When I call `tff.learning.from_compiled_keras_model` and include a dummy batch, I get this error: `ValueError: Layer model_1 expects 2 inputs, but it received 1 input tensors. Inputs received: [<tf.Tensor 'packed:0' shape=(2, 20) dtype=int64>]`.

The model accepts two numpy arrays as inputs, so I defined my dummy_batch as:
```
x = tf.constant(np.random.randint(1,100, size=[20]))
collections.OrderedDict([('x', [x, x]), ('y', x)])
```

I dug around a little bit and saw that eventually, `tf.convert_to_tensor_or_sparse_tensor` gets called on the input list (in the `__init__` for `_KerasModel`), and that returns a single tensor of shape (2,20), instead of two separate arrays or tensors. Is there some other way I can represent the list of inputs to avoid this issue?@pgoldberg,

This is definitely something we can take a look at. Attaching a colab with a repro and some quick investigation, and assigning myself.

Colab: 

https://colab.research.google.com/drive/1E7VS3AXICnwmcHXwYum5avh_8K_oLuFyPeter, see [this commit](https://github.com/tensorflow/federated/pull/431/commits/1a87f07eb65962ce300ddfc0f5b96e54468cca24); it should contain the bugfix that you need.Thank you!"	3	2019-05-12 01:49:41	2019-05-15 17:09:54	2019-05-15 17:09:54
https://github.com/tensorflow/federated/issues/399	[]	How adpot other datasets	"How adpot other datasetsHi there, does the current version support other dataset customized by ourselves. The data shown in the tutorials are in HDF5 format and I have a hard time adapting mine to the pipeline.This is probably a good question for StackOverflow, where there is a `tensorflow-federated` tag. A similar question has already been asked in [Create a custom federated data set in TensorFlow Federated](https://stackoverflow.com/questions/55434004/create-a-custom-federated-data-set-in-tensorflow-federated), which might give some of the information you are looking for. 

The `tff.simulation.ClientData` interface supports any file format. The `0.4.0` release contains two implementations: [tff.simulation.HDF5ClientData](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/HDF5ClientData) and [tff.simulation.FilePerUserClientData](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/FilePerUserClientData).

If you have further questions about how to use those APIs, the TFF team would encourage using StackOverflow so the community can benefit from the recorded answers in the future."	1	2019-05-07 04:49:33	2019-05-07 15:48:54	2019-05-07 15:48:54
https://github.com/tensorflow/federated/issues/373	[]	Why TFF runs slowly even with only one client?	"Why TFF runs slowly even with only one client?I found TFF is extremely slow even if I only simulated with one client. In addition, when I insert tf.print() in function ""def __call__(self, dataset, initial_weights)"" in the class ClientFedAvg in federated_averaging.py, running the script in federated_learning_for_image_classification.ipynb will print several times in each round with only one client. But this function should be executed once in each round with each client. I don't know whether it is a bugHi. The runtime we initially included is minimal - it was originally intended primarily for testing and small-scale experiments. Support for higher-performance simulations is coming. This is not to say we cannot make the reference runtime faster. We have not invested much in trying to memoize or cache things to avoid repeated calls like those you described. Perhaps you can help by posting here stack traces from these repeated calls, so that we can see exactly when they happen and diagnose this (I do not remember off the top of my head). Thanks!
Thanks for the reply!
For federated_learning_for_image_classification tutorial, set NUM_CLIENTS = 1 and insert tf.print(""call ClientFedAvg"") at the beginning of function ""def call(self, dataset, initial_weights)"" in the class ClientFedAvg in federated_averaging.py. Then I run the tutorial and get the following results:
call ClientFedAvg
call ClientFedAvg
call ClientFedAvg
round  0, loss=<loss=3.1112683>
call ClientFedAvg
call ClientFedAvg
call ClientFedAvg
round  1, loss=<loss=2.9389212>
call ClientFedAvg
call ClientFedAvg
call ClientFedAvg
round  2, loss=<loss=2.666009>
call ClientFedAvg
call ClientFedAvg
call ClientFedAvg
round  3, loss=<loss=2.1996822>
I think with NUM_CLIENTS = 1, it should print ""call ClientFedAvg"" only once. But I got three each round. It means that it trains repeatly from same initial weights in vain for each client in each round. For another case I wrote, with NUM_CLIENTS = 1 it printed more than 10 times. Thanks!After dividing into this problem, `client_outputs.__getattr__` was called by three times in `run_one_round_tff` in optimizer_utils.py: `client_outputs.weights_delta_weight`, `client_outputs.weights_delta`, and `client_outputs.model_output`, so `ClientFedAvg` was evaluated by three times.This is to be expected in the interpreted runtime, as it is naively interpreting the syntax trees TFF generates.

Have you tried using `tff.framework.set_default_executor(tff.framework.create_local_executor())`?please what is version of cuda that requires TFF?
I would like to execute this tutorial but I find incompatibility in cuda versionI think we can consider this issue closable."	6	2019-04-29 11:44:21	2020-03-09 04:34:53	2020-03-09 04:34:52
https://github.com/tensorflow/federated/issues/361	[]	Using GPU produces error after upgrading to 0.4.0	"Using GPU produces error after upgrading to 0.4.0I updated TFF to 0.4.0 yesterday and found that the image classification tutorial cannot be run correctly now (which worked in previous TFF versions). After executing 

> #@test {""timeout"": 600, ""output"": ""ignore""}
state, metrics = iterative_process.next(state, federated_train_data)
print('round  1, metrics={}'.format(metrics))

The following error message occurs. I believe this problem is also partly mentioned [here](https://github.com/tensorflow/federated/issues/345). Plus, I am interested in knowing of optimizing GPU usage for TFF possibly by looping on the clients in parallel (as the previous [closed issue](https://github.com/tensorflow/federated/issues/278)). Or is there any advice on speeding up the federated training? Now it takes really long time when having a large neural network. Thank you!

> ---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1318       return self._call_tf_sessionrun(
-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)
   1320 

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1406         self._session, options, feed_dict, fetch_list, target_list,
-> 1407         run_metadata)
   1408 

InvalidArgumentError: Could not colocate node with its resource and reference inputs; devices /job:localhost/replica:0/task:0/device:CPU:0 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.
	 [[{{node ReduceDataset}}]]
	 [[{{node subcomputation/StatefulPartitionedCall_1}}]]
	 [[{{node subcomputation/StatefulPartitionedCall_1}}]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-16-8cee327e1bd3> in <module>()
      1 #@test {""timeout"": 600, ""output"": ""ignore""}
----> 2 state, metrics = iterative_process.next(state, federated_train_data)
      3 print('round  1, metrics={}'.format(metrics))

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/function_utils.py in __call__(self, *args, **kwargs)
    598     context = self._context_stack.current
    599     arg = pack_args(self._type_signature.parameter, args, kwargs, context)
--> 600     return context.invoke(self, arg)
    601 
    602 

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in invoke(self, fn, arg)
    698       else:
    699         computed_arg = None
--> 700       result = computed_comp.value(computed_arg)
    701       py_typecheck.check_type(result, ComputedValue)
    702       type_utils.check_assignable_from(comp.type_signature.result,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in <lambda>(x)
    841       return ComputationContext(context, {comp.parameter_name: arg})
    842 
--> 843     return ComputedValue(lambda x: self._compute(comp.result, _wrap(x)),
    844                          comp.type_signature)
    845 

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    782                             computation_types.FunctionType)
    783     if comp.argument is not None:
--> 784       computed_arg = self._compute(comp.argument, context)
    785       type_utils.check_assignable_from(computed_fn.type_signature.parameter,
    786                                        computed_arg.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
--> 746       return self._compute_tuple(comp, context)
    747     elif isinstance(comp, computation_building_blocks.Reference):
    748       return self._compute_reference(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_tuple(self, comp, context)
    800     result_type_elements = []
    801     for k, v in anonymous_tuple.to_elements(comp):
--> 802       computed_v = self._compute(v, context)
    803       type_utils.check_assignable_from(v.type_signature,
    804                                        computed_v.type_signature)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute(self, comp, context)
    742       return self._compute_compiled(comp, context)
    743     elif isinstance(comp, computation_building_blocks.Call):
--> 744       return self._compute_call(comp, context)
    745     elif isinstance(comp, computation_building_blocks.Tuple):
    746       return self._compute_tuple(comp, context)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _compute_call(self, comp, context)
    789     else:
    790       computed_arg = None
--> 791     result = computed_fn.value(computed_arg)
    792     py_typecheck.check_type(result, ComputedValue)
    793     type_utils.check_assignable_from(computed_fn.type_signature.result,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in <lambda>(x)
    869         arg_type = comp.type_signature.parameter
    870         return ComputedValue(
--> 871             lambda x: my_method(fit_argument(x, arg_type, context)),
    872             comp.type_signature)
    873       else:

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in _federated_map(self, arg)
    912     fn = arg.value[0]
    913     result_val = [
--> 914         fn(ComputedValue(x, mapping_type.parameter)).value for x in arg.value[1]
    915     ]
    916     result_type = computation_types.FederatedType(mapping_type.result,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in <listcomp>(.0)
    912     fn = arg.value[0]
    913     result_val = [
--> 914         fn(ComputedValue(x, mapping_type.parameter)).value for x in arg.value[1]
    915     ]
    916     result_type = computation_types.FederatedType(mapping_type.result,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in <lambda>(x)
    773           'but found \'{}\' instead.'.format(computation_oneof))
    774     else:
--> 775       return ComputedValue(lambda x: run_tensorflow(comp, x),
    776                            comp.type_signature)
    777 

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/reference_executor.py in run_tensorflow(comp, arg)
    342     if init_op:
    343       sess.run(init_op)
--> 344     result_val = graph_utils.fetch_value_in_session(sess, result)
    345   return capture_computed_value_from_graph(result_val,
    346                                            comp.type_signature.result)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/graph_utils.py in fetch_value_in_session(sess, value)
    759       if not tf.contrib.framework.is_tensor(v):
    760         raise ValueError('Unsupported value type {}.'.format(str(v)))
--> 761     flattened_results = sess.run(flattened_value)
    762 
    763     def _to_unicode(v):

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-> 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

InvalidArgumentError: Could not colocate node with its resource and reference inputs; devices /job:localhost/replica:0/task:0/device:CPU:0 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.
	 [[{{node ReduceDataset}}]]
	 [[node subcomputation/StatefulPartitionedCall_1 (defined at /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/tensorflow_deserialization.py:122) ]]
	 [[node subcomputation/StatefulPartitionedCall_1 (defined at /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_federated/python/core/impl/tensorflow_deserialization.py:122) ]]

We believe GPU support may be working again in release `0.5.0`, would you try upgrading?Closing as this seems to be handled with relatively few issues by the `local_executor`."	2	2019-04-24 23:51:44	2019-12-06 20:02:33	2019-12-06 20:02:33
https://github.com/tensorflow/federated/issues/326	[]	is it work in win10	"is it work in win10Hi,

We haven't tested anything on Windows machines, so I think we would say this is unsupported.

However, since we publish a pip package under tensorflow-federated, in theory at least this should be cross-platform.

Feel free to submit fixes for anything you may encounter deploying on Windows!"	1	2019-04-10 12:43:33	2019-05-03 17:57:51	2019-05-03 17:57:50
https://github.com/tensorflow/federated/issues/258	[]	Keras set_weights function not working with batch normalization layer in the network	"Keras set_weights function not working with batch normalization layer in the network`model.set_weights()` function from Keras is not working if the network consists of batch normalization layer(s). I looked into the issue, I believe, it is due to having an incorrect order of array elements as returned by `tff.learning.keras_weights_from_tff_weights(state.model)` function which does not match the output of `model.get_weights()`. Also, the underlying reason could be that the `state.model` contains separate tuple for trainable and non-trainable weights.


Any updates on this? Thank you in advance. Thank you for the report @aqibsaeed, apologies for the delay.

I've been trying to reproduce this with little success. Might you be able to provide a minimal code that defines an example model and input data to reproduce this? Which TFF release you see this behavior?

Its also not entirely clear to us (yet) how the batch normalization parameters should be handled in the federated setting.

I used the [colab notebook](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) provided on tensorflow.org/federated website. 

```
def create_compiled_keras_model():
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(64, activation='linear', kernel_initializer='zeros', input_shape=(784,)),
      tf.keras.layers.BatchNormalization(),
      tf.keras.layers.Activation('relu'),
      tf.keras.layers.Dense(64, activation='linear', kernel_initializer='zeros'),
      tf.keras.layers.BatchNormalization(),
      tf.keras.layers.Activation('relu'),
      tf.keras.layers.Dense(10, activation=tf.nn.softmax, kernel_initializer='zeros')
  ])
  
  def loss_fn(y_true, y_pred):
    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(
        y_true, y_pred))
 
  model.compile(
      loss=loss_fn,
      optimizer=gradient_descent.SGD(learning_rate=0.02),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
  return model
```

After one round of training as described in the notebook, I tried to set the weights as follows and got the error pasted below: (Am I not setting the weights correctly?)
```
n_model = create_compiled_keras_model()
n_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))
```

`ValueError                                Traceback (most recent call last)
<ipython-input-31-06625dec9d3d> in <module>()
      1 nglobal_model = create_compiled_keras_model()
----> 2 nglobal_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in set_weights(self, weights)
    406         tuples.append((sw, w))
    407       weights = weights[num_param:]
--> 408     backend.batch_set_value(tuples)
    409 
    410   def compute_mask(self, inputs, mask):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in batch_set_value(tuples)
   2858   if ops.executing_eagerly_outside_functions():
   2859     for x, value in tuples:
-> 2860       x.assign(np.asarray(value, dtype=dtype(x)))
   2861   else:
   2862     with get_graph().as_default():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in assign(self, value, use_locking, name, read_value)
    913     with _handle_graph(self.handle):
    914       value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)
--> 915       self._shape.assert_is_compatible_with(value_tensor.shape)
    916       assign_op = gen_resource_variable_ops.assign_variable_op(
    917           self.handle, value_tensor, name=name)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)
   1021     """"""
   1022     if not self.is_compatible_with(other):
-> 1023       raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
   1024 
   1025   def most_specific_compatible_shape(self, other):

ValueError: Shapes (64,) and (64, 64) are incompatible`



To debug this issue I tried printing weight matrix length for each layer, it seems the order of weight matrix as returned by `tff.learning.keras_weights_from_tff_weights()` is different than `base_model.get_weights()`. 

```
base_model = create_compiled_keras_model()
random_weights = base_model.get_weights()
for rw in random_weights:
  print(len(rw))
```
> 784
64
64
64
64
64
64
64
64
64
64
64
64
10

```
federated_weights = tff.learning.keras_weights_from_tff_weights(state.model)
for fw in federated_weights:
  print(len(fw))
```
> 784
64
64
64
64
64
64
64
64
10
64
64
64
64@aqibsaeed thank you for sharing the model you used and the steps you have already taken, they were very helpful!

It looks like `tf.keras.Model.weights` and `tf.keras.Moldel.get_weights()` are returning the weights in different order. I suspect `set_weights()` is expecting the order returned by `get_weights()`, but TFF is pulling from `.weights` parameter.

I'll look into a fix for this.

```
federated_weights = tff.learning.keras_weights_from_tff_weights(state.model)
for f_w, k_w in zip(federated_weights, base_model.weights):
  print('{} == {}: {}'.format(f_w.shape, k_w.shape, f_w.shape == k_w.shape))
```

> (784, 64) == (784, 64): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64, 64) == (64, 64): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64, 10) == (64, 10): True
> (10,) == (10,): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64,): True

```
for f_w, k_w in zip(federated_weights, base_model.get_weights()):
  print('{} == {}: {}'.format(f_w.shape, k_w.shape, f_w.shape == k_w.shape))
```
> (784, 64) == (784, 64): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64,): True
> (64, 64) == (64,): False
> (64,) == (64,): True
> (64,) == (64, 64): False
> (64,) == (64,): True
> (64, 10) == (64,): False
> (10,) == (64,): False
> (64,) == (64,): True
> (64,) == (64,): True
> (64,) == (64, 10): False
> (64,) == (10,): FalseThank you very much! @aqibsaeed 

(**Note:** this checked-in to source, but isn't in the pip package yet. The next release will be in a week or two)

`tff.learning.framework.ModelWeights` has a new interface method called `assign_weights_to` ([api_docs](https://github.com/tensorflow/federated/blob/master/docs/api_docs/python/tff/learning/framework/ModelWeights.md)) which should be a workaround in the short term.

The code  would look something like:

```
keras_model = tf.keras.models.Sequential(...)
tff_weights = tff.learning.keras_weights_from_tff_weights(state.model)
tff_weights.assign_weights_to(keras_model)
```

Please also be aware that BatchNorm has trainable variables (beta, gamma) and non-trainable variables (mean, variance). How these should be aggregated at the global model is still an open question, and likely the vanilla FedAvg algorithm in the FL API may not be doing what is desired.Sounds good! Thank you very much for the prompt action. I think this issue can be closed now. Hello
I followed the above discussion on keras models with batch_normalization, this is the relevant part of the code:
```
for round_num in range(2, 70): 
    FLstate, FLoutputs = trainer_Itr_Process.next(FLstate, federated_train_data)   
    FLlosses_arr.append(FLoutputs.loss)
    tff_weights= tff.learning.keras_weights_from_tff_weights(FLstate.model)
    tff_weights.assign_weights_to(tff_weights, Local_model_Fed)
```


I appreciate the help with the following error:
```
Traceback (most recent call last):
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 2225, in <module>
    main()
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 2218, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 1560, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\pydevd.py"", line 1567, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Users\ezalaab\.p2\pool\plugins\org.python.pydev.core_7.1.0.201902031515\pysrc\_pydev_imps\_pydev_execfile.py"", line 25, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:\Users\ezalaab\Documents\eclipse-workspace\MANA-FederatedLearning\location-based-federated-learning\Test\FederatedLearningTFv3.py"", line 299, in <module>
    tff_weights.assign_weights_to(tff_weights, Local_model_Fed)
AttributeError: 'list' object has no attribute 'assign_weights_to
```"	8	2019-03-20 20:58:09	2019-06-01 16:52:28	2019-04-01 17:30:47
https://github.com/tensorflow/federated/issues/236	[]	Tensor is not an element of this graph	"Tensor is not an element of this graphI've tried to reproduce a tutorial ""Federated Learning for Image Classification"" in Colab [1] and encountered a `ValueError` during execution

    iterative_process = tff.learning.build_federated_averaging_process(model_fn)

The error is following:

     ---------------------------------------------------------------------------
     ValueError                                Traceback (most recent call last)
     /usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
        1091             subfeed_t = self.graph.as_graph_element(
     -> 1092                 subfeed, allow_tensor=True, allow_operation=False)
        1093           except Exception as e:

     /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)
        3477     with self._lock:
     -> 3478       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
        3479 

     /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)
        3556       if obj.graph is not self:
     -> 3557         raise ValueError(""Tensor %s is not an element of this graph."" % obj)
        3558       return obj

     ValueError: Tensor Tensor(""Placeholder:0"", shape=(), dtype=float32) is not an element of this graph.

How it can be solved?


[1] https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynbThanks for the report @osanwe. We're are actively working on the next release, which will fix this.

The following link should work for the next few days, until the new release:
https://colab.research.google.com/github/tensorflow/federated/blob/test_237819542/docs/tutorials/federated_learning_for_image_classification.ipynbHi @osanwe, we updated to v0.2.0 earlier today; this notebook now works. Thanks for the vigilance!Hi, the above link is not working. I also faced the same issue."	3	2019-03-12 09:56:59	2022-07-01 00:29:20	2019-03-13 04:36:48
https://github.com/tensorflow/federated/issues/225	[]	No 'loss' attribute	"No 'loss' attributeI was able to run the Federated Image classification tutorial last week. But now this error is popping:
![tff_eror](https://user-images.githubusercontent.com/8556809/53893611-6e00ec80-4026-11e9-9a05-e03eed235a5d.PNG)
Testing #226, will merge once all tests pass#226 is merged, this error should be fixed.Thanks!"	3	2019-03-06 15:42:24	2019-03-06 17:21:07	2019-03-06 17:21:07
https://github.com/tensorflow/federated/issues/220	[]	Issues with enabling eager execution.	"Issues with enabling eager execution.I was following the tutorials included with the repo, The one on https://github.com/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb to be specific, and on executing 
`example_element = iter(example_dataset).next()`

I got the following error 
`RuntimeError: dataset.__iter__() is only supported when eager execution is enabled.`

While following the tutorial I did run `tf.enable_eager_execution()`, which didn't return any unusual output, but `tf.executing_eagerly()` still returns False. 

TF Version 1.13.1 not sure, what was wrong. Works on a different VM."	1	2019-03-05 07:33:07	2019-03-05 09:17:28	2019-03-05 09:15:43
