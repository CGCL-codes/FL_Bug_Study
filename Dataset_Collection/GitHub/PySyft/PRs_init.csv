url,label,title,all_text,comments,merged,created_time,updated_time,closed_time
https://github.com/OpenMined/PySyft/pull/6642,[],M1 Hot fix,"M1 Hot fix## Description
Added ruamel installation for M1.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2022-07-19 09:37:29,2022-07-20 03:25:30,2022-07-20 03:25:30
https://github.com/OpenMined/PySyft/pull/6637,[],Small typo,"Small typo## Description
Simply solved a small typo.

## Affected Dependencies
None.
",1,True,2022-07-15 10:46:48,2022-07-15 10:57:40,2022-07-15 10:57:40
https://github.com/OpenMined/PySyft/pull/6631,[],Trying fixed docker cache filter,"Trying fixed docker cache filter## Description
Trying fixed docker cache filter

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2022-07-08 04:40:31,2022-07-11 11:03:51,2022-07-11 11:03:51
https://github.com/OpenMined/PySyft/pull/6595,[],[WIP]Fix Incorrect privacy budget spent,"[WIP]Fix Incorrect privacy budget spent## Description
Since , we use FPT , which increases the range of the input tensor, this inturn causes large rdp constants, leading to high privacy budget spent, the PR aims to decode FPT values right before publish.

## Affected Dependencies
`Vectorized Publish`

## How has this been tested?
Existing Tests
",2,False,2022-06-13 10:15:37,2022-06-22 04:42:26,2022-06-22 04:42:26
https://github.com/OpenMined/PySyft/pull/6588,[],Upgrading cypress to fix json-schema security issue,"Upgrading cypress to fix json-schema security issue## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2022-06-08 07:43:34,2022-06-20 06:27:41,2022-06-20 06:27:40
https://github.com/OpenMined/PySyft/pull/6575,[],S.T.E.V.E. - Service for Tracking Errors and Verify Exceptions,"S.T.E.V.E. - Service for Tracking Errors and Verify Exceptions## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",3,False,2022-06-05 21:35:49,2022-07-08 14:46:32,2022-07-08 14:46:32
https://github.com/OpenMined/PySyft/pull/6573,[],Disable checking all domain health by default,"Disable checking all domain health by defaultnormally when we run network_client.domains it checks the health of all the domains before doing anything. as the number of domains increases (and we also discover some bugs/issues) this seems to be causing problems by default - disabling for now@madhavajay should we re-enable this, since all messages are sent with timeouts or at least have some sort of default timeout?Should we have a larger timeout? since it fetches all datasets so deserialization can take time if there are a large number of datasets uploaded.",1,True,2022-06-03 22:37:22,2022-06-07 07:15:36,2022-06-07 07:15:35
https://github.com/OpenMined/PySyft/pull/6567,[],Fix Network hangs Bug and add timeouts,"Fix Network hangs Bug and add timeouts## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

**Changes: **

- Add a 15 sec timeout to Signed with reply messages in msg_forwarding service.
- Add a timeout on sy.connect.
- add an ExceptionMessage if sy.connect time's out.
- add a retry mechanism to submitting association request, if failed to connect to network via sy.connect.
- Make ReceiveAssociationRequestMessage Without Reply.
- make ImmediateMessageWithoutReply message in syft_route to run async in celeryworker.

[ch-6297]
cc @IonesioJunior 

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
MAYBE:  Line 479, could we use the same vpn_status variable( in L462) to retrieve host ip , as self.vpn_status makes a new network call ,Yes, good catch. üëçFixed in this commit: https://github.com/OpenMined/PySyft/pull/6567/commits/7e031150a03d995c3824c66527465d573c896035",2,True,2022-06-01 16:07:51,2022-06-06 09:41:59,2022-06-06 09:41:58
https://github.com/OpenMined/PySyft/pull/6546,[],Patch dep torchcsprng for PySyft 0.5.0 on Apple M1 chips,"Patch dep torchcsprng for PySyft 0.5.0 on Apple M1 chips ## Description
The setup configuration file has been modified to allow the installation of `syft 0.5.0` on Apple with M1 chips.

While testing the change (i.e. _within an environment without neither `torchsnprng` nor `sympc` installed_), I also ended up adjusting the rules for a few packages in the env that caused some clashing leading to failures in all the `syft.test` env in `tox`  (more details below).

#### Dropping `torchsnprng` dependency

The change affects the `setup.cfg` file now including an extra configuration group for `torchcsprng`.
This group has been added to the `torch_ecosystem` requirements, and ignores the installation of `torchcsprng` if `platform_machine=='arm64'` (i.e. Apple M1)

#### Fixing Package versions

Some packages in  in `syft` and `dev` configuration led to version clashing when bluntly installing the latest versions of some packages (e.g. `nbconvert 6.5`, `black 22.0.3`).
Those clashing affected all the corresponding test env in `tox`, ultimately leading to errors in all the (`syft`) tests.

FYI, `tox` `syft.test.*` have been used to test the removal of `torchcsprng` from the deps. 

## Affected Dependencies

None, really. 
In fact, `torchcsprng` is currently used only in `SyMPC` which is only listed as optional requirement for `syft_0.5.0` (via `requirements.unstable.txt`), and not even included when installing `syft==0.5.0` directly from PyPi.

## How has this been tested?

Running and checking that all the `syft.test.*` in `tox` complete successfully. 

_Note_: I don't have Apple M1 chip computer, so I tested it the other way around, that is negating the condition in `setup.cfg` to have environments **without** `torchcsprng` installed.
 
_Take away_: With my surprise this was true also for `syft.test.libs`. 
Therefore, in this PR I also changed `tox.ini` getting rid of the `requirements.unstable.txt` dependency whenever not needed.

## Checklist
- [X ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [X ] My changes are covered by tests",2,True,2022-05-27 19:03:26,2022-06-08 04:04:57,2022-06-08 04:04:57
https://github.com/OpenMined/PySyft/pull/6530,[],Fixed lazy repeat array subtraction,"Fixed lazy repeat array subtraction## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2022-05-26 16:04:08,2022-05-27 05:11:38,2022-05-27 05:11:37
https://github.com/OpenMined/PySyft/pull/6522,[],Fixing empty gamma tensor state,"Fixing empty gamma tensor state## Description
If a data scientist tries to publish a PhiTensor, or a PhiTensor with public values added to it, the state tree will be empty when `publish` is called, resulting in failures. This PR adds a small check for this and rectifies the state tree.

## Affected Dependencies
Nothing new.

## How has this been tested?
Re-running failed tests

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2022-05-25 15:59:24,2022-05-26 04:43:55,2022-05-26 04:43:55
https://github.com/OpenMined/PySyft/pull/6490,[],DataSubjectList Fix ,"DataSubjectList Fix ## Description
When performing operations between multiple `PhiTensors`, the `DataSubjectLists` did not combine, but rather took on the `DataSubjectList` of the first `PhiTensor` involved in the operation (i.e. if `gamma = phi1 + phi2`, then `gamma.data_subjects` would be equal to `phi1.data_subjects` but it should reflect that it has data belonging to data subjects in phi2 as well. 

This had been (silently) causing inaccuracies in the publish method, which our existing PhiTensor and GammaTensor tests didn't catch. 

## Affected Dependencies
This mainly affects DataSubjectLists and GammaTensors, and may result in changes to PhiTensor, PhiTensorAncestors, and a few other classes.

## How has this been tested?
Tested in notebooks both by directly creating PhiTensors, as well as by creating a Syft Tensor and calling `.private.`

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,False,2022-05-15 04:07:11,2022-06-24 22:49:40,2022-06-24 22:49:40
https://github.com/OpenMined/PySyft/pull/6487,[],small notebook fixes,"small notebook fixes## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2022-05-13 07:32:47,2022-05-13 11:02:29,2022-05-13 11:02:29
https://github.com/OpenMined/PySyft/pull/6480,[],Fix for stack Course Tests,"Fix for stack Course Tests## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

[ch-6044]

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2022-05-12 06:19:47,2022-05-13 03:48:28,2022-05-13 03:48:27
https://github.com/OpenMined/PySyft/pull/6470,[],Fix bug with race condition with flag `install=false` on remote machine,"Fix bug with race condition with flag `install=false` on remote machine## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2022-05-10 10:58:08,2022-05-11 05:07:22,2022-05-11 05:07:21
https://github.com/OpenMined/PySyft/pull/6464,[],local user group and hagrid launch message,"local user group and hagrid launch messageSupport for user account management in Windows 10 Home Edition is not well explained on Microsoft pages too. So, adding those two lines of text (as a Note) will save time for those who are trying hagrid for the first time. 

Also, the hagrid launch error message is cryptic - just changing to 'domain' works.

## Description
Those notes will help save time for those Windows 10 users with Home edition, who are eager to get hagrid launch to happen successfully.

## Affected Dependencies
No dependencies

## How has this been tested?
I verified the steps -",1,True,2022-05-08 07:08:21,2022-05-09 06:07:48,2022-05-09 02:18:09
https://github.com/OpenMined/PySyft/pull/6462,[],np.dot implementation for Phi and Gamma Tensors ,"np.dot implementation for Phi and Gamma Tensors ## Description
Dot implementation for phi and gamma tensors

## Affected Dependencies
None 

## How has this been tested?
Yes, tests has been added for both

## Checklist
- [ x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ x] My changes are covered by tests
We might be losing information this way- for instance the `func` class variableBe sure to remove print statements :)Calculating the metadata might be more complicated then this- consider the case where `self` has min_val is 1, and max_val is 10, and `other` has a min_val and max_val of -1 and -2 respectively :) I believe that if `other` is a numpy array, it shouldn't have a `.child`. Has this been tested?It just occurred to me- although I really like this implementation because it's very explicit, but perhaps we could simplify this to `if self.data_subjects.one_hot_lookup != other.data_subjects.one_hot_lookup`That could also reduce the number of `elif` conditions we have to test forthis is totally valid, but it might be better to compare against `reference_data.shape` since that is our ground source of truth :) 
(The reasoning being that if for some reason PhiTensor.child was not initializing properly, this test could alert us to that)I would rename this test to reflect that you're also testing the `sum` operation :)Hhmm looks like all gamma tests test the sum operation as a way to ensure that the gamma tensor was properly implemented.Nicely done! You might need to manually cast this as a `float` to get rid of the `# type: ignore` since we try to only use that as a last resort üëç Two of these are redundant and not needed- we typically only need to check all scenarios if `other` is a DP Tensor and has min/max val metadata of its ownMight need to manually wrap this in `float()` to avoid type errors Unfortunately the min_vals may become greater than the max_vals if other has negative numbers, so we have to do something similar to what you did above",2,False,2022-05-06 23:29:55,2022-07-13 16:41:14,2022-07-13 16:41:13
https://github.com/OpenMined/PySyft/pull/6456,[],Fixed issue with tempdir and public key in HAGrid,"Fixed issue with tempdir and public key in HAGrid- Updated network operator notebook for AdAstra
- Pinned cryptography breaking paramiko
  see: https://github.com/paramiko/paramiko/pull/2039
- Added /ping endpoint so that NetworkRegistry works on --headless

## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2022-05-06 04:55:48,2022-05-06 04:56:00,2022-05-06 04:55:59
https://github.com/OpenMined/PySyft/pull/6448,[],Extend rdp cache to dramatically improve accuracy of DP system,"Extend rdp cache to dramatically improve accuracy of DP system## Description
The cache used to speed up the privacy budget calculation was not granular enough when the value of the RDP constants was between 0-1. This resulted in every query having a minimum privacy budget spend of ~7.766, even if it was supposed to spend significantly less.

This PR extends the cache significantly to include (as of now) 1.2M cached values. The minimum privacy budget spend will be reduced from 7.766 to ~0.05. [Numbers may change in the future depending on user feedback.]

## Affected Dependencies

The cache file located in `syft/src/syft/cache` will be replaced, and its size will increase by a factor of 4, from around 2.3MB to around 9.2MB.
This may also solve some other bugs in the codebase- by default, admin accounts have a PB of 5.55, and since the minimum PB spendable (prior to this PR) was  7.766, there may have been things breaking.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

Currently this has been prototyped in notebooks, but will add dedicated tests as I incorporate changes into the codebase. 

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
:heart:",1,True,2022-05-05 07:11:27,2022-05-19 15:11:20,2022-05-19 15:11:19
https://github.com/OpenMined/PySyft/pull/6444,[],Fix for registering users from notebook using `sy.register`,"Fix for registering users from notebook using `sy.register`- grid: refactor register api in grid to create new user
- syft: refactor register method in grid to convert url to grid url

## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

[ch-5187]

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2022-05-04 14:01:11,2022-05-05 22:26:16,2022-05-05 22:26:16
https://github.com/OpenMined/PySyft/pull/6421,[],Missing `f` prefix on f-strings fix,Missing `f` prefix on f-strings fixFixes #6420,1,True,2022-04-23 22:21:00,2022-05-04 04:42:15,2022-05-04 04:42:15
https://github.com/OpenMined/PySyft/pull/6415,[],Launch mutiple nodes in azure using hagrid,"Launch mutiple nodes in azure using hagrid## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

[ch-5688]

```
hagrid launch domain to azure --jupyter --node_count=2
```
Launching multiple VMs
![multiple_vm_table](https://user-images.githubusercontent.com/11032835/164732031-6a1c977d-6e69-4320-86df-1d766876b41c.png)

VM Launch Completed
![vm_launch_completed](https://user-images.githubusercontent.com/11032835/164732087-dc8901b1-f977-4080-9f21-bf06feb941ad.png)

VM (any) Launch Fails
![fails_to_launch](https://user-images.githubusercontent.com/11032835/164732176-f40d9833-1e00-4185-8d95-78cee4c706bf.png)

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Adding this sleep, for now, it tries to install hagrid, while the build directory for hagrid is still in progress.",1,True,2022-04-20 14:21:36,2022-05-04 00:25:10,2022-05-04 00:25:09
https://github.com/OpenMined/PySyft/pull/6410,[],Correcting minor bugs in osx tutorial,"Correcting minor bugs in osx tutorial## Description
Proposing some changes:
Conda environment is set up as `syft_env` in the beginning but referenced as `syft` later. So adding the `_env` where it is missing.
Maybe I missed something, but `pip install syft` is not mentioned in the tutorial. I have added it as step 5.5.

Also, the tutorial is overly specific for MacOs11_5_1. But I have completed it successfully on macOs Monterey 12.1. Maybe consider changing the requirements?


## Affected Dependencies
Only /docs

## How has this been tested?
I ran the tutorial successfully in my machine with the proposed changes

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2022-04-18 19:35:03,2022-05-04 04:25:06,2022-05-04 04:25:06
https://github.com/OpenMined/PySyft/pull/6407,[],add hagrid version command,"add hagrid version command## Description
This PR is to add a `hagrid version` command to Hagrid which prints the hagrid version to the cli.
I'm not sure this is a good idea to have syft imported directly into HAGrid. Even when we add the ability for HAGrid to manage syft environments for us we probably won't want to import syft itself.I think users expect to type something like `version` and in that case perhaps we can just print the hagrid version since all the other information is already available under `hagrid debug`.Remove unused commented lines to keep our PRs to only include changes that are being used.Okay, thanks Madhava! @madhavajay I was actually thinking the same since we have `hagrid debug` happening, we could add anything there and keep `--version` to just print hagrid version. I'll do that.@madhavajay, right also since we are dumping all other stuff on the `debug` command, I'll strip it off. ",1,True,2022-04-15 07:09:54,2022-04-29 23:17:28,2022-04-29 23:17:27
https://github.com/OpenMined/PySyft/pull/6406,[],Apply to Network Bug,"Apply to Network Bugcreate a notebook to demonstrate the network apply is working fine

## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

[ch-5605]

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,True,2022-04-14 10:43:27,2022-04-25 01:01:57,2022-04-25 01:01:56
https://github.com/OpenMined/PySyft/pull/6385,[],SeaweedFS Tech Debt,"SeaweedFS Tech Debt## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

https://app.shortcut.com/openmined/story/5094/feature-add-seaweedfs-to-syft-grid


- [x] Changes to Python .send for REPTs & NDEPT uses new FileUpload API (make sure this can be toggled)
- [x] Python Client handles errors like timeout on presigned
- [x] Add functionality to delete data from blob store if the corresponding dataset/pointer is deleted on .get()
- [x] Spicy Swan Upload Test
- [x] Spicy Swan e2e Test for Large Action results being stored in Blob storage
- [x] Create a task that runs in the Domain and checks the upload bucket for old uploads and deletes them
- [x] Add proper permissions to UploadDataMessage

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Deleted this method, cause I do not feel the need to create empty datasets anymore, since `load_dataset` is now very fast.Doesn't this mean that objects are no longer deleted when a user calls .get()?@madhavajay they are deleted, but not inside the `get_object_action.py`. The logic to delete is moved to a different message. Refer to:
https://github.com/OpenMined/PySyft/pull/6385/files#diff-b52976171f0fc89f259716fc13dd749fdbf453a72a5cfe2be9537e07a68d56feR238I guess the `CreateDatasetMessage: create_dataset_msg,` on the `dataset_manager_service` side can be left for now until we refactor the services?We still need `CreateDatasetMessage: create_dataset_msg` as it is still being used by `load_dataset` to store proxy obj in the database.Yes, awesome. I can see now there is a seperate step which calls an explicit Delete this is great and much much better! ‚ù§Ô∏è",1,True,2022-03-30 08:51:00,2022-04-14 00:17:34,2022-04-14 00:17:33
https://github.com/OpenMined/PySyft/pull/6337,[],SeedweedFS Integration,"SeedweedFS Integration## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
Q: Any specific reason we set endpoint as `weed` here and as `localhost` in file packages/grid/backend/core/config.py ?and for password also?MAYBE: could we rename to seaweedfs?MAYBE: could we rename to seaweedfs?Q:Any specific reason we limit master server to 1024 MB?
or it because it diverts all storage to volume server, we could keep a low size for master server
I think in grid/core/config.py
https://github.com/OpenMined/PySyft/blob/d004b6127fa0b37aa0d81c573a1299a7981bf897/packages/grid/backend/grid/core/config.py#L97
The password was set to None, should we change it to ""admin""Fixed by [4949dc8](https://github.com/OpenMined/PySyft/pull/6337/commits/4949dc80b35837b60bdc1967d04ac8ab16ab1ab0)Fixed by [b0c2b88](https://github.com/OpenMined/PySyft/pull/6337/commits/b0c2b8871940bc8b36d6cc41cd103b36dcb4fadd)Great observation points right there. Ideally the seaweed variables will be loaded using environment variables, but as we were developing this new functionality, we set these ‚Äã‚Äãstatic values for a moment. But as you can see in my last PRs this has been solved!

Again, thanks for taking some time to review this PR. ‚ù§Ô∏è This option actually makes the master server stop directing writes to oversized volume servers (In this case >1GB). It helps to keep the volume servers balanced. The value itself (1GB) was arbitrarily chosen from what I know. But we can do benchmark tests to check what would be the best value.Q: Why do we pass settings to Object Store, I think we are not using it now, or will it be used later?@rasswanth-s we will be using this later, there is some work still left on refactoring this part of the code.Nice Documentation :rocket: NITPICK: maybe directly set the default value of dataset_name as `""""`Q:Any semantic reason we add blob to the prefix of the url?I Imported  it here , to have upload service in `globals()`
Should find a better way to do this.Not having it in global namespace caused `index_syft_by_module_name` to fail in deserialize.Yes this serves as reverse proxy at traefik+1",2,True,2022-03-01 08:07:08,2022-03-22 02:04:24,2022-03-22 02:03:52
https://github.com/OpenMined/PySyft/pull/6335,[],Fix DatabaseManager typo.,"Fix DatabaseManager typo.## Description
I think it is a typo.

## Affected Dependencies

## How has this been tested?

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2022-02-28 11:55:03,2022-02-28 15:18:47,2022-02-28 13:50:13
https://github.com/OpenMined/PySyft/pull/6333,[],Mock notebooks for Login Flow and Account Info Update,"Mock notebooks for Login Flow and Account Info Update## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
<p>When verbose=True will the process just take longer?</p><p>Wondering if the user will remember to set verbose to FALSE without documentation. Is that a typical property the user would expect to clarify for logging in.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='3'/><p>Awesome üôå</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='5'/><p>Ahh I see. Forgive me if I'm not up to speed in Python yet. So if verbose=FALSE will they not be able to get error messages?</p><p>Is the idea that we ask them to put in verbose to help troubleshoot when issues happen?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='9'/><p>To clarify as a guest user is the idea that they can search for datasets on this domain but cannot do anything else?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='12'/><p>Just going to give some ideas here for potential scenarios that could be addressed:</p><ol><li>Error handling for when a domain is offline</li><li>Error handling for when a network is offline</li></ol><p> </p><p><strong>Ideas &amp; Questions:</strong></p><ul><li>Do we have a method that will return a list of available actions to run if so we can mention that method in the ""Successfully Logged In"" message. So a ""Connecting to https://ca.openmined.com  Logging into Canada Domain...  Logged in ü•≥!!  Logged in as sheldon@caltech.edu Please use the command: <code>command.name</code> at anytime to see a list of commands available"" or something to that effect. Thoughts? </li><li>General question is it possible for the same user account to be logged into under 2 different IP's simultaneously?  </li><li>How does a user log out of a domain? Or is that not necessary in this case?</li></ul>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='17'/><p><strong>Things that I'm thinking about for this:</strong></p><ul><li>How can we enforce secure password standards like minimum 7 characters with special characters etc?</li><li>Is there a way to encourage a Data Scientist to give further contextual information to a Domain Owner like ""Institution"" or ""Profile"" </li></ul>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='2'/><p>Should/Can we specify when the domain is offline?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='3'/><p>Suggestion:</p><ul><li>A user with this email already exists on { node.name }. You can try logging in by...(login instructions)</li></ul><p> </p><p>Reason:</p><ul><li>May be nice to remind the user which domain they are trying to register to</li><li>Would be nice to remind them on how to login since they seem to already have an account</li></ul>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='5'/><p>For now let's specify that...</p><ul><li>Congratulations !!! Sheldon Cooper is successfully registered to the Canada Domain node. Logging in....Success.</li></ul><p>This way we can avoid confusion for the user on whether they registered to the network or the domain. Basically a reinforcer that they registered to this domain node.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='6'/><p>Additionally some things have been on the mind that I'm not sure if we should tackle now or later is...</p><ul><li>Should we limit the attempts to log in with incorrect passwords within a given timeframe?</li><li>What happens if they forget their password?</li></ul><p> </p><p>I'm curious to know what your thoughts are on the priority of these items and any ideas you have on how we should start addressing them.</p><p><code>verbose=False</code> will turn off any type of acknowledgement statements/messages for the user. e.g. when a user verbose logs in with <code>verbose=True</code>,p they we will see acknowledgement messages of connecting to domain and with which email they are logged in. So, verbose=True  provides more details/info on the operation being performed. (Helps user during troubleshooting).</p><pre class=""ql-syntax"" spellcheck=""false"">Connecting to https://ca.openmined.com  
Logging into Canada Domain...  Logged in ü•≥!!  
Logged in as sheldon@caltech.edu  
</pre><ol><li>Yes, <code>verbose=True</code> provides more information/details on the operation being performed, helps a user during troubleshooting.</li><li>For the second comment, great questions there:</li></ol><ul><li class=""ql-indent-1"">Should we limit attempts... : I think restricting attempts are majorly implemented to prevent malicious attempts to login. It will be a good security measure to do so. On a priority level I feel its a good to have type feature and not very urgent.</li><li class=""ql-indent-1"">Forget password ... : Yeah, I feel this should be implemented on a higher priority. Because, there is no way where we can reset a user's password (even if I am an Admin). We can have <code>reset_password</code> in both UI and notebook.</li></ul><p>Yes correct, they can only explore datasets but cannot access them. They will need to register themselves to the domain in order to do so.</p><p>Yes, these are some great edge cases, thanks Kyoko for pointing them out. I will add them.</p><p> </p><p>Ideas &amp; Questions:</p><ul><li>Yes we can definitely have something like <code>command.help</code> to list the available commands. Although, python provides a <code>help</code> method that can be used to accomplish a similar thing. So, <code>help(command)</code> list all the methods that can access through the command along with their usage. Possibly we can build something like <code>command.help</code> that executes 	 <code>help(command)</code> functionality . But yes, I will add this as part of the message.</li><li>By different IP's do you mean that can a user to login into two domains at the same time ?? If that the case then yes. A user can login into any number of domains at the same time.</li><li>Log out functionality in notebooks is not required, cause a user is logged in as long as the notebook is alive. When a notebook dies, a user can not access their logged  session. They  need to start a new one. Although, a similar think called auto session expiry can be something we can think of in future if there is a use case where we need to logout a user even from notebooks.</li></ul><p>Nice, I will update the message.</p><p>üëç</p><p>I think we can do that. Will add a case to showcase that.</p><ul><li>Yes, we can enforce some secure password standards. We will have to build a password validator and it can raise an error if the password does not match the standards. Do we have something in the UI, if so we can use the same in notebooks, if not we can decide on a common thing (possibly: one upper case, one special character, one numerical value, and minimum 7 characters).</li><li>Either we can make certain fields compulsory during registration itself like Institution. Or prompt the user to update/complete their profile or certain fields in the profile whenever their perform a login. These are two ways I can think of w.r.t notebooks.</li></ul><p>Gotcha ^_^</p><p>Yeah cool might create a Feature request story so that we can then loop in Baye and Madhava around the topic. ^_^</p><p>Story added in shortcut: https://app.shortcut.com/openmined/story/5227/account-settings-reset-password</p><p>Great ^_^</p><p>Thanks Shubham this helps clarify.^_^</p><ul><li>If there is already a Python function then no need to create a command ^_^</li><li>By different IP's I'm trying to see if there are edge cases or security vulnerabilities so by different IP's I mean a user on one device tries to login to say Sheldon's account on Canada Domain and a user on a different device also tries to login to Sheldon's account on Canada's domain. Is that currently possible to do?</li><li>As far as logging out thanks for clarifying wasn't sure of the natural behavior there ^_^</li></ul><p>Yeah great ideas. So...</p><ul><li> Nothing in UI to that effect yet but have already discussed with Baye and Thiago and think that should be added as a To-Do in the backlog: https://app.shortcut.com/openmined/story/5229/chore-define-password-requirements</li><li>Thinking that fields like ""Institution"" should not be required but that we could do your second idea of prompting the user to fill out their profile.</li><li class=""ql-indent-1"">Maybe something along the lines of ""Introduce yourself to the Domain Owner by filling out your profile. These properties will be seen by the Domain Owner when they are triaging your result requests.""</li></ul>@Kiaka007 Yes, a user's account can be accessed from two or more different devices/IPs simultaneously. Currently, there is no restriction.<p>Great !!! I will add these changes w.r.t Complete profile message with the Account update Flow, cause at that point I also be able to indicate the steps to update information. Will add this reference to the Account update ticket.</p><p>üëç</p><p>Thanks @shubham there's nothing wrong with that by itself I just wanted to know so I can further think of edge cases ^_^ If you think of any just let me know ^_^ </p><p>Got it ^_^</p><p>Let's add some detail to the syft.register prompt... like ""As a guest, you can look around and see what data is on this Domain. When you're ready to begin your data science project, create an account by calling syft.register().</p><p> </p><p>Alternatively it might be nice to be able to call register on the guest account handle a well, just to spare having to type in all the url and port and stuff again.</p><p>Noteworthy that sy.networks[0] logs in as a guest user to the network. Should we introduce this concept yet?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='14'/><p>To rift off of Andrew...</p><blockquote>Logged in as GUEST USER</blockquote><blockquote>Explore! See what datasets are on this domain by using the command <code>data_scientist_node.datasets</code> When you're ready to begin your data science project you can create an account by calling <code>syft.register()</code></blockquote>To clarify and @shubham3121 correct me if I'm misreading this but it's logging them in as a guest user to the domain not the network. So this is the same scenario as above just using the network as a proxy to find the domain. üëçüèæ <p>Yes, the statement <code>network_client = sy.networks[0]</code>  (this is already implemented) logs a user as  a guest a user and is used as a proxy to find the domain. Otherwise, no other operation can done on the network node.</p><p>The <code># Call the .register method</code> comment should probably include information about what the <code>.register</code> method is going to do (register with the website?). Love the inclusion of potential calls they might want to make as a guest user.  </p><p>After talking it over with Shubham we thought that we should perhaps NOT offer a support email in this case (because the domain node may not exist, and we don't know if it has a network node, etc.), we should offer some general assistance (check your URL, contact the source where you obtained the URL) instead. If they are getting the URL incorrect that's probably the only feasible approach.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='15'/><p>This email gets pulled from the data / domain owner user information set in the UI? (Shubham suggested an explicit field called Support Email.) Unless the domain doesn't exist. In which case.... the network support email?</p><p> </p><p>Also, if the user is getting the URL wrong (as we discussed, if they're getting a 404 error) we want to nudge them to check the URL. So ""seems to be offline at the current moment. Please check your URL and ensure that it's correct.""</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='16'/><p>In this case, if there's an error we may not have a support email, etc. As with the login notebook, we probably want to provide some general troubleshooting steps (check your URL, check in with the source of the URL) and leave it at that.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/6333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='9'/><p>I see I think that makes sense ^_^</p><p>Same as above I'm down to pivot in that direction. Although might want to include text that gets at the idea that they may have to wait for the node to be online. As in if they do have the correct URL and the domain is simply not online at the moment they should be aware that it may not be a user error but just that the node is not online and they need to check back in later.</p>",3,True,2022-02-28 07:40:46,2022-05-04 05:57:45,2022-05-04 05:57:44
https://github.com/OpenMined/PySyft/pull/6309,[],Add stub for public divide for share tensor,"Add stub for public divide for share tensor## Description
Add the divide operator to the SMPC -- this will be required for the Fixed Precision Tensor.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- It is still WIP

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2022-02-20 20:27:50,2022-03-09 06:41:34,2022-03-09 06:41:33
https://github.com/OpenMined/PySyft/pull/6262,['bug '],Add test and fix for store object mutation.,"Add test and fix for store object mutation.## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

A guest user is able to mutate an object owned by the data owner.

Shortcut Links:
- https://app.shortcut.com/openmined/story/4457/guest-mutation-security-fix
- https://app.shortcut.com/openmined/story/4918/bug-object-mutation-exploit

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
TODO: remove this.@shubham3121 , are we clear with those questions we earlier had based on the algorithm?",2,True,2022-01-21 04:37:04,2022-02-16 00:00:04,2022-02-16 00:00:03
https://github.com/OpenMined/PySyft/pull/6254,[],FIX: new budget calculated incorrectly for budget requests in domain GUI client,"FIX: new budget calculated incorrectly for budget requests in domain GUI client## Description
This PR fixes *new budget* calculation on the Budget Request GUI page.
closes: #6253

## How has this been tested?
On a test node, a screenshot is attached.
<img width=""1344"" alt=""Screenshot 2022-01-16 at 23 45 17"" src=""https://user-images.githubusercontent.com/48272539/149677422-3f347ed1-8f86-4e36-b494-a4bdbe0dbf8a.png"">


## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,True,2022-01-16 20:47:14,2022-02-18 01:39:44,2022-02-18 01:39:44
https://github.com/OpenMined/PySyft/pull/6250,[],Fix Windows Tests,"Fix Windows Tests## Description
Windows tests debugging.",1,False,2022-01-13 14:16:06,2022-01-13 18:15:47,2022-01-13 18:15:47
https://github.com/OpenMined/PySyft/pull/6226,[],WIP feat: Tailscale and Traefik on k8s (devspace),"WIP feat: Tailscale and Traefik on k8s (devspace)## Description

WIP

## Affected Dependencies

The path prefix for the API in the headscale container wasn't being stripped and I didn't find a feature to do it with ingress-nginx; also Traefik is already used for the docker-compose setup and it's easier to ensure consistent configuration by using the same tool instead of translating routing configuration between Traefik and something else):
- added Traefik helm chart (pulled by devspace)
- removed minikube ingress addon (ingress-nginx)

I don't see recommended / pinned versions of `minikube`, `devspace`, etc. so I've just used whatever was the default installed version at the time of developing the feature. This is bad because things may randomly break or not work for others and even if the issue is unrelated it makes reproducing and debugging issues that much harder.

## How has this been tested?
On my workstation by manully running pytest commands, etc.
Needs more testing and automation - I'm having difficulties with local testing due to high memory usage while I'm also worried about running these tests directly on CI because the runners may get broken and impact other's development. I will try to make things run with less memory on my computer. Could someone tell me how much memory you'd expect these integration tests to normally need?

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
This part may not be so simple - connecting the 2 clusters to see the node port service to establish VPN connectivity may not be as simple as connecting the tailscale container of the docker compose setup below. I will experiment with this locally with some smaller minikube clusters so that I can actually run 2 of them.constants like this can go away once the fixtures actually work",2,False,2021-12-06 09:51:31,2022-02-16 00:18:27,2022-02-16 00:18:26
https://github.com/OpenMined/PySyft/pull/6218,[],Feat: Update Networks welcome page,"Feat: Update Networks welcome page## Description
Quick (and temporary) fix to the Networks start page.",1,False,2021-12-01 21:36:37,2021-12-01 22:01:13,2021-12-01 22:01:13
https://github.com/OpenMined/PySyft/pull/6199,['documentation '],Update installation docs,"Update installation docs## Description
Update the installation tutorials by:
- Add M1 instructions
- FIx formatting of paragraphs and issues pointed to by course participants
- Use the same naming for virtual_env and domain node as in the course website and notebooks

## Affected Dependencies
The gh-pages branch has already been updated accordingly.

## How has this been tested?
The changes can be checked easily at https://openmined.github.io/PySyft/install_tutorials/osx_11_5_1.html

## Checklist
- [x ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x ] My changes are covered by tests",1,True,2021-11-26 13:21:34,2022-02-14 01:47:40,2022-02-14 01:47:39
https://github.com/OpenMined/PySyft/pull/6192,[],Reenable PA - bools got fixed in the new release,"Reenable PA - bools got fixed in the new release## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
‚ù§Ô∏è",1,True,2021-11-22 12:00:03,2022-01-25 01:31:02,2022-01-24 14:28:39
https://github.com/OpenMined/PySyft/pull/6177,[],[WIP] feat: configure traefik self-signed TLS,"[WIP] feat: configure traefik self-signed TLS‚ö†Ô∏è üöß This is work in progress!

## Description

Exposed on port 443 and routes to front and back-ends.

## Affected Dependencies
- Self-signed certificates (i.e. `openssl`)

## How has this been tested?

You can either generate a self-signed cert with SAN extension or Traefik can generate a default self-signed certificate.
The current Traefik config assumes you generate one.

```shell
# generating self-signed certs with SAN extension for 127.0.0.1 in the right place
mkdir packages/grid/traefik/certs
cd packages/grid/traefik/certs
curl -sS https://raw.githubusercontent.com/antelle/generate-ip-cert/master/generate-ip-cert.sh |
    bash -s 127.0.0.1
```

```shell
# deploy with docker compose
cd packages/grid
source .env && docker compose -p eu --profile frontend up -d
```

After the frontend loads, open `https://127.0.0.1` in a browser. There will be a warning or error about the certificate not being trusted. Add the certificate to your trusted certificates - either from the browser settings or  for the whole system.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,True,2021-11-12 07:38:26,2021-11-29 06:27:01,2021-11-29 06:27:01
https://github.com/OpenMined/PySyft/pull/6173,[],Update L3_DataPreparation.ipynb,"Update L3_DataPreparation.ipynbAdding changes requested by Y/K review process.

## Description
Additional information / sections / description / links added.

## How has this been tested?
- Tested by running in the notebook.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-11-11 01:49:52,2021-11-11 02:06:52,2021-11-11 02:06:51
https://github.com/OpenMined/PySyft/pull/6169,[],Updating 0.6.0,"Updating 0.6.0## Description
Lots of updates including fixes for HAGrid on Windows.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-11-10 11:38:23,2021-11-10 11:38:51,2021-11-10 11:38:51
https://github.com/OpenMined/PySyft/pull/6167,[],HAGrid Windows,"HAGrid Windows## Description
- Removed non windows compatible dependencies from hagrid
- Added is_windows function to detect windows
- Added get_environment
- Fixed emoji style issue by removing unused virtualbox / vagrant check

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-11-10 00:07:11,2021-11-11 07:12:31,2021-11-10 11:36:14
https://github.com/OpenMined/PySyft/pull/6166,[],[W.I.P.] Windows Installation Notebook,"[W.I.P.] Windows Installation Notebook## Description
Adding more information on all the steps needed to get Hagrid up and running on Windows using WSL for Course 3. Still a work in progress. 

## Affected Dependencies
None

## How has this been tested?
Running the notebook

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,False,2021-11-09 22:32:30,2021-11-23 02:43:26,2021-11-23 02:43:10
https://github.com/OpenMined/PySyft/pull/6156,[],[WIP] Update notebooks so that a bug can be reported.,[WIP] Update notebooks so that a bug can be reported.See shortcut issue: 3761,4,False,2021-11-05 04:27:48,2021-11-29 06:23:16,2021-11-29 06:23:01
https://github.com/OpenMined/PySyft/pull/6153,[],Creating hotfix to correct issue on azure vm where db migration fails,"Creating hotfix to correct issue on azure vm where db migration fails## Description
Creating hotfix to correct issue on azure vm where db migration fails

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-11-05 02:12:59,2021-11-05 03:15:31,2021-11-05 03:15:31
https://github.com/OpenMined/PySyft/pull/6148,[],Index objects with ID without having to pull back entire database to ‚Ä¶,"Index objects with ID without having to pull back entire database to ‚Ä¶It was really bugging me that domain_client.datasets[-1]['asset_name'] was CRAZY slow on the second run. I dug in and found code that was just written naively (probably prototype code that has lived on too long) so that if pulls back every object in the database to the client and then does the searching client side when you want to select an asset....

So this makes it so that if you ask for a pointer to an object with a specific id... it only returns that object. 

While i'm here I also added a .block to publish() in a couple places. This method isn't yet asynchronous for some reason so we need to put this in just to prevent things from breaking.

Fixes: [ch3659]",2,True,2021-11-04 23:28:57,2021-11-05 02:38:02,2021-11-05 02:38:01
https://github.com/OpenMined/PySyft/pull/6140,[],Hagrid to throw error without launching if there's not enough ram,Hagrid to throw error without launching if there's not enough ram[ch3661],2,True,2021-11-03 14:32:38,2021-11-03 17:56:28,2021-11-03 17:56:28
https://github.com/OpenMined/PySyft/pull/6139,[],Fix: UI compatible with dev,"Fix: UI compatible with dev## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-11-03 14:10:09,2021-11-05 03:33:30,2021-11-05 03:33:30
https://github.com/OpenMined/PySyft/pull/6134,[],Fix: domain config formData,"Fix: domain config formDataCloses [ch3699]

## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-11-03 05:59:43,2021-11-03 11:53:50,2021-11-03 11:53:50
https://github.com/OpenMined/PySyft/pull/6133,[],Fix: add support for updating institution and website fields,"Fix: add support for updating institution and website fieldsCloses [ch3695]

## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-11-03 05:50:41,2021-11-03 13:52:58,2021-11-03 13:52:58
https://github.com/OpenMined/PySyft/pull/6130,[],Fix: change roles update route from PUT to PATCH,"Fix: change roles update route from PUT to PATCHCloses [ch3688]

## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-11-03 05:20:50,2021-11-03 11:50:56,2021-11-03 11:50:56
https://github.com/OpenMined/PySyft/pull/6117,[],Update Hagrid docs to contain Kubernetes deployment,"Update Hagrid docs to contain Kubernetes deployment## Description
Added documentation on deploying locally and on GKE, which is currently deployed at https://openmined.github.io/PySyft/deployment/index.html#installation.

## Affected Dependencies
No affected deps.

## How has this been tested?
- Run the updated changes to see the page is formatted nicely without any errors.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-11-01 12:45:00,2021-11-01 18:28:39,2021-11-01 18:28:34
https://github.com/OpenMined/PySyft/pull/6114,"['bug ', '0.6.0 alpha']",Fix object dataset relationship deleted on data request.,"Fix object dataset relationship deleted on data request.## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

- If the DS requests the dataset itself and the DO approves the requests, then the object dataset relationship is deleted.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
Since the BinObject is deleted, the corresponding BinObjDataset is also deleted due to cascading effect. Thus we need to recreate the BinObjDataset for the new BinObject being created.",5,True,2021-11-01 06:32:44,2021-11-02 14:44:13,2021-11-02 14:44:12
https://github.com/OpenMined/PySyft/pull/6112,[],Add Batched SMPC Action,"Add Batched SMPC Action## Description
Create a `SeqBatchAction Message` for SMPC actions.

The scope of the actions is the following:
* The celery worker will try to execute a group of actions -- all at once
* if one action fails then do not try to execute the next actions since they are dependent on one another

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Same functionality as `SMPCActionMessage` only that it keeps a list of `SMPCActionMessages`Some of the maps that were kept in `smpc_action_message.py` file should be kept separatedQ: George, I forgot could you reiterate why we are clubbing `spdz_mask`, `spdz_multiply` , In the current DB logic, when action fails, we store it in the  DB part,we retry the action when a new object arrives,
George, I think when when spdz multiply fails,this keeps the whole SMPCBatchMessage in DB, when a new message arrives,, `spdz_mask` would be executed again, there would be another roundtrip of communication by spdz mask .I only skimmed the PR, maybe I am missing something?Yep, my point is that we should not try to execute an action that is dependent on the previous one - in our case, `spdz_multiply` is dependent on `spdz_mask`.
In our current approach we do:
```
execute spdz_mask --> Fail --> add to DB
execute spdz_multiply --> Fail --> add to DB
```
When we get an object we will try to execute both `spdz_mask` and `spdz_multiply`

Now we do:
```
try to execute spdz_mask --> Fail --> Store spdz_mask, spdz_mul in the DB
```
When an item arrives we try again `execute spdz_mask` and if success `spdz_mul` - if `spdz_mul` fails then we will store only the `spdz_mul` in the DB.

This can be extended to ""N"" actions that are sequentially dependent on one another.

This PR still keep the same number of actions in the DB, but what I think are the improvements:
1. It will not hit the DB that much -- since we store all the messages at once (we require only 1 connection, not 2 as before)
2. we will try to execute only 1 action and if it fails we ""put on hold"" more `SMPCAction` messages.

The logic to store in the DB is done using `try catches` which in Python are expensive, but they are `pythonic` (zen of python says that is better to ask for `forgiveness` than `permission`)
This `BatchSeq Action` might not have a big impact on the multiplication since we have only 2 messages, but when we move the `gt` operation to be entirely done by actions (for the moment we still have that ping-pong between the DS and DOs - the actions generated by the `gt` functionality will be kept - and there are more than 2 :smile: in a `BatchSeqAction`)
This was my  first doubt george, 
`When an item arrives we try again execute spdz_mask and if success spdz_mul - if spdz_mul fails then we will store only the spdz_mul in the DB`
By the DB logic, if the spdz mask succeeds and spdz mul fails, since this came from a single `BatchSeqAction` , this would keep spdz mask and spdz multiply both in the DB , and spdz mask would be executed again when a new object arrives.
Great thought , george single message make great sense, as  currently have the RunClassMethodSMPCAction is put in queue, which actually creates SMPCActionMessages and put it at back of queue, this create a lot of hassle, which slows our runtime.I think by this we could execute spdz mask and spdz directly in one action directly in RunClassMethodSMPC itself. I think also by this we would not have to couple the SMPC logic in the backend stream which we discussed, as we execute directly
This should perfectly fit the greelet logic I am working on.Discussed in slack",6,True,2021-10-31 23:46:17,2021-11-03 04:30:08,2021-11-03 04:30:07
https://github.com/OpenMined/PySyft/pull/6076,[],[Back-End] - Adjust permissions so a Data Scientist (or any user?) can update their own information,[Back-End] - Adjust permissions so a Data Scientist (or any user?) can update their own informationStory details: https://app.shortcut.com/openmined/story/1545,1,True,2021-10-25 05:48:43,2021-10-25 06:52:44,2021-10-25 06:52:43
https://github.com/OpenMined/PySyft/pull/6071,[],"Iteration 5 Private Compare,","Iteration 5 Private Compare,## Description
Fixes races conditions in SMPC Actions, by raising `ObjectNotInStore` Error, to retry that action.",1,False,2021-10-23 08:00:16,2021-10-23 08:09:43,2021-10-23 08:09:43
https://github.com/OpenMined/PySyft/pull/6064,[],WIP write repo/branch/hash to file,"WIP write repo/branch/hash to file## Description
WIP, writes repo/branch/hash information to a file. Also provides a route to read the current git info.

_Write_
```sh
curl <domain>/api/v1/git \
  -X PATCH \
  -d '{ ""repository"": ""<repo>"", ""branch"": ""<branch>"", ""hash"": ""<hash>"" }' \
  -H ""Content-Type: application/json""
```
_Read_
```sh
curl <domain>/api/v1/git
```

## How has this been tested?
- No auto tests yet

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2021-10-21 01:46:53,2022-02-16 00:12:03,2022-02-16 00:12:02
https://github.com/OpenMined/PySyft/pull/6052,[],Private mul iteration5,"Private mul iteration5## Description
Client serialization and protobuf modification


Q: Why do we need this? Each `SMPCActionMessage` should only be sent to the node that should run itSORRY: Completely my fault...I should have probably done this better...I am not sure why I added proto here.No worries georgeYeah correct ranks to run actions ,good catchremoved itQ: For what is `msg_id` needed?",1,True,2021-10-16 10:17:18,2021-10-16 12:08:50,2021-10-16 11:20:29
https://github.com/OpenMined/PySyft/pull/6049,[],SPDZ multiply code,"SPDZ multiply code## Description
SPDZ multiplication code

Merge after `rasswanth_iteration_2` branch, if the changes are not conflicting.Probably this should be another error -- something like `ObjectNotInStore` (we kind of tying the mpc logic to the celery worker and we should not do that because the celery worker should be genericAbsolutely :+1:",1,True,2021-10-14 19:12:29,2021-10-15 03:04:39,2021-10-14 22:20:22
https://github.com/OpenMined/PySyft/pull/6041,[],feature: cleanup duet integration test files,"feature: cleanup duet integration test files## Description
Save duet notebooks information into a temporary directory.
Close #5726.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,False,2021-10-12 13:23:29,2021-11-03 00:20:09,2021-11-03 00:20:08
https://github.com/OpenMined/PySyft/pull/6039,[],Docker Buildx Cache,"Docker Buildx Cache## Description
One more attempt at fixing the docker build cache.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
Q: Any semantic reason we are switching from `docker-host` to `host.docker.internal`?If it worked we could have eliminated the `docker-host` sidecar container which also sometimes fails to start, however upon testing I found that even with the special linux extra_hosts field the resulting IP is still wrong and doesn't work so I have reverted it back to `docker-host`. See more here: https://stackoverflow.com/questions/48546124/what-is-linux-equivalent-of-host-docker-internal",2,True,2021-10-11 05:13:12,2021-10-11 09:41:28,2021-10-11 09:41:28
https://github.com/OpenMined/PySyft/pull/6009,[],Replacing Global Variables in pytest files with fixtures,"Replacing Global Variables in pytest files with fixtures## Description
We ran into latent bugs while testing `SingleEntityPhiTensor` and `RowEntityPhiTensor` due to using global variables instead of pytest fixtures. This was probably done to speed up the testing process.

In this PR, I removed ALL global variables from both test files and replaced them with safer Pytest fixtures.

## Affected Dependencies
No new dependencies; however, all of the PRs concerning new operations for `SingleEntityPhiTensor` and `RowEntityPhiTensor` will have to be completely rewritten due to these changes.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

Actually running the tests:
`pytest row_entity_phi_test.py`
`pytest single_entity_phi_test.py`
Linting:
`pre-commit run --all-files`

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-09-28 03:44:42,2021-09-30 18:40:56,2021-09-30 18:40:56
https://github.com/OpenMined/PySyft/pull/6002,[],[W.I.P.] Patch flaky SEPT tests on Windows by checking for dimension size,"[W.I.P.] Patch flaky SEPT tests on Windows by checking for dimension size## Description
Currently, four tests for SingleEntityPhiTensor fail on Windows when the tensor data generated by the pytest fixture is of size (1, 1):

- test_ne_shapes
- test_compress
- test_partition
- test_partition_axis

## Affected Dependencies
No changed or affected dependencies.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

I generated 1 million random tensors using the same method as was done in the pytest file, and none of them failed the dimensions check.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Q: Should we remove this file?
Strange as to why it is generating tensor of (1,1) at this point
Sometimes, the CI additional has randomness, It happened many times in SyMPC also, if we run locally  for zillion times it passes, but in CI it fails Great catch, I just deleted it :)",1,True,2021-09-24 13:42:18,2021-09-24 19:29:32,2021-09-24 19:29:08
https://github.com/OpenMined/PySyft/pull/5960,[],[WIP] Data Subject Annotation Wizard and related UX improvements,"[WIP] Data Subject Annotation Wizard and related UX improvements## Description
Building on my recent PRs, this PR will build a simple text-based wizard to help people annotate their private tensors with differential privacy metadata even when they aren't particularly familiar with differential privacy. Here's a snippet from the wizard so far.

```
	=====================================================================
	Welcome to the Data Subject Annotation Wizard!!!
	=====================================================================

	You've arrived here because you called Tensor.private() without
	passing in any entities! Since the purpose of .private() is to add
	metadata for the support of automatic differential privacy budgeting,
	you need to describe which parts of your Tensor correspond to which
	real-world data subjects (entities) whose privacy you want to
	protect. This is the only way the system knows, for example, that it
	costs twice as much privacy budget when twice as much of your data
	(say, 2 rows instead of 1 row) refer to the same entity.

	Entities can be people (such as a medical patient), places (such as a
	family's address), or even organizations (such as a business, state,
	or country). If you're not sure what kind of entity to include, just
	ask yourself the question, ""who am I trying to protect the privacy
	of?"". If it's an organization, make one entity per organization. If
	it's people, make one entity per person. If it's a group of people
	who are somehow similar/linked to each other (such as a family), make
	each entity a different group. For more information on differential
	privacy, see OpenMined's course on the subject:
	https://courses.openmined.org/

	Since you didn't pass in entities into .private() (or you did so
	incorrectly), this wizard is going to guide you through the process
	of annotating your data with entities.

	In this wizard, we're going to ask you for *unique identifiers* which
	refer to the entities in your data. While the unique identifiers need
	not be personal data (they can be random strings of letters and
	numbers if you like). It is ESSENTIAL that you use the same
	identifier when referring to the same entity in the data that you
	never accidentally refer to two entities by the same identifier.
	Additionally, if you plan to do any kind of data JOIN with another
	dataset, it is ESSENTIAL that you are using the same unique
	identifiers for entities as the data you're joining with. Since these
	unique identifiers may be personal information, PySyft might not be
	able to detect if two tensors are using different identifiers for the
	same person.

	So, in this tutorial we're going to be asking you to specify Unique
	Identifiers (UIDs) for each entity in your data. This could be an
	email, street address, or any other string that identifies someone
	uniquely in your data and in the data you intend to use with your
	data (if any).

	Do you understand, and are you ready to proceed? (yes/no)
	 
	Excellent! Let's begin!


	_____________________________________________________________________

	Question 1: Is this entire tensor referring to the same entity?

	Examples:
	 - a single medical scan of one patient
	 - a single spreadsheet of proprietary statistics about a business
	 - a tensor of facts about a country

	(if the tensor is about one entity, but it also contains multiple
	other entities within, such as a tensor about all the customers of
	one business, ask yourself, are you trying to protect the people or
	the business)

	If yes, write the UID of the entity this data is about, otherwise
	write 'no'.
	 no
	_____________________________________________________________________

	Question 2: Does each row correspond to an entity (perhaps with
	occasional repeats)?

```",1,True,2021-09-07 01:35:03,2021-09-16 22:00:42,2021-09-16 22:00:41
https://github.com/OpenMined/PySyft/pull/5954,[],SimpleService and NodeRunnableMessageWithReply,"SimpleService and NodeRunnableMessageWithReply## Description
In offline discussions, it has been raised that our use of Service and Message objects (the latter of which requires creating custom protobufs for each message type) is extremely cumbersome. Instead, it has been proposed that we should simply have Message types which themselves have a method that knows how to execute some functionality when the message is received by a Node. 

This PR is an initial step in this direction. A project which moves all of our services (or rather removes many of them) into this kind of model is a massive project. The desire of this PR is to make such a project incremental. I have created a service called ""SimpleService"" and installed it on the Domain node. Any subclass of NodeRunnableMessageWithReply can automatically be sent from a Client to a Domain, and NodeRunnableMessageWithReply is a subclass of RecursiveSerde.

Future work will attempt to use this service and message type to port existing functionality or to implement new functionality with ideally greatly reduced effort from our previous setup.


NOTE: I also discovered a bug in 'dev' wherein database tables are not being properly created. I fixed it.
NOTE: I also developed a suspicion that our service-based auth decorators might not be working correctly.",4,True,2021-09-04 18:43:10,2021-09-04 21:21:58,2021-09-04 21:20:52
https://github.com/OpenMined/PySyft/pull/5952,[],Fixed Intergration Tests SMPC,"Fixed Intergration Tests SMPC## Description
Modified to use `get_copy` in reconstruct.



## How has this been tested?
Locally

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",5,False,2021-09-03 10:37:03,2021-09-05 03:24:56,2021-09-04 21:22:39
https://github.com/OpenMined/PySyft/pull/5951,[],fix: remove blank notebook,fix: remove blank notebook,1,True,2021-09-03 08:53:13,2021-09-04 21:23:12,2021-09-04 21:23:11
https://github.com/OpenMined/PySyft/pull/5948,[],"Added support & tests for np operations (eq, ne, add, sub, repeat, pos) to S.E.P.T, R.E.P.T","Added support & tests for np operations (eq, ne, add, sub, repeat, pos) to S.E.P.T, R.E.P.TAdding support & tests for np operations (eq, ne, add, sub) to S.E.P.T, R.E.P.T, I.G.TGood question. We really should add `__eq__` to the new serde interface because its crucial that equality is defined on a class so that we can confirm that the serialization works as expected. We probably need to do this through the whole tensor chain since each tensor can have arbitrary attributes with values which need comparison.awesome workThis assignment will mean that `third_tensor` and `reference_tensor` will point to the same variable in memory and have the same id() so comparing it later should always be True, was that intended?Test functions don't need to return anything. You can change the return type to -> None as well.Test functions don't need to return anything. You can change the return type to -> None as well.same as aboveIf we want to make sure you cant compare with different owners, should we also have a test that shows you CAN compare with the same owner and that the `__eq__` passes?You can remove the type ignore if we tell mypy what the type is like this:
```
def test_eq_ndarray(row_data: List) -> None:
    """"""Test equality between a SEPT and a simple type (int, float, bool, np.ndarray)""""""
    sub_row_data: SEPT = row_data[0]

    reference_tensor = REPT(rows=sub_row_data)
    assert sub_row_data.child == reference_tensor, ""Comparison b/w REPT and ""
```Added!Unfortunately I'm not sure that this can be the way in which we handle equality. This checks to see if the polynomial itself is equal but what we actually need to do is check whether the underlying result of the polynomial is equal. One could have two different polnyomials return the same value (they could both return 0, for example)Probably need to comment this out for now. see __eq__ for comment.Any reason this was deleted?I'm confused about what's going on here.(we can't do IntermediateGammaTensor until we modify with the clip at teh end of the polynomial as discussed at the last meeting I attended.)I fully agree, I haven't modified the code for IntermediateGammaTensors since before we had that meeting, and reduced the scope of this PR to just the methods for SingleEntity and RowEntity Phi TensorsThis would fail if self.child and other.child have shapes that are not equal, but are still broadcastable, so I rewrote them to check for broadcastability, and if self.entity == other.entity:

[Link to relevant lines](https://github.com/OpenMined/PySyft/blob/a13862e62d75dd1cc3e3f2c64625f9abfb757320/packages/syft/src/syft/core/tensor/autodp/single_entity_phi.py#L381)if `other` is an acceptable_simple_type (int, float, bool, np.ndarray) then there are two scenarios:

- if it's a float/int/bool, then the comparison is doable, and no additional checks need to be made
- if it's a numpy array, then we need to check that self.child.shape and other.child.shape are broadcastable. If it's not, we raise an exception and tell the user that __eq__ can't be performed because the shapes are not broadcastable. If they are, we save the result of `self.child == other.child`, and return a SingleEntityPhiTensor whose child equals this result.

Sorry if my changes and thought process isn't clear! I can add more comments inline to explain.Ah makes sense - actualy what confuse dme was the way Github printed it. The code is clear.Marking as resolved since there's another PR open for equality for IntermediateGammaTensors""self"" should be self.gamma.child instead (or actually you should first run self = self.gamma and then run what you ran here)interesting!invert isn't the right method here. you wanat (1 - x).  invert is 1/x",1,True,2021-09-02 04:21:08,2021-09-23 19:36:27,2021-09-23 19:36:26
https://github.com/OpenMined/PySyft/pull/5931,[],Prepare for ADP+SMPC Demo branch Merge into dev,"Prepare for ADP+SMPC Demo branch Merge into devLots of work to do to get this branch ready. Creating this PR to help track it.What advantage does this offer over running the normal tox task and should we move this into a `scripts` folder?is this comment still valid?is this comment still valid?Should we put these into Backlog items?Should we put these into Backlog items?Remove these commented imports?Extract to Settings in config.py?Put pool size into config.py SettingsWhats happening with the upcast? Does .content contain a bunch of PyPrimitives?Should this be added to the backlog?Should this be added to the backlog?What does this comment mean?Does this need to be expanded on in the Backlog?Add temporary_box to all primitive types in the backlog?Do these need to be exposed? Are there any circular import issues?Add the backup attr cache to the backlog? This relates to calling super class op implementations like `__add__` when implementing subclass Tensor types.Add to backlog?Lets clean up this commented print code if possible.Should we just remove this for now?Remove?Can budget comparison be <= or must be <?Remove prints and possibly log?Add to backlog?Add to backlog?Add to the backlog proper support and conversion of these numpy types.Fix by properly supporting numpy scalars.Add to backlog?Add to Backlog?This is currently broken as GammaScalar requires prime on init.Do we need to implement the whole poly serde stuff in the backlog?Should this be uncommented and fixed or removed?Does this need to be improved or better documented?Add to backlog?Do we know the value / key types of these tuples and dicts?Are the polys from a known library and type?What are f and rranges?Remove unused code?Are we able to fix the Scalar type here?Do we know what the second part of this tuple return type is?What are the keys and values of prime2symbol?Are these types broader like np.int np.float, should we introduce a more generic numeric type?We need to handle this serde complexity jump as a tech debt story in the backlog.Can we remove this?Can we remove this?Do these need deleting?Do these need deleting?remove?Remove?Remove?What is the tech debt here thats needs fixing and can we put it in the backlog?What is the tech debt and can we add it to the backlog?More tech debt for the backlog?Should the db init param be removed from this and superclass Node?Are these services coming back?This seems like it needs changing?Should we make client serializable?Perhaps the code changed but the comment is out of date?These shouldn't be ignoredThese shouldn't be ignoredThese shouldn't be ignoredWhats the issue here?Are these only numpy arrays or possibly also other Tensor chain types?Remove print statements?are these only numpy arrays?Is this Tuple[int]?Do we need to create a more general solution for this to support the other combinations and put that in the backlog?Does this need to be uncommented and fixed?remove or include with the above fix?Remove prints?Should we catch and raise this problematic shape state?Remove?Comment still valid?Comment still valid?Should be Tuple[Type], what is the type in dims, is this just Tuple[int] shape?These can be declared as *args: Tuple[Any, ...], **kwargs: Any to avoid having to use ignore.Remove commented line?args and kwargs are not int, change to Tuple[Any, ...] and Any*args is variadic so Tuple[Any, ...] is probably more correct.Perhaps just use TypeTuple[int] and cast to tell mypy thats what it is.Whats going on here?What about the commented out code?No longer valid comment? Remove commented code?Should this todo be added to the backlog?Add to backlog.Should this be added to the backlog and comment removed?Should we add this to the backlog?Add np.int64 / precision tensor handling plus tests for these overflow issues to backlog.Is this code still needed?Does this need to be handled in the backlog?remove?Remove?Refactor?Remove?Add some tests for this to the backlog?DoneYupfixedFixedfixedfixedfixeddoneI'd like to leave this here - we're not sure where we're going to put it yet but we need to have both documentation and implementation for how we handle shape information on pointers.fixedremovedadded to https://app.clubhouse.io/openmined/story/1128/tech-debt-for-adp-smpc-demo?stories_sort_by=priority&stories_group_by=WORKFLOW_STATEaddednah just removing it for nowaddedI'm eliminating for now as we're exclusively focusing on np.int32 support and will tackle int64 separately.removedaddedremovedremovedNah I think this is how it should work for now unless you're referring to some refactor i'm not aware of.removedWe've got a comprehensive testing plan for all this - will remove the TODO for now,.",1,True,2021-08-29 00:18:56,2021-09-02 23:33:30,2021-09-02 23:33:28
https://github.com/OpenMined/PySyft/pull/5927,[],Add 'publish' method to share tensor,"Add 'publish' method to share tensor## Description
 - Add `publish` method to share to be executed on all shares when `MPCTensor` calls it.

## How has this been tested?
- Added test for ShareTensor, although that method should not be reached because `publish` should be treated by the `ADPTensor` which is before the `ShareTensor` (in the chain of tensors)

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
@iamtrask @madhavajay I think those were some bugs on the `demo` branch?Also here",3,False,2021-08-26 18:31:55,2021-08-28 22:50:36,2021-08-28 22:50:36
https://github.com/OpenMined/PySyft/pull/5903,[],Add build-essential to dependencies in order to fix building error,"Add build-essential to dependencies in order to fix building error## Description
The library bplib fails on building when docker image is trying to be created. This cause pip tries to install other versions of bplib and then tries to get all other versions of the other dependencies, taking hours to try to build the image.

Adding build-essential allows to build bplib and then the image is created in a few minutes.

## Affected Dependencies
build-essential 

## How has this been tested?
Executing 
docker build -f docker/syft.Dockerfile --build-arg GPU=false -t openmined/syft:latest -t openmined/syft:`python VERSION` .
as explained in docker/syft.Dockerfile and getting the image created.


## Checklist
- [X] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [X] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [] My changes are covered by tests",1,False,2021-08-11 11:17:49,2021-08-16 05:07:09,2021-08-16 05:07:09
https://github.com/OpenMined/PySyft/pull/5884,[],Hot Fix for unboxed PyPrimitive upcasting,"Hot Fix for unboxed PyPrimitive upcasting## Description
- Fixed issue where temporary_box in dict isn't checked before del
- Switched pytest.mark.skip for broken tests to pytest.mark.xfail

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-08-05 05:26:30,2021-08-05 09:10:11,2021-08-05 06:44:44
https://github.com/OpenMined/PySyft/pull/5876,[],Codebase fixes to get the Friday trade_demo working,"Codebase fixes to get the Friday trade_demo working## Description
This friday we have an important demo which is being worked in in the notebook series ",1,True,2021-08-04 13:44:43,2021-08-04 16:24:59,2021-08-04 16:24:58
https://github.com/OpenMined/PySyft/pull/5861,[],mypy fix for syft,"mypy fix for syft## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2021-07-30 10:55:23,2021-08-05 07:41:21,2021-08-05 07:41:20
https://github.com/OpenMined/PySyft/pull/5847,[],Madhava/hot fix backend issue,"Madhava/hot fix backend issue## Description
Fixes issue which breaks grid from starting. Simple syft Dict vs typing.Dict issue.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-07-28 21:44:30,2021-07-28 22:01:49,2021-07-28 22:01:49
https://github.com/OpenMined/PySyft/pull/5846,[],Updating tox fix,"Updating tox fix## Description
Updates to this PR: https://github.com/OpenMined/PySyft/pull/5844

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-07-28 01:40:25,2021-07-28 01:44:02,2021-07-28 01:44:02
https://github.com/OpenMined/PySyft/pull/5844,[],fix to allow execute cd command in tox.ini  Ubuntu,"fix to allow execute cd command in tox.ini  Ubuntu## Description
Fix to allow execute cd command in tox.ini in Ubuntu

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2021-07-28 00:38:38,2021-07-28 01:41:41,2021-07-28 01:41:41
https://github.com/OpenMined/PySyft/pull/5831,[],fixing mypy for grid,"fixing mypy for grid## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-07-23 08:28:25,2021-07-27 07:29:03,2021-07-27 07:29:03
https://github.com/OpenMined/PySyft/pull/5829,[],fixing mypy for hagrid,"fixing mypy for hagrid## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-07-22 11:40:51,2021-07-27 07:19:49,2021-07-27 07:19:49
https://github.com/OpenMined/PySyft/pull/5822,[],Sympc tensor grid,"Sympc tensor grid## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
This is not working!! It works only for Hagrid this versionNote that there's also a packages/syft/proto/core/node/common/action/smpc_action.proto file which appears to be empty. Let's remove it.Is there a reason there is both ""tensor"" and ""array""?I'm surprised this works with missing number values. I thought proto didn't like that. Any reason we got rid of entity and id?(this might not have been your doing?)let's move these imports to the top of the fileHow big of a project is it to handle this TODO before we merge. If we don't handle it let's definitely create a Story in Clubhouse before merging so we don't forget.This is not a good place for this run_class_smpc_method function - definitely let's find a better place for it.

klass.py is about code generating pointers but in the case of ShareTensor we're not code generating anything. We just need to create a pointer subclass and put methods on it that we want. This documentation is incorrect.Can we get some comments and documentation on this service explaining what's going on? It's pretty complex :)Maybe a bit of documentation explaining this tensor here would be good.niceAcceptable @madhavajay this is ok to be removed?Those needs to be put backThose needs to be removedYep. I need to remove themI think this was from a branch where I had also the DP code. I will revisit this logicHmm...the work here is from the branch you had with DP and a lot of the work from here is your work -- I think this one I commented because I only wanted to leave the SMPC part in and then maybe get the DPIt should not be that big.

The party that has the MPCTEnsor would send in the kwargs the seed and each party would generate the UUIDsSure! I will have in the morning a sync with @madhavajay only to be sure that I might not overcomplicate stuff with the `celeryworker` and the MPCTensor and tomorrow I will get this done.Ahhh..it is because the child could be a Tensor or an array -- I am not sure which approach should we keep (I think this was something that I have seen in the DP branch and I carried with it)
If you look here - basically there is an `if...else` depending on the child [here](https://github.com/OpenMined/PySyft/pull/5822/files#diff-37698445e24479cdf1304dfe050c1beb3d4e6463eca4245b8a52591b1d8028fd) at the serializationAh right - that makes sensebrilliant <3This showcased a lot of problems for the allowed methods.@tudorcebere @madhavajay I added here not to garbage collection because sometimes it was triggered by the worker (probably because working with pointer more than usually)Yeah custom tensor proto is recursive and can be another Tensor all the way to the bottom until it finally must be a numpy array. Turtles most of the way down. üê¢ üåèQ: Why do we have ShareTensor inside FP Tensor.Should it be like FP tensor inside ShareTensor.Q: Should we also have ring_size in ShareTensor?MAYBE: We could remove the above repeated commentsQ: Why do we disable garbage collection?MAYBE : We could have the import at top of module.Q:Why do we generate it sequentially without assignment of the generator.Q: why do we disable garbage collection here also?YepI think for the moment this won't matter -- this is a branch that diverged a lot from ""dev"" and when we go back we would have that field (from what I remember we have it in dev)Here we have FPTensor --> ShareTensor  (you need to convert to integer and the we secret share it -- and the result is still an int tensor)We have a remote pointer and then we ask to do a remote operation -- at that point, we lose the ""pointer"" locally and we also send a ""GarbageCollect action"" -- that action might be executed before doing the actual operation (and we lose the data we want to do the operation on)We use this to be ""in sync"" with the parties and the intermediary result from the shares.
All the parties (+ the orchestrator) have a seed and they generate ""id_at_locations"" in a deterministic and synchronized way.

The parties might have intermediary results (and they generate ids for those results).  The orchestrator discards the ids for those intermediary results and keeps only the last id for the result.
I get a recursive import :(The same reason as bellowdoes this invert the bool? Should it possibly just be pointable=False incase the bool is already False?Maybe we can do a quick find replace on the copied `RunClassMethodAction` string inside this file?Should we just remove commented out unused code we have in git history?This breaks the tests/syft/core/tensor/autodp/single_entity_phi_test.py test?Looks like its being called in transformers, so im adding it back.Why does this need to be removed? And are you asking or saying?I have done this.I have disabled these tests so this can be off for now.Not the scope of this PR, but we really need to clear the serialize/deserialize functionsShould we raise an exception when rank is not provided or assign a default value?Q: Why is it insecure, since we beaver triples, I think it should be secure.We generate for all parties and take only specific rank, would all the parties have the same seed.Should we have  `slots`  or we will keep it for a later PR?Can we have the default base as 2,it would be effective for shift operations?Same here , could we have default base as 2?I think we should remove the error logs in the notebook before merge.Having numpy also , includes unsigned integers(not available in torch), which helped me a lot  in protocol implementations.What errors george, is it resolved?I think we can add ```__slots__``` later or maybe not at all. ```__slots__``` can have a disadvantage when used in an inheritance scenario and in this case we (the SMPC team) do not fully control what happens with the ```PassthroughTensor```)Because the parties might know how the shares from the other parties might look.
Since they all get the same seed and generate shares (even thought they do not keep them -- they could still ""look"" at them)Now it works -- 7 days ago there were some ```allowlist``` related problemsI am not sure if I understand what you mean.
Here I only want to make sure that we set `pointable` accordingly to the `gc_enabled` parameter. If we don't want to garbage collect the parameter we should set `pointable to False` (which is the invert of the `gc_enabled` variable)Rank should always be provided. It does not have any default value.Yep - all parties have the same seed and generate the same ""shares"", but they keep only specific shares depending on their rank (if you look at line 118)This is used for the moment since it would be simpler to visualize the result (if you have a float number)Is this resolved?Is this resolved?Is this resolved?Is this resolved?Is this resolved?Is this resolved?Is this resolved?",5,True,2021-07-20 19:02:34,2021-08-03 10:20:32,2021-08-03 10:20:31
https://github.com/OpenMined/PySyft/pull/5821,[],Flake 8 fix 0.6.0,"Flake 8 fix 0.6.0## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-07-20 12:26:29,2021-07-22 10:17:09,2021-07-22 10:17:09
https://github.com/OpenMined/PySyft/pull/5796,[],[Snyk] Security upgrade pillow from 6.2.2 to 8.3.0,"[Snyk] Security upgrade pillow from 6.2.2 to 8.3.0<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>



#### Changes included in this PR

- Changes to the following files to upgrade the vulnerable dependencies to a fixed version:
    - packages/syft/examples/duet/mnist/original/requirements.txt



#### Vulnerabilities that will be fixed





##### By pinning:
Severity                   | Priority Score (*)                   | Issue                   | Upgrade                   | Breaking Change                   | Exploit Maturity
:-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------
![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"")  |  **566/1000**  <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.6  | Buffer Overflow <br/>[SNYK-PYTHON-PILLOW-1316216](https://snyk.io/vuln/SNYK-PYTHON-PILLOW-1316216) |  `pillow:` <br> `6.2.2 -> 8.3.0` <br>  |  No  | No Known Exploit 

(*) Note that the real score may have changed since the PR was raised.




Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the effected dependencies could be upgraded.


Check the changes in this PR to ensure they won't cause issues with your project.



------------



**Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*

For more information:  <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIwN2M1ZjZmOC1kNWE2LTRiNzAtYmMxZS04YjFkZmVkYWNiMDYiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjA3YzVmNmY4LWQ1YTYtNGI3MC1iYzFlLThiMWRmZWRhY2IwNiJ9fQ=="" width=""0"" height=""0""/>
üßê [View latest project report](https://app.snyk.io/org/openmined/project/beaa9361-e859-4f25-9e9f-25910b90ae9f)

üõ† [Adjust project settings](https://app.snyk.io/org/openmined/project/beaa9361-e859-4f25-9e9f-25910b90ae9f/settings)

üìö [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities)

[//]: # (snyk:metadata:{""prId"":""07c5f6f8-d5a6-4b70-bc1e-8b1dfedacb06"",""prPublicId"":""07c5f6f8-d5a6-4b70-bc1e-8b1dfedacb06"",""dependencies"":[{""name"":""pillow"",""from"":""6.2.2"",""to"":""8.3.0""}],""packageManager"":""pip"",""projectPublicId"":""beaa9361-e859-4f25-9e9f-25910b90ae9f"",""projectUrl"":""https://app.snyk.io/org/openmined/project/beaa9361-e859-4f25-9e9f-25910b90ae9f?utm_source=github&utm_medium=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-PILLOW-1316216""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""priorityScore""],""priorityScoreList"":[566]})",1,False,2021-07-17 00:06:32,2021-07-26 01:13:20,2021-07-26 01:13:20
https://github.com/OpenMined/PySyft/pull/5788,[],hagrid create and destroy local vm,"hagrid create and destroy local vmWe need the ability for [hagrid](https://github.com/OpenMined/PySyft/tree/demo_strike_team_branch_4/packages/hagrid) to perform basic deployment and teardown of single VM nodes locally, which can subsequently be provisioned with Domain/Network functionality (perhaps using a tool like ansible).

**Acceptance Criteria:**

- this feature shouldn't require any additional local file configuration
- this feature should be usable out of the box immedaitely after running ""pip install hagrid"" (see [ch65])
- this feature should ask for any additional information it needs in the command prompt (login information, what machine to use, etc.). 
- this feature should provide good defaults and there should be an optional place to put local file configuration for inputs
- should use the same grammar as cloud deployments (but just replace the eword ""azure"" or ""aws"" with ""local""). 
- it should be possible to run this command multiple times and specify different ports for multiple VMs to run on
- if a port is not available - it should automatically iterate up one until it finds an available port.
------

If you would like to claim this issue, you must first open a pull request with the following attributes:
- **Title:** [WIP] hagrid create and destroy local vm
- **Branch merging into:** [ iamtrask/ch67/hagrid-create-and-destroy-local-vm](https://github.com/OpenMined/PySyft/tree/iamtrask/ch67/hagrid-create-and-destroy-local-vm)
- **Description:** must include the text ""[ch67]"" and ""#5789""

If you do not claim this issue by creating a pull request, others will assume you are not working on it and will pick up the task themselves (regardless of whether you comment on this issue).",2,False,2021-07-14 21:18:24,2021-07-28 03:40:39,2021-07-28 03:40:39
https://github.com/OpenMined/PySyft/pull/5784,[],[WIP] hagrid create and terminate vms on azure/aws/gcp,"[WIP] hagrid create and terminate vms on azure/aws/gcpWe need the ability for hagrid to perform basic deployment and teardown of single nodes on AWS/Azure/GCP, which can subsequently be provisioned with Domain/Network functionality (perhaps using a tool like ansible).

**Acceptance Criteria:**

- this feature shouldn't require any additional local file configuration
- this feature should be usable out of the box immediately after running ""pip install hagrid"" (see [ch65])
- this feature should ask for any additional information it needs in the command prompt (login information, what machine to use, etc.). 
- this feature should provide good defaults and there should be an optional place to put local file configuration for inputs like username/password relating to a cloud provider (but this should be optional as mentioned)
- all three cloud providers should be configurable using the same CLI grammar",2,False,2021-07-14 20:24:12,2021-07-28 03:40:53,2021-07-28 03:40:53
https://github.com/OpenMined/PySyft/pull/5773,[],Add a first example of SMPC Tensor,"Add a first example of SMPC Tensor## Description
Add the SMPCTensor Proof of Concept with a dummy example

## How has this been tested?
- There is an example ```test_mpc_tesnsor.py``` - more tests would be added in the following days.
- TODO: Add tests for it.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
let's put a TODO comment here in the code to route based on something that partial string search.love thislove thisgreat abstraction - i remember talking about thisI'm surprised you needed to re-write all this logic. Could you not have simply called the node service which can execute actions?Let's wrap this constructor in a Tensor.share() methodvery niceI will check this out :+1: I am not sure in this case what the service would be -- I reach this point after:
```RunClassMethodAction``` (here I get all the actions - SMPCActions for the current node) --> I send them to SMPC Service and they execute it (and enter here)Q: Does it raise circular import error when kept at file start?Hmm - it should not",1,False,2021-07-13 18:39:10,2021-07-20 19:10:05,2021-07-20 19:10:05
https://github.com/OpenMined/PySyft/pull/5767,[],[Snyk] Security upgrade pillow from 6.2.2 to 8.3.0,"[Snyk] Security upgrade pillow from 6.2.2 to 8.3.0<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>



#### Changes included in this PR

- Changes to the following files to upgrade the vulnerable dependencies to a fixed version:
    - packages/syft/examples/duet/dcgan/original/requirements.txt


<details>
<summary>‚ö†Ô∏è <b>Warning</b></summary>

```
torchvision 0.5.0 requires pillow, which is not installed.

```
</details>


#### Vulnerabilities that will be fixed





##### By pinning:
Severity                   | Priority Score (*)                   | Issue                   | Upgrade                   | Breaking Change                   | Exploit Maturity
:-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------
![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"")  |  **566/1000**  <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.6  | Buffer Overflow <br/>[SNYK-PYTHON-PILLOW-1316216](https://snyk.io/vuln/SNYK-PYTHON-PILLOW-1316216) |  `pillow:` <br> `6.2.2 -> 8.3.0` <br>  |  No  | No Known Exploit 

(*) Note that the real score may have changed since the PR was raised.




Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the effected dependencies could be upgraded.


Check the changes in this PR to ensure they won't cause issues with your project.



------------



**Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*

For more information:  <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlMjY4OGUyNy1iYWZlLTRlNzAtODc5Ni03MDUwNDI2ZmJhNDMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImUyNjg4ZTI3LWJhZmUtNGU3MC04Nzk2LTcwNTA0MjZmYmE0MyJ9fQ=="" width=""0"" height=""0""/>
üßê [View latest project report](https://app.snyk.io/org/openmined/project/0b795ab3-264d-400d-8560-18bcfbe844b6)

üõ† [Adjust project settings](https://app.snyk.io/org/openmined/project/0b795ab3-264d-400d-8560-18bcfbe844b6/settings)

üìö [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities)

[//]: # (snyk:metadata:{""prId"":""e2688e27-bafe-4e70-8796-7050426fba43"",""prPublicId"":""e2688e27-bafe-4e70-8796-7050426fba43"",""dependencies"":[{""name"":""pillow"",""from"":""6.2.2"",""to"":""8.3.0""}],""packageManager"":""pip"",""projectPublicId"":""0b795ab3-264d-400d-8560-18bcfbe844b6"",""projectUrl"":""https://app.snyk.io/org/openmined/project/0b795ab3-264d-400d-8560-18bcfbe844b6?utm_source=github&utm_medium=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-PILLOW-1316216""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[566]})",1,False,2021-07-06 23:38:53,2021-07-26 01:13:15,2021-07-26 01:13:15
https://github.com/OpenMined/PySyft/pull/5765,[],[Snyk] Security upgrade pillow from 6.2.2 to 8.3.0,"[Snyk] Security upgrade pillow from 6.2.2 to 8.3.0<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>



#### Changes included in this PR

- Changes to the following files to upgrade the vulnerable dependencies to a fixed version:
    - packages/syft/examples/duet/vae/original/requirements.txt


<details>
<summary>‚ö†Ô∏è <b>Warning</b></summary>

```
torchvision 0.5.0 requires pillow, which is not installed.

```
</details>


#### Vulnerabilities that will be fixed





##### By pinning:
Severity                   | Priority Score (*)                   | Issue                   | Upgrade                   | Breaking Change                   | Exploit Maturity
:-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------
![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"")  |  **566/1000**  <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.6  | Buffer Overflow <br/>[SNYK-PYTHON-PILLOW-1316216](https://snyk.io/vuln/SNYK-PYTHON-PILLOW-1316216) |  `pillow:` <br> `6.2.2 -> 8.3.0` <br>  |  No  | No Known Exploit 

(*) Note that the real score may have changed since the PR was raised.




Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the effected dependencies could be upgraded.


Check the changes in this PR to ensure they won't cause issues with your project.



------------



**Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*

For more information:  <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2NGE1MTIxMy1lNzc2LTQ2MWItYjEwOS0yNjZiMzIyNTQ2YzQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjY0YTUxMjEzLWU3NzYtNDYxYi1iMTA5LTI2NmIzMjI1NDZjNCJ9fQ=="" width=""0"" height=""0""/>
üßê [View latest project report](https://app.snyk.io/org/openmined/project/1597d015-c616-45be-91fd-059e68e63943)

üõ† [Adjust project settings](https://app.snyk.io/org/openmined/project/1597d015-c616-45be-91fd-059e68e63943/settings)

üìö [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities)

[//]: # (snyk:metadata:{""prId"":""64a51213-e776-461b-b109-266b322546c4"",""prPublicId"":""64a51213-e776-461b-b109-266b322546c4"",""dependencies"":[{""name"":""pillow"",""from"":""6.2.2"",""to"":""8.3.0""}],""packageManager"":""pip"",""projectPublicId"":""1597d015-c616-45be-91fd-059e68e63943"",""projectUrl"":""https://app.snyk.io/org/openmined/project/1597d015-c616-45be-91fd-059e68e63943?utm_source=github&utm_medium=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-PILLOW-1316216""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[566]})",1,False,2021-07-06 21:29:03,2021-07-26 01:13:11,2021-07-26 01:13:10
https://github.com/OpenMined/PySyft/pull/5764,[],[Snyk] Security upgrade pillow from 6.2.2 to 8.3.0,"[Snyk] Security upgrade pillow from 6.2.2 to 8.3.0<h3>Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>



#### Changes included in this PR

- Changes to the following files to upgrade the vulnerable dependencies to a fixed version:
    - packages/syft/examples/duet/mnist/original/requirements.txt


<details>
<summary>‚ö†Ô∏è <b>Warning</b></summary>

```
torchvision 0.5.0 requires pillow, which is not installed.

```
</details>


#### Vulnerabilities that will be fixed





##### By pinning:
Severity                   | Priority Score (*)                   | Issue                   | Upgrade                   | Breaking Change                   | Exploit Maturity
:-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------
![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"")  |  **566/1000**  <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.6  | Buffer Overflow <br/>[SNYK-PYTHON-PILLOW-1316216](https://snyk.io/vuln/SNYK-PYTHON-PILLOW-1316216) |  `pillow:` <br> `6.2.2 -> 8.3.0` <br>  |  No  | No Known Exploit 

(*) Note that the real score may have changed since the PR was raised.




Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the effected dependencies could be upgraded.


Check the changes in this PR to ensure they won't cause issues with your project.



------------



**Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*

For more information:  <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJmNTdmMDBiZC03Y2EwLTQ5NzQtYmY1OC1jNzFmODYxZjY3Y2UiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImY1N2YwMGJkLTdjYTAtNDk3NC1iZjU4LWM3MWY4NjFmNjdjZSJ9fQ=="" width=""0"" height=""0""/>
üßê [View latest project report](https://app.snyk.io/org/openmined/project/beaa9361-e859-4f25-9e9f-25910b90ae9f)

üõ† [Adjust project settings](https://app.snyk.io/org/openmined/project/beaa9361-e859-4f25-9e9f-25910b90ae9f/settings)

üìö [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities)

[//]: # (snyk:metadata:{""prId"":""f57f00bd-7ca0-4974-bf58-c71f861f67ce"",""prPublicId"":""f57f00bd-7ca0-4974-bf58-c71f861f67ce"",""dependencies"":[{""name"":""pillow"",""from"":""6.2.2"",""to"":""8.3.0""}],""packageManager"":""pip"",""projectPublicId"":""beaa9361-e859-4f25-9e9f-25910b90ae9f"",""projectUrl"":""https://app.snyk.io/org/openmined/project/beaa9361-e859-4f25-9e9f-25910b90ae9f?utm_source=github&utm_medium=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-PILLOW-1316216""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[566]})",1,False,2021-07-06 20:42:25,2021-07-26 01:13:07,2021-07-26 01:13:06
https://github.com/OpenMined/PySyft/pull/5733,[],Releasing 0.5.0rc3,"Releasing 0.5.0rc3## Description
- sy.load is now automatic
- apache arrow torch serde
- python 3.9 and pydp support
- updated dev tooling with tox
- new docker containers for grid
- added docker-compose example
- unified versioning system for 0.5.0rc3
- grid supports PostgreSQL
- lots of bug fixes

## Affected Dependencies
Stuff

## How has this been tested?
CI

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-06-25 07:07:48,2021-06-25 07:08:31,2021-06-25 07:08:31
https://github.com/OpenMined/PySyft/pull/5727,[],Improved Docker and grid local dev with tox,"Improved Docker and grid local dev with tox## Description
- Improved Docker and grid local dev with tox
- Copied @koenvanderveen Grid Network and Domain connection notebook
- Added print private exception in debug mode on server side
- Split up docker files into two stages
- Set flask.app.url_map.strict_slashes = False
- Fixed bug with CreateInitialSetUpResponse return message key
- Set grid unit tests to run without random order (possible db state)

## Affected Dependencies
Stuff

## How has this been tested?
Locally a lot.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-06-24 11:41:53,2021-06-25 06:11:00,2021-06-25 06:11:00
https://github.com/OpenMined/PySyft/pull/5676,"['code quality ', 'documentation ']",Update GA to check docstring diff only,"Update GA to check docstring diff only## Description
Documentation is as important as the code. At this moment syft has not the code documented as would be expected, however is not realistic to start to document the whole code.

With this PR an step `Run flake8` in a  github-action is created to check the documentation, namely, only new lines of code will be reviewed by flake8. In this way, if a developer adds a new funcionality but does not affect to the previous one, he is only forced to document the code he/she has included.

## Affected Dependencies
The needed libraries are installed in the step itself.

## How has this been tested?
1. I have run the command `flake8 src/syft/__init__.py` and flake8 complains about the following:
```
src/syft/__init__.py:1:1: D205 1 blank line required between summary line and description
```
See that the flake8 error is in the line number 1.

2. I have added a `foo` function in the previous `__init__` file with a bad format to check that the Github Action only complains about this function, but does not raise the previous error
```
src/syft/__init__.py:108:1: E302 expected 2 blank lines, found 1
def foo(a:str):
^
src/syft/__init__.py:108:10: E231 missing whitespace after ':'
def foo(a:str):
         ^
src/syft/__init__.py:109:1: D300 Use """"""triple double quotes""""""
    ""wrong docstring""
^
src/syft/__init__.py:109:1: D400 First line should end with a period
    ""wrong docstring""
^
src/syft/__init__.py:109:1: D403 First word of the first line should be properly capitalized
    ""wrong docstring""
^
1     D300 Use """"""triple double quotes""""""
1     D400 First line should end with a period
1     D403 First word of the first line should be properly capitalized
1     E231 missing whitespace after ':'
1     E302 expected 2 blank lines, found 1
```
See that flake8 complains about lines >= 108 but does not say anything about line 1.
(see this [link](https://github.com/OpenMined/PySyft/runs/2864595066?check_suite_focus=true) to check the whole action)

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,False,2021-06-19 11:06:31,2021-08-05 08:17:03,2021-08-05 08:17:02
https://github.com/OpenMined/PySyft/pull/5670,[],[WIP] Code Cleanup from Demo Push,"[WIP] Code Cleanup from Demo Push## Description
These two weeks have been a focus on delivering single-node demos for a variety of folks. Additionally, we've recently done the monorepo merge between PyGrid and PySyft. This PR is mostly focused on reducing some technical debt which was discovered in the process of putting together end-to-end demos on the new monorepo.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
NIT: We might want to stick to python scripts so we can be cross-platform?NIT: We might want to stick to python scripts so we can be cross-platform?The Romanian Reaper is pleased üòÜ Can we get the erorr as well?Can we add a flag for this? Usually, when we comment out things in the codebase, we are going to leave them commented for ages.Can we remove these?This should be removed.Same as above.Same as above.Same as above.ci-grid is used only in CI, this shouldn't be commented out.Could you explain why this was a problem? It might be a good thing to do, not exactly sure about the use case/if it works as you are expecting.can we actually get the logging of the error so we don't hide unwanted behaviour?I am not quite ahppy with this, because the node should be able to pick it's storage in a more generic way. We should split functionality here 100%.For example here, we are making a hard commit that we want sqlite, but maybe in two months we would like to experiment with Redis. It would be nice to be able to receive an additional optional argument in the constructor like `Memory Storage Backend` in which we pick multiple implementations (that's why we have the storage interface). Nice work, but we should totally wrap this up into something else, the node shouldn't be aware of this.We should keep some similar API in which you select the storage backend. More details above.Isn't this quite duplicated work with the one in PyGrid?This needs refactoring to be able to customize with the name. Let's keep this as open as possible, we are working on adding Apache Arrow + Apache Flight which will make dataset streaming E X T R E M E L Y fast (on normal machines, the task will become network bound, not CPU bound as it is currently).Why can't we reuse the Pointer generation API? I would like to reuse or pointer creation mechanism so we rely on existing working code.Can you move this to `syft/lib/ast/util.py`?Can you explain this change? Not sure why do we need it.kinda neat change, I must admitthis should be changed into `ValueError(""Not protobuf decoding scheme set for Tensor."")`This should be all already in dev from @eelcovdw ?Same as above.Same as above.Why are all of these commented out? Might be the transformer work.Before merge, we should stick only to the relevant notebooks (like create a story of 3 notebooks that make some functionality). Currently, there are way too many notebooks.is this not still a python script?ah - are you referring to windows?Ah we can just remove that code entirely. Removing now.Probably - it was pulled from that branch. Could be a merge issue.Probably - it was pulled from that branch. Could be a merge issue.Probably - it was pulled from that branch. Could be a merge issue.I think ""experimental"" can be messy though, no? (agreed in general though)Fixed!This functionality is temporarily disabled because there is a conflicting dependency between pygrid-cli related code and jupyter notebook. FixedfixedWill rewrite most of this class next.Ah so SQLAlchemy can work with many sql backends - and in this case this is just one of the ways to ask for an in-memory SQL store.(this was resolved through an offline discussion between Tudor and myself. This is duplicate because we're in the process of moving this class from Grid -> Syft)100% I hate this class but it gives us the UX we want for now while we 100% replace it with something better later.Would love to if we can, although I do have lots of custom UX things that the pointer generation API probably won't get us (like printing things as numpy tables etc.)(outdated - came from anotheor PR)can't take credit - inherited from another prTudor and I chatted about this offline - SQLAlchemy is generic enough but this interface should be cleaned up with a wrapper class around TableBase, and the other SQLAlchemy specific pieces here.Covered in conversation - SQLAlchemy wrapped by Manager classes will provide this.",2,False,2021-06-17 23:21:23,2021-06-26 01:15:58,2021-06-26 01:15:58
https://github.com/OpenMined/PySyft/pull/5664,[],"bugfix, base tokenizer not in AST","bugfix, base tokenizer not in AST## Description
previous PR changed tokenizer serde to upcast tokenizers to baseclass, I forgot to add the baseclass to allowlist.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Local

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,False,2021-06-15 19:04:46,2021-06-23 18:43:33,2021-06-23 18:42:58
https://github.com/OpenMined/PySyft/pull/5658,[],Fixing pydp tests,"Fixing pydp tests## Description
Fixing pydp tests

## Affected Dependencies
None

## How has this been tested?
Locally and CI

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,True,2021-06-14 04:20:14,2021-06-14 05:21:10,2021-06-14 05:21:10
https://github.com/OpenMined/PySyft/pull/5636,[],[WIP] Plan Result permissions ,"[WIP] Plan Result permissions ## Description
Make sure that a data owner can say: if you run this plan on this data, you can download the result without requesting it

## How has this been tested?
notebook, will add tests laterQ: Shouldn't we check that all the attributes have this permission?

I am thinking at the following scenario:
```
a_ptr = a.send(duet)
# b_ptr is privata data owner by the DataOwner
(a_ptr + b_ptr) # This has read permission
```

^ we could have something like this from a plan perspective, right?In this case, the result permissions would mean:

""if we run `__add__` on `a_ptr`, with as input `b_ptr`, the result can be downloaded"", so it makes sense that `(a_ptr + b_ptr)` is downloadable, would you agree?I agree, only if both `a_ptr` and `b_ptr` are downloadable.
If `a_ptr` was not downloadable and the result was (and also `b_ptr` was downloadable) this might leak info about `a` - the other party could do `result - b_ptr` and would find `a`Its the other way around, `a_ptr` is downloadable, `b_ptr` is not. However, maybe this is not a good example, because this only works for plans. And for plans, the ""result-b=a"" logic may or may not hold, the point is, you should think about that when giving permission.

I think the example we should look at is

```
plan_ptr=plan.send(duet) #downloadable
# x_ptr is private
train_ptr.set_result_permissions(""__call__"", {""x"": x_ptr}, ds.verify_key) # set by data owner
y_ptr = plan_ptr(x_ptr) # called by ds
y = y_ptr.get() # called by ds, and no need to request download permission
``` 

It may be that y_ptr reveals some information about x_ptr, but thats beyond the point (the plan could be a machine learning model, or some statistic, it could be anything). However, this is no different from an ad hoc download requests.

If the plan is really doing x_ptr +3, or something else that reveals too much information about the inputds, the data owner should consider denying that request.",5,False,2021-06-03 11:27:03,2021-10-12 05:58:47,2021-10-12 05:58:47
https://github.com/OpenMined/PySyft/pull/5622,[],0.5.0rc2 one more time,"0.5.0rc2 one more time## Description
Hopefully the build version issue is fixed.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-06-01 08:52:54,2021-06-01 09:00:28,2021-06-01 09:00:28
https://github.com/OpenMined/PySyft/pull/5609,[],Test network utility,"Test network utility## Description
Basic network diagnostics test utility. 

- [x] CI test
- [x] Check local signaling server
- [x] Loopbackloopback=True still uses the same signalling server it just exchanges the IDs automatically using a file, so we should still check all of these main url service checks. There is the case where a user can run their own signalling server so it might be nice to be able to pass that in as a parameter and have these checks run against it to make sure that the python process can read the url on the port you supply.

like:
```
sy.test_duet_network(network_url=""http://192.168.0.2:5432"")
```Not sure how I missed those üòûÔ∏è. Fixed.",3,True,2021-05-28 05:06:22,2021-06-28 22:55:59,2021-06-28 22:55:59
https://github.com/OpenMined/PySyft/pull/5602,"['priority: 2 - high :cold_sweat:', 'documentation ']",Update Hyperledger Aries Example to fix Breaking Changes to Controller package,"Update Hyperledger Aries Example to fix Breaking Changes to Controller package## Description
We recently introduced some breaking changes to the aries_cloudcontroller package. I have been through the tutorials and updated code appropriately so it all works. Plus I now fixed the packaged to a specific version so this will hopefully remain stable in the future.

I linked to a deployed version of the OM Duet Authority - http://139.162.224.50/. Would be good to put behind a proper domain and add SSL etc but this will do for now. Just to show people taking the example what something more ""real"" might look like.

I added a notebook to allow people who didn't take the full course to still get a credential on their mobile which they can use to authenticate against the OM Duet Authority website.

And made some minor updates to the Readme.

@madhavajay I think the course comes out Wednesday so be good to get this in ASAP. No changes to any Syft code so would suggest just merge ;)

## Affected Dependencies
None

## How has this been tested?
- Worked through the full tutorial

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",3,True,2021-05-24 16:15:08,2021-06-01 02:21:13,2021-06-01 02:21:12
https://github.com/OpenMined/PySyft/pull/5591,[],Moving CDC data into external repo,"Moving CDC data into external repo## Description
Moving CDC data into external repo.
Fixes broken CDC test.

## Affected Dependencies
None

## How has this been tested?
Locally and CI

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-05-20 01:15:54,2021-05-20 01:16:28,2021-05-20 01:16:28
https://github.com/OpenMined/PySyft/pull/5574,[],Remove redundant time.sleep(),"Remove redundant time.sleep()## Description
Closes #5394 
Removed/commented time.sleep() where unneeded. Added comments where the command can't be removed.

## Affected Dependencies
src/syft/grid/duet/__init__.py 
Some tests

## How has this been tested?
Tests were run for the affected files to ensure expected behaviour.
NOTE: Current test failure seems to be due to CI issues. The tests ran fine locally before updating branch from source.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,False,2021-05-15 17:06:05,2021-08-11 01:34:30,2021-08-11 01:34:30
https://github.com/OpenMined/PySyft/pull/5567,[],0.5rc2,"0.5rc2## Description
- Recursive Serializable Plans with ResNet 18 example
- Stability fixes
- PyGrid fixes
- New Mono Repo structure
- Renamed master to main
- Added torch==1.8.1 support

## Affected Dependencies
Release

## How has this been tested?
CI

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-05-14 10:36:33,2021-05-14 10:42:26,2021-05-14 10:42:26
https://github.com/OpenMined/PySyft/pull/5565,[],Removing time.sleep,"Removing time.sleep## Description
#5394
Removed `time.sleep()` from the  `Pointer class` from `pointer.py` and `OpenGridTokenFileExchanger` from  `exchange_ids.py`. As of now I haven't experienced any time lag after removing it. Creating this PR for testing any error/bug on github tests.

## Affected Dependencies
`src/syft/core/pointer/pointer.py`
`src/syft/grid/duet/exchange_ids.py`

## How has this been tested?
A seperate script was made where I made requesting access to the pointer with and without `time.sleep` and I could not find any sort of time lag.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,False,2021-05-13 20:51:16,2021-06-26 21:04:20,2021-06-26 21:04:20
https://github.com/OpenMined/PySyft/pull/5552,[],PyGrid client minor fixes,"PyGrid client minor fixes## Description



## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)",2,False,2021-05-09 22:52:53,2021-05-10 23:46:56,2021-05-10 23:46:56
https://github.com/OpenMined/PySyft/pull/5540,[],[WIP] Create Protobuf Object for BERT Iterator,"[WIP] Create Protobuf Object for BERT Iterator## Description
@AlanAboudib Creates a `.proto` file for the BERT Iterator in SyferText, so it can be used with Duet. Fixes Issue #5539",3,False,2021-05-05 23:52:01,2021-06-22 06:26:18,2021-06-22 06:26:18
https://github.com/OpenMined/PySyft/pull/5529,['documentation '],Add autoflake,"Add autoflake## Description
Add autoflake to precommit

## Affected Dependencies
None

## How has this been tested?
I have included into `SyMPC` as well (see https://github.com/OpenMined/SyMPC/pull/139) 

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
Q: Could we also add unused imports? (also, I think we should also use recursive?)- Remove (python standard) libraries is used by default. Remove other libraries could raise unexpected errors I think

- yes, I will include recursive
For 1 I think we can put a ""noqa"" like text next to the import.",6,False,2021-05-01 22:57:01,2021-05-06 04:49:50,2021-05-06 04:49:49
https://github.com/OpenMined/PySyft/pull/5528,"['priority: 3 - medium :unamused:', 'documentation: docstring']",update python-linting GA,"update python-linting GA## Description
Enable `pydocstyle` and `darglint` and simplify `python-linting` job GA.
Most of the checks have been delegated to `pre-commit` via `pre-commit/action@v2.0.0`

For the reviewer:
This way developers are forced to run `pre-commit` locally to see if they have errors. If `syft-core` team thinks this can be a blocker, please comment and close the PR.


## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
I have added a dummy file to test:
https://github.com/OpenMined/PySyft/runs/2484002854?check_suite_focus=true
![image](https://user-images.githubusercontent.com/16245436/116795723-4f4bf280-aad7-11eb-8e3d-55fc4a63232e.png)


## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
Q: Why not remove those?",1,False,2021-05-01 21:42:01,2021-05-28 12:22:50,2021-05-28 12:22:49
https://github.com/OpenMined/PySyft/pull/5520,"['bug ', '0.5']",Fix model serialization in MCFL python notebook (fixes #5460),"Fix model serialization in MCFL python notebook (fixes #5460)## Description
Model serialization scheme was updated in the last moment before 0.5.0rc1, 
but I forgot to change it in part of the MCFL notebook that demonstrates low-level interaction with Grid where model ser-de was made ""manually"" not using syft routines.
This change doesn't affect any functionality.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
Ran create plan & execute plan notebooks against pygrid 0.5.0rc1

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
Add these changes to `tests/syft/core/fl/model-centric/mcfl_create_execute_plan_test.py`?",3,False,2021-04-27 13:39:14,2021-08-05 08:11:21,2021-08-05 08:11:21
https://github.com/OpenMined/PySyft/pull/5505,[],Fix rl notebooks,"Fix rl notebooks## Description
fix an error and remove unused attributes.

## Affected Dependencies

## How has this been tested?

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,False,2021-04-24 07:56:26,2021-04-24 15:04:23,2021-04-24 15:04:23
https://github.com/OpenMined/PySyft/pull/5499,[],[WIP] Fix remote binary operators ,"[WIP] Fix remote binary operators ## Description
Fix #4774 

## To Discuss
Possible approaches to fix this:
- [currently implemented] After `RunClassMethodAction`, validate the pointer for binary ops (i.e. set pointer type to what remote client has it as, instead of referencing the AST tree).
This results in two messages being sent to the client.

- Add another `RunClassMethodAction` as a `with_reply` message such that it returns the actual result type after executing the action. Use this for binary ops.
This would reduce the pointer validation message being sent again, but might have other implications I'm missing.

## How has this been tested?
Tests pending.",2,False,2021-04-22 05:42:28,2021-09-03 08:59:36,2021-09-03 08:59:36
https://github.com/OpenMined/PySyft/pull/5495,[],Public Key Infrastructures Course: Hyperledger Aries Duet Session Auth Layer,"Public Key Infrastructures Course: Hyperledger Aries Duet Session Auth Layer## Description
I replaced the self-sovereign-identity folder with the hl-aries-credential-exchange folder. This is now up to date and uses the Syft 0.5 rc version. Readme in the folder contains instructions for running the example which should stand independently of the course. 

Be grateful is someone can have a go at following these to run the example.

## Affected Dependencies
None

## How has this been tested?
- Example notebooks tested and working

We could remove this now",1,True,2021-04-21 18:36:04,2021-05-03 06:04:48,2021-05-03 06:04:48
https://github.com/OpenMined/PySyft/pull/5494,"['bug ', 'improvement ', 'good first issue :mortar_board:', 'priority: 4 - low :sunglasses:', 'severity: 4 - low :sunglasses:']",Fix notebook Remote Medical Data Science Grid Mock,"Fix notebook Remote Medical Data Science Grid Mock## Description

This just removes an extra return within a string in a notbook JSON. The notebook file is corrupted and doesn't load without this fix.

## Affected Dependencies
None

## How has this been tested?
- Just try to load the notebook either into a Google Colab or through `jupyter notebook`.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [-] My changes are covered by tests",2,True,2021-04-21 14:49:49,2021-04-27 06:20:38,2021-04-27 06:20:38
https://github.com/OpenMined/PySyft/pull/5493,[],[WIP]Add support for Object Attributes in AST,"[WIP]Add support for Object Attributes in AST## Description
Closes #5338 

This PR intends to add attributes to AST which are bound to the object not the class. For eg: `torch.nn.Linear(1,2).weight`.
Currently, trying out approach suggested by @madhavajay in #5338. 

### Fix

1. Enable addition of object attributes in AST
2. Add flag to such object attributes in their respective classes
3. Attach Function/Static Attribute to pointer to enable retrieval during execution time

## How has this been tested?
- Will write tests in `tests/syft/lib`

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,False,2021-04-21 14:30:03,2021-05-05 12:37:38,2021-04-27 06:17:04
https://github.com/OpenMined/PySyft/pull/5488,[],Audit duet tutorials,"Audit duet tutorials## Description
Change notebooks (mnist+dcgan) for gpu compliance
closes #5487 

## Affected Dependencies
None

## How has this been tested?
```
  python ./scripts/mnist.py
  python ./scripts/nb_duet_test.py
  pytest -m duet 
```

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
<p>Remove/Clean/Repeat, if not this would be on the <code>dev</code>  branch</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5488/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='4'/><p>Why is this information is removed? Was not ok?</p><p> </p><p>input is Z, going into a convolution</p><p>&nbsp;&nbsp;&nbsp;&nbsp;----------------------------------------------------------------</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Layer (type)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Output Shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Param #</p><p>&nbsp;&nbsp;&nbsp;&nbsp;================================================================</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConvTranspose2d-1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 112, 4, 4]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;179,200</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BatchNorm2d-2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 112, 4, 4]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;224</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReLU-3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 112, 4, 4]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConvTranspose2d-4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 56, 7, 7]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;56,448</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BatchNorm2d-5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 56, 7, 7]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;112</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReLU-6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 56, 7, 7]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConvTranspose2d-7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 28, 14, 14]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25,088</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BatchNorm2d-8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 28, 14, 14]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;56</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReLU-9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 28, 14, 14]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConvTranspose2d-10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 1, 28, 28]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;448</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tanh-11&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-1, 1, 28, 28]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0</p><p>&nbsp;&nbsp;&nbsp;&nbsp;================================================================</p><p>&nbsp;&nbsp;&nbsp;&nbsp;Total params: 261,576</p><p>&nbsp;&nbsp;&nbsp;&nbsp;Trainable params: 261,576</p><p>&nbsp;&nbsp;&nbsp;&nbsp;Non-trainable params: 0&nbsp;&nbsp;&nbsp;&nbsp;</p><p>&nbsp;&nbsp;&nbsp;&nbsp;""""""</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5488/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='16'/><p>Same</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5488/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='17'/><p>It was for the original version, just the user couldn't generate it without additionnal packages. </p><p>Also in the example above I've used the resize transform so it isn't anymore.</p>Thanks for the comment!Thanks for the comment!",9,False,2021-04-20 08:18:11,2021-08-31 01:34:45,2021-08-31 01:34:44
https://github.com/OpenMined/PySyft/pull/5486,[],[WIP] Duet SSI Fix,"[WIP] Duet SSI Fix## Description
This PR is a rebase and fix of this older PR: https://github.com/OpenMined/PySyft/pull/4911

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
Locally

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,False,2021-04-20 07:19:30,2021-04-23 01:47:28,2021-04-23 01:47:27
https://github.com/OpenMined/PySyft/pull/5483,[],[WIP] Fix rl notebook,"[WIP] Fix rl notebook## Description
fix a bug within the actor_critic notebook
remove unused attributes in reinforcement_learning notebook

## Affected Dependencies


## How has this been tested?

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",3,False,2021-04-20 02:39:25,2021-04-21 14:50:25,2021-04-21 14:50:24
https://github.com/OpenMined/PySyft/pull/5477,[],Fix tutorials SyMPC,"Fix tutorials SyMPC## Description
Fix the SyMPC VMs and Duet tutorials.

At one point we need to make a request to our own duet store -- at that point, we check if the node we respond is the same as the ""we"" and in that case, we made a request to our own store and we can give ```Request.Accepted```.

The problem is because requesting the ```len``` of an object triggers a ```request``` and then a ```get```.
The same behavior can be seen using:
```
ptr = duet.python.List([1,2,3])
ptr.get() ## This works because we do not request, we simply get the object


BUT, doing this:

# This won't work because we would do a get(request_block=True)
# and we do not have the logic for what happens when
# node X request an object from node X
len(ptr)
```

## How has this been tested?
- Running the tutorial

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
It should also approve the requests made by simple clients to be consistent with the case when we create an object and simply ""get"" it without approval?
Or should we change the behavior for ""get""?Well, currently we are using VirtualMachines as placeholders to simulate the permission mechanism as well, (we should probably create test-specific nodes at some points). Until then, normal clients are going through the permission scheme, while the root clients get auto-approved?",1,True,2021-04-15 20:41:04,2021-05-06 12:07:13,2021-05-06 12:07:13
https://github.com/OpenMined/PySyft/pull/5474,[],"[WIP] torchvision mean, std","[WIP] torchvision mean, std## Description
Added all accepted data types in ``Normalize.mean``  and ``Normalize.std`` which are accepted by ``as_tensor()``. issue #5031 

## How has this been tested?
- ``pre-commit run --all-files``

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Shouldn't these be UnionGenerator? example:
```
allowlist[""torch.Tensor.max""] = UnionGenerator[
    ""syft.lib.python.Bool"",
    ""syft.lib.python.Float"",
    ""syft.lib.python.Int"",
    ""torch.return_types.max"",
]
```Is there any specific need for this? The error is 
`` mean is not an attribute of Normalize``They are attributes of normalize class. Anything that pass as a ``tensor`` is mean, std's return type.allowlist return type is set to a single one.",2,False,2021-04-14 15:13:22,2021-04-27 06:09:08,2021-04-27 06:09:08
https://github.com/OpenMined/PySyft/pull/5463,[],[WIP] Allowlist test with cuda,"[WIP] Allowlist test with cuda## Description
1.All parameter combinations were tested on the CPU and GPU separately and marked ""skip_no_cuda"" if no GPU device was available.
2.A CUDA (true/false) entry is added to the not_available rule to indicate whether the parameter is not available on the CPU or CUDA.
3.Exceptions from syft are treated separately and added with ""reason:not_supported_syft"" to the entry not_available.
4.Fixed some parameter errors in some of the allowlist_test.json functions, such as ""inverse"" with tensor_cube as the parameter.
5.Add rules to generate sparse tensor and quantized tensor.
6.If an inappropriate _arg causes the error, the not_available entry takes a ""reason:bad_input.""
7.All entries in the allowlist_test.json that do not equal the current torch version(lte_version=gte_version=torch version) are temporarily removed and regenerated except for untested, added_feature, and deprecated rules.

## Affected Dependencies

## How has this been tested?

## Checklist
- [*] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [*] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [*] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [*] My changes are covered by tests
Typo?typo...",3,False,2021-04-12 08:15:59,2021-06-22 06:19:55,2021-06-22 06:19:55
https://github.com/OpenMined/PySyft/pull/5455,[],Forward plan (SyModule),"Forward plan (SyModule)This PR is an attempt to solve the following challenges around SyModule
- Creating 1 model as source of truth, not a local and a remote model
- The ability to compose multiple SyModules into one bigger model
- A convenient API for defining complex models, load pre-trained weights, and then fine tune the pretrained model using SyModule
- The ability to pass a SyModule as a default argument to a plan 

This PR is based on earlier work by @xutongye around serialization of nn.module and introduces:

1) SyModule
- - the forward method is automatically converted into a plan **after initialization of an object**, by using the state in the SyModule in the Plan.
- - SyModule can contain nn.Parameters, nn.Modules from the ast, and nested SyModules
- - The most tricky part of this PR is the ""recompile"" step. We currently recompile (relinking the obj state to the plan) the SyModule on **deserialization**, as the state may have changed compared to when the forward `Plan` was constructed. We might want to handle this in a different place, or store a `needs_recompile` attribute and check before execution. The implementation details are in the lowest part of the `proto2object` for SyModule
2) SySequential, inheriting from SyModule which allows to chain multiple SyModules
3) Nested plans, in order to wrap a SyModule.forward call in a plan

<p>would be really cool if this dummy data could be automatically generated based on the input argument variables</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5455/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='30'/><p>train_pointer is accepting actual variables as input instead of pointers to variables already on the remote machine. This makes me think that the plan is actually being executed locally on pointers instead of the actions actually being sent to the remote machien to be executed there. This would mean that there's still a tremendous amount of network traffic (one message per action) when running a plan as opposed to only saying ""hey run this plan on these variables"" being the only message actually sent over the network.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5455/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='41'/><p>Calling model.send() in the plan feels strange to me. If we're sending the plan to a remote worker and running it there then why would the plan (which is already on the remote machine) send data somewhere else?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5455/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='16'/><p>glad to se this is working without model.send() although i'm not sure where the model or its parameters are being sent to the remote machine. where is this happening?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5455/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='15'/><p>both, because model is an argument to the plan it is sent to the plan vm automatically. model is a symodule and therefore uses the object2proto in lib/torch/module line 419</p><p>This notebook was experimental and is outdated, please check the Sy Resnet nb</p><p>Yes, the plan pointer does accept variables (instead of pointers), however I think it would also accept pointers. But it does not mean that those variables are not sent and executed remotely (I am not sure how to interpret ""local"", is a VM also local?). When we execute the plan pointer by passing variables we don't need a lot of traffic, because we only serialize the arguments, not the actions in the plan. <strong>However, </strong>we currently use a hack for local execution of plans, which actually involves sending them to an ephemeral machine. We should get rid of that hack at some point.</p><p> </p><p>Also, this notebook is outdated </p><p>Agreed, I do think this would be a nice stand alone ticket so we might leave it our for now (in this PR)?</p><p>ahhhhh!</p>I think a requirement for this is that our API for calling the forward plans exactly matches the api for calling the torch.nn.Module.forward (I think this is a good idea anyway), right now there is a difference:

SyModule.forward
```
out = self.layer(x=x)[0]
```
torch.nn.Module.forward
```
out = self.layer(x)
```",3,True,2021-04-09 14:01:59,2021-05-06 03:26:21,2021-05-06 03:26:21
https://github.com/OpenMined/PySyft/pull/5447,[],[WIP] fix - TorchModule - __setattr__,"[WIP] fix - TorchModule - __setattr__## Description
This PR is for issue [#5036](https://github.com/OpenMined/PySyft/issues/5036). Looking for guidance @tudorcebere 

## Affected Dependencies
None.

## How has this been tested?
Have ran [this test](https://github.com/OpenMined/PySyft/blob/55bb023f6cc1f1778dc5f5354ab681c4028c3ed9/tests/syft/lib/torch/module_test.py#L83) to test the changes.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,False,2021-04-05 15:20:07,2021-05-06 07:42:40,2021-05-06 07:42:40
https://github.com/OpenMined/PySyft/pull/5416,[],PyGrid client Minor Fixes / Enhancement,"PyGrid client Minor Fixes / Enhancement## Description
Adjust the `syft.grid.client.get_instances` method to show available cloud instances in a DataFrame in a proper structure.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",5,True,2021-03-31 19:19:17,2021-04-01 09:16:32,2021-04-01 09:16:31
https://github.com/OpenMined/PySyft/pull/5410,[],Fix MCFL on Grid Side,"Fix MCFL on Grid Side## Description
Making changes to syft to match the PyGrid MCFL branch and integration tests.

## Affected Dependencies
Syft, Grid the whole world! 

## How has this been tested?
Integration tests and CI.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-03-31 03:24:42,2021-06-29 18:01:43,2021-03-31 08:05:39
https://github.com/OpenMined/PySyft/pull/5406,[],Add simple gc strategies,"Add simple gc strategies## Description
Add a strategy like design pattern to the GC logic.
Currently, it implements two strategies:
 - `simple` - the one that we had before when one pointer goes out of scope a message would be sent to the DO to delete the object.
 - `batched` - send a message when a number of messages have been collected.

TODO:
 - add `benchmarking`

## Affected Dependencies
- GC logic that was used before

## How has this been tested?
- Added tests
- TBD: Add benchmark.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
typo in docstringDo we need both in your opinion?I think we might need the ```threshold_total``` in case the DS is talking with a lot of DOs (and it might accumulate a lot of pointers? even if they don't take as much space as the real data, they might also add up) - and in case we never reach the limit for ```trehshold_client``` we might consume memory on the DS side?

Thinking also about this now...I think I would remove this, this is premature optimization and we should add code only in case we need it (for the moment there is no indication that this might happen).NIT: more like a personal preference maybe, but it might make sense to live under `core/pointer`? If you feel like this is the right spot, leave it here.Q: Not sure how to solve the circular dependency more nicely :\@tudorcebere this is the way that I would think most people would use the gc strategyI like this as well, but we need really nice docs on it on how to actually use them/how to implement one. Most likely, people won't be aware of this feature, but it's nice to have the option to do it. (Maybe we want to cut network usage and never free anything, stuff like that).Yep - I would do that in the ```__init__.py``` file from ```garbage_collection``` dir",3,True,2021-03-30 15:27:05,2021-04-06 11:03:30,2021-04-06 11:03:30
https://github.com/OpenMined/PySyft/pull/5402,[],[WIP] fix - TorchModule - __setattr__ ,"[WIP] fix - TorchModule - __setattr__ ## Description
This PR is intended to investigate  and fix issue #5036. Looking for guidance @tudorcebere 

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2021-03-30 11:36:59,2021-04-02 14:24:40,2021-04-02 14:24:33
https://github.com/OpenMined/PySyft/pull/5385,[],load multiple libs in one call,"load multiple libs in one call## Description
Fix #5344
Enable loading multiple libraries in a single `syft.load` call.

## How has this been tested?
`tests/syft/api/load_lib_test.py`

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Q: Why is the need to convert to string?NIT: Leave it like ```lib=lib```Could you add in the type hints that we want specifically strings?
`List[str], Set[str]` etc.

It has to be clear for the user that it shouldn't pass the module itself.We shouldn't convert to string, we need to check all the arguments (even if nested) are strings.

In other words:

```
if isintance(args[0], str):
   ...
elif is_iterable(args[0]):
   # iterate through this and check if the components are strings and load them
else:
   # errorCould we get another test that checks for different errors when an invalid string or an invalid object is passed?To ensure only string objects get passed.

Now checking using `isinstance` before calling `_load_lib`, to better handle error reporting.Done.Added error testing. But this requires #5397 to be merged first.Done.Done.",1,True,2021-03-28 17:28:56,2021-03-31 16:41:45,2021-03-30 10:59:25
https://github.com/OpenMined/PySyft/pull/5384,[],fix#5365 skip rewrapping iter,"fix#5365 skip rewrapping iter## Description
Fix #5365

`__iter__` gets wrapped multiple times due to 
https://github.com/OpenMined/PySyft/blob/261869e50852a24b2d76f3b44a5819050acd9eb8/src/syft/ast/klass.py#L338-L342

Fix: skip re-wrapping if already wrapped

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
NIT:
```
qual_name = getattr(iter_target, ""__qual_name__"", None)
if qual_name and ""wrap_iter"" in qual_name:
    return
```",4,True,2021-03-28 09:02:55,2021-04-19 17:11:59,2021-03-30 12:11:16
https://github.com/OpenMined/PySyft/pull/5383,[],"torchvision test parameters, min_version requirements","torchvision test parameters, min_version requirements## Description
Fix #5325 
Refactor torchvision test parameters & fix min_version requirements

Fix #5368 
Support PIL Image in torchvision tests.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
We have PIL image types now so this could potentially be enabled?Yes, done.",1,True,2021-03-28 08:26:21,2021-04-05 10:38:00,2021-04-05 10:38:00
https://github.com/OpenMined/PySyft/pull/5378,[],Raise Reference error to prevent spurious network call,"Raise Reference error to prevent spurious network call## Description
Fixes issue #5364 
Pointer is invalidated if `.get()` is called once.

## Affected Dependencies
None

## How has this been tested?
``` python
import syft as sy
client = sy.VirtualMachine().get_root_client()

int_ptr = client.syft.lib.python.Int(0)
result = int_ptr.get() # delete_obj = True by default

# Does not make a network call and 
# Raises a ReferenceError
int_ptr.get()
```

Raises a reference Error in the second call and preventing a network call.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,True,2021-03-26 18:23:31,2021-03-29 11:50:35,2021-03-29 11:50:35
https://github.com/OpenMined/PySyft/pull/5376,[],API of user-defined model & Plan,"API of user-defined model & Plan## Description
This PR is directly based on #5342 . 

### How does the API look like?
```python
class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = torch.nn.Linear(4,2)
        self.fc2 = torch.nn.Linear(2,1)

    def forward(self, x=torch.rand(4), torch=torch): # << The only difference is here.
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x

@make_plan
def train(dl=dl, model=MyModel()):
    optimizer = ROOT_CLIENT.torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0)
    criterion = ROOT_CLIENTtorch.nn.CrossEntropyLoss()
    
    for x, y in dl:
        out = model(x=x) # << use keyword args, not position args
        loss = criterion(out, y)
        loss.backward()
        optimizer.step()
        
    return model

local_model = MyModel()
remote_model = local_model.send(alice_client)
train_ptr = train.send(alice_client)
m_ptr = train_ptr(dl=remote_dl, model=remote_model)
m_get = m_ptr.get()
assert isinstance(m_get, MyModel) # the type of model got back is the class you defined
assert type(m_get.forward).__name__==""method"" # the forward of m_get is a method, not a plan.
```
So, as you can see, the way we define our model is almost the same as in pytorch, except two requirements in forward method:
1. It must have a default value for `x`. This is necessary for us to create plan from the forward method.
2. It must have an argument `torch`, and also give default value to it. This is necessary because we want to replace `torch` by `remote_torch` in plan. With `torch` as an argument, we can replace it simply by passing in `remote_torch` to the `torch` argument.

### How I do that?
#### 1. How to call child modules of a remote model, i.e., `model_pointer.fc1(x)`?
An user defined model class is not pre-defined in our source code, which means we can't add it to AST. So, the `model_pointer` doesn't have attribute named `fc1` when it's created. We need to attach an attribute `fc1` to  `model_pointer`. 
To do that, one way is to send `local_model.fc1` and get a pointer `fc1_pointer`, and then do `remote_model.fc1=fc1_pointer`. But, this means we send `local_model.fc1` two times, one time when we send `local_model`, second time when we send `local_model.fc1`. Doing so, means `remote_model.fc1_pointer` is pointing to a different remote `nn.Linear` object, i.e., not the child module `fc1` of the remote model. Then, when training, the optimizer is updating the parameters of which `fc1`? So, this is a problem. Maybe we can find a solution to this problem. But let's find a better way, where there is no such a problem.
My solution is, we don't need to send `local_model.fc1` to get `fc1_pointer`, we directly copy the pointer `remote_model` to get `fc1_pointer`, and give an attribute `attribute_name=""fc1""` to `fc1_pointer`. With this new attribute `attribute_name=""fc1""`, we can directly access the child module `""fc1""` of the remote model. So, `fc1_pointer` is pointing to the child module of the remote model, not a different `Linear` object.
#### 2. How is the forward plan created?
The forward method of our model is sent as a plan object. So, we create a plan from the forward method. So, we need the forward method to have default value for it's `x` argument. And we need the forward method to have a `torch` argument, which is replaced by `remote_torch` when creating the plan object.
One problem is that when making a plan, it needs a function, not a method. But `forward` is a mothod. How we get a function? It's easy, we can call a method from the class name, `MyModel.forward(self=model, x=x, torch=torch)`, then it acts like a function.
#### 3. When is the forward plan created?
We create the forward plan inside `send` method, i.e., the forward plan is created when you call `my_model.send()`. I think this is the good time to create the plan, because this is the time the plan is needed.
#### 4. The forward plan will not be created multiply times.
It's slow to create a plan object, especially when the function is big. So, we store the plan object once it's created the first time, and re-use it when it's needed later.
```python
local_model = MyModel()
assert not hasattr(local_model, ""_sy_forward_plan"")
remote_model = local_model.send(alice_client) # a plan is created
assert hasattr(local_model, ""_sy_forward_plan"")
remote_model = local_model.send(alice_client) # _sy_forward_plan is used
```
#### 5. How to transform a plan to a function?
When a model is sent, the forward method becomes a plan on remote side. When a remote model is got back, is the forward attribute of the got back model a plan or a method? It's a mothod again.
When a model is sent, we record the class in a global dict named `CUSTOM_MODELS`, which maps a class name to a class. For example, `CUSTOM_MODELS[""MyModel""]=MyModel`.
So, when we get back a remote model, we can get the class of that model from `CUSTOM_MODELS`, because we know the class name from `proto.module_type`. So, we can create exactly a `MyModel` object. And it's forward attribute is a method naturally.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- examples/experimental/tongye/torch_nn_Module.ipynb
- examples/experimental/tongye/MLP Plan.ipynb

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
What is attribute name? Can't we deduce it from `points_to_object_with_path`?Would you mind adding:
* a test with a very dummy model (in can have like an internal FC of [32, 2]) with random data/labels to add a test that the forward pass is not breaking?
* add a few lines of docstrings?As I understand it, they are different. Let's use an example, 
```python
m = nn.Sequential()
m.add_module(""fc1"", nn.Linear(4,2))
m.add_module(""fc2"", nn.Linear(2,1))
m_ptr = m.send(duet)
```
Then, for `m_ptr`, the `points_to_object_with_path` of it would be like `torch.nn.Sequential`. Then, imagine if we want a pointer pointing to `m.fc1`, we can create a Pointer same as `m_ptr` but has `attribute_name=fc1`.sure, will do.Could you help me understand what this is doing?It would be great if this could be 1) documented 2) moved into the `Plan` class so it does not complicate the user facing API. Note that right now it would also not solve the issue when you would create a Plan by hand.Also check if this works if the plan output is a single object (instead of a list)",1,True,2021-03-26 05:39:30,2021-04-23 09:02:04,2021-04-23 09:02:03
https://github.com/OpenMined/PySyft/pull/5363,[],Iterator/GC fixes to solve store __delitem__ errors,"Iterator/GC fixes to solve store __delitem__ errors## Description
Fixing a few iterator return types

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
Theres some code below that I wrote:
```python
try:
    if hasattr(_obj_ref, ""__len__""):
        max_len = _obj_ref.__len__()
except AttributeError:
    # I am not sure why this happens on some types
    pass
```

I can't remember but I am pretty sure this happens, maybe its on generators which cant give you their length because its not known, or something like that, either way, I think its possible that there is no known `len` sometimes. Should we add some `try` and logging just in case so we can spot it when it blows up?Maybe we should add generators as a separated supported type instead?",1,True,2021-03-24 11:47:14,2021-03-25 19:40:46,2021-03-25 19:40:46
https://github.com/OpenMined/PySyft/pull/5337,[],add numpy.ndarray,"add numpy.ndarray## Description
Support numpy.ndarray

## Affected Dependencies
numpy

## How has this been tested?

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
If we add a custom Numpy proto which contains an additional field which is the numpy type as a string.

Then we should be able to do something like this:
```python
import numpy as np
import torch

# our unsupported numpy array
x = np.array([0, 1, 2], dtype=np.uint16)
npdtype = x.dtype

# refactor to a dict of uint -> int types for matching
if npdtype == np.uint16:
    x = x.astype(np.int16)

y = torch.from_numpy(x)

# serialize along side the original dtype
ser_y = y
ser_numpy_dtype = npdtype.name


# deserialize on other side
y = ser_y
numpy_dtype = ser_numpy_dtype

# conver to numpy first
z = y.numpy()

# get numpy type from string
npdtype = np.dtype(numpy_dtype)

# down sample
z = z.astype(npdtype)

assert z.all() == x.all()
assert z.dtype == np.uint16
```

Leave off complex, unicode, str and object for now and just throw exceptions and we can make it seperate tasks to handle them as a proper Numpy Array serde protobuf.Make sure to move the import numpy as np and other np calls to inside the test.
That way when the test collection process checks for `numpy` and if it doesn't find it it wont crash collecting the test file. In reality no one will not have numpy but its still how the vendor lib tests are done.Thank you!Please correct me if I'm wrong, could I also ignore uint and raise exceptions or should I make a custom proto which supports uint now?I would add it now since the majority of things we want numpy for is numbers so it would be good if we dont hit any of these by accident and have it not work.

To add the custom proto just create one that looks something like this:
```proto
syntax = ""proto3"";

package syft.lib.numpy;

import ""proto/lib/torch/tensor.proto"";

message NumpyProto {
  syft.lib.torch.TensorData tensor = 1;
  string dtype = 2;
}

```

Then run: `scripts/build_proto.sh`.Thank you for your clear instruction! I implemented this, and it worked! We can now use uint!",8,True,2021-03-22 05:58:06,2021-03-25 03:23:41,2021-03-25 03:23:40
https://github.com/OpenMined/PySyft/pull/5333,[],Opacus and Plan use case demo,"Opacus and Plan use case demo## Description
This PR adds a notebook to demonstrate a use case with Opacus and Plan. The notebook is heavily based on #5230 .

And also, this PR fix the `VerifyAll/VerifyAll()` inconsistancy, by introducing a constance `VERIFYALL`.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- examples/experimental/tongye/opacus_with_Plan/

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
<p>nit: spelling (""trainning"")</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='8'/><p>spelling: <code>lenth</code>  -&gt; <code>length</code>  <span class=""ql-cursor"">Ôªø</span></p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='9'/><p>But this is the pointer to the <strong>Dataset</strong>, not the dataloader. This length is the total number of examples, whereas if we check the length of the dataloader, we'd get the total number of batches.</p><p> </p><p>If we want to pass a pointer to the dataset instead that is totally fine, but then we should change the text cells to reflect that it's not in fact a dataloader :) </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='11'/><p>The super long duration might be an artifact of passing the dataset length instead of the dataloader length: in this case, batch size is 64 so there's going to be 64x the batches :D </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='12'/><p>Why not use real data since we have the pointer anyway? We can still limit the batches to the first 100</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='13'/><p>line 38: this is more pythonic: <code>data, target = data_tuple</code>.  You could also do this in the for loop directly:</p><pre class=""ql-syntax"" spellcheck=""false"">for _batch_idx, (data, target) in enumerate(dl):
</pre><p> </p><p>Same for line 48:</p><p> </p><p><code>eps, best_alpha = privacy_engine_ptr.get_privacy_spent(args[""delta""])</code></p><p> <span class=""ql-cursor"">Ôªø</span></p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5333/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='16'/><p>yeah, thanks</p><p>thanks</p><p>right, it's a typo, I did want length of dataloader, it should be <code>dl_length=train_loader_ptr.<strong>len</strong>()</code></p><p>yeah, we will see after fix the dataset length typo</p><p>It's how you use the Plan api for the moment. When define a function as a Plan, you should give default values to it's arguments, and the defaut value should be local, serializable, and sendable. So, we don't have the data locally, and a DataLoader is not serializable and sendable for the moment (we are trying to support that in #5314). So I use a List to fake it.</p><p>thanks</p><p>Yes this confusing part of the API is high on the change list. üòä</p><p>Sorry, this is a hangover from before we had Tuples and destructuring so its not intentional, we need to make sure this still works as expected because destructuring might be a little bit magic..... Actually I just tested and while destructuring does work it also seems to cause some small issue with the GC, so ill go make a ticket for that now. Thanks. üòä</p>",2,False,2021-03-21 02:44:28,2021-04-27 05:42:23,2021-04-27 05:42:23
https://github.com/OpenMined/PySyft/pull/5277,[],Fixing issue 5167,"Fixing issue 5167## Description
solves issue #5167 
## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-03-09 14:22:24,2021-03-19 05:11:22,2021-03-19 05:11:22
https://github.com/OpenMined/PySyft/pull/5266,['documentation '],Add docstring coverage,"Add docstring coverage## Description
Documentation is a very importante point if we want to see `openmined` used by people. 
This PR adds a docstring coverage workflow where the PR will be:
* Accepted if the docstring coverage is higher or the same
* Denied if the docstring coverage is reduced

This PR is part of `docs_team`. If reviewers think this is nice. The same PR will be done in rest of `openmined` projects.

## Affected Dependencies
None

## How has this been tested?
I have tested adding a dummy `foo.py` with empty docstring, in this case the PR fails, when I delete this dummy file the workflow is ok.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",5,True,2021-03-07 19:38:33,2021-03-10 06:34:50,2021-03-10 06:34:50
https://github.com/OpenMined/PySyft/pull/5261,[],fixed code quality issues,"fixed code quality issues### Description
Hi :wave: 
I ran the [DeepSource](https://deepsource.io) analysis on the forked copy of this repo and found some interesting [code quality issues](https://deepsource.io/gh/withshubh/PySyft/issues/?category=recommended).

##### Motivation and Context
Some of the issues I have fixed boost minor performance and a few removes the anti-patterns in code.

#### Summary of changes

- Remove length check in favour of truthiness of the object
- Remove unnecessary use of comprehension
- Use `callable()` to check if the object is calllable
- Replace ternary syntax with if expression
- Add the bound instance as method parameter
- Fix dangerous default argument
- Add .deepsource.toml

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)

We run mypy with `--no-implicit-optional` so this will probably fail in our linter step.
I think the best way to refactor this would be to introduce an Optional or just remove the two if statements since they are not needed.same as aboveThis used to be an Optional so i think it has just been wrongly changed, but perhaps it can be refactored by removing the default below... We need to be careful here since the concrete child classes actually pass through None:
```python
def __init__(
        self,
        *,  # Trasterisk
        name: Optional[str] = None,
        network: Optional[Location] = None,
        domain: Optional[Location] = None,
        device: Optional[Location] = None,
        vm: SpecificLocation = SpecificLocation(),
        signing_key: Optional[SigningKey] = None,
        verify_key: Optional[VerifyKey] = None,
    ):
        super().__init__(
            name=name,
            network=network,
            domain=domain,
            device=device,
            vm=vm,
            signing_key=signing_key,
            verify_key=verify_key,
        )
```

So I think these are actually correct.

```python
# must be supplied and defaults to {}
vms: [Dict[UID, VirtualMachine]] = {}
```
If we call this parent constructor with a `None`, we need to then detect that down stream and overwrite it since an explicit None here from the subclass won't use the default...
This is also a super special case where we actually need the self as part of the args to support older python versions.Interestingly, here we are actually testing for that attribute not that it is in itself `callable`, either is fine, but I wouldn't call this incorrect.",11,True,2021-03-06 08:14:45,2021-03-27 07:00:21,2021-03-27 07:00:20
https://github.com/OpenMined/PySyft/pull/5250,[],Torch vision api's added,"Torch vision api's added## Description
Solves issue #5186 
Added list of datasets from https://pytorch.org/vision/stable/datasets.html
Added list of transforms from https://pytorch.org/vision/stable/transforms.html
## Affected Dependencies
None
## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Q and NITPICK: Could we leave this as simple ```torch``` or ```th```?Yes, I'll make the changeMoving this down a line looks like a mistake?Shouldnt we match the torchvision capabilities to the torchvision version? Occasionally there is more than 1 torchvision version for a torch version like 0.8.0 and 0.8.1 due to a bug fixes.It might be better to leave this in but commented out so that its easy to spot what needs to be done to fix it.
e.g.

```python
torchvision.transforms.functional.to_pil_image(a)
<PIL.Image.Image image mode=RGBA size=736x700 at 0x11599C908>
```
```
# allowlist[""torchvision.transforms.functional.to_pil_image""] = ""PIL.Image.Image""
```Yeah, I am not sure what caused it, I'll make the changeI tried comparing the Torchvision version but I was facing some issues, I'll re-check@madhavajay `torch.__version__` is commented in `torch/allowlist.py`. Any specific reason why? No reason at all, the functionality to make it work was only recently added so it must have just been forgotten.
```
>>> client.torch.__version__.get()
'1.7.1'
```I uncommented it and it works. üòä",15,True,2021-03-04 07:05:16,2021-03-19 17:43:14,2021-03-19 06:31:01
https://github.com/OpenMined/PySyft/pull/5245,[],fix #5208 handler serde error,"fix #5208 handler serde error## Description
lack of @bind_protobuf cause the error below when access duet.requests.hander.
```
TypeError: You tried to deserialize an unsupported type. This can be caused by several reasons. Either you are actively writing Syft code and forgot to create one, or you are trying to deserialize an object which was serialized using a different version of Syft and the object you tried to deserialize is not supported in this version.
```

in util.py:validate_field, ""if(object)"" gets False when ""object"" is an empty list and errors appear.
```
[CRITICAL][logger]][4080] Object <GetAllRequestHandlersResponseMessage: xxxx> has no handlers field set.

PySyft/src/syft/logger.py in traceback_and_raise(e, verbose)
     57     except BaseException as ex:
     58         logger.debug(""failed to print exception"", ex)
---> 59     raise e
     60 
     61 

TypeError: exceptions must derive from BaseException
```


## Affected Dependencies
None.

## How has this been tested?
```
duet.requests.handler
```

## Checklist
- [‚àö] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [‚àö] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [‚àö] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [‚àö] My changes are covered by tests",1,True,2021-03-03 04:24:00,2021-03-04 13:09:20,2021-03-04 13:08:53
https://github.com/OpenMined/PySyft/pull/5230,[],Plan API,"Plan API## Description
Aims to implement #5224 

## Affected Dependencies

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
local_model should be passed in as an argument, no? Otherwise the plan might never be able to find it when the plan is used in the wild somewhere.can this plan be run multiple times? let's add a test to be sure.Yes it can, there is already a nb in this pr doing that. Will add a testYes that would be better. However, I ran into problems with sending sy.modules, by passing them as arguments, because it relies on the object2proto function, which is not implemented for sy.modulesSo i think one of my next projects might be to make it serializable. If you have any ideas here let me knowI am not a 100% sure, but I dont think the plan would be unable to find the model on a different Node (I assume that is what you mean by _in the wild_). Not that in this test we also send the model to another node after creation. I think it all works because when we send the model during plan creation, we record a save_object_action on the PlanVM.We are working on serializable nn.Module and then a refactor of Sy.Module for this.",6,True,2021-03-01 09:50:35,2021-03-23 08:42:36,2021-03-23 08:42:35
https://github.com/OpenMined/PySyft/pull/5229,[],notebook example for remote DataLoader,"notebook example for remote DataLoader## Description
Closes #4901 

This PR is mainly based on @uid42 's implementation on #4901. 
Since there has been no activity for more than one month, I created new PR based on his code. 
Although I solved the problem of retrieving the remote model, I also have trouble sending list pointer. 

As @uid42 says in #4901, this notebook works in 0.4, but doesn't work in 0.3.  

## Affected Dependencies
no

## How has this been tested?
google colab and jupyter lab

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,False,2021-03-01 06:28:47,2021-03-24 04:10:54,2021-03-24 04:10:53
https://github.com/OpenMined/PySyft/pull/5228,[],Added Slice object in syft,"Added Slice object in syft## Description
This PR adds a slice object to syft. It is related to [this issue](https://github.com/OpenMined/PySyft/issues/5082). 

## How has this been tested?
It hasn't been tested yet. I will add tests as soon as I get a review on this PR.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Error:
```
src/syft/lib/python/slice.py:151:20: F632 use ==/!= to compare constant literals (str, bytes, int, float, tuple)
```

Change to 
`assert step != 0`I am not sure why you have used `NULL` instead of `None`. Any specific reason? If not replace it with `None` and that should solve following logs 
```
src/syft/lib/python/slice.py:38:21: F821 undefined name 'NULL'
            start = NULL
                    ^
src/syft/lib/python/slice.py:39:20: F821 undefined name 'NULL'
            stop = NULL
                   ^
src/syft/lib/python/slice.py:40:20: F821 undefined name 'NULL'
            step = NULL
```

and some more similar errors, have a lookI see an error because these are not used 
```
src/syft/lib/python/slice.py:23:1: F401 '.util.downcast' imported but unused
from .util import downcast
^
src/syft/lib/python/slice.py:24:1: F401 '.util.upcast' imported but unused
from .util import upcast
```I don't see getitem and iter on the slice Class. Am I missing something?Can't these just be normal `ints`?yes, that won't be a problem but i just thought if it might help with faster computationThe problem would be that we need to convert the ints into syft.lib.python.Ints then to protos and bytes, where as i'm sure the built in protobuf integer type is extremely fast and lightweight.Alright!I still get this error. They are present in the slice Class.",19,True,2021-02-28 12:12:51,2021-03-24 02:49:44,2021-03-24 02:34:37
https://github.com/OpenMined/PySyft/pull/5222,[],Add metaclass for custom tensor,"Add metaclass for custom tensor## Description
Create a metaclass that would add the methods and properties to the custom tensor.
This is needed when creating the AST and doing remote operations (TODO: need to add an example).

## Affected Dependencies
- Custom tensors

## How has this been tested?
- Added test to validate properties
- Added test to validate that only the forwarded methods work

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
- [ ] Add remote tensor testsThey have nothing to do with the PR description.
Those were some simple fixes shown by flake",3,False,2021-02-26 09:12:02,2021-03-19 09:30:18,2021-03-19 09:30:18
https://github.com/OpenMined/PySyft/pull/5212,[],Information Flow,"Information Flow## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-02-23 23:36:42,2021-03-06 16:06:19,2021-03-06 16:06:19
https://github.com/OpenMined/PySyft/pull/5209,[],splitnn fix,"splitnn fix## Description
Cherry picked this PR: https://github.com/OpenMined/PySyft/pull/5205

## Affected Dependencies
None

## How has this been tested?
Jupyter 

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-02-23 07:30:06,2021-02-23 07:30:28,2021-02-23 07:30:28
https://github.com/OpenMined/PySyft/pull/5206,"['bug ', 'priority: 2 - high :cold_sweat:', 'severity: 3 - medium :unamused:']",Fix/Update PyGrid User Messages,"Fix/Update PyGrid User Messages## Description
Fix/Update PyGrid User Messages due to the changes done on storable objects (add `@bind_protobuf` decorator)


## Checklist
- [X] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [X] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [X] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2021-02-22 07:47:07,2021-02-25 02:37:06,2021-02-25 02:37:05
https://github.com/OpenMined/PySyft/pull/5194,[],CI: simplify install versions,"CI: simplify install versions## Description

unify scripts for installing Torch versions and related packages
after merging #5192 we can also drop PT version from the chafe file name

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
We had timeout issues with pip, are you sure the job task timeout will do the same thing?The reason we xargs the supported_libs is that they aren't available on all OSes or Python versions, so this allows them to silently fail to install and then those tests will simply not run.typo: installaha, then you can still set it with Action timeout and continue on failermy bad, thx```suggestion
```
this is not needed since you have cat in the step abovein combination with `continue-on-error: true` it should
but if you silently accept missing some packages isn't it making the test weak, imagine that none of the packages is installed, you skip them in testing and show as all pass... so you think all is fine, but in fact, you just don't know...```suggestion
```just a question, why not split the requirements file into mandatory and development and then in setup load the mandatory, then you have to keep updated packages only on one place, something in directions #5193 That was for debugging, I was trying to figure out why it was failing. Turns out it was the Windows python install not matching the os.linesep and installing CUDA instead of CPU.Yeah i saw that separate PR but I guess we can discuss that in the other PR.The problem I had was that if you pip install a .txt file and one of the packages fails it aborts the whole install. I could not find another way to ignore individual failures except to `xargs`.You can test this with torch <= 1.6.0 and or python 3.6 and the sympc library.
```
@pytest.mark.vendor(
    lib=""sympc"", python={""min_version"": (3, 7)}, torch={""min_version"": ""1.6.0""}
)
```The actual tests will fail. The only tests which get skipped are ones which don't match the matrix config:
```
@pytest.mark.vendor(
    lib=""sympc"", python={""min_version"": (3, 7)}, torch={""min_version"": ""1.6.0""}
)
```So in this case python 3.6 and torch <= 1.6.0 simply wont run that test and wont import sympc so it wont fail. But for any version where its expected it will fail because the lib won't be there.ok, now I understand what you mean, but do you know what package fails on what os?
what we did was filtering od adjustment based on OS / versions...sure :] that is why I made it separate :]",2,True,2021-02-19 10:44:31,2021-02-23 08:41:57,2021-02-23 08:27:06
https://github.com/OpenMined/PySyft/pull/5188,[],StorableObject refactor (Cherry picked),"StorableObject refactor (Cherry picked)## Description
The aim of this PR is to simplify things around StorableObject, including StorableObject, send, get, SaveObjectAction, GetObjectResponseMessage, maybe more.

Cherry Picked this: https://github.com/OpenMined/PySyft/pull/5104

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
I want to approve this PR only after reading the first file!

Golden work:fire: :smile: Ideally here we should be able to use the AST to solve the types, the main problem being that we don't have the wrapper types in our AST. Do you think we can find a workaround on pydoc.locate (maybe add the wrapper types in the AST/in some kind of cache). Not a must for this PR, but it's something worth to think on.:fire: :firecracker: This still exists, I just move it to another place, `generate_wrapper.py`  :)I think we can use function `index_syft_by_module_name`. I already add wrapper classes to AST path. But not really add them, just fake it like it's there. Please take a look inside `index_syft_by_module_name`. For exmaple, torch.deviceWrapper has a ast path syft.wrappers.torch.deviceWrapper",2,False,2021-02-19 02:15:42,2021-02-20 12:44:24,2021-02-20 12:44:24
https://github.com/OpenMined/PySyft/pull/5165,[],Fixed return type on opacus get_privacy_spent,"Fixed return type on opacus get_privacy_spent## Description
- Updated notebooks
- Added workaround to cast numpy floats and ints to python

## TODO
- [x] Refactor dynamic method resolution
- [x] Fix SyModule with PrivacyEngine
       Init method is expecting object or pointer but we have neither
       Perhaps we could create a fake top level module on the remote side and get a pointer to it?
- [ ] Add Integration Test Checkpoints to notebooks

## Affected Dependencies
None

## How has this been tested?
Notebooks

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-02-17 04:36:44,2021-03-03 08:09:47,2021-03-03 08:09:47
https://github.com/OpenMined/PySyft/pull/5153,[],[WIP]Fixing issues which are highlighted by bandit,"[WIP]Fixing issues which are highlighted by bandit## Description
This PR is for #5149. I have removed the assertion code and used python inbuilt module secretes for random number generation. Could you review my work and suggest to me any changes to this, so that I can try to implement the same to fix other issues of bandit too.


## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,True,2021-02-13 21:21:53,2021-02-19 07:38:29,2021-02-19 07:30:48
https://github.com/OpenMined/PySyft/pull/5151,[],fixes broken request name with tag,"fixes broken request name with tag## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests


Fixes #5102",3,False,2021-02-13 08:51:12,2021-03-02 10:48:31,2021-03-02 10:48:31
https://github.com/OpenMined/PySyft/pull/5125,[],Issue 5075 - Investigate pytest warnings,"Issue 5075 - Investigate pytest warnings## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests

## Summary
This time I run: 

`pytest -m 'fast or slow' --no-cov -n auto`

as @madhavajay suggest, and was raised 4116 warnings, with this smalls changes that I made the warnings decreased to 24 warnings.

What I did was to change the direct casting using int(), for using the built-in function __int__().

I'll open issues for this 24 warnings that I couldn't solve.

Closes #5075",1,True,2021-02-09 01:18:58,2021-02-11 07:12:45,2021-02-11 07:12:45
https://github.com/OpenMined/PySyft/pull/5121,[],duet notebook examples (closes #4798 and #4796),"duet notebook examples (closes #4798 and #4796)## Description
Closes #4798
Closes #4796 

I have written notebooks to fix the following issues. I tested them on jupyter lab and google colab., and both of them worked



## Affected Dependencies
No

## How has this been tested?
google colab and jupyter lab
## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,False,2021-02-08 02:41:59,2021-02-08 03:13:58,2021-02-08 03:13:58
https://github.com/OpenMined/PySyft/pull/5119,[],#5075 - Solve warnings on tests and open issues for the unsolved,"#5075 - Solve warnings on tests and open issues for the unsolved## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests

## Summary

Solved some warnings, decreasing from 29 to 23 warnings.
On the solutions, add async for tests that need and change the direct cast for float as float() to .__float()__
the reason for the float changes is that the ability to return an instance of a strict subclass of float is deprecated, and may be removed in a future version of Python.
For the unsolved warnings most of them is for the specific use of asyncio, so  I'll be opening issues for them.",1,True,2021-02-07 01:16:26,2021-02-08 05:03:20,2021-02-08 05:03:19
https://github.com/OpenMined/PySyft/pull/5117,[],Add attributes to allowlist accessible through objects(closes #5031 and #5032 ),"Add attributes to allowlist accessible through objects(closes #5031 and #5032 )## Description

Closes #5031 
Closes #5032 

Currently, the problem with adding attributes in `allowlist` dictionary is that only those attributes can be added which are static and only accessible by class. This is not possible  if you will try to access any attribute of `torchvision.transforms.Normalize` like `inplace`  or `mean`. You get an error as shown below:-

![](https://user-images.githubusercontent.com/33565881/105841499-b77fc980-5ffa-11eb-9aae-c387903632d8.png)

These attributes can only be accessed when we pass the required positional arguments list as given below. The trick is to pass these positional arguments whenever we call the attributes like `mean` and `std`

![](https://user-images.githubusercontent.com/33565881/105841531-c49cb880-5ffa-11eb-8c95-d6d63e97f070.png)

### Fix
My proposed fix adds two new attribute to `src/syft/ast/attribute.py`  -  `self.require_pargs` and `self.parg_list` .
`self.require_pargs` is set to `True` is that attribute requires any positional arguments to access its sub-attributes, otherwise by default it is set to `False `. `self.parg_list` contains list of all the positional arguments that need to be passed into the attribute so that we can access its sub-attributes, by Default it is empty. As all components of AST like `Module` , `Class` , `Callable`  inherit from `attribute` class they inherit these 2 parameters also.

We check if `self.require_pargs` is set to `True` and pass `self.parg_list` into the class as shown in the code so that we can access its internal sub-attributes during using `getattr()`.

### How to use
Specified in the code. In case of `torchvision.transforms.Normalize` we need to pass in 2 attributes `mean` and `std` . Add name of attribute( in this case `torchvision.transforms.Normalize`) and list of positional arguments as corresponding entry in this case `[(1, 2, 3), (1, 2, 3)]` . Have a look at `src/syft/lib/torchvision/allowlist.py ` . 


## Affected Dependencies
Added new dict module_pargs in which modules that require positional arguments to access attributes can list their positional arguments.

## How has this been tested?
Wish to get some guidance for added fix and then write tests

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
Please don't commit the Pipfile as its an opinionated virtualenv / package management file which isn't relevant to the source code repo itself.The return type depends on what is passed in. It seems that its allowed to be any scalar or sequence. We need to use the UnionGenerator here.",4,False,2021-02-06 03:03:09,2021-03-02 13:48:57,2021-03-02 10:46:36
https://github.com/OpenMined/PySyft/pull/5110,[],updated pre commits hooks version in .pre-commit,"updated pre commits hooks version in .pre-commit## Description
trying to solve  #5099 by updating packages 



## How has this been tested?
pytest -m fast -n auto comm


## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2021-02-04 08:50:56,2021-02-05 14:35:00,2021-02-05 05:41:40
https://github.com/OpenMined/PySyft/pull/5097,[],[WIP]Duet tests review,"[WIP]Duet tests review## Description
 - [ ] Fix flaky test. fixes https://github.com/OpenMined/PySyft/issues/5024
 - [ ] Fix sanity test.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,False,2021-02-02 18:44:49,2021-02-03 10:11:24,2021-02-03 10:11:24
https://github.com/OpenMined/PySyft/pull/5095,[],"[wip] Add SyftTensor, FloatTensor, DataTensor, IntegerTensor","[wip] Add SyftTensor, FloatTensor, DataTensor, IntegerTensor## Description
aims to implement #5046

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Would `callable` replace isfunc? Also is there any need to some how create a dynamic wrapper for @property or `getset_descriptor` attributes?updatedQ: Why not use?
```
if isinstance(self.child, (FloatTensor, IntegerTensor))
```
In that case there will be only 1 ```if``` statement

Also for the other cases as well.Q: @tudorcebere we should remove the ```syft_decorator```, right?Note that one comparison  is on `self.child`, the other is on `other`Sorry for not giving more information!

I meant this if and the next one.
``` 
if isinstance(self.child, FloatTensor) and isinstance(other, SyftTensor):
  return SyftTensor(child=self.child @ other.child)
if isinstance(self.child, IntegerTensor) and isinstance(other, SyftTensor):
  return SyftTensor(child=self.child @ other.child)
```

Changed to:
``` 
if isinstance(self.child, (FloatTensor, IntegerTensor)) and isinstance(other, SyftTensor):
  return SyftTensor(child=self.child @ other.child)
```

Don't forget about this :DTurns out this isn't possible due to Mypy, I get the error:
```
Both left and right operands are unions
```

I tried creating a Union type but isinstance can't work with ""Typing"" stuff. There could be a way around this but for now I think its okay.I removed all of these. üòä",1,True,2021-02-02 15:25:31,2021-02-16 08:21:17,2021-02-16 08:21:17
https://github.com/OpenMined/PySyft/pull/5089,[],Add attributes to allowlist which can be accessed through objects (closes #5031 and #5032 ),"Add attributes to allowlist which can be accessed through objects (closes #5031 and #5032 )## Description

Closes #5031 
Closes #5032 

Currently, the problem with adding attributes in `allowlist` dictionary is that only those attributes can be added which are static and only accessible by class. This is not possible  if you will try to access any attribute of `torchvision.transforms.Normalize` like `inplace`  or `mean`. You get an error as shown below:-

![](https://user-images.githubusercontent.com/33565881/105841499-b77fc980-5ffa-11eb-9aae-c387903632d8.png)

These attributes can only be accessed when we pass the required positional arguments list as given below. The trick is to pass these positional arguments whenever we call the attributes like `mean` and `std`

![](https://user-images.githubusercontent.com/33565881/105841531-c49cb880-5ffa-11eb-8c95-d6d63e97f070.png)

### Fix
My proposed fix adds two new attribute to `src/syft/ast/attribute.py`  -  `self.require_pargs` and `self.parg_list` .
`self.require_pargs` is set to `True` is that attribute requires any positional arguments to access its sub-attributes, otherwise by default it is set to `False `. `self.parg_list` contains list of all the positional arguments that need to be passed into the attribute so that we can access its sub-attributes, by Default it is empty. As all components of AST like `Module` , `Class` , `Callable`  inherit from `attribute` class they inherit these 2 parameters also.

We check if `self.require_pargs` is set to `True` and pass `self.parg_list` into the class as shown in the code so that we can access its internal sub-attributes during using `getattr()`.

### How to use
Specified in the code. In case of `torchvision.transforms.Normalize` we need to pass in 2 attributes `mean` and `std` . Add name of attribute( in this case `torchvision.transforms.Normalize`) and list of positional arguments as corresponding entry in this case `[(1, 2, 3), (1, 2, 3)]` . Have a look at `src/syft/lib/torchvision/allowlist.py ` . 


## Affected Dependencies
Added new dict module_pargs in which modules that require positional arguments to access attributes can list their positional arguments.

## How has this been tested?
Wish to get some guidance for added fix and then write tests

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
I am having a feeling that we might want the ```allowlist``` to become a more ""standalone"" class such that we might add more things to it? (like we have the ```module_pargs```)
What do you think @madhavajay, @tudorcebere ? or maybe I am overthinking this?",8,False,2021-02-02 08:41:51,2021-02-16 04:07:11,2021-02-16 04:07:11
https://github.com/OpenMined/PySyft/pull/5088,[],Result of func/method/property/attribute inherites upstream tags,"Result of func/method/property/attribute inherites upstream tags## Description
This may close #5076 .

---
### Motivation
Consider the following example:
```python
# DO side
torch.Tensor([1, 2, 3]).send(duet, searchable=True, tags=[""a""]) 
torch.Tensor([4, 5, 6]).send(duet, searchable=True, tags=[""b""])

# DS side
a = duet.store[""a""]
b = duet.store[""b""]
c = a + b
c.get(reason=""some reason"") 
```
Then what should the DO do to accept the request to `c`? One way is to call `duet.requests[0].accept()`, this is the manual way. Another way, which is a automatical way, is to use request handlers, in this way, the DO can call `duet.requests.add_handler(action=""accept"", tags=??)`. Wait, what's the tags for `c`? No, there are no tags for `c`.

This is the problem this PR aims to solve.

### New features this PR introduces:
#### 1. Fix a bug.
A bug is encountered and then get fixed. The bug is the following: 
```python
# DO side
torch.Tensor([1,2,3]).send(duet, searchable=True, tags=[""a""]
torch.Tensor([4,5,6]).send(duet, searchable=True, tags=[""b""]
# DS side
duet.store[""a""].tags # you get [""a""]
duet.store[""b""].tags # you still get [""a""]
```
The reason for this bug is that if the key passed in to duet.store[key] is very short, for example key=""a"", then there are chances that it can be matched with any id string very easily, so it return the first element in duet.store. This is fixed.
#### 2. obj.property/attribute inherites tags of obj, plus name of property/attribute
For example:
```python
# DO side
torch.Tensor([1,2,3]).send(duet, searchable=True, tags=[""a""])
# DS side
a = duet.store[""a""]
ptr = a.requires_grad # ptr points to an object whose tags=[""a"", ""rerquires_grad""]
```

#### 3. obj.method(args) inherites tags of obj, tags of args, plus name of method
```python
# DO side
torch.Tensor([1,2,3]).send(duet, searchable=True, tags=[""a""])
Int(2).send(duet, searchable=True, tags=[""b""])
# DS side
a = duet.store[""a""]
b = duet.store[""b""]
ptr = a.pow(b) # ptr points to an object whose tags=[""a"", ""b"", ""pow""]
```
#### 4. func(args) inherites tags of args, plus name of func
```python
# DO side
torch.Tensor([1,2,3]).send(duet, searchable=True, tags=[""a""])
torch.Tensor([4,5,6]).send(duet, searchable=True, tags=[""b""])
# DS side
a = duet.store[""a""]
b = duet.store[""b""]
ptr = a + b # ptr points to an object whose tags=[""a"", ""b"", ""__add__""]
```



## Affected Dependencies
No.

## How has this been tested?
- examples/experimental/tongye/inherit_tags/

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,False,2021-02-02 04:20:24,2021-02-16 03:51:18,2021-02-16 03:51:17
https://github.com/OpenMined/PySyft/pull/5071,[],Duet example - DCGAN Draft - review request #4798,"Duet example - DCGAN Draft - review request #4798## Description
I'm currently working to fix #4798 . 
I almost finished the implementation for MNIST. I tested my notebook on my local laptop and Google Colab, and both of them worked.
Could I ask someone to review this PR?

![fake_samples_0_910](https://user-images.githubusercontent.com/54299088/106352534-5839f600-6327-11eb-8592-968e4df82aa0.png)
(iteration = 1 on google colaboratory)


## Affected Dependencies
no

## How has this been tested?
jupyter lab and google colaboratory

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",5,False,2021-01-29 11:37:26,2021-02-08 02:52:17,2021-02-08 02:51:50
https://github.com/OpenMined/PySyft/pull/5067,[],Autoapprove requests made by root clients #5015,"Autoapprove requests made by root clients #5015## Description
When .get is called with the argument ""request_block=True,"" a ""RequestMessage"" is sent to the remote side, and then a ""RequestAnswerMessage"" is sent repeatedly to check whether the request has been approved. 
There is no remote side when using the VirtualMachine, and it has no service response to these two messages.
In this PR, two services are added, check if the root client makes the request and approve it by returning a RequestStatus.Accepted. 

## Affected Dependencies
When a guest client calls .get(), it can get the variable normally, but if it calls .get(request_block=True), it gets None(see pointer.py:get).

## How has this been tested?
To reproduce this issue:
```
import syft as sy
alice = sy.VirtualMachine(name=""alice"")
alice_client = alice.get_root_client()

alice_client.torch.Tensor([1, 2, 3]).get(
    request_block=True,
    name=""Test""
)
'''
error occurs:
""KeyError: ""The node <UID: xxx> of type <class 'syft.core.node.vm.vm.VirtualMachine'> cannot process messages of type <class 'syft.core.node.domain.service.request_message.RequestMessage'> because there is no service running to process it.<class 'syft.core.node.domain.service.request_message.RequestMessage'>""
""
'''
```

To test if it is fixed:
```
import syft as sy
import torch
alice = sy.VirtualMachine(name=""alice"")
alice_client = alice.get_root_client()

p = alice_client.torch.Tensor([1, 2, 3])
t = p.get(request_block=True, name=""Test"")
assert torch.equal(t, torch.Tensor([1, 2, 3]))

alice_guest = alice.get_client()
p = alice_guest.torch.Tensor([1, 2, 3])
t = p.get(request_block=True, name=""Test"")
assert t is None
```

## Checklist
- [*] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [*] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [*] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [*] My changes are covered by tests",2,False,2021-01-26 08:48:21,2021-02-04 03:03:02,2021-02-04 03:03:01
https://github.com/OpenMined/PySyft/pull/5066,[],[WIP] Test #5032,"[WIP] Test #5032## Description
try to closes #5032 
This PR hopes to solve Issue #5032 .

## Affected Dependencies
No

## How has this been tested?
we can see that Normalize property exists on torchvision.transforms like ToTensor.
Also, I have confirmed that inplace property only exists on object, not the class (transforms.normalize). 
So, I removed the comment for this issue.

I am a beginner at pysyft and git, so please guide me if I am going in the wrong direction. 

![2021-01-26 (5)](https://user-images.githubusercontent.com/54299088/105850691-676a2c80-6025-11eb-9eb2-e2ad07622b32.png)



## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x ] My changes are covered by tests",6,False,2021-01-26 00:24:57,2021-02-09 06:26:15,2021-02-09 06:26:15
https://github.com/OpenMined/PySyft/pull/5064,[],[WIP]  Testing #5033,"[WIP]  Testing #5033## Description
try to closes #5033  
This PR hopes to solve Issue #5033  .

## How has this been tested?
-below is  the tests that you ran to verify the changes.
- and instructions so we can reproduce.
- we can see that Compose.transforms property only exists on the object(cn) not on the class(Compose).
![pysyft_dev #5033](https://user-images.githubusercontent.com/56379013/105653239-4bf60900-5ee1-11eb-8bba-189ff1f20674.png)
in above screenshot we can see that Compose takes list of transforms(i give two transforms above) so `allowlist[""torchvision.transforms.Compose.transforms""] = ""syft.lib.python.List""` must be true but I don't knowwhy test are failigs after removing comment before this command in allowlist.py

also need some help to create a test case from this one and  in which file we have to create a test for these



## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",5,False,2021-01-25 02:15:58,2021-02-09 06:09:37,2021-02-09 06:09:37
https://github.com/OpenMined/PySyft/pull/5061,[],[WIP] Removing xfail annotations ,"[WIP] Removing xfail annotations ## Description
I have found 4 tests with ""xfail"" markers. One of them is test_free_after_iterating().

The test_free_after_iterating function, as I understand is checking whether the iterator is empty(deallocated) after getting exhausted. The support.check_free_after_iterating function seems to be failing the tests. So I implemented a for loop to check for the same. Tell me if I understood the function wrongly.
Fixes #4974 
## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?

`pytest tests/syft/lib/python/dict/dict_test.py -k test_free_after_iterating`

This gives xpassed verdict.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",4,False,2021-01-23 16:21:16,2021-02-05 05:47:47,2021-02-05 05:47:35
https://github.com/OpenMined/PySyft/pull/5059,[],Fix mean and std in Torchvision[Draft],"Fix mean and std in Torchvision[Draft]## Description
Closes #5031 

 This PR hopes to solve Issue #5031 . I have currently written a fix in `src/syft/lib/torchvision/allowlist.py ` to assign `syft.lib.python.Tuple` to mean and std in `torchvision.transforms.Normalize`. Pls do have a look at the code and do provide me with some guidance on whether I am approaching in the right direction or not.. Thanks

## Affected Dependencies
None for now

## How has this been tested?
Currently No tests have been written as I wish to seek guidance for direction in which I am going.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2021-01-22 17:30:19,2021-02-02 08:44:32,2021-02-02 08:44:32
https://github.com/OpenMined/PySyft/pull/5050,[],Try Fix Windows,"Try Fix Windows## Description
Please include a summary of the change, the motivation, and any additional context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2021-01-22 00:03:39,2021-01-22 00:14:21,2021-01-22 00:14:21
https://github.com/OpenMined/PySyft/pull/5021,[],add paper link,add paper linkadd a direct link to the paper as a .pdf file and fix a typo.,1,False,2021-01-17 12:12:01,2021-01-18 10:58:19,2021-01-18 10:58:18
https://github.com/OpenMined/PySyft/pull/5020,"['documentation ', 'status: review needed :raising_hand:', 'status: in progress :star2:', 'sympc']",Add matmul duet tutorial,"Add matmul duet tutorial## Description
Add `matmul` operation in SMPC Duet tutorials

- [x] 1-DS-1-DO
- [x] 1-DS-2-DO

This PR closes https://github.com/OpenMined/SyMPC/issues/43

I have also updated the description of the notebooks and the `supported_libs.txt` requirements.

## Affected Dependencies
-`sympc`

## How has this been tested?
Launch a jupyter notebook with all dependencies installed and follow the instructions of the notebooks

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
<p>MAYBE: The first link is not pointing to the correct address.</p><p>Also, could you remove the ""ipynb"" extension from the second one.</p><p> </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5020/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='1'/><p>NITPICK: Could you fix the link :D</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5020/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='11'/><p>TYPO: subtranc -&gt; substract</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5020/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='24'/><p>NITPICK: Could you remove the extension from the last notebook?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5020/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='1'/><p>Q: I think we can remove those two cells right? (this one and the one below) - since we print the store in the next step</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5020/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='17'/><p>Also this cell</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5020/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='18'/><p>NITPICK: To test ""matmul""</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/5020/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='31'/><p>Done. Thanks!</p><p>Done. Thanks!</p><p>Done. Thanks!</p><p>Done. Thanks!</p><p>Done. Thanks! (in the cell from Matrix multiplication section)</p>",7,True,2021-01-17 01:46:53,2021-01-20 11:52:15,2021-01-20 11:52:15
https://github.com/OpenMined/PySyft/pull/4987,[],Support Syft 0.3.0 with torch 1.4.0,"Support Syft 0.3.0 with torch 1.4.0## Description
Wish to solve #4963 (building Syft 0.3.0 with torch 1.4.0) with this PR. 
## Checklist
- [X] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [X] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,False,2021-01-02 22:06:11,2021-01-06 11:20:31,2021-01-05 05:51:06
https://github.com/OpenMined/PySyft/pull/4985,[],Fix typos in example notebooks and readme,"Fix typos in example notebooks and readme## Description
Fixed some typos in the examples and the contributing guide link in the readme.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",3,True,2021-01-01 19:19:45,2021-01-20 06:53:54,2021-01-20 06:53:54
https://github.com/OpenMined/PySyft/pull/4959,[],Data Science Grid Mock API - schema mockup concepts,"Data Science Grid Mock API - schema mockup concepts## Description
This PR is the result of a discussion in slack that can be followed [here](https://openmined.slack.com/archives/C6AN4BLS2/p1609068420425800). The idea is to bring a set of tools to help the adoption of existing schemas and tackle the problem of combining datasets that use different schemas or compare inference results of models with different schemas.

## Affected Dependencies
Just a discussion mockup, does not apply.

## How has this been tested?
Just a discussion mockup, does not apply.

## Checklist
- [X] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [X] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [X] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [X] My changes are covered by tests",2,False,2020-12-29 12:40:30,2021-02-05 05:07:33,2021-02-05 05:07:33
https://github.com/OpenMined/PySyft/pull/4955,[],Fix the example document more friendly,"Fix the example document more friendly## Description
give more information about the network-url in the example.
when i run the examples, I occurred some connection error. it is more friendly to tell the new use launch a local network.



## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,False,2020-12-28 11:39:25,2021-01-15 00:18:08,2021-01-15 00:18:08
https://github.com/OpenMined/PySyft/pull/4937,[],Fix all warnings in syft_0.3.0 documentation - first partial implementation,"Fix all warnings in syft_0.3.0 documentation - first partial implementation##  Description 
- Partial fix for #4518
- Warnings due to formatting errors in docstring were fixed

## Affected Dependencies
- none

## How has this been tested?
- Fast tests were run: 1911 passed, 19444 deselected, 3 xfailed, 23 warnings
- Same outcome as on forked dev branch
- No additional tests added or required since only changes in comments

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,True,2020-12-22 12:37:48,2021-01-22 06:03:36,2021-01-15 01:30:12
https://github.com/OpenMined/PySyft/pull/4926,[],added location and locations printouts to remote and SMPC-encrypted nn.Module,"added location and locations printouts to remote and SMPC-encrypted nn.Module## Description
This adds a new `__str__()` function to `torch.nn.Module` which adds a location printout to every shared model. In the case of an SMPC-encrypted model it outputs all the different locations. Should serve as a small convenience function.

## Affected Dependencies
None.

## How has this been tested?
- Tested the output of normal tensors, a remote tensor, a SMPC-encrypted tensor, a normal nn.Module, a nn.Module with FixedPrecisionTensor weights, a remote nn.Module and an SMPC-encrypted nn.Module.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,True,2020-12-17 17:52:29,2020-12-18 08:05:04,2020-12-18 08:04:38
https://github.com/OpenMined/PySyft/pull/4905,[],add github benchmark-actions support,"add github benchmark-actions support## Description
this PR fixed #4654 

## Affected Dependencies
pytest-benchmark

## How has this been tested?
tested on local though need review for .yml :)

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2020-12-12 09:12:11,2020-12-13 05:19:26,2020-12-13 05:19:26
https://github.com/OpenMined/PySyft/pull/4882,[],Len and Iter final touch,"Len and Iter final touch## Description
A followup PR that presents the bug on the store.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

",1,True,2020-12-01 13:41:22,2020-12-02 12:05:51,2020-12-02 12:05:50
https://github.com/OpenMined/PySyft/pull/4877,['bug '],Fix issue with logging bool being backwards,"Fix issue with logging bool being backwards## Description
- Fix issue with logging bool being backwards
- Added some comments and included in MNIST to expose to users

## Affected Dependencies
None

## How has this been tested?
Jupyter Notebooks

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2020-12-01 03:21:10,2020-12-02 03:56:20,2020-12-02 03:56:20
https://github.com/OpenMined/PySyft/pull/4868,[],Added the upgradation info : PySyft 0.2.x to 0.3.x,"Added the upgradation info : PySyft 0.2.x to 0.3.x## Description
This PR introduces the upgradation from PySyft 0.2.x to PySyft 0.3.x with reference to the issue #4820",1,False,2020-11-28 15:16:43,2020-12-21 19:35:47,2020-11-30 11:00:12
https://github.com/OpenMined/PySyft/pull/4838,[],Fixed PyPI README by adding the complete URL ,"Fixed PyPI README by adding the complete URL ## Description
fixes #4833",2,True,2020-11-21 05:18:53,2020-11-24 05:54:17,2020-11-24 05:54:17
https://github.com/OpenMined/PySyft/pull/4827,[],Fix StorableObject lossing .data .description .tags after serde,"Fix StorableObject lossing .data .description .tags after serde## Description
Fixes OpenMined#4791

Issue OpenMined#4791 says that in `PySyft/syft_0.3.0 branch`, a `StorableObject` will loss it's `.data`, `.description` and `.tags` attributes if you serialize it into bytes and then deserialize it back. But as what I found, this issue also happens in `PySyft/master` branch, so I direct this PR to master branch. And I found that this issue also exists when you serialize a `StorableObject` to `proto`, not only when to bytes.

What I found is that after serialization, the `.description` and `.tags` information disappear in the bytes or proto. And in the descrialization process, the `.data` information is dropped.

Why does the  `.description` and `.tags` information disappear in the serialization process? The problem is in the `_object2proto` method of `StorableObject`. In this function, at ""Step 4: save the description into proto"", it checks `if hasattr(self.data, ""description"")`. And as `hasattr(self.data, ""description"")` is `False`, no `description` attribute is set to proto. So, what I think it should be checking is `if hasattr(self, ""description"")`. And this is also the thing happens at ""Step 5: save tags into proto if they exist"". So that's the reason why the  `.description` and `.tags` information disappear in the serialization process, and how to fix it.

Then why the `.data` information is dropped in the deserialization process? The problem is in the `get_data_protobuf_schema` method of `StorableObject`. In another function `_proto2object`, this function `get_data_protobuf_schema` is called to get the schematic_type of `StorableObject`. As `get_data_protobuf_schema` returns `None`, the `data` information is dropped. So what I do is I change this `get_data_protobuf_schema`  function to let it return a `StorableObject_PB`. 

So that's how I fix this bug.

But there is one thing I am not sure about. These codes that I found leading to this bug, they seem to be written that way on purpose. Because they do not seem like typos. So I am not sure what consequences changing them.


## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Add a test function in `storable_test.py'
- Assert that the '.id', '.data', '.description' and '.tags' are the same between the initial obj and after serde it.

## Checklist
- [ x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x ] My changes are covered by tests",6,True,2020-11-18 04:18:32,2020-11-26 01:11:58,2020-11-25 14:05:11
https://github.com/OpenMined/PySyft/pull/4801,[],debug pointer.svd (OpenMined#4616),"debug pointer.svd (OpenMined#4616)## Description
Fixes #4616
When execute `pointer_tensor.svd()`, it returns 4 pointers, while what is expected is 3. And the 3rd one points to nothing. That's issue #4616.
I found that the reason is that there is bug in function `def de_register_obj()`. This function has a line `if hasattr(obj, ""id"")`, which is useless when `obj` is of type TorchTensor. Why it is useless is because a TorchTensor object always has `id` property, as you can see in it's definition. So I change the condition to be checking `has_attr = hasattr(obj, ""_id"") if isinstance(obj, torch.Tensor) else hasattr(obj, ""id"")`.
And why this bug is the key to #4616? Because `hasattr(obj,""id"")` will run `obj.id()`. And in `obj.id()`, an `id` will be created for obj. And that `id` created is the 3rd poiner, which points to nothing.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Add a test unit `test_remote_svd`

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2020-11-10 07:58:00,2020-11-16 09:09:45,2020-11-16 09:09:45
https://github.com/OpenMined/PySyft/pull/4781,[],Fix the h5py version,"Fix the h5py version## Description
It seems that 5 days ago a [new version of h5py](https://github.com/h5py/h5py/releases) was released.
The test was failing at the following line.
```
if '/' in name:
```
with the error:
```
TypeError: a bytes-like object is required, not 'str'
```

The problem was that ""name"" is a bytes values.
This simply pins the version to the previous one.

## Affected Dependencies
H5Py -  Pythonic interface to the HDF5 binary data format

## How has this been tested?
- The failing tutorial test should pass

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2020-11-04 17:38:35,2020-11-05 07:45:57,2020-11-05 07:45:57
https://github.com/OpenMined/PySyft/pull/4773,[],Add .ndim and .T to Tensor (OpenMined#4617),"Add .ndim and .T to Tensor (OpenMined#4617)## Description
Fixes #4617 
Add .ndim and .T functionality to FixPrecisionTensor and PointerTensor.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- test units

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
Test the value for transposeHere the same, test ""y"" not the shape of ydone in a new commit, by testing `x.fix_prec().T==x.T.fix_prec()`done in a new commit, by testing `bob_xT.get()==x.T` and `bob_x.get()==x`",2,True,2020-11-04 07:07:49,2020-11-08 22:26:51,2020-11-08 22:26:51
https://github.com/OpenMined/PySyft/pull/4752,[],fix comparison between FPT and AST,"fix comparison between FPT and AST## Description
this PR fixes #4705 
comparison function of additive_share.py is modified.

## Affected Dependencies
none

## How has this been tested?
- test cases are added

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Q: Could you move those to another test function? (because this function would be called 2 times - one with protocol = ""fss"" and once with protocol ""snn""). You can also parameterize that function with only ""fss""Done",1,True,2020-11-01 16:14:26,2020-11-05 13:00:01,2020-11-05 12:59:50
https://github.com/OpenMined/PySyft/pull/4714,[],Fix Flake8 C408 Error,"Fix Flake8 C408 Error## Description
C408 causes the test to fail in the current CI test. I solved this problem.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2020-10-26 12:01:29,2020-10-26 13:48:42,2020-10-26 13:31:40
https://github.com/OpenMined/PySyft/pull/4700,[],"Primitive support: Int, Float, List, Bool","Primitive support: Int, Float, List, Bool## Description

Tons of changes, finished Int, Float, List, Bool. Pushing now to unblock other people.

To go: Dict, String, Complex. Currently commented out the String changes, as they are failling and in unittest format.
",1,True,2020-10-23 13:45:29,2020-10-23 19:46:04,2020-10-23 19:46:04
https://github.com/OpenMined/PySyft/pull/4697,[],"Fix for RNN training on websockets, added instructions for Windows workaround installation","Fix for RNN training on websockets, added instructions for Windows workaround installation## Description

- Fixed Recurrent Neural Network training on local and remote websockets in response to issue #4656 
- Currently, it is impossible to install PySyft in Windows because PyTorch 1.4.0 is not available. Added a workaround to install PySyft on Windows machines in INSTALLATION.md.


## Affected Dependencies
No additional dependencies affected

## How has this been tested?
- Working locally with websockets
- Test cases passing


## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",5,True,2020-10-22 08:49:56,2020-11-02 15:46:12,2020-11-02 15:46:12
https://github.com/OpenMined/PySyft/pull/4655,[],changed all asset to if/raise to prevent disable of assert during PYTHONOPTIMISE env,"changed all asset to if/raise to prevent disable of assert during PYTHONOPTIMISE env## Description
Changed all assert blocks to if/raise block. This is important as assert checks can be globally disabled with the -O and -OO flags, as well as the PYTHONOPTIMIZE env var. This PR is against an open issue, [Don't use `assert` for non-debugging data validation](https://github.com/OpenMined/PySyft/issues/3480) 

## Affected Dependencies
For now, commented out the assert blocks and wherever error statement wasn't present, added an error statement based on understanding I could make of it 

## How has this been tested?
- ran the standard `python setup.py test`, which was a success
Q: Could you thorw dirrectly?
```raise RuntimeError(f""Number of results found {len(results}}. Expected 1"")Q: Also hereQ: Could we have 2 ```if``` statements here? one for the input and one for the weightQ: Could you specify here given inputs are not supported for ```nb_channels_in``` - the same for the ones bellowQ: Could you specify here that ```weight```, not ```input```Q: Also here, specify ```nb_channels_in``` (not the value, but the name of the variable)Q: Also hereQ: given tensor shapeQ: Could you rewrite this with an
```if not (padding == 0 and ceil_mode...) <--- it shows that we support only one settingQ: Why not use f-string?sure",5,True,2020-10-16 04:09:42,2020-10-20 13:14:41,2020-10-20 13:14:41
https://github.com/OpenMined/PySyft/pull/4653,[],Add remote share for RST,"Add remote share for RST## Description
Add the possiblity to serialize/deserialize the RST

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Added a test to validate the changes

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by testsTODO: After [this](https://github.com/OpenMined/syft-proto/pull/1450) gets merged replace itdon't forget the tag & description params (like for AST)I wonder if all shares should not always have the same `garbage_collect_data` ?
If so, you can put the assert all equal test on all the shares attributes```suggestion
        Gradient makes no sense for Replicated Shared Tensor, so we make it clear
        that if someone query .grad on a Replicated Shared Tensor it doesn't error
``````suggestion
        """"""Calling backward on Replicated Shared Tensor doesn't make sense, but sometimes a call
        can be propagated downward the chain to an RST (for example in create_grad_objects), so
```Alternatively you can maybe import AdditiveSharingTensor and just copy the method?
`grad = AdditiveSharingTensor.grad` ?```suggestion
    def simplify(worker: AbstractWorker, tensor: ""ReplicatedSharingTensor"") -> tuple:
        """"""
        This function takes the attributes of a ReplicatedSharingTensor and saves them in a tuple
        Args:
            tensor (ReplicatedSharingTensor): a ReplicatedSharingTensor
        Returns:
            tuple: a tuple holding the unique attributes of the replicated shared tensor
``````suggestion
    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> ""ReplicatedSharingTensor"":
        """"""
            This function reconstructs a ReplicatedSharingTensor given it's attributes in
        form of a tuple.
        Args:
            worker: the worker doing the deserialization
            tensor_tuple: a tuple holding the attributes of the ReplicatedSharingTensor
        Returns:
            ReplicatedSharingTensor: a ReplicatedSharingTensor
```:+1:Hmmm...I am inclined to keep them separated such that each RST is not coupled with the AST (even if the RST is an AST behind)I can put for all the shares to have the same garbage collect data :DDonemay be --> doesn't throw an error also the line ""returns grad and can't be set"" doesn't make much sense Copy pasted it from Additive Sharing Tensor. Fixed it now :D (also in AST)```suggestion
        not throw an error
```Why is copy() needed, I thought copy() wasn't an inplace op?Docstring should only be at the begining of a functionwhy do you set `gc` and not `.garbage_collect_data` ?```suggestion
        not throw an error
```Add `worker`Can you also test call .share before .send if it works? (otherwise just leave it as a TODO)Hmm...isn't it tested at line 275? (I send a pointer and call share on it?)At one point when I send the same ```share[i]``` to 2 workers and it messes up with the garbage collection :\ (from my understanding this is because only one of the pointers should control the garbage collection - but sending it to two workers it means that one has the ```gc``` flag ```on``` and the one ```off```)

With the ```copy``` I make sure there are no changes in the process generated by the ```send``` methodThis is because ```share``` is a wrapper.
And in the ```native``` we have:
```
@gc.setter
def gc(self, flag):
     self.garbage_collection = flag
...
@garbage_collection.setter
def garbage_collection(self, flag):
      if not self.has_child():
          if hasattr(self, ""ptr"") and self.ptr is not None:
              self.child = self.ptr
      self.child.garbage_collect_data = flag
```
Basically we would do. ```share.child.garbage_collect_data = flag```Removed :DAh ok that's nice! Thanks :) I was saying call .share() and then call .send()Ok this is weird indeed!",2,True,2020-10-14 23:12:53,2020-11-11 07:03:09,2020-11-11 07:03:09
https://github.com/OpenMined/PySyft/pull/4640,[],Updated simplify and detail for dflclient,"Updated simplify and detail for dflclient## Description
Corrected DataCentricFLClient serialization. the method, the previous method has bugs and didn't serialise it correctly.
More specifically running the below code raises an error and connection breaks but the method in this PR will solves that
```
x = torch.tensor([1,2,3,4,5]) 
x = x.send(bob) 
x = x.share(bob,alice)
```
This comes after testing and discussion from @IonesioJunior and me.

## Affected Dependencies
List any dependencies that are required for this change.


## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2020-10-12 15:32:05,2020-10-13 03:03:35,2020-10-13 03:03:35
https://github.com/OpenMined/PySyft/pull/4636,[],Added value of epochs in the tutorial,"Added value of epochs in the tutorial## Description
While running the tutorial, an error I received was that the value of epochs was missing. Hence I added the value of epochs based on the sample output.

## Affected Dependencies
None

## How has this been tested?
- The final output of the code is as given in the earlier sample output.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- x ] My changes are covered by tests",2,False,2020-10-10 13:19:39,2020-10-10 13:21:56,2020-10-10 13:21:35
https://github.com/OpenMined/PySyft/pull/4635,[],Add Notebook comparing PySyft SMPC protocols: FSS and secureNN,"Add Notebook comparing PySyft SMPC protocols: FSS and secureNN## Description
This notebook will give an overview and a very quick explanation over/of the different SMPC protocols that are currently implemented in PySyft. It'll also elaborate on **what kind of Machine Learning you can conduct in an encrypted fashion using them**, along a comparison of their resulting performance to each other and to the non-encrypted scenario. The protocol explanations should mainly serve to give a high-level understanding of what the main **crypto-slang** stands for, how the terms relate to each other and what ressources serve as good starting points to dig deeper! 

Closes #4063

* **Current status:** After familiarizing with the different protocols on a high level I've written the explanations which aim at giving a quick overview of the protocols and should give a quick understanding of what different models, optims, losses can be used with the different protocols. 
* **Next steps:** Incorporate feedback from @LaRiffle @gmuraru @somiljain7 if any. Find suitable examples and implement them using PySyft (incl. additional explanations). Use these + explanations in papers to explain/showcase the performance of the protocols for different computations/applications. 

## Affected Dependencies
None.

## How has this been tested?
- Follow instructions in notebook and run cells. 

## Checklist 
*Still WIP.*
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
<p>TYPO: the first category -&gt; the first category.</p><p>At point 3, I think in our scenario we do the reconstruct only on a ""worker"" (but all workers can do that reconstruct)</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='3'/><p>TYPO: randomely -&gt; randomly</p><p>Maybe we can also add that the offline phase exists to speed up the online phase (you actually pointed this out).</p><p>But, we can also generate Beaver Triples during the online phase - but this would impact the <code>online</code>  performance</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='4'/><p>NITPICK: Sounds a little off ""the server that serves"" - maybe ""the party that serves""?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='5'/><p>TYPO: as a circuit consisting of different logical gates (e.g. AND, XOR, etc.). &lt;-- dot at the end :D</p><p>withouut -&gt; without</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='6'/><p>Q: I think it might be better to provide the link from arxiv</p><p>TYPO: a dot after (state-of-the-art ML protocol before SecureNN).</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='9'/><p><em> circuit-gabling-based SMPC -&gt;  circuit-garbling-based SMPC</em></p><p><em>consider adding a link to: </em></p><p><em>GC wiki: </em>https://wiki.mpcalliance.org/garbled_circuit.html </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='6'/><p>1) typo in efficent</p><p> </p><p>2) If the protocol is for : 3 parties with one dishonest party in the malicious adversary model - how is this protocol be made to work with 2 parties + 1 crypto-worker? </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='9'/><p>somes typos: randomely, appraoches, calcultaion, <em>complexcity</em> : I often copy paste my text in a google doc to have spell checking and fight against simple typos ;) </p><p> </p><p>For multiplication, there is a communication step where masked values are revealed</p><p>assumingly securenn is not in the malicious model right?</p><p>""These are being trusted of not collaborating with each other"" not necessarily! The exact term is colluding.</p><p>You can say that it is resistant to some level of collusion (we often consider that we have a <em>honest majority</em>), and can be<u> seen as</u> a <strong>trusted execution environment</strong> as long as thee isn't too much collusion. </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='0'/><p>good explanation!</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='5'/><p>there is also a version with 3 parties + 1 crypto provider I think in SecureNN</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='9'/><p>For mat mul, it could be good to add that the beaver triples extend to matrices</p><p> </p><p>the ""Low Level"" components of SecureNN are not necessarily useful unless you explain precisely how they use each other to implement ReLU for example</p><p> </p><p>You mention that division is possible, it is indeed with SecureNN but it is incredibly slow -  you can mention it</p><p> </p><p> </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='10'/><p>Might be interesting to point out that indeed FSS doesn't require lots of communication rounds. But is computationally more intensive than SecureNN (because we call a PRG <code>G</code> many times)</p><p> </p><p>For Dopout, i guess we could implement it, just no one took the time to do so :) </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='14'/><p>Erratum: we support dropout for secureNN + FSS</p><p>Alright typos fixed, I'll remember to paste everything into GDocs from now on :D </p><p>I've also added the two remarks about that only one worker does the constructing in our case and about the additional communication step. </p><p>Ok interesting, thanks! I'll change that.</p><p>Ok thanks, I'll make that clearer!</p><p>:D Aight!</p><p>Thanks! </p><p>Thanks, very helpful website!</p><p>I didn't find the paper on arxiv, but I changed the link to the link mentioned in the code. </p><p>Well the secureNN requires an honest majority for all parties<em> that store a share</em> - that doesn't include the crypto-worker. So 2 parties + 1 crypto-worker where one party is dishonest isn't considered a honest majority which means secureNN can't safely be used as far as I understand. </p><p>They describe the system as a 3PC-protocol, and the algorithms always consider two parties holding the secret shares (referred to as P0, and P1) and one party generating the random primitives (P2).</p><p> </p><p>Do you have some resource pointing to the 4PC-version which I should include or should I simply state it? </p><p> </p><p>Alright, thanks! </p><p>Do you know why specifically the division is slow? </p><p> </p><p>Also the idea about the ""Low Level"" components was to give more advanced users the possibility to themselves assess whether special computations are possible with the implemented low level algorithms of the protocol. Detailed explanations about how the algorithms work might scare of the less experienced users, how about I could include the graph showing which low level components work together and are based on each other (Fig.3 in the paper). What do you think? </p><p>Alright perfect, I'll add that. </p><p>Ahh - sorry. It was here: https://eprint.iacr.org/2017/396.pdf</p>Maybe you can just mention that the 4PC version exist without detailing yeah, maybe that's not the most important par of SecureNN (and we don't implement it!)<p>&gt; Do you know why specifically the division is slow?</p><p> </p><p>Yeah because it you check the implem, you can see that it makes roughly n call to the comparison, and because the comparison is already a bit expensive, it gets terrible^^</p><p> </p><p>Yeah Fig 3 is helpful I agree! But even for advanced users, we don't expect them to reuse parts of the low level components to build new computations, they should instead rely on the the Relu, Div, and MaxPool I would say</p>Oh, interesting! Thanks for the explanation :) Sounds good, I'll do that! Ahh ok because the division is implemented as subtraction and then bitwise comparison and shifting, right?

Ok so I think I'll just shift the low-level parts with the figures to an extra section at the end for users who are more interested and want to have a starting point to understand the implementations and the papers. üëç Don't add this line, the tutorials shouldn't be ignored :) <p>similair -&gt; similar</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='1'/><p>(Only inference, we use pre-trained models) -&gt; in the intro you mention doing training</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='2'/><blockquote>Does the compression also needs to be turned off, if so, why? (as in Th√©o's tuto)</blockquote><p>It makes the code faster, it's not compulsory</p><p> </p><blockquote>if resnet is in train() mode it can't run an encrypted forward pass</blockquote><p>Training resnet is not supported so far (and too expensive)</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='7'/><p>Do you really want to feature  remote encryption? Maybe just mention it as a side note as it makes just more code.</p><p>Also, if you want to use encryption_kwargs, use it on all cells also the previous one for example.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='9'/><p>You can add a line to explain what all those are: (Wrapper)&gt;AutogradTensor&gt;FixedPrecisionTensor&gt;[AdditiveSharingTensor]  </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='11'/><p>you coud reuse your encryption kwargs in <code>secret_share</code>  </p><p> </p><p>CrossEntropyLoss is a function or  a class? Maybe add it to pysyft in a different PR :) </p><p> </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='15'/><p>""<em>This reduces communication complexity significantly."" -&gt; can be removed</em></p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='17'/><p>we don't see the loss reducing, why is so?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='27'/><blockquote><strong>Encryption with n parties possible (n &gt;= 2)</strong></blockquote><p>It mostly 2 party I'd say</p><p> </p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='29'/><p>why is it commented?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='31'/><p>SSN + CELoss + Avg Pool is missing :(</p><p> </p><p>Test Accuracy is the same of SNN with MSE and CE ? I would expect the Accuracy of SNN + CE to match the one of FSS + CE</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='36'/><p>You can remove the TODO: just the protocol calls a PRG many times :) </p><p> </p><p>Mention that this table is extracted from the AriaNN paper maybe</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='39'/>Ahh yes, that was just for convenience for myself :D I wanted to delete that anyways as soon I'm done with the tutorial. üëç <p>Alright, thanks for the explanations! </p><p>I 'll try to make it more compact. I think it can be important to show the specific difference between these two encryptions because following the existing tutorials one might be confused or even not really be aware of the two different ways for encryption. </p><p>I just realized that using encryption_kwargs for both scenarios isn't that compact, because <code>.share()</code> expects no ""workers"" argument but rather just the list of workers, whereas <code>.encrypt()</code> expects a list of workers under the keyword ""workers"". I don't know if that's supposed to be like that, would probably make sense to standardize that, what do you think? +1</p><p>Done. </p><p>I'll add a PR for nn.MSELoss() and nn.CrossEntropyLoss() :+1 </p><p> </p><p>Do you mean just writing <code>secret_sharing(**meta)</code>  given meta wouldn't contain the precision_fractional? </p><p>Done</p><p>Changed it to ""<strong>Encryption with n parties possible (mainly n==2)"" </strong>because theoretically the protocol would allow it right? </p><p>So that people can decide whether to use AvgPool or MaxPool. In this case the last test I ran was with AvgPool, so mainly that's the reason. I can also run the last experiment with MaxPool. :+1</p><p>Yes true I found it odd too. I'll run a check again :+1</p><p>Done. </p><p>yes this the reason for the low acc in the second last discussion you started I guess. Re-running the training, but still I don't understand why sometimes things like that occur, this is why I maybe suggested an issue with GarbageCollector in my notebook (because usually I don't get these kind of error if I restart the notebook and directly train only one model)</p><p>Ok I ran it again and the loss was 715 and 715 for both epochs, still the test-acc. was 0.14 which should be correct, so I guess the optim together with the quite small batch size just leads to bad first steps. </p><blockquote>SSN + CELoss + Avg Pool is missing </blockquote><p>nearly there :) </p><blockquote>I would expect the Accuracy of SNN + CE to match the one of FSS + CE</blockquote><p>Re-training yielded 0.14 so all good. :+1</p><p>Concerning showcasing the remote encryption an alternative would also be that you include the local encryption part in your tutorial Part 11 bis - given of course you also think that this direct comparison would make sense and doesn't yet exist in a notebook. :+1</p><p>his means that usually it is considered that there exists a <em>honest majority</em> of servers which are trusted of not collaborating with each other</p><p>^^ should we also say here that they do not divert from what they should do in the protocol?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='0'/><p>TYPO: I think here there is an extra ""*""</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='1'/><p>Add a dot (.) at the end of "" about <a href=""https://blog.openmined.org/federated-learning-types/"" target=""_blank"">different kinds of Federated Learning</a>)""</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='6'/><p>(the first category) &lt;-- also a dot here</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='18'/><p>(i.e. only two servers are involved)  &lt;--- also here a dot at the end?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='21'/><p>Q: I am using ReviewNB but the images are not appearing? (Should the link be updated or it would be ok after merge?)</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='38'/><p>Q: Also for this iamge</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='39'/><p>Q: With the PySyft Projects should we better point here ""https://github.com/OpenMined""</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4635/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='40'/><p>Will do that :+1</p><p>Done.</p><p>Done.</p><p>Also done :D </p><p>Yes forgot to update!</p><p>Yes I agree, changed that. </p>",6,True,2020-10-10 12:28:17,2020-11-11 12:00:41,2020-11-11 12:00:41
https://github.com/OpenMined/PySyft/pull/4612,[],Bandit fix,"Bandit fix## Description
Fixes failing security check on 0.0.0.0 interface binding in demo network.py

## Affected Dependencies
None

## How has this been tested?
Locally

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2020-09-27 15:17:07,2020-09-27 16:52:17,2020-09-27 16:52:14
https://github.com/OpenMined/PySyft/pull/4600,[],Make MNIST pass again!,"Make MNIST pass again!Fixed a few bugs in the MNIST demo regarding optimizer and scheduler. Continued the awesome work made by Jay.

## Description
Please include a summary of the change, the motivation, and any additionl context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,False,2020-09-25 18:16:29,2020-09-25 19:45:06,2020-09-25 19:45:06
https://github.com/OpenMined/PySyft/pull/4599,[],Optim scheduler fix,"Optim scheduler fix## Description
Making optim and scheduler work in the MNIST demo.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2020-09-25 16:38:28,2020-09-25 16:59:26,2020-09-25 16:59:26
https://github.com/OpenMined/PySyft/pull/4550,"['bug ', 'priority: 3 - medium :unamused:', 'severity: 2 - high :cold_sweat:', 'status: completed :heavy_check_mark:']",update compatibility with pygrid,"update compatibility with pygrid## Description
Add request id in the model centric notebooks

## Affected Dependencies
model centric fl worker
## How has this been tested?
- unit tests
- running the notebooks

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",5,False,2020-09-11 21:13:27,2020-09-13 09:20:47,2020-09-13 09:20:47
https://github.com/OpenMined/PySyft/pull/4539,[],Add a new tutorial,"Add a new tutorial## Description
This is a new tutorial and provides appropriate changes to make it run

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
NITPICK: I know it should not matter in computational speed, but could we use ```{...}``` for ```set```MAYBE: Keep another variable, ```protocol_lower``` and do only once ```protocol_lower = protocol.lower()```MAYBE: We have this in ```precision.py```
```
def reciprocal(self, method=""NR"", nr_iters=10):

Args:
             method:
             'NR' : `Newton-Raphson`_ method computes the reciprocal using iterations
                     of :math:`x_{i+1} = (2x_i - self * x_i^2)` and uses
                     :math:`3*exp(-(x-.5)) + 0.003` as an initial guess by default
         
             nr_iters:
                 Number of iterations for `Newton-Raphson`
         Returns:
             Reciprocal of `self`
         """"""
 
         if method.lower() == ""nr"":
             new_self = self.modulus()
             result = 3 * (0.5 - new_self).exp() + 0.003
             for i in range(nr_iters):
                 result = 2 * result - result * result * new_self
             return result * self.signum()
```

Are the results better/computed faster if you use this?Interesting, I should compare!
But the key the fast computtaion here is caching the result, for batchnorm at inference that's always the inverse you're trying to do :) <ul><li>Where a shared input [[x]] is applied on a public function --&gt; I think it should be the other way around</li><li>alpha is called a random mask (when I hear mask I always tend to consider it like an ""index-value"" selector)? - maybe a random number?</li><li>x = y + 1 shouldn't continue on the previous line?</li><li>[[f_alpha]]_j (maybe specify that j is in {0, 1}) or explicitly specify [[f_alpha]]_0 and [[f_alpha]]_1 (also, thinking of it, those function shares, can be owned by some other workers, right?)</li><li>Maybe: moving the ""function shares [[f_alpha]] is also shared between Alice and Bob"" where you give the example</li></ul>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='1'/><p>Q: Should we add a comment why we do <code></code><code>torch.set_num_threads(1)</code><code></code></p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='3'/><p>TYPO: Wan't --? Do you want? or simply Want?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='7'/><p>TYPO: here --&gt; from here</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='10'/><p>TYPO: consecuting -&gt; consecutive?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='11'/><p>Q: This seems counter-intuitive? right?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='14'/><p>TYPO: the data and the model would already be located on the workers</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='15'/><p>Q: Maybe it is worth to show how encrypted_data and encrypted_model look?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='21'/><p>Q: Do you want to leave the path from your computer?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='23'/><p>TYPO: I might be wrong. one by tab -&gt; one per tab</p><p>There is also a ModelCentricFLClient (if you change the model_owner with that...it might still work?) &lt;-- I think it gives a better representation on what client has what data</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='27'/><p>I think we should remove the errors</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='1'/><p>I think we should remove the errors</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/4539/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='1'/>Sure!<p>Thx!</p><p>Thx!</p><p>thx!</p><p>Ok I will expand a bit on it</p><p>Thx!</p><p>I didn't want to explain in depth fixed precision and secret sharing for people who just want to ""encrypt"" (and don't want to read section 1), so I didn't want to both by printing the stuff</p><p>Lol yeah that's a good point! Not sure if I can remove the warning though</p><p>Isn't the ModelCentricFLClient really different fomr the DataCentric? It used to be the StaticFL worker right?</p><p>I sh*t this file should go away</p><p>I sh*t this file should go away</p><p>OK I 've tried to improve!</p><p> </p><ul><li>Where a shared input [[x]] is applied on a public function --&gt; I think it should be the other way around. --&gt; I wouldn keep this for secureNN because they both to the same ""public"" protocol to evaluate the function, and have a secret shared input</li></ul><p> </p>Hmmm..I am not sure @cereallarceny might know more :DHmm...you can delete the message (manually) or what I did for Crypten tutorials was to simply edit the path and add ""intentionally_changed_path"" (or something like this :D)Might be worth checking the following:
* if changing the initial guess gives better results? (like the difference between the logits would be smaller)
* if removing the ```if...else``` changes the speed -- sometimes computers do this ```branch prediction``` thing.
CPUs are using a pipeline for fetching data/fetching instructions/etc and they might run code before it needed to be run (predict that you might take the ""if"" or ""else"" branch). If the prediction is correct you get ""full speed"" ahead, but the problem comes in when that prediction is bad, in which case they need to evict all the results from the pipeline (and if this is the case) it might introduce some latency.

I do not know all the ""internals"" of python to know what it does with the ```if....else```, but it might be worth a shot removing the ```if...else``` and check if it speeds things up

**PS**: Sorry for the long post OK I'll do it!Integrated :) 
I'll keep the if else statement for the moment because this function isn't called so many times
I could improve the initiatal guess indeed for my method, i'll investigate this separateely I thinkYes, ModelCentricFLClient is related with static FLThanks! :)",4,True,2020-09-07 20:57:25,2020-09-11 16:37:58,2020-09-11 16:37:54
https://github.com/OpenMined/PySyft/pull/4538,[],Fixing broken link to tutorial 4,Fixing broken link to tutorial 4,1,False,2020-09-07 15:37:21,2020-09-11 16:20:19,2020-09-11 16:20:19
https://github.com/OpenMined/PySyft/pull/4522,[],Added support for FixedPrecisionTensor chain in ReplicatedSharingTensor (RST) ,"Added support for FixedPrecisionTensor chain in ReplicatedSharingTensor (RST) ## Description

> Added support for FixedPrecisionTensor chain in ReplicatedSharingTensor (RST) 

> Modified add and sub of FPT to support FPT chain in RST

> Modified falcon files to support (Wrapper)> [ReplicatedSharingTensor]

this PR closed #3931 

## Affected Dependencies
None

## How has this been tested?
Test cases are added for the required update

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Q: What happens here if you don't select the child? (it should be a simple Wrapper, right?)Q: @abogaziah @LaRiffle what do you think about changing in the AST the name from ```field``` to ```ring_size```?the xor function calls the __eq__ of native, which as far as i know calls the overload 
```
@wraps(getattr(tensor_type, method_name))
    def overloaded_native_method()
```
which removes the wrapper and tried to use the eq operation on RST (which has not been implemented yet) 
which ultimately throws up the error 
`AttributeError: 'ReplicatedSharingTensor' object has no attribute 'eq' `Q: I think we can remove those now :D, Muhammed made some changes to fix the multiplicationWhat is happening here...I think..is that when ```other``` get ```wrapped``` ```isinstance(other, LongTensor)``` returns true and we do the ```eq```. I will think of a way to avoid this and will ping you on Slack :DQ: If you leave it ```c_L``` what happens?okay :)Done!Q: Can you remove this?You should be able to pass ```c_2```, right?
This applies to all instances where we use ```.child```Q: Also it conv2d might also need the same type of logic (with wrapper check)done, not sure how did I left it there working on itQ: Could you introduce a test for this?Q: It doesn't work with c_2? (because now we should have the logic to check if is a wrapper or not)Q: Also, could you add a test for thisQ: Also for thisMAYBE: Use ```torch.allclose```. Also for the othersDoneWe can leave this as it is now and we will have at one point a big round of refactoring the Tensors and then we would discuss more regarding the APIokdonedonedonedonedoneMAYBE: Please remove the prints :D",4,True,2020-09-02 14:43:54,2020-12-12 08:03:01,2020-10-08 08:09:00
https://github.com/OpenMined/PySyft/pull/4501,[],[WIP] Add support for FixedPrecisionTensor chain [Replicated Sharing Tensor (RST)],"[WIP] Add support for FixedPrecisionTensor chain [Replicated Sharing Tensor (RST)]## Description
Added support for the FPT chain in RST and updated add and sub to support these operations in FPT chain. 

On completion this PR closes #3931 

## Affected Dependencies
None

## How has this been tested?
for [Replicated Sharing Tensor (RST)] FixedPrecisionTensor chain 
> Test cases are added for .share while using precision
> Test cases are added for add and sub  

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
this condition is not reachable, you have already checked it previously.how is that useful?fixed pointed arithmetic is not test thought it's the purpose of that issue, numbers after decimal point should be addedwhat is that?how is that useful?this is tight coupling which should be avoided, but why would you swap them anyway?why would you mul -1?this was added temporarily as few of the tests are written using reconstruct(), which fails as it gets called on native.py, in future, it will be good to go with .get() instead of reconstruct()Sorry, fixed in latest commitits been added so that the  _hook_method_args of overload_method can know Put back SyftTensor on the tensors found in the response. if this was not used the output of add or sub would have (Wrapper)>[ReplicatedSharingTensor] instead of (Wrapper)>FixedPrecisionTensor>[RaplicatedSharingTensor] after the operation.a.reconstruct() <-- this is a wrapper
and in native we do not have the reconstruct method
so it fails, thus as I mentioned earlier we should use .get() universally in future. because putting ""-"" was giving an error (i can't remember the error, for now, I'll tell you soon though.the reason can be found in the above code for AST, the swap has been done for the same reason. 
now about I disagree, you ""get"" a remote tensor but you ""reconstruct"" a shared tensor, calling everything get is the easy way, not the correct one, 
can you explain further? give an example of a test case that fails?this is not the right place, it breaks the level of abstraction torch hook is a lower-level abstraction so RST should not know it exists.![Screenshot 2020-08-30 at 12 17 27 AM](https://user-images.githubusercontent.com/28955148/91644057-31188900-ea56-11ea-854d-ec4c36297bcb.png)

if that is not included  this error will prompt up with ""reconstruct""fixed![Screenshot 2020-08-30 at 12 31 19 AM](https://user-images.githubusercontent.com/28955148/91644288-23640300-ea58-11ea-8ffd-99d1f6e5b2b8.png)
here is the error!@gmuraru can you help me this issue?Yep - this is needed because when we add the wrapper we check what ```custom tensors``` exists - if we do not have this here it would not know how to create the ```(Wrapper)>RST``` - this is added in all our custom tensors that need to be wrappedA better approach to this (for all syft framework) it should be to add something ""invasive"" like:
```
1. get all classes that inherit from AbstractTensor
2. call this ```default_register_tensor``` on all of them
```
But I think another issue might be in order to do this -- doing one now.Hmm...thinking of this. Could we add a reconstruct method in ""native"" and have a check -- we can do the reconstruct only if we have ```AdditiveSharingTensor``` or ```ReplicatedSharingTensor```fixed in the latest commit! fixed in latest commit!we are doing this in other issue, right? @gmuraru @abogaziah I think you can remove the else here - if it enters the throwdone :)",1,False,2020-08-29 15:58:47,2020-09-02 13:29:05,2020-09-02 13:14:00
https://github.com/OpenMined/PySyft/pull/4484,[],Falcon module structure & design ,"Falcon module structure & design ## Description
In this PR we initiate the file structure of the classes the Falcon protocol will use, establish their interfaces, and how they will communicate.
 
I have made three new classes:
1- Flacon tensor: extends ReplicatedSharingTensor to implement the tensor operations that are specific to Falcon protocol and not related to the replicated secret sharing scheme
2- Falcon: that's actually a module, not a class, it will overload the torch.nn.Functional module, hence it will reimplement its functions like conv2d, maxpool, batchnorm, etc..
3- FalconHelper: will include subroutines and helper functions of Falcon, we made it to hide details to make levels of abstractions .since Falcon isn't a class, we can't do that in the Falcon module and that's the job of this class 

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
I thought more on this and having a ```FalconTensor``` (having the suffix tensor on it) it means we can apply all operations from torch to this, but ```xor``` is not a method from a tensor.

My idea is to have the ```xor``` functionality in the ```falcon.py``` (receives 2 RST and applies the xor).

**LE**: Also, when we are wrapping the tensors we will have
```Wrapper>FPT>FalconTensor``` and applying xor to this it would not work, since ```xor``` should be an operation that also exists in the ```wrapper``` (and the wrapper kind of mimics the tensor for torch) - so I think we should not have this FalconTensor class, but stick with the base operations/methods from torch and the other ones to be written under in ```falcon.py```that's an excellent point!NITPICK: One space at the end - but I can fix that in my PRQ: Why remove those?NITPICK: We can remove the ```.```I think it won't work, you can either import an installed module or specify the directoryI have no idea :D I added them back",2,True,2020-08-26 11:49:53,2020-08-30 12:35:51,2020-08-30 12:35:51
https://github.com/OpenMined/PySyft/pull/4476,[],ImportError due to Pillow version in torchvision --problem solved,"ImportError due to Pillow version in torchvision --problem solved## Description
I was getting the following error due to the version of Pillow in torchvision: 

> ImportError: cannot import name 'StringType'

from the following line
`from torchvision import datasets, transforms`

Adding the following cell at top of the notebook solved this issue:
`pip install 'syft[udacity]'
pip install ""pillow>7""`

## How has this been tested?
Only small change is made,no test required.",3,False,2020-08-25 08:31:50,2020-09-05 17:23:26,2020-09-05 17:23:26
https://github.com/OpenMined/PySyft/pull/4474,[],Falcon - Select share + evaluating,"Falcon - Select share + evaluating## Description
This PR implements the share selection from the Falcon paper and the **evaluation** part.
Fixes the first point and the last from from #3852

## How has this been tested?
- Added test to validate it works

## Checklist
- [X] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [X] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [X] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [X] My changes are covered by tests
@abogaziah changed the name to field to be consistent with the ```AdditiveSharingTensor``` + it seems also in CrypTen they use the same name attributetypo: x_share or *y_share*I couldn't find the secret sharing scheme you are using (can you share a link?), but if we want to be consistent with the protocol it should be  
1) replicated secret sharing , same as for `b` 
2) it should be maliciously secure secret sharingthis function is returning two different sharing which is not indicated from the name of the function and also makes it less general. why not return a single sharing and call the function twice? as a general comment: The sharing used in ReplicatedSharingTensor (https://github.com/OpenMined/PySyft/pull/3856/files) is for semi-honest adversary and falcon is secure against malicious adversary...(Optional): When using annotations, it is cleaner to use `Tuple` from typing instead of the tuple syntax from python.I don't think that you need to share a 1 for that. a much more efficient way will be to do (see Falcon paper, Linear operations): 
party1 share =  1-2[beta1]
party2 share =  -2[beta2]
party3 share =  -2[beta3]
(Optional): Ret type annotation.(Optional): Ret type annotations(Optional): Ret type annotationsI do not agree with that, field stands for the whole arithmetic field ]0, Z [, which makes it an incorrect and misleading name for the variable that represents Z only. also, the paper uses ""ring size"" so I think we should stick to itDoneYep - currently the RST and the Falcon protocol are ""combined"" inside the RST (the \pi_mul operation is encapsulated in the RST) - if we add more protocols over RST we can have something like we have in the ```aditive_sharing_tensor.py```
```
@crypto_protocl(""fss"")
def add(self, other):
    # here we define how our operation over the AST should work if we use the FSS protocol
```Hmm..in that case I think I can completely remove this function - doing that nowRemoved this functionI renamed the file and it seems it got all the changesThis is not really the ring size - is the maximum value for the random value (for the common randomness)Will create a ticket for adding annotations to all the elements in the falcon related implementationYep - I left a comment there - basically doing this I think becomes a ""little"" intrusive with the RST - but it is worth thinking if we can rewrite this - hopefully, we will finish falcon soon and we will start benchmarking it and see where we can ""get some juice""Made an issue for adding types annotations for all stuff related to falcon [here](https://github.com/OpenMined/PySyft/issues/4505)how would you know they going to sum up to ring size if you don't know what is the ring size
( alpha1 + alpha2+ alpha3 )% ring size should be zerothat's a subroutine, it should be moved to the helper classselect share is a subroutine not protocol, it would be better to describe the arguments and the return directly
like
 return: x if b=0. y if b=1
and remove the rest of the commentit's not a share, it's a shared value, it would be better to call them x, y, bthis is not coherent with the interface please remove it and just use torch.ones.shareevaluate is like the worst name ever a routine can have üòÑ that's basically what every routine in the world does, it evaluates some expression, it doesn't tell you anything about what this routine does.
this also should be moved to the helper classif you noticed this routine determines the negativity of x, if beta is an even number this will return x, if beta is odd it will return - x, you can choose a name that explains this functionality please remove the ""share"" suffix please remove the docstring, xor is self-documented it doesn't need comments, ""you should see every comment as a failure to express yourself  with code""same expression will work on ints, this expression adds not-needed overheadplease remove this, we need the interface to be as small as possible, you can just use torch.ones().share()We should not use ```FalconHelper``` for simple ints. We should use ```int_a``` ^ ```int_b```We need to do docstring because if we move to ```syft_0.3.0``` that will fail if we do not have every function documented. For a newcomer, the documentation might help.Yep - tried to follow the model from ```AST```, but we might refactor thatWill doWill rename itWill doDoneYep...it is. will redoIt is a check to make sure we are doing everything correctly - we could remove this after we finish the implementation.Thinking of a conditional negate - I do not know if this is the best name for itwhy is this a global variable thought it's only in xor?value should be always RST, other could be RST for private xor, or long tensor or int for public xor remove comments before merging to the codebase should the size be 1 or x.shape?remove commentsI know that negate_con doesn't mean conditional negative, do not use abbreviations just use the whole word until you come up with a better nameremove abbreviationswe can use public multiplication and addition here to save some communication rounds: (beta*-2)+1we can use public operations here: d= -1*c_L +1Yep - we can do. I had a feeling that it would require multiple multiple rounds of communications if we reorder the operations, but the public multiplication is done locally.The same for this.Will removeHm, I made it like this to be able to have ```xor(int, RST)``` - I can remove itReading that part of the paper again - yeah it should be ```x.shape```Will doI'm working on RST  to enable executing 1-c_L directly, it should be ready shortly Hmm..this is not a global variable, it is accessed only using the class, but I think you are right, we can move it inside the xor methodI have all those types in place - which look pretty bad - let's keep them in place only to make sure we define the protocol correctly and then remove themP1 has share x1, x2; P2 has x2,x3 and P3 has x3 and x1 - when adding a constant we will have
P1 has share x1 + c, x2, P2 has share x2,x3 (nothing changed) and P3 has x3 and x1 + c```suggestion
            returns x if beta=0, or -x if beta=1
```
this should say what the function does not how it's done
```suggestion
        """""" determines the sign of x,  positive if beta is 0 or negative if beta is 1
```RST only supports torch.long, deterministic, little, and fast interface is better than a slow wide one, we will use FPT to support floating-point arithmetic  (which will pass torch.long to this class)tests should not include any iterations nor conditions (from clean coding book üòÑ) just simple expressions that tell what the interface does last request: let's move the assertion inside the body of the code, the tests should only test the interface and helper functions, we can use assertions in code during construction it would make it easier to find and remove them before moving it to production
for example, put (assert x.ring_size == 2) inside the xor function instead of if statements and remove the corresponding test case 
also better manage whitespace, related expressions should be in a continuous chunk
It is not ok to have asserts in the ""real"" code because they can be removed if you run with the ```-O``` or ```-OO``` flag.Hmm...in that case when generating the shares we should raise an exception is the passed Tensor is a different type than long, right?Yep - I will manually create the ```TEST_VALS``` list with all values we exceptIf ```generate_shares``` is public shouldn't we move the ```__validation``` function here?
I will also remove line 51I added that in my last PRyes that's the point, you should remove the asserts before pushing to production, asserts makes it easier to do that if you want your checks not to be moved to production to improve performanceit's not part of the interface, I left it public just to test itQ: This looks kind of *ugly* but this is how black formatted it :\Q: Do you prefer this to be split into 3 asserts? (one for each type?) @abogaziah there's no difference, anyone would work no problem :D",2,True,2020-08-25 07:19:25,2020-09-01 15:43:51,2020-09-01 15:43:50
https://github.com/OpenMined/PySyft/pull/4469,[],Fix syft-proto version,"Fix syft-proto version## Description
Keep track of the syft-proto version and not the commit.


## How has this been tested?
- There are no error messages when sending stuff to workers.

## Checklist
- [X] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [X] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [X] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [X] My changes are covered by tests",1,True,2020-08-24 15:12:03,2020-08-24 21:18:35,2020-08-24 21:18:35
https://github.com/OpenMined/PySyft/pull/4465,[],Add benchmark graph and script for tanh function approximation methods,"Add benchmark graph and script for tanh function approximation methods## Description
Following https://github.com/OpenMined/PySyft/pull/4031, this PR includes:

- Scripts for plotting benchmark graph for tanh function approximations.
- Benchmark graph as suggested in issue https://github.com/OpenMined/PySyft/issues/3999

Closes: https://github.com/OpenMined/PySyft/issues/3999

## How has this been tested?
- Same as https://github.com/OpenMined/PySyft/pull/4031, no tests are including for benchmarking.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests

```suggestion
data format: list[('method_name', precision value),]
```Incorrect doc-strings :stuck_out_tongue: Ah! You're right! Fixed this now.Ok :) changed.",5,True,2020-08-23 16:39:21,2020-08-24 10:55:30,2020-08-24 10:55:30
https://github.com/OpenMined/PySyft/pull/4463,[],Throw error if protoc not found,"Throw error if protoc not found## Description
Could not find an issue to link this to.
Give a message if the ```protoc``` file is not found on the system.

## Affected Dependencies
No dependencies are affected

## How has this been tested?
- Running ```pre_commit.sh``` without ```protoc``` installed.

## Checklist
- [X] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [X] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [X] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [X] My changes are covered by tests - manually tested",1,True,2020-08-22 23:39:39,2020-08-24 15:30:35,2020-08-24 15:30:35
https://github.com/OpenMined/PySyft/pull/4065,[],Added support for negative numbers in reciprocal method #4048,"Added support for negative numbers in reciprocal method #4048## Description
This PR fixes #4048
Added support for negative numbers when use different methods in Reciprocal.
Uses a signum function to achieve this
Suggestions are welcome! 

## Affected Dependencies
- None

## How has this been tested?
-  No additional tests are needed. Changed the default value in test_reciprocal(): function to include negative numbers too
-  Ran pytest test/torch/tensors/test_precision.py without issue
## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
In general, `NR` is much faster than `division`. Is there a reason for changing this? :smile: 
This part duplicates many parts with `NR`. Can you remove this duplication? üòé Yes will do that!! 
https://github.com/OpenMined/PySyft/issues/4036#issuecomment-675880294...  I saw this and made a change. I think now keeping it as NR should not be a problem.```suggestion
        sgn = (self > 0) - (self < 0)
```

Seems a lot more intuitive, straightforward this way to me```suggestion
        """"""
```
You don't need this unless you are using special notations/math in the docstrings```suggestion
        """"""
```

Here too:heart: agreed! lesser code this wayokay!okay",3,True,2020-08-22 13:44:20,2020-08-23 10:50:12,2020-08-23 10:50:12
https://github.com/OpenMined/PySyft/pull/4064,[],"Added scripts, graphs for benchmarking AST operations","Added scripts, graphs for benchmarking AST operations## Description
Scripts and Graphs are added for benchmarking AST operation based on two currently available protocols (fss, snn).
currently added scripts and graphs are added for:-

- [x] share_get

- [x] max_pool2d

- [x] avg_pool2d

- [x] batch_norm

Improvement suggestions are welcomed :) 

This PR Partially fixes #4000 

## Affected Dependencies
None

## How has this been tested?
- This addition doest requires an additional test but an average of 10 timestamps have been computed to get an accurate result.
- Benchmark graphs may be different for different system specifications on which the scripts are run though they should show a similar trend. 
## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",6,True,2020-08-22 10:56:20,2020-08-25 12:14:00,2020-08-25 12:13:59
https://github.com/OpenMined/PySyft/pull/4047,[],[WIP] fixed the precision issue,"[WIP] fixed the precision issue## Description

Resolves #3950 

Aims to solve 

- [x] math operations between fixed-precision and non-specified fix precision. The wrapper to convert from non-fixed precision to fixed-precision casted the type but did not actually change the tensor value

- [ ] `PureFrameworkTensorFoundError` which seems to be an issue when the first arg is not a fixed precision but the second one is (does not happen if both are fixed precision)

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Unit tests

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
@abogaziah 

This is the specific section I changed. After changing this the test cases for `add`, `subtract`, `mult` (scalar, I haven't checked matrix mult) and `div` work as expectedyou should not call a variable _ , pick a good naming of what that variable is you should not import a class in the middle of the code, imports are placed in the header of the file (first few lines)Great, it works ‚úå, we just need to change the design of the solution",1,False,2020-08-20 01:48:20,2020-09-01 18:27:06,2020-09-01 18:27:06
https://github.com/OpenMined/PySyft/pull/4041,[],Typos fix,"Typos fix## Description
Simple Typos fixes

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- There is no test required since only comments are changed

## Checklist
- [X] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [X] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [X] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [X] My changes are covered by tests",1,True,2020-08-19 09:07:12,2020-08-19 23:32:21,2020-08-19 13:46:34
https://github.com/OpenMined/PySyft/pull/4038,[],Pointer docs,"Pointer docs## Description
Adding documentation and syft decorator on pointers in syft 0.3.0.Q: Can't we add this at the top? or dependency resolution conflict?let's include the actual .request_access() method name here. Possibly with an exxample code snippet!... ""for more information on how subclasses of pointer are automatically generated..., please see <ast module discussing automatic generation... probably st.klass>""

This notion that ""this Pointer class is never directly use but is subclassed"" should probably be added to the story at the top.let's include an example code snippet here.It's a circular dependency issue at the moment - but it should hopefully be a fixable one.PR welcome - but be wared - here be dragons",1,True,2020-08-18 22:18:28,2020-08-20 18:10:58,2020-08-20 18:10:58
https://github.com/OpenMined/PySyft/pull/4035,[],FIX Return invalid dtype when MPC is applied,"FIX Return invalid dtype when MPC is applied## Description
**Issue:**
this was the original issue #3982  raised there was a mismatch in dtype after using the share()

![](https://user-images.githubusercontent.com/39186433/89849067-a9f89380-dbc2-11ea-84aa-6bf791a46b78.png)

**Reason:**
The create_wrapper class which was used to create the wrapper was returning tensor with a dtype torch.float32.

**Fix**
sending the `type=self.dtype` as a parameter in `shared_tensor.wrap()` from the `share()` solves the issue.
`wrap()` function calls another function `sy.framework.hook.create_wrapper(type, **kwargs)` which is appended below
![Screenshot 2020-08-20 at 4 42 36 PM](https://user-images.githubusercontent.com/28955148/90763352-2ba79a00-e304-11ea-93df-132d956a6fd4.png)
in `create_wrapper(cls, wrapper_type)` function if wrapper_type is None we get` torch.Tensor()` in return which is by default of dtype `torch.float32` (which is our issue). This was easily fixed by sending the dtype from the `share()` fucntion which ultimatly reaches to` create_wrapper(cls, wrapper_type)` and sets the dtype.


This pull request closes #3982

## Affected Dependencies
None

## How has this been tested?
one new test case has been added in` test_share_get()` function of the `test_additive_shared.py` file.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2020-08-18 12:19:03,2020-08-22 05:34:34,2020-08-22 05:34:34
https://github.com/OpenMined/PySyft/pull/4032,[],Made tests fault tolerant to missing crypten module for windows,"Made tests fault tolerant to missing crypten module for windows## Description
Resolves #4025 by skipping crypten test files and adding expected to fail in serde tests if operating system is Windows.

## How has this been tested?
My own system was failing some tests and `test_inv_sym` goes on forever, so I ran only crypten and serde tests by temporarily moving other test files to another destination.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Could you add the `reason` attribute to these?Of course, added attribute",17,False,2020-08-17 17:09:54,2020-11-10 09:05:33,2020-11-10 09:00:57
https://github.com/OpenMined/PySyft/pull/4031,[],"Add Benchmark Graph, Scripts for Sigmoid Approx. Methods","Add Benchmark Graph, Scripts for Sigmoid Approx. Methods## Description

- Added scripts for plotting benchmark graph for sigmoid function approximations.
- Added a benchmark graph as suggested in issue #3998 
- Added files for organizing all the benchmark data in the same file.
This pull request closes #3998 

## Affected Dependencies
matplotlib

## How has this been tested?
- This issue does not require a test script although the average of the 10 iterations has to taken to get the best benchmarks.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
I think all these need to be rewritten as literals, since the [tests are failing](https://github.com/OpenMined/PySyft/pull/4031/checks?check_run_id=994680515).  Are you using the [pre-commit package](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md#setting-up-pre-commit-hook)?done! Thanks :)Could you also add the time? (probably it is in ms)
Like: ```Execution Time (ms)```> Could you also add the time? (probably it is in ms)
> Like: `Execution Time (ms)`

Sure :)",7,True,2020-08-17 17:05:25,2020-08-22 10:33:06,2020-08-19 19:50:53
https://github.com/OpenMined/PySyft/pull/4021,[],Fast FixedPrecisonTensor Reciprocal Method (150x faster),"Fast FixedPrecisonTensor Reciprocal Method (150x faster)## Description

issue: https://github.com/OpenMined/PySyft/issues/3993

We can use reciprocal 150 times faster on average. Implementing division of AST with reciprocal can make existing very slow division much faster. In addition, many slow operations can be made very quickly.

This implementation referred to the implementation of CrypTen.

https://github.com/facebookresearch/CrypTen/blob/3c4e53aa4910117e4e136691cb09ea7a7e001029/crypten/mpc/mpc.py#L1029

```python
x = torch.tensor([1., 2., 3.]).fix_prec().share(bob, alice, crypto_provider=theo)

# Existing method (using division)
x.reciprocal().get().float_prec() # 27 seconds
# tensor([1.0000, 0.5000, 0.3330])

# New Method (using NR)
x.reciprocal(method=""nr"").get().float_prec() # 0.18 seconds
# tensor([1.0000, 0.5000, 0.3330])

# New Method (using LOG) / Approximation
x.reciprocal(method=""log"").get().float_prec() # 0.31 seconds
#  tensor([1.0000, 0.4650, 0.3530])
```

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
MAYBE: Use 
```
tensor = torch.tensor([1.0, 2.0, 3.0])
x = tensor.fix_prec()

result = x.reciprocal().float_prec()
assert (result == tensor.reciprocal()).all()
```Because the torch tensor uses the Floating Point, the original method of using division also has an error. That's why you can't use this method. üòÑ 

```python
tensor = torch.tensor([1.0, 2.0, 3.0])
x = tensor.fix_prec()

result = x.reciprocal().float_prec()
assert (result == tensor.reciprocal()).all() # -> tensor([True, True, False])
# tensor([1.0000, 0.5000, 0.3333]) == tensor([1.0000, 0.5000, 0.3333]) == 
```
Ahh...don't use equal, use: [```torch.isclose```](https://pytorch.org/docs/stable/generated/torch.isclose.html)I applied this change request and added a docstring üòÑ",10,True,2020-08-15 16:09:10,2020-08-18 07:57:19,2020-08-18 07:29:01
https://github.com/OpenMined/PySyft/pull/4020,"['bug ', 'priority: 2 - high :cold_sweat:', 'status: review needed :raising_hand:', 'severity: 2 - high :cold_sweat:', 'status: in progress :star2:', 'status: investigating :mag:']",Grid clients serialisation correction,"Grid clients serialisation correction## Description
Register `DataCentricFLClient` in serde for serialization.

## Affected Dependencies
`DataCentricFLClient` cannot be serialised currently, as it's not registered in serde.
which causes error while doing SMPC as sharing of tensor raises the error during serialization.
Code : 
`x = torch.tensor([1,2,3,4,5])
x = x.send(bob)
x = x.share(bob,alice)`

Error:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-66c2dc1e8ea9> in <module>
      3 
      4 x = x.send(bob)
----> 5 x = x.share(bob,alice)

~/anaconda3/envs/pysyft/lib/python3.8/site-packages/syft/frameworks/torch/tensors/interpreters/native.py in share(self, protocol, field, dtype, crypto_provider, requires_grad, no_wrap, *owners)
    916                 {""requires_grad"": requires_grad} if isinstance(chain, syft.PointerTensor) else {}
    917             )
--> 918             shared_tensor = chain.share(
    919                 *owners,
    920                 protocol=protocol,

~/anaconda3/envs/pysyft/lib/python3.8/site-packages/syft/generic/pointers/pointer_tensor.py in share(self, *args, **kwargs)
    382             raise RuntimeError(""Error, share must have > 1 arguments all of type syft.workers"")
    383 
--> 384         response = self.owner.send_command(self.location, ""share"", self, args, kwargs)
    385         return response
    386 

~/anaconda3/envs/pysyft/lib/python3.8/site-packages/syft/workers/base.py in send_command(self, recipient, cmd_name, target, args_, kwargs_, return_ids, return_value)
    514                 cmd_name, target, args_, kwargs_, return_ids, return_value
    515             )
--> 516             ret_val = self.send_msg(message, location=recipient)
    517         except ResponseSignatureError as e:
    518             ret_val = None

~/anaconda3/envs/pysyft/lib/python3.8/site-packages/syft/workers/base.py in send_msg(self, message, location)
    312 
    313         # Step 1: serialize the message to a binary
--> 314         bin_message = sy.serde.serialize(message, worker=self)
    315 
    316         # Step 2: send the message and wait for a response

~/anaconda3/envs/pysyft/lib/python3.8/site-packages/syft/serde/serde.py in serialize(obj, worker, simplified, force_full_simplification, strategy)
     43         strategy = serialize
     44 
---> 45     return strategy(obj, worker, simplified, force_full_simplification)
     46 
     47 

~/anaconda3/envs/pysyft/lib/python3.8/site-packages/syft/serde/msgpack/serde.py in serialize(obj, worker, simplified, force_full_simplification)
    335 
    336     simple_objects = _serialize_msgpack_simple(obj, worker, simplified, force_full_simplification)
--> 337     return _serialize_msgpack_binary(simple_objects)
    338 
    339 

~/anaconda3/envs/pysyft/lib/python3.8/site-packages/syft/serde/msgpack/serde.py in _serialize_msgpack_binary(simple_objects, worker, simplified, force_full_simplification)
    289     # 2) Serialize
    290     # serialize into a binary
--> 291     binary = msgpack_lib.dumps(simple_objects)
    292 
    293     # 3) Compress

~/anaconda3/envs/pysyft/lib/python3.8/site-packages/msgpack/__init__.py in packb(o, **kwargs)
     33     See :class:`Packer` for options.
     34     """"""
---> 35     return Packer(**kwargs).pack(o)
     36 
     37 

msgpack/_packer.pyx in msgpack._cmsgpack.Packer.pack()

msgpack/_packer.pyx in msgpack._cmsgpack.Packer.pack()

msgpack/_packer.pyx in msgpack._cmsgpack.Packer.pack()

msgpack/_packer.pyx in msgpack._cmsgpack.Packer._pack()

msgpack/_packer.pyx in msgpack._cmsgpack.Packer._pack()

msgpack/_packer.pyx in msgpack._cmsgpack.Packer._pack()

msgpack/_packer.pyx in msgpack._cmsgpack.Packer._pack()

msgpack/_packer.pyx in msgpack._cmsgpack.Packer._pack()

msgpack/_packer.pyx in msgpack._cmsgpack.Packer._pack()

msgpack/_packer.pyx in msgpack._cmsgpack.Packer._pack()

TypeError: can not serialize 'DataCentricFLClient' object
```








Q: This is ok?this was not changed by me,  actually, i only changed two files, probably some error while I rebased my PySyft repo, I changed serde.py and grid.clients.datacentric files I will update this.Q: Shouldn't we have a ```syft-proto``` for this?Q: if we reach here doesn't mean we already are hooked?yup we have created but I am not sure how to access that hook from here, also creating a new one should not raise any issue. I will add the code in syft-proto, but for now, this would work also it's easier to use this function if add any new class which need to be serialised, we don't need to wait fr adding it to syft-proto. Could you create an issue in ```PySyft``` with good first issue regarding this.I think you can access it from the worker.The problem is that `detail` is called by a gridnode on the simplified objects it receives over the network from client(so we don't have control from client side over the behaviour of the function.). 
and you cant pass the hook to it, (unless I serialise the hook instance in `simplify ` function which we will mean we are essentially sending hook over the network which is not required. okayand it's a class method which takes simplified input as arguments.It can't be accessed using ```syft.local_worker.hook```?@Nilanshrajput I think that @gmuraru is right. Please make this modification before merging.got it, updatedMAYBE: uncomment thisyup,i commented while local testing forgot to remove it",11,True,2020-08-15 09:11:28,2020-08-20 20:14:33,2020-08-20 20:14:33
https://github.com/OpenMined/PySyft/pull/3990,[],Remote addition using pointers on Duet,"Remote addition using pointers on Duet## Description
In this PR - I have finished the serde requirements (and fixed a few bugs) necessary to perform remote addition on a duet node.",2,True,2020-08-11 22:24:40,2020-08-12 16:53:49,2020-08-11 22:32:37
https://github.com/OpenMined/PySyft/pull/3989,[],tensor.send(duet) and ptr.get() both work with duet (with tests),"tensor.send(duet) and ptr.get() both work with duet (with tests)## Description
In this PR I fixed a small bug which was preventing .get() serde to work correctly.
",1,True,2020-08-11 19:48:52,2020-08-11 21:50:43,2020-08-11 20:09:51
https://github.com/OpenMined/PySyft/pull/3960,[],Overload mod operator for the fixed precision tensor,"Overload mod operator for the fixed precision tensorAs pointed out in https://github.com/OpenMined/PySyft/issues/3958 modular operations over fixed precision tensors are broken. This PR overloads the mod operation over fixed precision tensors in [`syft/frameworks/torch/tensors/interpreters/precision.py`](https://github.com/arturomf94/PySyft/blob/d75a2ba0d4542faf1e5d019c00a9d77868f4d723/syft/frameworks/torch/tensors/interpreters/precision.py#L76) and adds a corresponding test in [`test/torch/tensors/test_precision.py`](https://github.com/arturomf94/PySyft/blob/d75a2ba0d4542faf1e5d019c00a9d77868f4d723/test/torch/tensors/test_precision.py#L40)

Closes: https://github.com/OpenMined/PySyft/issues/3958would you just change ""other"" to ""devisor"" for readabilityis there a reason why you decided to copy the object and not modify the self.child directly and return self? I expect that this modification will break the same functionality if the devisor was another FPT or torch.Tensor
please test the case of 
x = torch.tensor([1, 2, 3]).fix_prec()
y =  torch.tensor([3]).fix_prec()
z= (x%y).float_prec()
assert (z == torch.tensor([1.0, 2.0, 0.0])).all()
That's a good shout. Will add that test too. For inmutability of `x` in `x % y`. Would you say it's best to return `x`?I've added a test for this now.calling a function on a tensor is supposed to modify the child without explicitly mentioning it, for instance, ""result %= divisor"" should do the same, and it's more readable. no, it would be better to init a new instance :) Agree. I'll change that. Actually, `result %= divisor` ends up being an infinite recursion. So I left it as it was: working with the `.child`. If there's a better way to avoid the recursion without referencing the child please let me know.hmm.. makes sense, let's keep it that way Q: Can we do it with how we have the ```add```, ```sub``` operator?
Use @overloaded.methodQ: Could you also add tests if the operators are ```AST``` and ```FPT```?I tried this, but the tests fail and I don't know how to do a mod operation where AST is the dividend and FTP the divisor... Does this work when both the dividend and the divisor are ASTs?I think `%` doesn't even work for AST and scalars! For instance, consider the following: 

```python
x = torch.tensor([1, 2, 3]).share(bob, alice, crypto_provider=james)
(x % 3).get() 
```

This turns out to be `tensor([1, 2, 3])`, which is not right üòï ‚ùó ‚ùì

In fact, it's inconsistent: sometimes I get something like `tensor([4, 2, 3])`. What does this mean? üò± Lol - never mind. I forgot to use fixed precision I think...I saw the tests [here](https://github.com/OpenMined/PySyft/blob/aafa70d096acf22d7754da8b7b0037f74d12313a/test/torch/tensors/test_additive_shared.py#L1045) for the mod operation on ASTs and tried to replicate it somehow. Let me know what you think.",1,True,2020-08-08 01:04:08,2020-08-11 15:18:14,2020-08-11 13:55:48
https://github.com/OpenMined/PySyft/pull/3959,"['bug ', 'improvement ']",Update PyGrid URLs and cleanup Model-Centric notebooks a bit,"Update PyGrid URLs and cleanup Model-Centric notebooks a bit## Description
PyGrid model-centric API URLs were updated from `model_centric` to `model-centric`.
This PR makes the change to FL client.
Also URLs are fixed in Model-centric notebooks and they made more up-to-date (e.g. pygrid gateway -> pygrid node).

## Affected Dependencies
n/a

## How has this been tested?
Executed model-centric notebooks with the latest PyGrid.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",2,True,2020-08-07 11:11:15,2020-08-07 18:50:27,2020-08-07 18:50:15
https://github.com/OpenMined/PySyft/pull/3955,[],The `__repr__` on the `Network` class must return a `repr_str`,"The `__repr__` on the `Network` class must return a `repr_str`## Description

The `__repr__` method only generated a string and did not return it. I fixed it üòÑ 

Thank You!

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2020-08-06 16:05:06,2020-08-07 14:01:19,2020-08-07 12:38:09
https://github.com/OpenMined/PySyft/pull/3951,[],Fix CrypTen tutorials,"Fix CrypTen tutorials## Description
We deprecated PyGridNode and we need to update the CrypTen tutorials to make use of the new PyGrid repo.
Update according to the new PyGrid repo.

## How has this been tested?
- The tests are passing

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
<p>Any idea why time goes up here? is it the same env as before?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/3951/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='21'/>There was added some more logic into the plans building process and the CrypTen commit which we use might not be the same one which we used when we did our benchmarking.
When I did the benchmarking I used the latest master and when I merged the changes I used the latest commit from master to pin it to that ""version"". It might not be the same.",2,True,2020-08-05 22:29:05,2020-08-06 10:26:23,2020-08-06 10:26:23
https://github.com/OpenMined/PySyft/pull/3945,[],quick fixes on types,"quick fixes on typesBug fix. Bug causes code to fail to initialize a node as it doesn't find the ""self.id"" property.
Also, some address types inclusion.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2020-08-05 14:50:19,2020-08-05 15:06:10,2020-08-05 15:06:10
https://github.com/OpenMined/PySyft/pull/3941,[],black src; black tests,black src; black testsFix linting errors.,1,True,2020-08-05 13:55:07,2020-08-05 13:55:32,2020-08-05 13:55:28
https://github.com/OpenMined/PySyft/pull/3904,[],fix for copy,"fix for copy## Description
This is a partial solution for #3845. I was able to reproduce the issue for `copy` using: 
```
import torch
import torch.nn as nn
import syft as sy

hook = sy.TorchHook(torch)
bob = sy.VirtualWorker(hook, id=""bob"")

model = nn.Linear(2, 1)
model_ptr = model.send(bob)
model_copy_ptr = model_ptr.copy()
```
The suggested fix solves it. 

However, I was not been able to reproduce the error for calling `clone()` on pointer.  It seems like a different bug. 
## Affected Dependencies
For this fix to work `import torch` is required in pointer_tensor.py

## How has this been tested?
There is not much testing required. I checked the above code works and nothing else in the library is broken. 

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
```suggestion
    model_bob.get()
    pred_remote = model_bob_copy(data_bob)
    pred_remote = pred_remote.get()
```The suggested change introduces an error. I investigated and this is what I think is happening: 
1) There is no straightforward way to copy a model in torch. 
2) Upon trying to copy a model in syft, under the hood you actually call clone. 
3) The difference is that (from the code): Clone should keep ids unchanged, contrary to copy.
4) Therefore we are actually creating two pointers to the same location. Once we ""get"" the object, it releases the other pointer as well.
5) Tensor on the other hand, do allow copy(). This code will work as expected:   
```
t = torch.tensor([3, 1.0, 2])
p1 = t.send(alice)
p2 = p1.copy()
p1.get()
p2.get()
```
6) In the example from 5 If we replace the `copy()` with `clone()` the same error will occur since p2 will have the same id as p1.",3,True,2020-07-27 23:39:34,2020-07-29 12:10:32,2020-07-29 12:10:31
https://github.com/OpenMined/PySyft/pull/3894,[],Crypten Merge to Master,"Crypten Merge to Master## Description
Merge the CrypTen branch into Master
**THIS WILL DROP SUPPORT FOR PYTHON3.6**

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
 - Unit tests for CrypTen

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
can we add a version?Can you fix a commit maybe?AddedSorry I meant should we add a commit number for the crypten master branch?
Ahh..sorry - meant to comment to the dill",2,True,2020-07-23 12:30:12,2020-07-26 21:14:58,2020-07-26 21:14:53
https://github.com/OpenMined/PySyft/pull/3860,[],test: Add RNN Plan example,"test: Add RNN Plan example## Description
I created a language model in a Plan. I was trying to mix the Creating a Plan tutorial and a word based language model from https://github.com/pytorch/examples/tree/master/word_language_model

This was tricky to get working because:
* I wanted to make it work like in the PyTorch Word Language Model example, using 3D tensors (sequence length x batch size x dims) but PySyft's autograd features did not work well with 3D tensors. Many classes assume that tensors are 2D when doing transpose e.g. `MatmulBackward`, `SumBackward`, `TBackward`.
* So I gave up on using 3D tensors and decided to only use the final result from the RNN (and not each output in the sequence). `GRU` from syft/frameworks/torch/nn/rnn.py won't work in a Plan because it uses `torch.sigmoid(z)` and maybe also because of chunking. Instead I made a custom GRU class that does `z.sigmoid()`.
  * I had other issues with PySyft's `rnn.py` in `_init_hidden` (making assumptions on the type of the input) and `_apply_time_step` (uses `stack` which seems like autograd doesn't support).
* Values were getting set to `None` by low-level PySyft operations.  E.g. `param.grad` was `None` after backprop. Sometimes this is expected like when using `nn.Embedding` but usually this was because of a bug somewhere. It was really hard to track these down. It seems like many operations in PySyft default to setting the result to `None` when something unexpected happen. This made something later error.
  * (corrected in another PR) Another example of this was `output += bias` (PyTorch's functional.py) was changing output to `None` when output and bias were `AutogradTensor`s.

## Affected Dependencies
(only required for translating the Plan to Tensorflow.js) An unreleased version of 3p0 with my translations for `select` and `__rsub__` is required to translate the Plan to Tensorflow.js. The example is lenient and will not fail if these translations are missing.

## How has this been tested?
I ran the file I submitted with pytest.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",4,True,2020-07-15 03:22:57,2020-07-24 19:07:28,2020-07-24 19:06:23
https://github.com/OpenMined/PySyft/pull/3850,[],Fix wording in Part 2 notebook,"Fix wording in Part 2 notebook## Description
Please include a summary of the change, the motivation, and any additionl context that will help others understand your PR. If it closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2020-07-10 13:32:31,2020-07-10 13:39:52,2020-07-10 13:39:48
https://github.com/OpenMined/PySyft/pull/3849,[],Fix wording in static FL Part 2 notebook,"Fix wording in static FL Part 2 notebook## Description
Few wording fixes

## Affected Dependencies
n/a

## How has this been tested?
n/a

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,False,2020-07-10 13:27:29,2020-07-10 13:31:41,2020-07-10 13:31:36
https://github.com/OpenMined/PySyft/pull/3843,[],New Fixes CrypTen branch,"New Fixes CrypTen branch## Description
Add the BCELoss (used in the benchmarks) and delete the plan at the end of the CrypTen computation.

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,True,2020-07-09 16:27:32,2020-07-13 04:20:42,2020-07-13 04:20:42
https://github.com/OpenMined/PySyft/pull/3842,['documentation '],Update Static FL Notebooks,"Update Static FL Notebooks## Description


Update static fl notebooks to match with recent changes on PyGrid.

Also:

- fixed speed test method for upload/download
- fixed authentication method

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- running notebooks w/ pygrid

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",2,True,2020-07-09 16:13:03,2020-07-10 13:20:53,2020-07-10 13:20:41
https://github.com/OpenMined/PySyft/pull/3830,[],Update install instructions in tutorial part 1,"Update install instructions in tutorial part 1## Description

This updates the instructions to install PySyft from Part 01 of the tutorial.

Previous: git clone, then install from source ```python setup.py install udacity``` (including udacity in the command now has an error) and link to the PySyft repo
Updated: ```pip install syft[udacity]``` (I would be ok making it just pip install syft) and link to INSTALLATION.md

I also update the translated notebooks and their dependencies (Python 3.6+, PyTorch 1.4)

Based on the updates to install/contribution pages on #3197 

## How has this been tested?

View on GitHub: https://github.com/mapmeld/PySyft/blob/master/examples/tutorials/Part%2001%20-%20The%20Basic%20Tools%20of%20Private%20Deep%20Learning.ipynb

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)",1,True,2020-07-07 17:22:16,2020-07-10 18:29:48,2020-07-10 18:29:47
https://github.com/OpenMined/PySyft/pull/3823,[],updating the websocket client's timeout,"updating the websocket client's timeout
## Description
Increasing the timeout will help in solving a lot of timeout issues especially with tensor.send() when the tensor is large and model.send() when the model is big.
This should solve issue #3813 and some other timeout issues.",5,False,2020-07-06 01:49:37,2020-07-10 23:51:10,2020-07-10 23:51:10
https://github.com/OpenMined/PySyft/pull/3822,[],CrypTen Plan Inference ,"CrypTen Plan Inference ## Description
Demo on how to run inference on data that is not found on the local machine + the model is not known.

## Attention
This PR must be merged after [this one](https://github.com/OpenMined/PySyft/pull/3817) and [this one](https://github.com/OpenMined/PySyft/pull/3692)


## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
<p>typo: ""... The workers that know ...""</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/3822/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='0'/><p>typo: ""...3001..."" an additional zero in the bash command and in the description</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/3822/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='1'/><p>typo ""... where each worker has a subset of the features we need for the training...""</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/3822/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='0'/><p>port 30001 -&gt; 3001</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/3822/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='2'/>DoneDone",2,True,2020-07-05 23:06:18,2020-07-08 18:50:00,2020-07-08 18:50:00
https://github.com/OpenMined/PySyft/pull/3821,[],Quick Fix CrypTen Demo Jails,"Quick Fix CrypTen Demo Jails## Description
Small fix needed because of this [PR](https://github.com/facebookresearch/CrypTen/pull/121)


## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",5,True,2020-07-05 20:01:23,2020-07-07 06:55:33,2020-07-07 06:55:33
https://github.com/OpenMined/PySyft/pull/3817,[],Use OnnxModels for private inference,"Use OnnxModels for private inference## Description
Define and use ```OnnxModels``` serialized models.
The parties know the model architecture and one of the parties has data that should be run through the model.
The local party does not have any idea about the architecture or the data.

The local party would only receive the result for the inference step.


## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Added test to check the functionality

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
This needs to be merged first.
```OnnxModel``` has protobuf + serde support.Test ProtobufTest MsgPackShouldn't we also store the model's weights as discussed in #3776 ?Merged to syft-proto you can update the requirement to master :) ```suggestion
    This method should disappear once CrypTen has support to load models
``````suggestion
    WARNING: All the workers that are part of the CrypTen computation
```Should this TODO info be at this place in the code?```suggestion
    This model will be known by a single party and when the party will call
    the crypten.load function it will deserialize the model to a PyTorch one
    and share it with the other parties.

    In this scenario, the worker that started the computation will not know about the
    instrinsics of the architecture and *only one* worker can have knowledge about the
    model
``````suggestion
from syft.frameworks.crypten import utils
from syft.generic.pointers.object_pointer import ObjectPointer
from syft.generic.abstract.sendable import AbstractSendable
from syft.workers.abstract import AbstractWorker
from syft.workers.base import BaseWorker
```The model here compared to the previous test could be private (ie hidden) to the local worker?
If so you can add a line to explain the difference with the previous test!Based on our discussion, I will create another PR for doing that -- will link it to #3776 when doing thatI think it is best to be in the ```OnnxModel.py``` file. Moving it.v0.4.10 of syft proto has been released with ONNX supportDone -- waiting for testsDone",2,True,2020-07-04 20:04:20,2020-07-07 12:10:17,2020-07-07 12:10:17
https://github.com/OpenMined/PySyft/pull/3807,['bug '],Fix issues in handle_func_command in syft tensors,"Fix issues in handle_func_command in syft tensors## Description
This cleans code in handle_func_command and fix bugs due to variable overwriting for syft tensors, and to overloading functions in native.py

PICKY Stuff: Here the comment should be removed, right? Since we look for ```native_{cmd}```haha no it's good to be picky. here the module `native_torch` defined in native.py overwrites native torch functions... Yeah it's a bit strange, I really had to modify a torch function (aka torch.roll) to behave slightly differently",3,True,2020-07-01 12:05:57,2020-07-14 08:47:59,2020-07-07 21:57:51
https://github.com/OpenMined/PySyft/pull/3803,[],Add linter/bandit and tests stubs,"Add linter/bandit and tests stubs## Description
- [x] Add tests stubs
- [x] Add linter and bandit as blockers
- [x] Add ```@final``` decorator to classes that should not be inherited anymore

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
@iamtrask @tudorcebere what do you think?
This says that this class should not be inherited anymore

Also, it could be applied to methods. More info, [here](https://www.python.org/dev/peps/pep-0591/)",1,True,2020-06-30 19:40:44,2020-07-06 20:48:43,2020-07-06 20:48:43
https://github.com/OpenMined/PySyft/pull/3798,"['bug ', 'improvement ', 'status: review needed :raising_hand:']","Fix Plan trace for inplace ops, add Role actions pruning","Fix Plan trace for inplace ops, add Role actions pruning## Description
Changes:
1. Inplace operations like `X +=1` were traced as `var1 = X.__iadd__(1)` where `var1` is redundant variable that also hides the fact that the result of operation is `X` itself. Hook method is updated to return self as result for inplace operations.
2. Add Role actions pruning, e.g. if some operations/placeholders are not connected to Plan outputs or inputs (via side-effect), these are removed.

## Affected Dependencies
n/a

## How has this been tested?
Unit tests

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
This is very nested and is not optimal for readability
```suggestion
            return_ids = set()
            if action.return_ids is not None:
                for ph in action.return_ids:
                    if isinstance(ph, PlaceholderId):
                        return_ids.add(ph.value)
```
Could you add a line to explain why we don't use action_rev_idx (as we use action_idx below) ? (I guess, because we did a reverse enumerate)Can you add a line to explain why you need to reverse?I'm surprised that it implicitely provides `output_connected_placeholder_ids` but why not!",1,True,2020-06-30 07:05:01,2020-07-05 12:41:33,2020-07-05 12:41:28
https://github.com/OpenMined/PySyft/pull/3789,[],Fixed bug in set_model_params() in test_translation.py,"Fixed bug in set_model_params() in test_translation.py## Description
Fixed bug in implementation of `set_model_params()` in `test/exection/test_translation.py`. See #3780 

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2020-06-25 18:42:19,2020-06-29 21:23:59,2020-06-29 21:23:54
https://github.com/OpenMined/PySyft/pull/3781,"['bug ', 'status: review needed :raising_hand:', 'severity: 2 - high :cold_sweat:']",Fixed bug in set_model_params() in Create Plan,"Fixed bug in set_model_params() in Create Plan## Description
Fixes #3780 

```python
def set_model_params(module, params_list, start_param_idx=0):
    """""" Set params list into model recursively
    """"""
    param_idx = start_param_idx

    for name, param in module._parameters.items():
        print('\t','name:', name, '\tparam_idx:', param_idx)
        module._parameters[name] = params_list[param_idx]
        param_idx += 1

    for name, child in module._modules.items():
        print(child, param_idx)
        if child is not None:
            param_idx = set_model_params(child, params_list, param_idx)

    return param_idx

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, 392)
        self.fc2 = nn.Linear(392, 64)
        self.fc3 = nn.Linear(64, 10)

    def forward(self, x):
        x = nn.functional.relu(self.fc1(x))
        x = nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = Net()
model_params = list(model.parameters())
set_model_params(model, model_params)
```
### Output
```bash
Linear(in_features=784, out_features=392, bias=True) 0
	 name: weight 	param_idx: 0
	 name: bias 	param_idx: 1
Linear(in_features=392, out_features=64, bias=True) 2
	 name: weight 	param_idx: 2
	 name: bias 	param_idx: 3
Linear(in_features=64, out_features=10, bias=True) 4
	 name: weight 	param_idx: 4
	 name: bias 	param_idx: 5
```

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests",1,True,2020-06-25 07:35:44,2020-06-25 11:18:16,2020-06-25 11:18:10
https://github.com/OpenMined/PySyft/pull/3713,[],fixing threepio import bug,"fixing threepio import bug## Description
This Pull request fix an import bug when `Command` from threepio is called. Command is no longer in threepio, now `command` contain that class. It solves #3712 

## How has this been tested?
- Before I was not able to install and import from master branch, now it works fine!

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [ ] My changes are covered by tests
I think it's not correct to assume there will be just 1 command in the list.",5,False,2020-06-16 19:16:14,2020-06-19 11:55:59,2020-06-19 11:55:59
https://github.com/OpenMined/PySyft/pull/3705,"['bug ', 'priority: 2 - high :cold_sweat:']",Detecting duplicate worker IDs in crypten computation,"Detecting duplicate worker IDs in crypten computation## Description
To refer to crypten parties running on specific workers, we can do whether with the rank (int) of the party or the id (str) of the worker running the party, thus we needed a kind of translation mechanism between worker id and rank, but when workers involved in the computation happen to have duplicate ids, ranks get translated badly and the computation will fail badly. This PR raise an error when a computation is about to start with duplicates ids.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2020-06-14 11:14:51,2020-06-14 12:37:54,2020-06-14 12:37:49
https://github.com/OpenMined/PySyft/pull/3690,[],Update test_fv.py,Update test_fv.pyHotfix: somehow I messed up with the values of tests and the tests failed after merged into master. A quick fix for the result values.,1,True,2020-06-11 07:05:55,2020-06-11 08:53:37,2020-06-11 08:53:31
https://github.com/OpenMined/PySyft/pull/3685,"['bug ', 'priority: 1 - immediate :fire:', 'severity: 1 - critical :fire:']",Removed `wrap()` on non-tensor pointers in `respond_to_search()`,"Removed `wrap()` on non-tensor pointers in `respond_to_search()`## Description

in `syft/workers/message_handlers:respond_to_search()`, all pointers are wrapped before being returned. This should be the case only on `PointerTensor` objects, not only all pointers.

This causes a fail in SyferText where we need to exchange object pointers that are not tensors.

## Affected Dependencies
None

## How has this been tested?
All existing tests

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
Q: if you remove this if completely are the tests failing?Just pushed that to testYeah...it is failling :(, it was ok before :+1: I reverted my changes and it still giving an error :O@AlanAboudib it seemed to be an intermittent and was not the result of your change.It was happening because of timeout when waiting for an async response. I re-ran the tests and all tests passed. You can make your change and then we can debug further if the issue happens again.Thank you @shubham3121, I will push the changes thenI think @shubham3121 referred to the changes with
```
    # Wrap only if the pointer points to a tensor.
            # If it points to a generic object, do not wrap.
            if isinstance(obj, PointerTensor):
                ptr.wrap()
```I was trying your idea of just remove `.wrap()` without adding my code. I just push my changes back now@AlanAboudib , Ques: You want to wrap Tensor objects (not necessarily PointerTensors objects) and not non Tensor objects, is that the behaviour you're looking for ?? 
If so, then the check should be `isinstance(obj, torch.Tensor)`. Because the currently the test is failing because the obj is an Tensor instance and not a PointerTensor object, as a result we are not wrapping it and hence results in error on calling `.shape`.Oh, my bad, I actually wanted to type:

```
if isinstance(ptr, PointerTensor):
```
Your solution works too. I will do

Thank you so much@AlanAboudib `ptr.wrap()` should ` prt = ptr.wrap()` as it is not an inline operation.",1,True,2020-06-09 12:30:06,2020-06-14 19:02:43,2020-06-14 19:02:39
https://github.com/OpenMined/PySyft/pull/3684,[],Update polynomial operations,"Update polynomial operations## Description
Closes #3683
Non-same size polynomials are shifted so that the Numpy can perform operations correctly. 

## How has this been tested?
Added tests with operations on non same size polynomials.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests
This block is only for Plaintext-Plaintext addition. The encryption-decryption process does not require this as the size of the polynomial are fixed and equal in that case",1,True,2020-06-08 16:52:16,2020-06-09 05:37:12,2020-06-09 05:16:51
https://github.com/OpenMined/PySyft/pull/3682,[],Add mean and var methods for AST,"Add mean and var methods for AST## Description
Add mean and var methods for AST

We can maybe having the  `m` and `sum_value` initialization to just happen just once, why include it in the loop?

```suggestion
        sum_value = share.sum(**kwargs)
        m = next(iter(shares.values())).numel() // sum_value.numel()
        for worker, share in shares.items():
```Can you provide some more info on this :smile: :+1: 

This seems to be the correct way of ""variance""
What does torch do different?By default pytorch mean is unbiased, which is useful for some stats estimations
https://pytorch.org/docs/master/generated/torch.var.html
I've added support of both biased and unbiased :) This biased, they do it unbiased by default",1,True,2020-06-07 21:24:53,2020-07-14 08:48:54,2020-06-10 15:17:07
https://github.com/OpenMined/PySyft/pull/3680,[],Fix decorator issues for FSS in AST,"Fix decorator issues for FSS in AST## Description
Small fixes to better use comp operators and relu with function secret sharing
",1,True,2020-06-07 21:17:24,2020-07-14 08:49:17,2020-06-19 20:22:51
https://github.com/OpenMined/PySyft/pull/3679,[],Fix dtype issues in AST,"Fix dtype issues in AST## Description
Fix severe dtype issues ",1,True,2020-06-07 21:14:01,2020-07-14 08:49:26,2020-06-18 15:35:32
https://github.com/OpenMined/PySyft/pull/3678,[],Small dtype fix in securenn,"Small dtype fix in securenn## Description
Just a small fix

",1,False,2020-06-07 21:06:20,2020-09-15 07:05:07,2020-06-10 14:36:17
https://github.com/OpenMined/PySyft/pull/3675,['bug '],Change `whitelist` to `allow_list`,"Change `whitelist` to `allow_list`## Description
Swaps out `whitelist` for a better term.

## Checklist
- [X] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [X] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [X] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)",1,True,2020-06-06 16:38:27,2020-06-07 18:55:41,2020-06-07 18:55:36
https://github.com/OpenMined/PySyft/pull/3673,[],Plans wrapped frameworks fix,"Plans wrapped frameworks fix## Description
Fix issue with wrapped frameworks from plans

## Affected Dependencies
List any dependencies that are required for this change.

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I have followed the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md) and [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have labeled this PR with the relevant [Type labels](https://github.com/OpenMined/.github/labels?q=Type%3A)
- [x] My changes are covered by tests",1,True,2020-06-05 21:19:25,2020-06-06 17:18:11,2020-06-06 17:18:11
https://github.com/OpenMined/PySyft/pull/3666,"['improvement ', 'documentation ']",Simplify encrypted aggregation in the tutorials,"Simplify encrypted aggregation in the tutorials# Pull Request

## Description
In [Part 10 - Federated Learning with Secure Aggregation.ipynb](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2010%20-%20Federated%20Learning%20with%20Secure%20Aggregation.ipynb), the first step of ""encrypted aggregation"" is that copying the selected parameter.
Since the **Fixed Precision** and **Encrypt** are not in-place operations.
Without copying the parameters, we can still keep the plain parameters in the local.
Therefore, in my opinion, this step can be omitted.

## Affected Dependencies
No

## Type of Change
Please mark options that are relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Documentation (non-breaking change which adds documentation)
- [x] Improvement (non-breaking change that improves the performance or reliability of existing functionality)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I did follow the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md)
- [x] I did follow the [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have added tests for my changes

## Additional Context
No",1,True,2020-06-03 16:43:38,2020-06-04 12:22:51,2020-06-04 10:41:18
https://github.com/OpenMined/PySyft/pull/3664,"['bug ', 'priority: 1 - immediate :fire:', 'severity: 1 - critical :fire:']",Update AbstractSendable's send() method,"Update AbstractSendable's send() method# Pull Request

## Description
the `send()` method of `AbstractSendable` used the `send_obj` of `BaseWorker` instead of `send()`. The former does not return a pointer while the latter does.

## Affected Dependencies
List any dependencies that are required for this change.

## Type of Change
Please mark options that are relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] Documentation (non-breaking change which adds documentation)
- [ ] Improvement (non-breaking change that improves the performance or reliability of existing functionality)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I did follow the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md)
- [x] I did follow the [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have added tests for my changes

## Additional Context
Please also include relevant motivation and context.
",3,False,2020-06-02 23:23:42,2020-06-03 17:13:44,2020-06-03 17:13:32
https://github.com/OpenMined/PySyft/pull/3657,[],Update `syft-proto` to 0.4.7,"Update `syft-proto` to 0.4.7## Description
Update to pull in Protobuf support for `BaseDataset`, `FixedPrecisionTensor`, and a fix for integer `Action` arguments.

## Checklist
- [X] I did follow the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md)
- [X] I did follow the [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have added tests for my changes",1,True,2020-06-01 17:50:39,2020-06-01 18:50:52,2020-06-01 18:50:48
https://github.com/OpenMined/PySyft/pull/3646,['documentation '],Update notebook/tornado versions,"Update notebook/tornado versions# Pull Request
Fixes #3508.

## Description
When you install the ```jupyter notebook``` it will install the latest version, but there are some issues regarding this that are detailed [here](https://github.com/jupyter/notebook/issues/3397)

## Type of Change
Please mark options that are relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] Documentation (non-breaking change which adds documentation)
- [ ] Improvement (non-breaking change that improves the performance or reliability of existing functionality)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## How has this been tested?
- Check the #3508 and validate that the notebook can be seen.

## Checklist
- [x] I did follow the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md)
- [x] I did follow the [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have added tests for my changes

## Additional Context
This PR should solve the problem with the notebooks.",4,True,2020-05-31 02:54:05,2020-07-01 08:08:52,2020-07-01 08:08:51
https://github.com/OpenMined/PySyft/pull/3644,['bug '],"serde, torch: fix typo in numpy tensor serializer.","serde, torch: fix typo in numpy tensor serializer.# Pull Request

## Description
typo while detaching tensor in ` numpy_tensor_serializer`

## Affected Dependencies
List any dependencies that are required for this change.

## Type of Change
Please mark options that are relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] Documentation (non-breaking change which adds documentation)
- [ ] Improvement (non-breaking change that improves the performance or reliability of existing functionality)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I did follow the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md)
- [x] I did follow the [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [x] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have added tests for my changes

## Additional Context
Please also include relevant motivation and context.
",1,True,2020-05-30 18:14:17,2020-05-30 20:34:12,2020-05-30 19:11:42
https://github.com/OpenMined/PySyft/pull/3621,[],Functional encryption She,"Functional encryption She# Pull Request


## Description
Please include a summary of the change and which issue is fixed. If this closes one or more open issues, [please tag them as described here](https://help.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

## Affected Dependencies
List any dependencies that are required for this change.

## Type of Change
Please mark options that are relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Documentation (non-breaking change which adds documentation)
- [ ] Improvement (non-breaking change that improves the performance or reliability of existing functionality)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [ ] I did follow the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md)
- [ ] I did follow the [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have added tests for my changes

## Additional Context
Please also include relevant motivation and context.
",1,False,2020-05-28 10:51:14,2020-05-28 10:53:01,2020-05-28 10:53:01
https://github.com/OpenMined/PySyft/pull/3617,[],Support serialization of lists in Arg,"Support serialization of lists in Arg# Pull Request

## Description
This PR extends protobuf serde to be able to serialize Arg of ""list"" type.
This is required for Actions that have arguments of ""list"" type.

## Affected Dependencies
https://github.com/OpenMined/syft-proto/pull/120
Note: requirements is modified to use https://github.com/OpenMined/syft-proto/pull/120 branch as dependency.

## Type of Change
Please mark options that are relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Documentation (non-breaking change which adds documentation)
- [ ] Improvement (non-breaking change that improves the performance or reliability of existing functionality)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## How has this been tested?
- Describe the tests that you ran to verify your changes.
- Provide instructions so we can reproduce.
- List any relevant details for your test configuration.

## Checklist
- [x] I did follow the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md)
- [x] I did follow the [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have added tests for my changes

## Additional Context
Needed for FL MNIST example.",1,True,2020-05-26 21:43:42,2020-05-29 00:34:46,2020-05-29 00:34:42
https://github.com/OpenMined/PySyft/pull/3614,['bug '],Fix federated_avg side effect,"Fix federated_avg side effect# Pull Request

## Description
This PR aims to fix `federated_avg` side effects. Instead of copying the model, create a new instance of the model object and copy all the `__dict__` from the model that the instance was created.

## Affected Dependencies
List any dependencies that are required for this change.

## Type of Change
Please mark options that are relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] Documentation (non-breaking change which adds documentation)
- [ ] Improvement (non-breaking change that improves the performance or reliability of existing functionality)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## How has this been tested?
- Running the steps described in #3613  

## Checklist
- [x] I did follow the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md)
- [x] I did follow the [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [x] I have added tests for my changes",2,True,2020-05-26 17:46:51,2020-05-27 13:32:46,2020-05-27 13:32:45
https://github.com/OpenMined/PySyft/pull/3609,[],added connection speed,"added connection speed# Pull Request

## Description
Implementing the connection speed test for fl worker

## Affected Dependencies
`time`, `random`

## Type of Change
Please mark options that are relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Documentation (non-breaking change which adds documentation)
- [ ] Improvement (non-breaking change that improves the performance or reliability of existing functionality)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## How has this been tested?
- TODO

## Checklist
- [x] I did follow the [Contribution Guidelines](https://github.com/OpenMined/.github/blob/master/CONTRIBUTING.md)
- [x] I did follow the [Code of Conduct](https://github.com/OpenMined/.github/blob/master/CODE_OF_CONDUCT.md)
- [ ] I have commented my code following the [OpenMined Styleguide](https://github.com/OpenMined/.github/blob/master/STYLEGUIDE.md)
- [ ] I have added tests for my changes

## Additional Context
Please also include relevant motivation and context.
`start()` - does it need parentheses? it's probably typoMAYBE: Use constant variable for 64 as there is for the others? It might also good to use a constant variable name for 64?
Also, it might be worth specifying the size at the end of the variable name, like ```MAX_BUFFER_SIZE_MB```?MAYBE: Some better name for ```20```?NIT:
```
if speed_history:
    avg_speed = sum(speed_history) / len(speed_history)
    return avg_speed
else:
    return -1
```",2,True,2020-05-26 10:37:35,2020-07-21 18:40:12,2020-06-09 08:11:15
https://github.com/OpenMined/PySyft/pull/3606,[],requirements.txt updated with tornado compatible notebook version,"requirements.txt updated with tornado compatible notebook version## Description

Thank you for your contribution to the PySyft repository.
Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context.
There was a version conflict with the installation of jupyter notebook with tornado version.
In order to overcome the problem, the requirements.txt is updated with a notebook <6 version so that it is compatible tornado version <5
If your pull request closes a GitHub issue, then set its number below.

Resolves # (issue)
3546

## Checklist:
* [x] My changes are covered by tests.
* [x] I have run [the pre-commit hooks](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md#setting-up-pre-commit-hook) to format and check my code for style issues.
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).

(See the [the contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md) for additional tips.)",1,True,2020-05-25 12:57:10,2020-05-28 17:11:32,2020-05-28 17:11:32
https://github.com/OpenMined/PySyft/pull/3605,[],pip-dep/requirements.txt updated with required notebook version,"pip-dep/requirements.txt updated with required notebook version## Description

Thank you for your contribution to the PySyft repository.
Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context.
There was a version conflict with the installation of jupyter notebook with tornado version. 
In order to overcome the problem, the requirements.txt is updated with a notebook <6 version so that it is compatible tornado version <5


If your pull request closes a GitHub issue, then set its number below.

Resolves # (issue)
3546

## Checklist:
* [x] My changes are covered by tests.
* [x] I have run [the pre-commit hooks](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md#setting-up-pre-commit-hook) to format and check my code for style issues.
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).

(See the [the contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md) for additional tips.)",4,False,2020-05-24 23:21:34,2020-05-25 12:57:41,2020-05-25 12:55:32
https://github.com/OpenMined/PySyft/pull/3599,[],Remove protocol parameter in decrypt,"Remove protocol parameter in decrypt## Description

The PR remove the need to a `protocol` parameter in `decrypt` and warn the user to not use it, without raising an error.

Resolves #3329 


## Checklist:
* [x] My changes are covered by tests.
* [x] I have run [the pre-commit hooks](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md#setting-up-pre-commit-hook) to format and check my code for style issues.
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).

(See the [the contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md) for additional tips.)
Beware of the case where we have an AutogradTensorI thought that passing tests meant it's working fine, but yeah there was no tests for decrypting an AutogradTensor. Fixed that and added test for decrypting with/without grad",1,True,2020-05-23 13:07:44,2020-05-28 15:41:18,2020-05-28 15:41:14
https://github.com/OpenMined/PySyft/pull/3595,[],Implement msgpack serde for RoleAssignments,"Implement msgpack serde for RoleAssignments## Description

We cannot implement protobuf serde because workers can't yet be bufferized.

## Checklist:
* [x] My changes are covered by tests.
* [x] I have run [the pre-commit hooks](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md#setting-up-pre-commit-hook) to format and check my code for style issues.
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).

(See the [the contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md) for additional tips.)
I think this is a typo, should be RoleAssignament's.I think there might be a hidden bug when settings obj refs as default arguments (might not occur here, but I would personally avoid it just for future usage).

```
class Test:
    def __init__(self, test={}):
        self.test = test       

    def add_elem(self, k, v):
        self.test[k] = v

    def print(self):
        print(self.test)

t1 = Test()
t1.add_elem(""1"", 1)
t1.print()
# {""1"": 1}

t2 = Test()
t2.add_elem(""2"", 2)
t2.print()
# {""1"": 1, ""2"": 2}         
```The class name is plural, so there is no typo, I think (English is not my native language so I may be wrong though ^^)Oh yep, I already had this kind of bug! Nice catch üëç Bizarrely, this is the correct punctuation in English. It's a shit language and I make no excuses.",1,True,2020-05-22 12:34:22,2020-05-27 14:12:34,2020-05-27 14:12:30
https://github.com/OpenMined/PySyft/pull/3593,[],Handle with grid unexpected errors properly,"Handle with grid unexpected errors properly## Description
Add a proper way to handle unexpected exceptions/errors.
- Handle with unexpected exceptions during syft.grid.register function.
- Handle with unexpected behavior due to notebook async constraints.
""to handle"" -> ""handling""",1,True,2020-05-22 06:41:15,2020-07-21 18:40:12,2020-05-22 14:00:29
https://github.com/OpenMined/PySyft/pull/3591,[],Avoids multiple peers registration in the same grid environment.,"Avoids multiple peers registration in the same grid environment.## Description

I just added a `_registered_peer` variable to hold the very first grid peer registration. So, any register made later, within the same runtime, the same instance is returned.

Resolves #3576 


## Checklist:
* [ ] My changes are covered by tests.
* [x] I have run [the pre-commit hooks](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md#setting-up-pre-commit-hook) to format and check my code for style issues.
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).

(See the [the contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md) for additional tips.)
I was wondering if there is a need to throw an warning/error in the user's face if he tries to register multiple times. I think just returning the registered peer for the first time would be okay.  What do you think?Yep, I think we can return the registered peer instance created by the first call. But we probably need to print a warning (not an exception or error message) to clarify to the user that they already have a registered peer.",1,True,2020-05-21 22:46:57,2020-07-21 18:40:12,2020-05-22 15:14:18
https://github.com/OpenMined/PySyft/pull/3589,[],Print useful information on syft.grid.register(),"Print useful information on syft.grid.register()## Description

Print useful information during `syft.grid.register` execution.

If your pull request closes a GitHub issue, then set its number below.

Resolves #3573",1,True,2020-05-21 18:58:16,2020-07-21 18:40:12,2020-05-22 01:19:12
https://github.com/OpenMined/PySyft/pull/3567,[],FL MNIST example with autograd tracing,"FL MNIST example with autograd tracing## Description
Attempt to use autograd tracing (in its existing form) to trace MNIST example for static FL.
Initially, there's unit test that should pass.
Next, notebooks should be updated.

## Checklist:
* [x] My changes are covered by tests.
* [ ] I have run [the pre-commit hooks](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md#setting-up-pre-commit-hook) to format and check my code for style issues.
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).

(See the [the contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md) for additional tips.)
Outstanding!!!Would it make sense for this to be a lambda? Seems like a single use function.Is this terminology intended to differentiate dummy inputs from Placeholders?üëç No longer needed or worth keeping?Ah no. Bad naming, probably. `return_placeholder` may sometimes contain values with int, bool types. I.e. not `PlaceHolders` at all. This happens because in `self._fetch_placeholders_from_ids()` we leave values that are not `PlaceholderId` type unchanged.

So what I want is just filter out non-Placeholders because they cause runtime error further in `PlaceHolder.instantiate_placeholders(...)`

Maybe I'm fixing problem in wrong place and I should just disallow such actions from tracing at all... Not sure.
Example of such action is when you trace code like:
```
if tensor > 0:
  ...
```
This will be traced into Action like `True = var_1.__gt__(0)`.
Maybe `return_placeholders` should be renamed to something like `return_values` then? That would make it possible to rename `true_placeholders` to `return_placeholders`, and then all the names would be more accurate.This div code was made for ints, but it seems to works fine for tensor (with scalar value at least).
@LaRiffle do you see any problem with removing this assert?",2,True,2020-05-20 21:46:21,2020-05-24 20:20:24,2020-05-24 20:20:20
https://github.com/OpenMined/PySyft/pull/3565,[],Full Demo: Training an Encrypted Neural Network across Workers using Crypten (with jails),"Full Demo: Training an Encrypted Neural Network across Workers using Crypten (with jails)## Description

This PR add an experimental notebook showing how to train an encrypted neural network using crypten, it's based on [Tutorial7 of CrypTen](https://github.com/facebookresearch/CrypTen/blob/master/tutorials/Tutorial_7_Training_an_Encrypted_Neural_Network.ipynb).

Two issues were also solved to implement the demo correctly. First an issue regarding how the crypten computation is initiated in the syft context, and a second regarding how virtual workers are used.

### First issue

CrypTen computation is started from a master worker that will be responsible of running the master party which needs to bind and listen to a port for synchronizing between distributed parties, our initial design start a party (rank 0) on the local worker to be the master that will do the previous job, this introduced some complication with the API as users will list workers in the `run_multiworker` decorator and doesn't expect an additional party to be running on the local worker. We solved this by removing the need to start a local party, but the ip address and port referred on the decorator would be the one used by the first worker in the list (more work on this need to be done).

### Second issue

Using virtual workers doesn't parallelize the computation as calling send will just call recv and the execution of the commands sent to the VirtualWorker will just run sequentially on the main thread. Using threads to call operations on VirtualWorker (which was our case) doesn't solve the issue as well as most computation is cpu-bound, and python doesn't support threads as expected because of the Global Interpreter Lock. So we switched to using processes instead, this introduces some extra cost for creating these processes, but this shouldn't be a big deal if the memory is shared using copy-on-write, a deeper analysis of this may be required only if this start popping up issues.

### Results
With the previous changes, we were able to reproduce the same training as in the CrypTen tutorial with little changes, the running time is almost the same even for VirtualWorker due to the parallelism using processes.


## Checklist:
* [x] My changes are covered by tests.
* [x] I have run [the pre-commit hooks](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md#setting-up-pre-commit-hook) to format and check my code for style issues.
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).

(See the [the contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md) for additional tips.)
Q: It would be a good idea to also add a title and an author (you :P)?Maybe also add some ""comments"" (I think you can do this in a follow up PR) regarding what is happening in each cell?",4,True,2020-05-20 21:19:54,2020-06-04 08:41:48,2020-06-04 08:41:42
https://github.com/OpenMined/PySyft/pull/3563,[],Solve webrtc notebook async calls,"Solve webrtc notebook async calls## Description

Apparently, the jupyter notebook has some limitations for running code asynchronously using the asyncio lib _(more details below)_. This PR aims to replace `asyncio.run()` calls to solve event loop issues during async calls inside of a notebook environment.

Issue details:
_""Unlike terminal IPython, all code runs on asyncio eventloop, so creating a loop by hand will not work, including with magics like %run or other frameworks that create the eventloop themselves.""_ , [IPython Docs](https://ipython.readthedocs.io/en/stable/interactive/autoawait.html?highlight=nest#using-autoawait-in-a-notebook-ipykernel)

Related issues on jupyter notebook repository:
[Can't invoke asyncio event_loop after tornado 5.0 update](https://github.com/jupyter/notebook/issues/3397)
[asyncio.run() fails in the notebook](https://github.com/jupyter/notebook/issues/3397)",1,True,2020-05-20 17:26:26,2020-05-20 22:32:34,2020-05-20 22:32:34
https://github.com/OpenMined/PySyft/pull/3560,[],Fix webrtc submodule path issues,"Fix webrtc submodule path issues## Description

The webrtc sub-module cannot be found if the python environment starts from a different root path. This PR aims to fix that problem.

",1,True,2020-05-20 14:14:57,2020-05-20 15:15:24,2020-05-20 15:15:24
https://github.com/OpenMined/PySyft/pull/3525,"['bug ', 'priority: 2 - high :cold_sweat:']",Fix stray objects from FSS `Worker/Plan` initialization,"Fix stray objects from FSS `Worker/Plan` initialization## Description

The extra objects initialized on every worker for FSS cause weird behavior in tests and very non-obvious object counts in unrelated parts of the code.

Resolves #3464

## Checklist:
* [X] My changes are covered by tests.
* [X] I have run [the pre-commit hooks](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md#setting-up-pre-commit-hook) to format and check my code for style issues.
* [X] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).

(See the [the contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md) for additional tips.)
```suggestion
        for worker in workers.values():
``````suggestion
        for worker in workers.values():
``````suggestion
        for worker in workers.values():
``````suggestion
        for worker in workers.values():
```",5,True,2020-05-13 21:58:36,2020-05-14 16:03:08,2020-05-14 16:03:08
https://github.com/OpenMined/PySyft/pull/3511,['bug '],Add a missing `NodeClient` import to `AbstractGrid`,"Add a missing `NodeClient` import to `AbstractGrid`## Description

This shows up as a `flake` linting error on several PRs after recent changes to the linting rules.

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [X] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).
",1,True,2020-05-12 14:55:54,2020-05-12 15:55:39,2020-05-12 15:55:38
https://github.com/OpenMined/PySyft/pull/3506,[],Updated PR Template,"Updated PR Template## Updated PR Template 

The PR template contained a bunch of check-boxes that never got checked, which made open PRs appear unfinished.  

Update: 1)  Removed type of change checklist as it is supposed to be identified with a label.
              2)  Included few of the tasks from contribution in checklist.

Fixes #3489

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).
I think we should remove those checks.
This check should already be done by ```flake8``` or ```black```, right?

I think those stuff should already be caught if we merge [this PR](https://github.com/OpenMined/PySyft/pull/3434)This doesn't seem necessary for all PRsAlso, could you remove this :DDone sir ! Q: Could you also remove this.
Since there are changes which might not require this thing :DSure !",1,True,2020-05-12 05:07:28,2020-05-12 19:07:28,2020-05-12 18:44:48
https://github.com/OpenMined/PySyft/pull/3500,[],Update Part 13c - Secure Classification with Syft Keras and TFE - Pri‚Ä¶,"Update Part 13c - Secure Classification with Syft Keras and TFE - Pri‚Ä¶‚Ä¶vate Prediction Client.ipynb

## Description

Thank you for your contribution to the PySyft repository.
Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context.

I have reviewed the bengali translation of this tutorial and fixed some major bugs and incoherence in language. 
If your pull request closes a GitHub issue, then set its number below.

Fixes # (issue)

## Type of change

Please mark the options that are relevant.

- [x] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,True,2020-05-11 02:59:47,2020-06-01 11:26:45,2020-05-11 14:35:44
https://github.com/OpenMined/PySyft/pull/3483,[],Added support for notebooks in Autoscale,"Added support for notebooks in Autoscale## Description

Added support for autoscale on notebooks, previously it could only be run using a python script.
Now it can be run using both script (.py) and notebook(.ipynb).

@hericlesme 
#3099(issue)
## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).`gclouds` -> `gcloud`Changed `gclouds` -> `gcloud`",2,True,2020-05-09 17:16:06,2020-05-16 17:09:40,2020-05-16 17:09:39
https://github.com/OpenMined/PySyft/pull/3482,['bug '],Fix bug in get() of nn.Module ,"Fix bug in get() of nn.Module ## Description
When a model with particular layers (BatchNorm2D) is sent to a worker, it produces a KeyError when `copy()` is called on them.

To reproduce:
```python
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.l = nn.Linear(10,10)
        self.bn1 = nn.BatchNorm2d(64)
 
model  = Model()         
model_copy = model.copy()  #works fine

model.send(alice)
model.get()

model.copy()   #throws KeyError Exception

```
This is due we were not retrieving buffers along with parameters.

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",3,False,2020-05-09 15:55:10,2020-05-10 05:30:43,2020-05-10 05:30:43
https://github.com/OpenMined/PySyft/pull/3479,"['bug ', 'status: stale :bread:']",[WIP] Fix Identity Operations,"[WIP] Fix Identity Operations## Description
> when .flatten() is called on AST or a remote torch tensor it removes the original tensor from the worker

> I think the issue is that in torch the .flatten operation on 1d array is actually an inplace operation. You can do the above example with .view(-1) and it will work fine.
I have fixed the issue locally by modifying the is_inplace_method in torch_attributes.py

This can be triggered by any operation that is an identity operation as while hooking and registering the processed tensor, (due to some GC reasons) the earlier version of the tensor is deleted (de-registered).

For an identity operation, this deletes the original tensor that we are wokring with hence giving rise to such a probem

Fixes #3439 

- [x] Fix the issues
- [x] Fix stuff I break due to this
- [ ] Add tests

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",3,False,2020-05-09 08:24:43,2020-07-06 00:08:30,2020-07-06 00:08:30
https://github.com/OpenMined/PySyft/pull/3473,"['bug ', 'testing ']",Store the message log in `BaseWorker` in an unserialized format,"Store the message log in `BaseWorker` in an unserialized format## Description

This bypasses some issues where serialization/deserialization changes
the state or type of the objects passed to the serdes. That also needs
to be fixed, but this is a smaller and still sensible change in the mean
time.

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [X] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).
nit: can you put the comment one line above so that it looks better? :)",1,True,2020-05-08 15:08:00,2020-05-10 17:04:56,2020-05-08 16:33:58
https://github.com/OpenMined/PySyft/pull/3466,[],Update Part 13b - Secure Classification with Syft Keras and TFE - Sec‚Ä¶,"Update Part 13b - Secure Classification with Syft Keras and TFE - Sec‚Ä¶‚Ä¶ure Model Serving.ipynb

## Description

Thank you for your contribution to the PySyft repository.
Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context.

I have reviewed the bengali translation of this tutorial and fixed some major bugs and incoherence in language.

If your pull request closes a GitHub issue, then set its number below.

Fixes # (issue)

## Type of change

Please mark the options that are relevant.

- [x] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,True,2020-05-08 05:10:45,2020-06-01 11:26:45,2020-05-08 12:33:01
https://github.com/OpenMined/PySyft/pull/3462,[],Add a check for docstring coverage to the Github Action tests,"Add a check for docstring coverage to the Github Action tests## Description

Check that docstring coverage hasn't decreased below the current level on each PR.

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [X] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).
",1,True,2020-05-07 15:29:39,2020-05-10 17:04:48,2020-05-07 15:59:13
https://github.com/OpenMined/PySyft/pull/3454,[],Add async send_command to websocket client,"Add async send_command to websocket client**:warning: Is needed for https://github.com/OpenMined/PySyft/pull/3337**

## Description

Improves async support for grid nodes, by allowing to send async commands simultaneously to several remote servers.

Waiting for a global strategy to add tests, as tests require setting up a grid network.


## How to use it


```python
import asyncio
import syft as sy
import torch as th
from syft.workers.node_client import NodeClient

hook = sy.TorchHook(th)

alice = NodeClient(hook, ""ws://localhost:7600"")
bob = NodeClient(hook, ""ws://localhost:7601"")
charlie = NodeClient(hook, ""ws://localhost:7602"")
crypto_provider = charlie

me = sy.local_worker

my_grid = sy.PrivateGridNetwork(alice,bob,charlie)

x = th.tensor([-1., 2])
p_alice = x.send(alice)
p_bob = x.send(bob)

# async send command
p = asyncio.run(
    alice.async_send_command(
        message=('abs', p_alice.child, (), {})
    )
)
print(p)
p.get()

# async dispatch to several workers
results = asyncio.run(
    charlie.async_dispatch(
        [alice, bob],
        [('abs', p_alice.child, (), {}),
         ('abs', p_bob.child, (), {})]
    )
)
```Why a `*args` for each of these methods?```suggestion
        Note: Is subclassed by the node client when you use the GridNode
```

Typo :grin: Can we have this printed only on when a `verbose` flag is set?:+1: `type(ret_val) == bytes`
When does this condition take place? (just curious... :thinking: )I would really love to see this in practice. I've had problems in achieving async comms over websockets before. Oh, good question! Will rm and see if it's ok:+1:So it should not be bytes, but if I rm step 4 in  `async_send_msg` I'll get a ""None"" as bytes. We had this in the past, so I kept it like in worker/ base.pyI have wanted to make this change for so long. üíØ",2,True,2020-05-06 08:43:33,2020-05-08 20:49:40,2020-05-08 20:49:39
https://github.com/OpenMined/PySyft/pull/3452,[],Send models with @run_multiworkers,"Send models with @run_multiworkers## Description

After introducing Jails and the mechanisms for sending Crypten models across workers, we needed a way to send those models while initializing a Crypten computation using our `@run_multiworkers` decorator. 

With this PR, a function decorated with `@run_multiworkers([ALICE, BOB], master_addr=""127.0.0.1"", model=model, dummy_input=dummy_input)` where `model` is a `torch.nn.Module` and dummy input is a `torch.Tensor` that can be evaluated using the `model`, every party running on different workers will have access to the `model` inside their functions, which then should be an equivalent Crypten model.

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,True,2020-05-05 19:01:27,2020-05-09 10:39:35,2020-05-09 10:39:34
https://github.com/OpenMined/PySyft/pull/3451,[],Bump version to `v0.2.5`,"Bump version to `v0.2.5`## Description

Thank you for your contribution to the PySyft repository.
Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context.

If your pull request closes a GitHub issue, then set its number below.

Fixes # (issue)

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,True,2020-05-05 18:06:00,2020-05-06 05:47:02,2020-05-05 18:35:44
https://github.com/OpenMined/PySyft/pull/3449,[],Update Part 13a - Secure Classification with Syft Keras and TFE - Pub‚Ä¶,"Update Part 13a - Secure Classification with Syft Keras and TFE - Pub‚Ä¶‚Ä¶lic Training.ipynb

## Description

Thank you for your contribution to the PySyft repository.
Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context.

I have reviewed the bengali translation of this tutorial and fixed some major bugs and language incoherence. 

If your pull request closes a GitHub issue, then set its number below.

Fixes # (issue)

## Type of change

Please mark the options that are relevant.

- [x] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,True,2020-05-05 05:01:50,2020-06-01 11:26:45,2020-05-08 12:32:00
https://github.com/OpenMined/PySyft/pull/3445,[],Update Part 12 - Train an Encrypted Neural Network on Encrypted Data.‚Ä¶,"Update Part 12 - Train an Encrypted Neural Network on Encrypted Data.‚Ä¶‚Ä¶ipynb

## Description

Thank you for your contribution to the PySyft repository.
Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context.

I have reviewed the bengali translation of this tutorial and fixed some major errors and incoherence in language.

If your pull request closes a GitHub issue, then set its number below.

Fixes # (issue)

## Type of change

Please mark the options that are relevant.

- [x] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,True,2020-05-04 05:37:35,2020-06-01 11:26:45,2020-05-08 12:30:25
https://github.com/OpenMined/PySyft/pull/3444,[],Update Part 11 - Secure Deep Learning Classification.ipynb,"Update Part 11 - Secure Deep Learning Classification.ipynb## Description

Thank you for your contribution to the PySyft repository.
Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context.

I have reviewed the Bengali Translation of the tutorial which had some major errors and incoherence. 

If your pull request closes a GitHub issue, then set its number below.

Fixes # (issue)

## Type of change

Please mark the options that are relevant.

- [x] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",3,True,2020-05-03 23:17:17,2020-06-01 11:26:45,2020-05-30 01:24:42
https://github.com/OpenMined/PySyft/pull/3442,['bug '],Fix `move` PT to its own location,"Fix `move` PT to its own location## Description

> the move() method intended to throw an error, when trying to move an object to a virtual worker that already possesses the object

Fixes #3435 

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).
I think this should be handle at a lower level, maybe on the reception of a Communication Message.
Otherwise, we may need this kind of check at several places (for different methods doing doing stuff with pointers). What do you think?There's a similar check in the de-serialization, but I don't think it's sufficient to resolve this issue. Not sure where the right place for this is. I suppose we could add a decorator that wraps the function with this check and the one above? Not sure how else to avoid duplication.I don't think it should be done in serde, it's too late at this point.
I think it should be done on the worker side, when a worker receives the order to send a tensor (basically all the communication action will resume to that at the end, right?) it should check the source and destination of the communication and do things accordingly to that.
But it's just how I see it, maybe there's better?I think this is even before a message gets sent; it's checking whether the worker has been asked to move a tensor to itself and bypassing sending a message to itself in that case. I can't think of a better place to do this yet, but I'm guessing we'll figure it out as we rework the communication methods for `Protocols`. ü§∑",1,True,2020-05-03 16:01:05,2020-05-12 14:51:11,2020-05-12 14:51:11
https://github.com/OpenMined/PySyft/pull/3428,[],Fix double tracing bug,"Fix double tracing bug## Description

There was a bug that actions were duplicated when tracing a torch function on Placeholders.

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).
Q: This is needed such that the local worker will run the plan using the actions? (not calling the function directly)Yes, just a pseudo hack to really use the actions traced and not the forward function",1,True,2020-04-30 07:50:30,2020-04-30 13:12:01,2020-04-30 13:12:00
https://github.com/OpenMined/PySyft/pull/3427,"['bug ', 'testing ']",Increase sigmoid exp tol,"Increase sigmoid exp tol## Description
The ```sigmoid_exp``` test is failing (sometimes).
This PR increases the tolerance such that the test will pass.

Fixes #3345

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I have added tests for my changes (already there)
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).

## Extra:

* Have seen this failure [here](https://github.com/OpenMined/PySyft/pull/3422/checks?check_run_id=632038986)

* Tested with the values from the issue and from the above test run:
* 0.0605 / (0.065 * 0.9885) = 0.94 (bellow threshold) (how it failed in the PR)
* 0.0605 / (0.065 * 0.9890) = 0.94 (bellow threshold) (how it failed in the issue description)",1,True,2020-04-30 05:21:58,2020-04-30 11:29:04,2020-04-30 11:29:04
https://github.com/OpenMined/PySyft/pull/3422,[],Fix plans,"Fix plans## Description
Plans changed such that the operations are not traced with the ```Tracer``` anymore. The ```PlaceHolder``` class is used to trace the operations.

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).Currently, we add the ```get_plain_text```to the ```PlaceHolder``` in the Torch Hook.This is needed such that when we build the plan, we use another set of methods (currently only ```load``` is changed).This will be also added to the PlaceHolder (such that we track this operation)Q: Why is this needed here?Currently, there is a wrapped framework needed (and that will track the operations) -- in the ```syft/execution/tracing.py```Maybe a line of comment would be helpful to better understand the purpose of thisAs well here: make sure to explain why we need to add frameworks kwargsyou can also put this as a comment in the code I'd sayDoneDone```suggestion
            # This is needed because at building we use a set of methods defined in syft (ex: load)
```",1,True,2020-04-29 11:30:30,2020-05-03 12:31:01,2020-05-03 12:31:00
https://github.com/OpenMined/PySyft/pull/3418,[],Remove unused method .value(),"Remove unused method .value()## Description

Thank you for your contribution to the PySyft repository.
Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context.

If your pull request closes a GitHub issue, then set its number below.

Fixes # (issue)

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,True,2020-04-28 19:27:54,2020-04-28 19:56:53,2020-04-28 19:56:53
https://github.com/OpenMined/PySyft/pull/3415,[],Modify Bengali translated notebooks,"Modify Bengali translated notebooks## Description
I worked on the modification of Bengali translated notebooks[From Part 01 to Part 10(except 3,4 and 5)] 

Fixes # (issue)
The issue was closed.
- issue [#3042](https://github.com/OpenMined/PySyft/issues/3042). 

## Type of change

Please mark options that are relevant.

- [x] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",7,True,2020-04-28 17:09:30,2020-06-01 11:26:45,2020-05-08 04:58:25
https://github.com/OpenMined/PySyft/pull/3411,[],Utility functions to convert pytroch model to onnx and load them back to crypten,"Utility functions to convert pytroch model to onnx and load them back to crypten## Description

In order to train crypten models on different workers, we need a way to send crypten models to those workers. Here we take advantage of the build of crypten models which converts a pytorch model into onnx first (bytes) then build a crypten model from those bytes. Our sender worker will then stop at converting the pytorch model to onnx, then send those bytes through the network to the receiving workers which will then build the crypten model, voila !

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,True,2020-04-26 17:21:43,2020-05-03 13:03:26,2020-05-03 13:03:25
https://github.com/OpenMined/PySyft/pull/3406,[],Crypten hook logic shifted to hook_crypten.py,"Crypten hook logic shifted to hook_crypten.pyFixes #3385
Refractor (code shifted to new file)

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).Here we should have those lines, only that we should call the ```hook_crypten_module``` that is defined in the other file.

Also, in this file, we should remove the ```_hook_crypten``` and ```_hook_crypten_module``` methods@IamRavikantSingh sorry...This is needed (my bad :( ). You can move it where @youben11 suggsted. We use another load function syft than what it is used in CrypTen because we use the tags to properly load the necessary data.",3,True,2020-04-25 18:51:40,2020-05-13 05:15:30,2020-05-13 05:15:29
https://github.com/OpenMined/PySyft/pull/3405,[],Auto-register Plan Torchscript translator,"Auto-register Plan Torchscript translator## Description

In #3360 torchscript translator is auto-registered on Plan inside `syft.execution.translation.torchscript`.
This was relying on the fact that `PlanTranslatorTorchscript` is imported somewhere in `__init__.py` files when Syft is imported. 
At some point `PlanTranslatorTorchscript` import was removed. 
This change registers `PlanTranslatorTorchscript` inside `syft.execution.plan`.

Additionally, it fixes FL Training Plan experimental notebooks to use `sy.make_hook` instead of `sy.hook`.

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",3,True,2020-04-25 02:26:54,2020-04-26 17:44:05,2020-04-26 17:44:04
https://github.com/OpenMined/PySyft/pull/3403,"['bug ', 'priority: 2 - high :cold_sweat:']","Garbage Collection in `execute_computation_action`, Fix Inplace-Operation Identification","Garbage Collection in `execute_computation_action`, Fix Inplace-Operation Identification## Description

Fixes #3397 
Fixes #3398 

~Currently breaks inplace computations. WIP üößüöß~
- [x] GC
- [x] Fix inplace operations

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).Does this inherently require copying because operations later in this function are in-place? Could it be avoided by changing the lines below that read something like `values = values.some_op()`?Could you add a test for this regex?What causes the original PT not to get GC'd? Unclear from this comment.If there's work left to be done, better to create an issue than a TODO.Removed (it was addressed)Updated :smile: Done :+1: Removed (redundant operation)üëç",2,True,2020-04-24 17:18:22,2020-04-28 15:09:39,2020-04-28 15:09:39
https://github.com/OpenMined/PySyft/pull/3399,[],Avg & Maxpooling with SMPC,"Avg & Maxpooling with SMPC## Description
resolves issue #2573 
overloading torch.nn.F.max_pool2d and avg_pool2d in order to use it in architectures like ResNet

## Type of change

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).```suggestion
from syft.frameworks.torch.nn.functional import conv2d
from syft.frameworks.torch.nn.functional import maxpool2d
from syft.frameworks.torch.nn.functional import avgpool2d
```
Just to be consistent with `__init__.py` files eveywhereIt be good if the `else` captures just the `mean` arg with other modes leading to raising Exceptions`torch.nn.functional.max_pool2d` currently supports both 3D and 4D tensors. So might be good to add compatibility for 3D tensors too. üëçüëçthat's an initial version to get it to work then we'll iterate on it why do you think that's necessary? is there a use case that benefits from that?well spotted! I actually thought about that but I made it that way to avoid writing a test case for the expectation üòÇ will do it next commitDoesn't change the functionality in anyway. It's just conventional to do so üòÑYou can use a more precise type of Error, ValueError might be appropriateCan we have this done before we merge? I guess it might be work if we have something like 
```python
if len(tensor.shape) == 3:
    return squeeze(pool2d(unsqueeze(tensor))
```
or something like that??",3,True,2020-04-23 22:07:04,2020-04-26 20:12:19,2020-04-26 20:12:19
https://github.com/OpenMined/PySyft/pull/3396,[],Fix bug with AST.dtype serde,"Fix bug with AST.dtype serde## Description

Thank you for your contribution to the PySyft repository.
Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context.

If your pull request closes a GitHub issue, then set its number below.

Fixes # (issue)

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,True,2020-04-23 16:44:54,2020-04-23 17:57:08,2020-04-23 17:40:39
https://github.com/OpenMined/PySyft/pull/3393,[],Test placeholder forward,"Test placeholder forwardAdd a missing test for checking if the placeholder (from the plans) is correctly forwarding an attribute if it is not found in its class.
Taken from [here](https://github.com/OpenMined/PySyft/pull/3377)Needed because this is run on the remote worker and it seems is not seen by the coverage package :\I'm probably missing something, but it doesn't seem like this test requires the other Crypten-related changes in this PR?Currently, I want to make sure I got the coverage rate > 95% (and see what else it fails on the tests). After that, I want to rebase the CrypTen branch with what it is on the master (and the test with placeholder should disappear).",4,False,2020-04-23 08:52:33,2020-06-04 09:49:51,2020-06-04 09:49:50
https://github.com/OpenMined/PySyft/pull/3392,[],"Drop `custom` support in SecureNN, Flexibility over dtype in ASTs","Drop `custom` support in SecureNN, Flexibility over dtype in ASTs## Description
Fixes #3389 

- [x] Assert SecureNN dtype is not `custom`
- [x] Flexibility over dtype in ASTs

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).Can we also support int32, int64 and the associated torch dtypes? https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtypeTo be more flexible for new users```suggestion
    assert (
        alpha_sh.dtype == x_sh.dtype == y_sh.dtype != ""custom""
    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""

``````suggestion
    assert (
        x_sh.dtype == y_sh.dtype != ""custom""
    ), ""`custom` dtype shares are unsupported in SecureNN, use dtype = `long` or `int` instead""
```",5,True,2020-04-23 08:24:26,2020-05-03 15:28:36,2020-05-03 15:28:36
https://github.com/OpenMined/PySyft/pull/3386,[],Adding a Protobuf/Msgpack Serializable Interface,"Adding a Protobuf/Msgpack Serializable Interface## Description

Currently, if you want to enrol a type into a protobuf schema, you have to edit syft/serde/protobuf files, which is not very natural and might not scale in time. This solution generates those structures at runtime by checking which class is implementing the interface.

~placed the interface in syft/interfaces, placing it in syft/serde will create some awkward cyclic 
dependency import problem.~ Solved this one by removing AbstractWorker from syft/serde/serde.py and making the msgpack import local.

~currently, I'm exploiting the fact that in __subclasses__ we find only the classes that directly inherits the interface. By doing this, I can be sure on the fact that the class won't rely on the implementation from its parent class and implement it's own. (so we won't have to deal with the inheritance chain at unbufferize). This idea could be bad, because of having to inherit every time ProtobufInterface. Any feedback would be great.~ Thanks @gmuraru + @karlhigley  for the cool tips! :)

* merged the two interfaces into one.

* removed global code might rely on runtime variables, wrote init functions for global variables in serde/probuf/serde.py, serde/protobuf/proto.py and serde/msgpack/serde.py. 

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).üëç 

Can we apply this same class to msgpack serialization with `simplify`/`detail` and call it something like `Serializable`?Sure! I could add a wrapper for libraries as well to have similar behaviour.Libraries?Instead of keeping a list with those objects from libraries (torch, numpy) we can use in msgpack/protobuf, it might be useful to have a wrapper with the same behaviour as this interface? Might be overkill.These should probably live in the `serde` package with the other serialization code.We have an outstanding TODO to vaporize `TrainConfig`, which is definitely out of scope for this PR, but I'd avoid importing it here just so we don't give the impression that people should be using it.For the sake of brevity, could we combine these interfaces and call the resulting class `SyftSerializable` or something like that?üöÄ üé∏ üöÄ üé∏ üëç Good catch!I could to that, but it will generate the horror named cyclic dependency, I could hack it with a try and catch, I saw it in a few places around the codebase. (or we want to get rid of such hacks?).Will do, the only problem is that because of global code usage in serde.py, the usage of that interface requires that all of the classes that uses the interfaces are loaded before the global code execution (by adding this import I forced those classes to be loaded for sure, I cand add that file in a more nested `__init__` one). I'll think of some workaround on it or maybe we want to remove the global code at the expense of some performance? (or maybe at least do some caching, like, we wait for actual execution and afterwards we store the result globally)I have no idea why I haven't done this in the first place. Nice one.This seems correct, given the focus of this PR. I think what's going to happen soon (due to work with `Protocols`) is that `Role` will become an `AbstractObject`. Anyway, üëç I think this is already covered, since `AbstractObject` is `SyftSerializable`.‚ù§Ô∏è I think this is already covered too. üéâ Could we call this function when the `msgpack` module is loaded? Laziness kinda makes sense, but it means we have to call this function in a bunch of different places.This looks a lot better. üëç Good catch! I think this has been the source of a lot of circular dependencies we've encountered. üëç Same thing here as with `msgpack`; thinking it would be better to call this function when the module is loaded so we can call it once and not to have to invoke it in all the serialization methods.üíØ If it won't block this PR, we could create an issue and kick this part over to the Crypto team.It looks like this changes the call signature of the serialization strategies, but I'm not sure I saw a change to the strategies themselves. Do these still match up?Yes, I wanted to detach the AbstractWorker import from the file to prevent the circular import, I'll think for a way to bring this back.This makes us rely on order of initialization and could create really awkard and hard to find bugs, for example, let's say that when we are loading the first time syft, syft/serde/msgpack/serde.py should be the last one to be loaded (so that all the classes should be loaded to actually get themselves enroled in the interface). I enjoy this lazy init because: 1. forces everybody to use the public API, not the global variables, 2. 100% safe, we are generating those global variables when syft is fully loaded and the interface has all it's subclasses enrolled. The other way would be to mess around in `__init__.py` files, I think it may be better to leave those alone.Makes sense re: avoiding a circular import. I just want to make sure these type annotations are correct(-ish). Maybe it could be `Callable[[object, object], bin]` for now?Do you think this should be an abstract class and its methods abstract methods?Is this here to avoid some kind of circular imports?I'd also prefer if it was done in an __init__.py file but if you think it's too complex, I'm ok with leaving it for now These are all good points! What if the global was an object with a getter that did the lazy initialization inside? Then we wouldn't have to call the `init_global_vars` function in so many places, because it would be built in to the getter.Oh, good catch. Yeah, this should never be instantiated directly. What do abstract methods look like in comparison to raising `NotImplementedError`?I think you just have to declare the method with an `abstractmethod` decorator and do nothing in it.
A difference is that an error will be raised as soon as you create an instance of a class inheriting from the abstract one if it doesn't implement all the abstract methods.
At least, that's what I remember from abstract classes in Python ^^That sounds good. üëç (The three of us write some pretty decent Python together.) üòÜ I'll create an issue after the merge.This will generate some headache on method resolution order (MRO), for example:

```
SyftSerializable(ABC)
AbstractWorker(ABC, SyftSerializable)
```

We should drop the ABC from AbstractWorker by doing this (and all the ABCs around the codebase that implements SyftSerializable). Might be a cool idea, not sure if we want it, not sure how it will behave when somethings like this happens, I need to make a few experiments: (here we can't drop Interface2 from the inheritance chain of Interface3)

```
Interface1(ABC)
Interface2(Interface1)
Interface3(Interface2, SyftSerializable).
```@karlhigley  any thoughts about this?Yes, as a temporary solution, we'll leave it here and figure out a solution to break those circular changes in a more elegant way.My problem is here:
```
from syft.serde.syft_serializable import SyftSerializable, get_protobuf_subclasses

print(len(get_protobuf_subclasses(SyftSerializable))) #12

class Example(SyftSerializable):
    @staticmethod
    def simplify(worker, obj):
        pass

    @staticmethod
    def detail(worker, obj):
        pass

    @staticmethod
    def bufferize(worker, obj):
        pass

    @staticmethod
    def unbufferize(worker, obj):
        pass

    @staticmethod
    def get_protobuf_schema():
        pass

print(len(get_protobuf_subclasses(SyftSerializable))) #13
```
If those globals were generated and a new class is added, the globals will not reflect this, but I figured a way to do this properly I think without having inits everywhere/checks/performance impact. (currently, i'm solving this only if the classes are a part of syft, but in time that might not be the case).What happens if we switch the order of the parent classes?
```
AbstractWorker(SyftSerializable, ABC)
```
Seems like MRO should work out in that case?",1,True,2020-04-21 20:33:43,2020-05-11 19:44:29,2020-05-11 19:44:29
https://github.com/OpenMined/PySyft/pull/3384,['bug '],[WIP] Add `Size` method to `TorchTensor` (native),"[WIP] Add `Size` method to `TorchTensor` (native)## Description
>After sending tensor to a worker the size() call returns torch.Size([0]) while .shape returns correct value.

Adds a `.size()` method to `TorchTensor`
Fixes #3382 

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,False,2020-04-21 17:46:28,2020-05-05 13:21:56,2020-05-05 13:21:25
https://github.com/OpenMined/PySyft/pull/3383,"['bug ', 'new feature ']",Fix `Plan` tracing that involves `backward()`/autograd,"Fix `Plan` tracing that involves `backward()`/autograd## Description

For training to work on web/mobile, we need `Plan` tracing to be able to handle training, which involves calling `backward()` on a model inside `Plan`. It's not working currently, so this PR adds a failing test as a seed for developing a fix.

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [X] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [X] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).üëç 

This seems like the kind of thing that could live in `Action`, so that it can be shared across `Action` types.Hadn't considered that we might want to do `copy`ing inside `Plans`!Not sure I follow this comment. What wouldn't work if the chain was `Placeholder > AutogradTensor > ... other tensors in chain ...`?This part seems partially redundant with the new method on `ComputationAction`. Can this delegate to the `__str__` methods on the component classes?

Also, can it be a `__str__` method itself?This seems like two tests rolled into one. The Torchscript translation part should probably go to `test_translation.py`.I recall you mentioned that you don't want to capture `send` action in `placeholder.send`, but rather in same place as other commands are captured.
Perhaps we should try doing same for copy?If placeholder comes before autograd in chain, trace will record `backward` method call on autograd tensor, but not anything that happens inside backward. So trace will just have `backward` command, which won't work in torchscript.
We'll probably want to do same thing with other custom tensor types, so all ops are captured right before the torch tensor.I think it's fine for now.Hmm, okay. Let's leave that for a later PR.I checked CommunicationAction, it seems to have different structure.
If we'll have such method in Action, it will need to distinguish between communication/computation.
It's probably better for communication to have own method?Some time earlier code representation was removed from `__str__` so I figured it's better to make it separate method.
Removed duplication by using communication action's method to create string representation.
Though, it's not very elegant because single communication action doesn't track placeholder's names, so we need to pass variable names.The structure is different, but they should at least partially converge. This method doesn't seem to be very structure dependent though, other than the list of attributes to `stringify`. Seems like there could be a common, general method backing them both, maybe with args to specify which fields to `stringify` or something.

Could happen here or be deferred for a later PR. ü§∑ Hum, I'm not sure having a trace_autograd attribute and a trace_autograd arg in build is the best solution. I think that you did that because of `__call__` but maybe both could use the attribute? Or `__call__` could also take it as an argument even if it seems oddWhy is this called code and not `__str__`?Just a side comment:
With these, are Placeholders now Tracers? ^^There was something like that in `__str__` before and it was removed. I guess it might be too large and inconvenient for str representation. `.code` is inspired by pytorch's torchscript module :)This part seems weird to me too. Starting to think `Placeholder` has too many responsibilities, and maybe we should split placeholding and tracing into separate classes.`trace_autograd` is Plan build option, so I added it as kward in `build` method.
And also it can be specified with decorator. So you can specify it when building the Plan both ways.Hmm, why Tracers? `.insert` is similar to `.create_from`, with extra arg to create Placeholder deeper in chain.
Because `Placeholders` weren‚Äôt designed to go deeper in the chain, they were supposed to hold the place of a tensor chain. Putting them deeper in the chain makes tracing work, but changes how the class is used in a way that makes the name inaccurate.(To be clear, I‚Äôm still okay with merging this and resolving the naming issue later, but I do see an issue. It might even be the one @Jasopaum is referring to.) :smile:I agree with what was said here üôÇ ",5,True,2020-04-21 15:08:38,2020-05-05 10:27:00,2020-05-05 10:26:59
https://github.com/OpenMined/PySyft/pull/3380,[],Add Syft version checking,"Add Syft version checking## Description

One of the most common errors reported by PyGrid users is the Syft version mismatch between the grid nodes and the user environment. The purpose of this PR is to provide a debugging mechanism for this error.

## Type of change

Please mark the options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [X] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",1,True,2020-04-20 15:10:55,2020-04-21 15:00:30,2020-04-21 15:00:30
https://github.com/OpenMined/PySyft/pull/3377,[],Add Crypten changes to master,"Add Crypten changes to master## Description
Update master with the core changes made on the crypten branch

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I have added tests for my changes -- the already existing tests should pass
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [x] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).When was this happening?Remind me what this was for? IIRC, it was proposed as a way to do tracing, but maybe it has other uses too. ü§î This one is for accessing methods that are necessary in the AbstractTensor class (the PlaceHolder is a child of that class).
In case we have ```PlaceHolder > CrypTensor```, we might call methods from CrypTensor that are not necessary for PlaceHolder.
Another way to do this is to hook all methods from CrypTensor (this to be done in the crypten branch), but in that scenario, we might have methods that might not be generic -- like ```get_plain_text``` (this is implemented only in CrypTensor)Copy pasted from the Crypten branch merge :3 
The trace for the error was this:
```

    @pytest.fixture(scope=""session"", autouse=True)
    def hook():
>       hook = TorchHook(torch)

test/conftest.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
syft/frameworks/torch/hook/hook.py:145: in __init__
    self._hook_native_tensor(crypten.mpc.MPCTensor, TorchTensor)
syft/frameworks/torch/hook/hook.py:277: in _hook_native_tensor
    self._hook_native_methods(tensor_type)
syft/generic/frameworks/hook/hook.py:102: in _hook_native_methods
    native_method = getattr(tensor_type, attr)
../../../miniconda3/envs/pysyft/lib/python3.7/site-packages/crypten/cryptensor.py:39: in __getattribute__
    dummy = cls(None)
../../../miniconda3/envs/pysyft/lib/python3.7/site-packages/crypten/mpc/mpc.py:73: in __init__
    super().__init__(requires_grad=requires_grad)
../../../miniconda3/envs/pysyft/lib/python3.7/site-packages/crypten/cryptensor.py:132: in __init__
    self._reset_gradients()
```

What (I think is happening) -- when we hook we use the ```getattr```  -- but that method is changed in the CrypTensorMetaclass (metaclass for the MPCTensor). The problem is when we hook a method and that method is called in the ```getattr``` (in getattr we call an already hooked method and we do not have the syft.hook initialized -- syft.hook is initialized at the end of the TorchHook init method)It is not necessarily a bug, but it might cause a name collision when import hook from syft (in case we do not have the hook initialized syft.hook will be a function)Medium-term plan for `Placeholder` is to make it a plain wrapper that no longer inherits from `AbstractTensor` and uses roughly this approach to forwarding method calls, so I'm not opposed in general. Just wondering why we need it in `master` if it's currently only used for Crypten, which couldn't be merged into `master` last I knew.I was thinking it is a good idea to sync with master the changes that affect the plans, roles or core functionalities (such that the master will have those changes as fast as possible).

Those changes are some (possible) solutions to some problems we faced when merging some PRs.
I was thinking it might be a good idea to have those changes (that affect the syft core) into the master branch.",2,True,2020-04-20 12:39:43,2020-04-24 12:36:19,2020-04-24 12:36:19
https://github.com/OpenMined/PySyft/pull/3376,[],Rework `PlanTranslatorTorchscript` so it doesn't strip `Parameters`,"Rework `PlanTranslatorTorchscript` so it doesn't strip `Parameters`Resolve Conficts, merge with master
>It turns out `jit.trace()` strips `Parameters` out of the tensor chain,
which causes issues with `fix_precision`. Don't fully understand why
`jit.trace()` does that, but operating on a copy of the original Plan
avoid mutating the original State tensors, which fixes the issue.
@karlhigley #3375",2,True,2020-04-19 18:20:42,2020-04-19 19:31:10,2020-04-19 19:31:10
https://github.com/OpenMined/PySyft/pull/3375,[],Rework `PlanTranslatorTorchscript` so it doesn't strip `Parameters`,"Rework `PlanTranslatorTorchscript` so it doesn't strip `Parameters`## Description

It turns out `jit.trace()` strips `Parameters` out of the tensor chain,
which causes issues with `fix_precision`. Don't fully understand why
`jit.trace()` does that, but operating on a copy of the original `Plan`
avoid mutating the original `State` tensors, which fixes the issue.

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [X] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [ ] I have added tests for my changes
* [ ] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).",4,False,2020-04-19 17:01:57,2020-04-19 18:35:59,2020-04-19 18:35:59
https://github.com/OpenMined/PySyft/pull/3374,[],Hooking crypten.nn.Module,"Hooking crypten.nn.Module## Description

In a parallel PR #3365 , I was wrapping the crypten model and adding functionalities such as sending and sharing crypten models. In this PR, I add the same features by hooking into `crypten.nn.Module` instead of wrapping it.

## Type of change

Please mark options that are relevant.

- [ ] Added/Modified tutorials
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

## Checklist:

* [x] I have added tests for my changes
* [x] I did follow the [contribution guidelines](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md)
* [ ] I have commented my code following [Google style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).Q: Maybe it is worth to have an issue created to move the ```CrypTen``` hooking functionality to a different file? (this file would then import specific functions and call them) -- Maybe in that way we have a clear separation between the ```TorchHook``` and the ```CryptenHook```?Q: Should be ```module``` or ```model```?I kept the same naming as the method hooked in torch.nn.Moduleyeah I definitely think so, and also giving the option to disable crypten hooking (`syft.TorchHook(th, hook_crypten=False)`) NIT: Big ""O"" for consistency :3Here should overload ```share```Q: The ```move``` operation changes the owner? If so maybe we should also check the ownerAgreed!These functions are identical for torch & crypten, maybe we could factorize their usage?Created this issue -- maybe we can add it as a comment?
https://github.com/OpenMined/PySyft/issues/3385Q: Can't we simply do ```return nn_self.parameters()[0].owner```Q: Also hereQ: Does it make sense to have a separate test for ```module.copy```?Pretty much the same, I just prefered followed the same style with the hooking of torch.nnYeah, I will add a separate test@LaRiffle I Agree ! would it make sense to put them in hook_module.py and import them to be used here?We agreed with George on refactoring all the functioning of crypten hooking to #3390",1,True,2020-04-19 08:33:25,2020-04-22 15:01:08,2020-04-22 14:54:08
https://github.com/OpenMined/PySyft/pull/3373,[],"Revert ""Translate Plan to torchscript""(#3361) and ""Handle torchscript arg""(#3360)","Revert ""Translate Plan to torchscript""(#3361) and ""Handle torchscript arg""(#3360)The failures in tests related to `Plans` of `nn.Module` where `nn.Parameter` not getting hooked successfully is attributed to changes in these commits. Somehow the tests related to the issue passed in the tests of this PR ü§îü§îü§î

#3360 
#3361 
## Tested with this Code
```python
import torch as th
import syft as sy

hook = sy.TorchHook(th)
bob = sy.VirtualWorker(hook, id=""bob"")
alice = sy.VirtualWorker(hook, id=""alice"")
sharan = sy.VirtualWorker(hook, id=""sharan"")
crypto_provider = sy.VirtualWorker(hook, id=""james"")

torch = th
syft = sy

class Net(sy.Plan):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = th.nn.Linear(1, 1)

    def forward(self, x):
        return self.fc1(x)


plan = Net()
plan.build(th.tensor([1.2]))
original_weight = plan.fc1.weight.clone()

plan.fix_precision()
plan.fc1.weight.float_prec_()
# ^^^ üíÄ Raises AttributeError: 'Parameter' object has no attribute 'child' üíÄ

assert (plan.fc1.weight - original_weight) < 10e-2
```
<br>

[An example of the tests failing](https://github.com/OpenMined/PySyft/runs/595917181)

I've made sure this test is **not flaky** by running in a loop each with a different seed (just to be on a safe side as anyway seeds won't affect the presence of attributes of an object üòÖüòÖ)",5,False,2020-04-18 15:50:54,2020-04-19 17:18:50,2020-04-19 17:18:50
https://github.com/OpenMined/PySyft/pull/3372,"['bug ', 'testing ']",Skip the privacy attack notebooks during notebook tests,"Skip the privacy attack notebooks during notebook testsThe black box model inversion notebook is downloading EMNIST and using
up all the space in the automated test environments.",2,True,2020-04-17 15:31:52,2020-04-17 16:18:38,2020-04-17 16:18:38
https://github.com/OpenMined/PySyft/pull/3370,['bug '],Don't assume `self` is in `known_workers` in `BaseWorker.serializer`,"Don't assume `self` is in `known_workers` in `BaseWorker.serializer`That assumption makes it possible to have a case where `known_workers`
is empty, which leads `len(frameworks) == 1` to be `False`, which
defaults to the wrong serialization strategy when `self.framework` is
`torch`.

Fixes #3359",3,True,2020-04-17 14:46:59,2020-04-17 16:20:02,2020-04-17 16:20:02
https://github.com/OpenMined/PySyft/pull/3366,['bug '],Restrict `pytest-randomly` version to avoid `3.3.0`,"Restrict `pytest-randomly` version to avoid `3.3.0``pytest-randomly 3.3.0` adds support for `pytest-xdist` but does so in a
way that requires `pytest-xdist` to be installed, which we don't need.
It's probably an accidental change, but until it's resolved we'll just
avoid that version (which breaks our builds.)",3,True,2020-04-15 15:00:26,2020-04-15 16:45:02,2020-04-15 16:45:02
https://github.com/OpenMined/PySyft/pull/3364,[],Add method to add/remove operations from base worker,"Add method to add/remove operations from base workerFixes #3363 

Some initial changes to add the mechanism to add/remove methods from a base worker such that it will be able to support the implemented CrypTen operations.

Currently, there is added only CrypTen support, but I choose to implement it in this way (is only an initial draft) such that we could easily add more frameworks in the future.

WIP: Need to add tests to make sure the add/remove mechanism work as intended.

Probably here the import could be globalUse your `supported_frameworks` dict :) maybe don't re-add it if it's already thereDoneWhat exactly is involved in 'Adding crypten support?"" Why would it not be supported by default?There was a discussion [here](https://github.com/OpenMined/PySyft/pull/3254).
Also, added screenshots - I hope I did not miss any information
[ss1](http://swarm.cs.pub.ro/~gmuraru/OpenMined/pic1.png)
[ss2](http://swarm.cs.pub.ro/~gmuraru/OpenMined/pic2.png)

**TL;DR:** We discussed that maybe the framework information might not be contained in the ```BaseWorker``` and we should have a mechanism to register those ""specific framework methods"". If this is not wanted, I can switch to something else  :smile: (Pinged @LaRiffle @karlhigley  in case I missed something)I left it into the method, because making it global it would create an import error.

```worker_support.py``` needs BaseWorker which is not defined :\This should be removed after merging the [Fix plan](https://github.com/OpenMined/PySyft/pull/3422) changesI wonder if we should remove crypten support?.search has changed and now no longer returns pointers right?it's request_search which doesWhat is the role of `types.MethodType` ?Ah ok :) https://stackoverflow.com/questions/37455426/advantages-of-using-methodtype-in-pythonI'm not sure this scales: maybe you could replace the list [add_support_to_worker, remove_support_from_worker] by a dict with ""standard"" keys, so that if you need to has more on them you don't mess up with indicesDoneDoneHmmm...```request_search``` is used if you want to take a plan from the worker that built the plan, right?

In this scenario, the plan is already sent after it was built (to each worker) and the worker will simply search for the plan in the objects they have.We can go for the 0.4.1 which got released after this oneQ: I didn't get why you always change ports George :pWhat about adding the framework name in these functions?

```suggestion
from syft.frameworks.crypten.worker_support import add_support_to_worker as add_crypten_support, remove_support_from_worker as remove_crypten_support


supported_frameworks = {}

if dependency_check.crypten_available:
    supported_frameworks[""crypten""] = {
        ""add_support"": add_crypten_support,
        ""remove_support"": remove_crypten_support,
    }
```Ahh...Because while testing I get the error ""port already in use"" (if the tests fail...I can change it back)Hmm, first I want to merge this [branch](https://github.com/OpenMined/PySyft/pull/3453) and then this should be fixed :DSo you suggest the following:
If we want to run with a plan/jail we add the support to workers and this support should not be removed, until explicitly the user requests thatDoneAh, that's because you have processes still running in the background. I have a script in my workspace that I call if processes keeps running, it's a single command script, might be helpful:

```bash
#!/bin/bash

kill -9 `ps |grep python | awk '{$1=$1};1' |cut -f1 -d ' '`
```### TODO: Don't remove this.Will change this back since the current PR does not have some changes required for tests to be able to pass -- will bump the version in a future PRAll good!",4,True,2020-04-15 13:05:40,2020-05-13 16:59:16,2020-05-13 16:59:16
https://github.com/OpenMined/PySyft/pull/3358,[],Adjust comparison thresholds in `test_instantiate_tfe_layer`,"Adjust comparison thresholds in `test_instantiate_tfe_layer`This test has a known issue with sampling from a normal distribution
that might land close to zero, making small absolute errors result in
large relative errors. This bumps the relative tolerance and adds an
absolute tolerance to prevent the test from accepting large absolute
errors on large values that result in small relative errors.

More on this in [Issue #3344](https://github.com/OpenMined/PySyft/issues/3344#issuecomment-612652546).",1,True,2020-04-14 17:47:21,2020-04-14 18:09:21,2020-04-14 18:09:21
https://github.com/OpenMined/PySyft/pull/3357,"['bug ', 'testing ']","Revert ""Translate Plan to torchscript on build time""","Revert ""Translate Plan to torchscript on build time""Reverts OpenMined/PySyft#3336

Unfortunately, this was merged when the tests weren't passing, and it breaks a few `Plan` tests that are now blocking other PRs. Reverting for now, until the `Plan` tests can be fixed.",3,True,2020-04-14 15:55:27,2020-04-14 16:18:38,2020-04-14 16:18:27
https://github.com/OpenMined/PySyft/pull/3356,"['bug ', 'testing ']",Fix flakiness in `test_local_remote_gradient_clipping`,"Fix flakiness in `test_local_remote_gradient_clipping`This test was implicitly relying on `torch.Tensor's` approximate `eq`
when comparing a tensor with a float. Instead, let's compare floats
directly, which makes this test always fail.",1,True,2020-04-14 15:27:29,2020-04-14 17:32:10,2020-04-14 17:32:10
https://github.com/OpenMined/PySyft/pull/3351,[],Changed costumer to customer,Changed costumer to customerFixed a minor spelling error.,1,True,2020-04-14 00:55:51,2020-04-14 14:37:21,2020-04-14 14:37:21
https://github.com/OpenMined/PySyft/pull/3346,[],Fix missing `device` param in `test_federated_client.py::test_fit`,"Fix missing `device` param in `test_federated_client.py::test_fit`This test was failing unexpectedly in the automated test suite, which
appears to be due to randomization sometimes leading the `loss` to
increase instead of decrease. In that circumstance, the test would try
to train the model further until the loss decreased, but the line that
trains the model again was missing the `device` param, causing the test
to fail intermittently.",1,True,2020-04-12 16:25:26,2020-04-12 17:05:46,2020-04-12 17:05:46
https://github.com/OpenMined/PySyft/pull/3343,['bug '],Fix Memory Leak in AST Operations,"Fix Memory Leak in AST OperationsFixes #3316 

## Current progress
- [x] Worker 1
- [x] Worker 2
- [x] Crypto Provider
- [x] Multi Party
- [x] Clean-Up
- [x] Tests


## The Change in Behaviour

<details>
  <summary>Code Used to Test</summary>
  
```python
import torch as th
import syft as sy

hook = sy.TorchHook(th)
alice = sy.VirtualWorker(hook, id=""alice"")
bob = sy.VirtualWorker(hook, id=""bob"")
crypto_provider = sy.VirtualWorker(hook, id=""james"")

torch = th
syft = sy


class Classifier(torch.nn.Module):
    def __init__(self, in_features, out_features):
        super(Classifier, self).__init__()
        self.fc = torch.nn.Linear(in_features, out_features)

    def forward(self, x):
        logits = self.fc(x)
        return logits


def mul(a, b):
    c = a * b
    return c


def add(a, b):
    c = a + b
    return c


alice.clear_objects()
bob.clear_objects()

a = torch.ones(1, 5)
b = torch.ones(1, 5)

a = a.encrypt(workers=[alice, bob], crypto_provider=crypto_provider)
b = b.encrypt(workers=[alice, bob], crypto_provider=crypto_provider)


classifier = Classifier(in_features=5, out_features=2)
classifier = classifier.fix_prec().share(bob, alice, crypto_provider=crypto_provider)


for i in range(3):
    print(""-"" * 20)
    print(f""Alice: {len(alice._objects)}"")
    print(f""Bob: {len(bob._objects)}"")
    print(f""Crypto Provider: {len(crypto_provider._objects)}"")

    c = classifier(b)  # <<<<< Just Toggle Between Operations
    # c = mul(a, b)
    # c = add(a, b)

    print(f""Alice: {len(alice._objects)}"")
    print(f""Bob: {len(bob._objects)}"")
    print(f""crypto_provider: {len(crypto_provider._objects)}"")

```
</details>

![Test](https://i.imgur.com/noAPMr5.png)

<details>
<summary>Same Table in MD</summary>

#### Sorry for the Ugly Table üòÖ
(\<br>'s don't go well in GitHub)
<table>
    <tr>
        <th>Operation</th>
        <th>Current Behaviour</th>
        <th>After Merge</th>
    </tr>
    <tr>
        <td>Addition</td>
        <td>
            <code>==========Iter1==========<br>
            Alice: 4<br>
            Bob: 4<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0<br>
            ==========Iter2==========<br>
            Alice: 5<br>
            Bob: 5<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0<br>
            ==========Iter3==========<br>
            Alice: 5<br>
            Bob: 5<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0</code>
        </td>
        <td>
            <code>==========Iter1==========<br>
            Alice: 4<br>
            Bob: 4<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0<br>
            ==========Iter2==========<br>
            Alice: 5<br>
            Bob: 5<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0<br>
            ==========Iter3==========<br>
            Alice: 5<br>
            Bob: 5<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0
            </code>
        </td>
    </tr>
        <tr>
        <td>Multiplication</td>
        <td>
            <code>==========Iter1==========<br>
                Alice: 4<br>
                Bob: 4<br>
                Crypto Provider: 0<br>
                !!OPERATION!!<br>
                Alice: 12<br>
                Bob: 6<br>
                crypto_provider: 1<br>
                ==========Iter2==========<br>
                Alice: 12<br>
                Bob: 6<br>
                Crypto Provider: 1<br>
                !!OPERATION!!<br>
                Alice: 19<br>
                Bob: 7<br>
                crypto_provider: 2<br>
                ==========Iter3==========<br>
                Alice: 19<br>
                Bob: 7<br>
                Crypto Provider: 2<br>
                !!OPERATION!!<br>
                Alice: 26<br>
                Bob: 8<br>
                crypto_provider: 3            
            </code>
        </td>
        <td>
            <code>==========Iter1==========<br>
            Alice: 4<br>
            Bob: 4<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0<br>
            ==========Iter2==========<br>
            Alice: 5<br>
            Bob: 5<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0<br>
            ==========Iter3==========<br>
            Alice: 5<br>
            Bob: 5<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0
            </code>
        </td>
    </tr>
    <tr>
        <td>nn.Linear</td>
        <td>
            <code>==========Iter1==========<br>
            Alice: 4<br>
            Bob: 4<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 9<br>
            Bob: 9<br>
            crypto_provider: 1<br>
            ==========Iter2==========<br>
            Alice: 9<br>
            Bob: 9<br>
            Crypto Provider: 1<br>
            !!OPERATION!!<br>
            Alice: 13<br>
            Bob: 13<br>
            crypto_provider: 2<br>
            ==========Iter3==========<br>
            Alice: 13<br>
            Bob: 13<br>
            Crypto Provider: 2<br>
            !!OPERATION!!<br>
            Alice: 17<br>
            Bob: 17<br>
            crypto_provider: 3
            </code>
        </td>
        <td>
            <code>
            ==========Iter1==========<br>
            Alice: 4<br>
            Bob: 4<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0<br>
            ==========Iter2==========<br>
            Alice: 5<br>
            Bob: 5<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0<br>
            ==========Iter3==========<br>
            Alice: 5<br>
            Bob: 5<br>
            Crypto Provider: 0<br>
            !!OPERATION!!<br>
            Alice: 5<br>
            Bob: 5<br>
            crypto_provider: 0
            </code>
        </td>
    </tr>
</table>
</details>Is this needed?Here I guess nothing has changedAs well herewhy do we need copy here?That's not needed finally?Yes. Works without it.
Removed it as it was breaking the inplace operations such as ```__iadd__```, etcDestroys the original share in the due to ```.remote_get()``` (next line) If it's not needed, let's remove it.I don't follow why `requires_grad` controls whether or not we register the response. What's the rationale for that? Can we make it clearer in this code?If these assertions are no longer passing with the changes in this PR, can we flip them around to `assert x.id_at_location not in bob._objects`? Seems like that would reflect the intuitive semantics of `move()`.‚ù§Ô∏è the new garbage collection tests!üëç üëç### If a tensor requires grad, it mustn't get destroyed till the `loss.backward()` is invoked. 
Basically, we shouldn't delete the tensor when `requires_grad` is `True`, but always when its `None` or `False`. So, for it to be preserved till backprop, the tensor has to be registered. Hmm. Well, maybe we could add a comment that conveys that part?

It would probably be clearer if the flag was called `backprop`, but that change would extend beyond the scope of this PR.Currently there is another PR in the happening that fixes some GC issues with the backprop and involves this part of the codebase. I will address your comment in them. üëç@karlhigley Done@karlhigley Done",2,True,2020-04-12 13:37:35,2020-04-20 11:47:33,2020-04-19 19:54:42
https://github.com/OpenMined/PySyft/pull/3342,[],Fix Part 3 - Grid applied to Smart Cities and Smart Homes,Fix Part 3 - Grid applied to Smart Cities and Smart HomesUpdate the notebook code snippet connecting the remote grid nodes between each other to aggregate the energy consumption value.,1,True,2020-04-11 20:30:42,2020-04-20 01:08:14,2020-04-20 01:08:13
https://github.com/OpenMined/PySyft/pull/3334,[],Whitelist `serde` from Random-Order Testing,"Whitelist `serde` from Random-Order TestingAddresses issue in `test_serde_simplification` which causes tests to fail.

The `serde` isn't supposed to be run in random order as the serialization has to be complete. But random testing includes asynchronous execution so _maybe_ `ser` isnt complete before `de`
",1,False,2020-04-10 08:55:42,2020-04-10 13:05:09,2020-04-10 13:05:09
https://github.com/OpenMined/PySyft/pull/3327,"['bug ', 'testing ']",Gradient clipping - add tolerance,"Gradient clipping - add toleranceThis PR is opened as a result of making the tests more robust.

There was a problem with one [test](https://github.com/OpenMined/PySyft/pull/3317) and this PR tries to solve the problemChange such that both return tensors",2,True,2020-04-08 14:42:20,2020-04-17 16:02:57,2020-04-17 16:02:57
https://github.com/OpenMined/PySyft/pull/3322,[],Remove unnecessary name off tutorials,"Remove unnecessary name off tutorialsThis PR is requested by @H4LL in #3166, just fix my previous mistakes. Ready to be merged directly.",1,True,2020-04-07 22:36:14,2020-04-17 16:12:22,2020-04-17 16:12:22
https://github.com/OpenMined/PySyft/pull/3288,[],Target correct notebook version,"Target correct notebook versionError coming up:
``` ERROR: syft 0.2.4 has requirement tornado==4.5.3, but you'll have tornado 5.1.1 which is incompatible.```",1,True,2020-04-01 17:15:35,2020-04-01 17:58:29,2020-04-01 17:58:29
https://github.com/OpenMined/PySyft/pull/3286,[],"Fix Circular import of RNNs, Correct Overloading","Fix Circular import of RNNs, Correct Overloading### Follow Up of #3269 

>  This file is normally inside the nn/ directory. In the precision.py file when we import nn, the nn/\_\_init\_\_.py gets called. Currently all other classes (ie Conv2d, AvgPool2d, etc) behave well when imported in the nn/\_\_init\_\_.py. 
But due to the RNN Classes requiring the import of FixedPrecisionTensor (line 14) and precision.py requiring the import of nn module, there is a problem.

Changes:
- [x] Move rnn.py back to nn/ (non-breaking)
<details>
  <summary> How? </summary>

# Here is How
([from docs](https://docs.python.org/2.0/ref/import.html))
> The first form of import statement (`import foo.bar`) binds the module name in the local namespace to the module object, and then goes on to import the next identifier, if any. 

Hence, by **changing the import from a bound to an unbound-to-module import**, we can workaround the circular import. 
More information [here](https://stackoverflow.com/a/22210807/8878627)
</details>

- [x] Changes requested by @Jasopaum in #3269 
- [x] Update tests to use the overloaded moduleCould you rename this to something else than `nn2`? `syft_nn` for instance, or just `nn` and `torch_nn` for the one aboveDoesn't it work with the import at the beginning of the file?No. This is the only configuration (afaik).
Otherwise a major refraction/making it independant of that requirement is required. Done üòä",2,True,2020-04-01 16:21:04,2020-04-17 16:07:38,2020-04-17 16:07:38
https://github.com/OpenMined/PySyft/pull/3285,[],Fix list tensor index,"Fix list tensor indexFix list as an index for syft tensors. Issue: #3259 also related:#2628

Problem solution: the logic from #2628 was mostly correct but it was placed in the wrong function. The return_list parameter should have been added to build_unwrap_args_with_rules which actually deals with function parameters.
Added a small test to check if tensors can be indexed with lists of indexes. 

Questions: Should we keep the changes from #2628 PR?Q: Why are we forcing here to return a tuple and a list?
**LE**: Forcing a list will have priority over a tuple?return_tuple actually does not have an effect on the current function, it is just forwarded to one_fold function. And the return list is just to force that the returned collection is a list. It sort of does but it's not explicitly enforced in this function.Since the `return_list` flag causes the function to coerce the return type to `list`, could we do without the flag and make this line something like:
```suggestion
        else lambda x: list(build_unwrap_args_with_rules(a, r, True))
```
If so, the lines below could also be removed.```suggestion
```",3,True,2020-03-31 22:53:09,2020-04-15 14:35:51,2020-04-15 14:35:51
https://github.com/OpenMined/PySyft/pull/3284,[],Generalize `Role` to any type of `Action` (computation or communication),"Generalize `Role` to any type of `Action` (computation or communication)Even though `Role` no longer directly references `torch`, this import can't be removed because `Actions` sometimes contain method calls like `torch.manual_seed()`, which require `torch` to be available in the scope where `eval` runs. Crypten runs into the same issue so we need a better solution for this, but I'll leave that for a future PR.Along related lines, sure would love to get rid of that `eval`. ü§î Wow, good catch! How did that happen?As the type of action will be automatically inferred, don't you think we could do it in the Role?I think it's been that way since the test was written. Probably just a copy/paste oops.We could! I figured this was in the spirit of putting the restriction that `Plans` can only contain `ComputationActions` in `Plan` instead of `Role`. I also kinda like that `Roles` don't know anything about `Action` types.We're going to have refactor this further one way or another, for sure. I was kinda leaning toward passing in an `Action` object (instead of the `log`.)Hum, but then, the Plan would have to build the `Action`s?
Also, what happens if we try to build a ComputationAction with communication stuff inside? ü§î We were already assuming that all actions created in a `Role` would be `ComputationActions`, so this is no worse in that sense. It‚Äôs actually more flexible because now `Protocols` could add `CommunicationActions` to a `Role` without changing the `Role` code.

`Plans` should never contain `CommunicationActions`, so I don‚Äôt see a problem there. We do need to add logic somewhere to determine which `Actions` are computations vs communications and create the right ones, but we need it for `Protocols` and we‚Äôre not there yet. ü§∑‚Äç‚ôÄÔ∏èNo, I agree with what you did! I was reacting to the ""passing in an Action object"".
And my second thought didn't have anything to do here, sorry, it was just a random question ^^And I'm starting to (slowly) build protocols so if you have any ideas about determining which actions are Computations and which one are Computations, I'm listening üòÑ Hmm, well, my first thought is that communication actions are all defined by PySyft and not by the various ML frameworks. Could we use that somehow? I mean, we can always just build a list of the communication actions and assume everything else is a computation action, but there might be a more elegant way to do it. ü§î ",1,True,2020-03-31 21:29:33,2020-04-01 19:18:39,2020-04-01 19:18:39
https://github.com/OpenMined/PySyft/pull/3276,[],FV homomorphic encryption scheme,"FV homomorphic encryption scheme#3097 Implementation of FV (Fan-Vercauteren) Homomorphic Encryption scheme.

Implemented:
- [x] Modulus (PolyModulus, CoeffModulus, PlainModulus)
- [x] IntegerEncoder
- [x] KeyGeneration
- [x] Encrypter
- [x] Decrypter 

@youben11 What's the purpose of the context class if it's only holding the parameters?

This should be with the other comments I think the parameters should be prefixed with a `_` here

```suggestion
        self._poly_modulus_degree = 2
```This is checking for a multiple of two and not a power of twoShouldn't this ValueError be raised after the power of two check?You can use [enumeration](https://docs.python.org/3/library/enum.html) classes hereHow can we benefit from existing implementations like `Crypto.Util.number.getPrime` here?This class will store more attributes in future as the work progress.Thank you for pointing it out.
Actually it is just a suggestion that it should be a power of 2 and in SEAL there is no checking for it. I have changed the section like this.
```
if value >= 2:
    self.__poly_modulus_degree = value
else:
    raise ValueError(""poly_modulus_degree must be at least 2"")
```https://github.com/OpenMined/PySyft/pull/3276#discussion_r401992509
I am not sure about how can we use `Crypto.Util.number.getPrime` because in the current implementation we need to generate primes with the difference of multiple of (2*polymodulus_degree)If you are following the SEAL architecture, then they require a power of two, you can't set the poly_modulus_degree to a value other than powers of two.I updated it for checking the value to be a power of 2.Can't we have the init function take keyword arguments to set the attributes on instantiation?It would be great if the secret_key is a special object and isn't just wrapped around a Plaintext.It would be great if the public_key is a special object and isn't just wrapped around a CipherText.Ok, I will create a new class for itOk, I will create a new class for itWhat's the purpose of round here?I was using a different way of finding random numbers earlier which requires round. I changed the way but forgot that round there üòÅüòÖAre these properties because you want them to be gettable but not settable?This should probably be a docstring.Are these methods functionally private? If so, should probably be prefixed with an underscore.(Matters because docstring coverage can be set to ignore private methods.)This pattern shows up way more in this PR than I've seen anywhere else in the code base. What's the intended purpose?Should be a docstringdocstringdocstringit is just a wrapper class. I am using for making the data(list) more meaningful. I am using the constructor for setting values.Actually, an instance of context object should not change its value once assigned.I like the wrapper class. üëç As I understand, the ciphertext is represented as two (and maybe more when multiplication happens) polynomials c0 and c1, can the documentation reflects that more?Q: why are we holding q_i/t and not q_i?What if the ciphertext has 3 elements? (it is a result of a ct x ct), shouldn't the decryption circuit be [c0 + c1 * sk + c2 * sk ^ 2]_q ?Shouldn't this be polynomial operations? Why is it implemented using element-wise operations?Encryptor maybe?Decryptor maybe?I think we don't need those ones, right? We can always check against Nonesame here, would be helpful  to know how data is organizedsame heresame hereq_i is available in encryption_param. I was holding it earlier but now I removed it and wherever needed I use by accessing from param.You are right. this should be c0 + c1*sk + c2 * sk^2...
Actually I was once thinking when ciphertext can go above 2 in size and for encryption-decryption with just 2, works fine so I just fixed it to 2

I will update itüëç""""""A wrapper class for representing plaintext elements.

    Attributes:
        data: A list of coefficient values of plaintext polynomial.
    
    Typical format:
    [c0, c1, c2...] where ci represents the coefficient of the polynomial.
    """"""""""""A wrapper class for representing public_key.

    Attributes:
        data: A list of values of the public key.
    
    Typical format:
    [ct0, ct1] : where ct0 represents `-(a*s + e)(mod q)` and ct1 represents `a`
    """"""""""""A wrapper class for representing secret_key.

    Attributes:
        data: A list of values of the secret key.

    Typical format:
    [x1. x2, x3...] : where xi is integer denoting -1, 0, 1 in respective modulus. 
    1 is represented as 1
    0 is represented as 0
    -1 is represented as (modulus-1)
    """"""""""""A wrapper class for representing ciphertext.

    Attributes:
        data: A list of lists of integers representing ciphertext.

    Typical format:
    [c0, c1, c2...] where ci represents polynomials(list of integers).
    """"""https://github.com/microsoft/SEAL/blob/f7d748c97ed841376c4a1cdec9e7c978f5e64a95/native/src/seal/decryptor.cpp#L101
That's because they are using NTT. We have two choices here:

1. Use NTT and let it as is
2. Switch to polynomial operations

In both cases it would be helpful to have operations like addition and multiplication between two ring elements (either they are NTT transformed or not) be separated into functions and used across the code base so that updating to NTT isn't a big deal.Ok, I add NTT transformationüëçsk_power can be just a list that we append power in the interationsthis is doing element-wise exponentiation, isn't it?need update I guessneed update -> [c0, c1, ...]I don't think it is element wise. Current secret key is a list of polynomials which is also a list. So I am taking power of every polynomials in secret key.But sk is itself a list of polynomials so taking power shouldn't be taking power of each polynomial individually?Ah yeah!so maybe `sk_power = [[] for _ in range(max_power)]` then you append lists there?",5,True,2020-03-30 14:27:38,2020-08-25 06:51:51,2020-05-29 19:44:26
https://github.com/OpenMined/PySyft/pull/3271,[],Remove implicit inplace fix_precision for PointerTensor,"Remove implicit inplace fix_precision for PointerTensorFixes #3261 Q: Is there a scenario where the ```PointerTensor``` is not a wrapper?
```
import torch
import syft

hook = syft.TorchHook(torch)
alice = syft.VirtualWorker(id=""alice"", hook=hook)

x = torch.Tensor([1,2,3])
ptr = x.send(alice)
ptr.child.fix_precision().get()
```Code outside this if block takes care of it (which is not modified).
`(Wrapper)>FixedPrecisionTensor>tensor([1000, 2000, 3000])` is the output, which is correct!Actually I'm not 100% comfortable with copying the  tensor.
What about this?
```python
if self.is_wrapper:
    child = self.child.fix_prec(*args, **kwargs)
    if no_wrap:
        return child
    else:
        return child.wrap()
```
If this works, then I guess we should go in that direction:)It works and this looks better approach!!",2,True,2020-03-29 14:41:59,2020-03-31 19:44:14,2020-03-31 19:44:14
https://github.com/OpenMined/PySyft/pull/3269,[],Fix overloading for torch.nn and Functional modules,"Fix overloading for torch.nn and Functional modules This PR fixes the following
- By #3253 the automatic overloading of torch modules was lost.
- By setting up the decorators and an efficient \_\_init\_\_.py, the functionality is restored.
- Also includes the overloading of AvgPooling and Conv2d from torch.nn which were earlier missing
- Move tests from test_precision related to torch APIs to test_nn.py and test_functional.pyI love thatThe problem in the rnn.py fileThe path of the file has to be updated in the omit path for the coverage to pass. Since the original path is used by the test coverage bot, it is failing.  Me toonit: can you put this line next to the one with AvgPool2d?I think you can simplify this `if`s and `else`sDo you think we could remove the import of FixedPrecisionTensor (not in this PR, of course)? I don't think RNN should deal with things at that low a level.",1,True,2020-03-29 03:44:31,2020-04-01 05:00:53,2020-03-31 19:49:17
https://github.com/OpenMined/PySyft/pull/3268,['documentation '],Added pull request template,"Added pull request template* Pull request templates are a nice feature that helps the contributor to add relevant information about their pull request. 
* They also help the reviewer to verify that the necessary guidelines of contributing have been followed. 
* Thus helping in maintaining the repo. :)It could be useful to have a ""Notebooks"" option as notebook-only PRs don't neatly fit into these optionsI guess they would fit into New feature. By notebooks only do you mean something in the lines of changes to tutorials? If required I can add an option such as ""Added/Modified a notebook in examples"".""New dependencies"" could probably be a checkbox in the `Type of change` list below.Having an entry for tutorials here would be good.The list of files changed is probably overkill, because I can see that from the diff. As long as people have added tests for their changes (as listed on the `Checklist` below) that probably covers 95% of what's useful as a PR reviewer. Since we run the repo tests automatically via Github Action, whatever tests get added need to pass in that environment.

I'd probably drop the `How has this been tested` section for now.I guess it isn't required. Removed it.",3,True,2020-03-28 18:59:17,2020-04-17 16:58:22,2020-04-17 16:08:34
https://github.com/OpenMined/PySyft/pull/3266,['bug '],Send Dataset_pointer,"Send Dataset_pointer- Added functionality to get a dataset from a remote federated_dataset object without breaking it.
if the user calls `fed_dataset[""alice""].get()` for example that would leave a pointer that points to noting and it will break the whole object 
- also updated the dataset_pointer representation to include (id, owner -> id at location, location)
- Solved issue #3267It's not the part of the code I know the best but do we want to have a more granular way of getting datasets?
For instance, what if a worker wants to get only one of its dataset in the FederatedDataset?Do we really want that? What is the usecase for `get_dataset()`?because if the user used `federated_dataset[worker].get()` that would leave it with a pointer that points to nothing. that will require the pointer to be deleted from the `datasets` dict if we want the `federated_dataset` object to still work. the user won't be able to do it if he doesn't have a deep understanding of the implementation because it will cause errors in other parts of his code like `dataset_loaders` for example. this is a way to keep the object always clean and ready to useit's not mine but I believe this implementation doesn't allow more than one dataset per worker since it uses a dict to store them, if it has {bob:ptr1, alice:ptr2} and you add alice:ptr3 it will replace ptr2 not add it",2,True,2020-03-28 14:43:54,2020-04-09 17:13:11,2020-04-09 17:13:11
https://github.com/OpenMined/PySyft/pull/3265,[],Merging tests run_websocket_server.py file ,"Merging tests run_websocket_server.py file Solves #2744 Q: Shouldn't there be 1/2 more level of indentation?corrected.Q: Shouldn't here be fl?I think here should be one level lower (4 instead of 5)corrected.I was running on an older version of syft-proto. It has to be fl according to new version.There is also another file in ```webosckets-mnist```. From a first look, they seem identical? Can't one be removedresolved.Even we merge this file and make a new file in the main repo. then there has to be a file in `websockets-example-MNIST` and `websockets_mnist` which call this `start_websocket_servers.py` in the main repo which calls `run_websocket_server.py`. So I think there will be a lot of arguments that will involve which will not make codebase shorter but also make it complex.So I think we have to leave it.Why not use directly the file from ```websockets_mnist```? what other files depend on this one?So you are saying for `websockets-example-MNIST/start_websocket_servers.py` we can use  `websockets_mnist/start_websocket_servers.py` ?Q: Why not simply remove this and in all the places where this was used ad the other one -- what I understand is that we currently have 2 python files and this one calls the other one. My idea is to delete this one and change all the occurrences where we called this file to the other oneI think we can delete this file because its folder doesn't contain a notebook in which it is used. Now, this file is of no use. So I am deleting it.",5,True,2020-03-28 07:56:48,2020-04-04 19:42:47,2020-04-04 19:42:46
https://github.com/OpenMined/PySyft/pull/3258,[],Model inversion attack tutorial,"Model inversion attack tutorial**This PR**:
Adds a notebook tutorial demonstrating a black box model inversion attack.
This attack creates a model which can recreate data from a target model's outputs.
Created for the Security & Identity team.

**To review**:
* Ensure you can run the notebook from scratch without errors
* Check for spelling and grammar mistakes

_There is no issue relevant to this PR._link is brokenI guess you don't need `*[ `
you could do :
```
torch.nn.Sequential(
     torch.nn.Conv2d(1, 32, kernel_size=5, padding=0, stride=1),  # first Conv layer
     torch.nn.ReLU(),
    etc
```same here `*[` is not neededMaybe this is less assertive but as suggestive: ""this attack could also be applied on much more sensitive data including face images or MRI records.""",3,True,2020-03-26 18:58:56,2020-03-30 19:47:06,2020-03-30 19:47:06
https://github.com/OpenMined/PySyft/pull/3256,[],Fix the placeholder serialization.,"Fix the placeholder serialization.Fixes #3214. 
The problem was that when reconstructing the network placeholders, the is_wrapper parameter from tensor was ignored.
The apparent issue with parameter serialization was due to the fact that when fetching a plan the wrapper parameter was incorrectly reconstructed as a plain parameter. This is actually done inside the placeholder deserialization function, where the ""is_wrapper"" field is stripped from the parameter tensors inside the instantiate method, hence the missing ""Wrapper"" when printing the deserialized network's parameters.Q: Are the state placeholder still not working? (what output we get if we uncomment this line)What's the problem with state placeholders?(I see that #3214 has something to do with `Parameters`, but not sure what about `Placeholders` is problematic.)Removing this seems questionable, because we do want to be able to instantiate a `Placeholder` with a wrapper tensor.Do we have to compare string representations here? Could we compare the state tensors directly?TIL about `starmap`. üòÉ",2,True,2020-03-26 14:57:58,2020-03-27 13:17:38,2020-03-27 13:17:25
https://github.com/OpenMined/PySyft/pull/3254,[],Init Crypten Plan + Rebased,"Init Crypten Plan + Rebased**Attention** Those changes are not meant to go into master, I will change the PR to target a different branch.

Rebased with master Crypten using Plans (also integrated with SyftCrypTensor).
 * Added the initial implementation (context setup with ```run_multiparty_computation``` and test file) from Ayoub
 * ```SyftCrypTensor``` will be a shell for the operations that will be done on the MPCTensor -- Integrated changes from Ajay
 * Traced the ```add``` and ```get_plain_text``` methods

How the tested function looks:
```
     @run_multiworkers([alice, bob], master_addr=""127.0.0.1"")
     @sy.func2plan()
     def plan_func():
         alice_tensor = crypten.load(""crypten_data"", 1)
         bob_tensor = crypten.load(""crypten_data"", 2)
 
         crypt = alice_tensor + bob_tensor
         result = crypt.get_plain_text()
         return result
```

How the plan looks
```
<Plan plan_func id:96985944591 owner:me built>
def plan_func():
    <syft.execution.placeholder_id.PlaceholderId object at 0x7f555de1f750> = crypten.load(crypten_data, 1)
    <syft.execution.placeholder_id.PlaceholderId object at 0x7f555de1f890> = crypten.load(crypten_data, 2)
    <syft.execution.placeholder_id.PlaceholderId object at 0x7f555de1f9d0> = _1.add(<syft.execution.placeholder_id.PlaceholderId object at 0x7f555de1fbd0>)
    <syft.execution.placeholder_id.PlaceholderId object at 0x7f555de1fb10> = _3.get_plain_text()
    return _4
```@youben11 I changed the load to look after the worker given a rank (the PR I merged into the CrypTen branch for plans did not have those changes and it fails when it looks for the tagged data on the ```syft.local_worker```)We should keep with commit '68e0364c66df95ddbb98422fb641382c3f58734c' (or any other that don't break with pysyft) for the moment, as the last version introduces some changes that requires torch nightly as well as other functionalities we doesn't support yet.I'm not sure about this, but if SyftCrypTensor is a tensor type that its only purpose is for building plans with crypten functionalities, then should we make it available in the API?Removed itWill try to keep track with the latest version of CrypTen (also removed the Python 3.6 Tests and TutorialTests from this PR) and if there are problems with the Torch, will go back to a more stable versionThis is needed because we return a ```SyftCrypTensor``` which is an ```AbstractTensor```Q: What is the better method to do it?
The problem is that when running the plan and we reach the ```get_plain_text``` instruction, ```_self``` is a ```PlaceHolder``` and ```.child``` is an ```MPCTensor```. The placeholder does not have the ```get_plain_text``` method and in the current implementation I try to resolve it by checking the childWhy is `SyftCrypTensor` not a `FrameworkTensor`? Is Crypten not a framework?If `Placeholder` isn't properly forwarding to the `child`, we should probably fix that in `Placeholder`.Q: Tried with a simple ```th.zeros([])``` (without explicitly specifying the shape). It seems that currently, the shape does not matter.Plans shouldn't know anything about specific frameworks, so we should avoid adding Crypten-specific code if we can help it. (@OpenMined/syft-core-team has an open issue to remove the PyTorch-specific code.)This seems like the kind of thing PySyft has left to be added on top by other libraries (like Grid.) I'm not opposed to having this functionality in PySyft, but the inconsistency strikes me as pretty weird and a sign that there may be a design issue here.`BaseWorker` intentionally doesn't know anything about frameworks, so would like to find a way to avoid letting that knowledge leak in. One possibility would be to create a `CryptenWorker` that overrides or adds to the message dispatch table.This implies we'd be removing support for Python 3.6 entirely. @iamtrask, thoughts on that?

If we want to keep 3.6 support for PySyft (but not Crypten), possible we could split the workflows into one for each Python version and exclude the Crypten tests in the 3.6 workflow.This is needed because after we built a plan the ```tensor``` (received as a parameter) is a ```MPCTensor``` (does not have the attribute ```is_wrapper```.
Another solution can be seen in a previous commit [here](https://github.com/OpenMined/PySyft/pull/3254/commits/d884650282b6cd45c8c80562cb431d9a81310988#diff-3fd9f3371ec1373832b99032d1212bad) (I kinda like more this approach)Either way is probably fine. Using `hasattr` protects us against future tensor types that don't have the attribute though.Sorry, this should target a separate branch -- the changes are like this because the latest CrypTen master has support for python >= 3.7Good idea :)@karlhigley moved the logic here.
We also need tome attributes from Placeholder (like the ```id```, ```tags```, the ```instantiate``` method, etc.).
and for the other attributes, we forward them to the child process.

Another solution for this would be to use a list of attributes that we know are called on the placeholder.
This list should be kept with the attributes like: ```id```, ```tags``` ... (all the parameters in the constructor) + the methods we define in the Placeholder.

What would look like then:
```
def __getattribute__(self, name):
    if name in [""id"", ""tags"", ....]:
       return object.__getattribute__(self, name)
   else:
       child = object.__getattribute__(self, ""child"")
       return getattr(child, name)
```I think you could maybe use `hasattr` to check if `Placeholder` defines `name` and delegate to the `child` otherwise.I'd really ‚ù§Ô∏è  for it to target the main branch. We want Crypten support, it's just a matter of figuring out how to get it integrated without upending the rest of the apple cart. We're squeezed on one side by Tensorflow (<= 3.7) and on the other side by Crypten. We really could narrow PySyft to 3.7 support only, just want to make sure that isn't going to break something else.The suggestion above doesn't work, because `hasattr` calls `__getattribute__`. Oops!Interesting point. Maybe it is not because currenly SyftCrypTensor is just a helper to build Plans for Crypten, it doesn't serve other purpose than building plans. But if it proves to be more useful, than it should be definitely a FrameworkTensorso do we need this one, or is the add call directly forwarded to the child?Sorry I didn't get the meaning of ""This seems like the kind of thing PySyft has left to be added on top by other libraries""
What would you think would be the best way to proceed here?Alternatively, how bad would that be to put the import in the `def run_crypten_party` ?
I've already seen things like this but I don't know how evil it isYes - I agree that we should be targeting the main branch. Having multiple separate version of PySyft is really a last resort nuclear option.

Or put another way - if we're worried about how PySyft master and Crypten changes will ""fit together"" - the answer is to merge small changes earlier not even bigger changes later (by having even more divergence through working with multiple branches)We have currently two blocking issues to merge this into master, the first is this python 3.6 support, the second is that crypten is now using torch nightly, and it will actually breaks badly if ran with torch==1.4.0I think the problem is that we are exposing some methods that are not necessarily needed by the ```VirtualWorker``` (@karlhigley this is the reason?)What do you think should be the resolution here?
I am thinking that will be a good idea to test with an older version of Crypten (that had support for pytorch 1.4) and target the Syft master? The downside of this is that we are not up-to-date with the Crypten changes.
It is forwarded to the child (in our case we do not have a child).

But, if I change in the ```__init__``` method to have this:
```
self.child = tensor
```
Then when building the plan, the traces look like this:
```
-> for log in sy.hook.trace.logs:
(Pdb) p sy.hook.trace.logs
[(('crypten.load', None, ('crypten_data', 1), {}), SyftCrypTensor>tensor(0.)), (('crypten.load', None, ('crypten_data', 2), {}), SyftCrypTensor>tensor(0.)), (('__add__', tensor(0.), (tensor(0.),), {}), tensor(0.)), (('get_plain_text', SyftCrypTensor>tensor(0.), (), {}), SyftCrypTensor>None)]
(Pdb)
```
And we get an error in the plan with:
```
 ValueError: The following tensor was used but is not known in this plan: 
E                   0.0
E                   Possible reasons for this can be:
E                   - This tensor is external to the plan and should be provided using the state. See more about plan.state to fix this.```

This is because (I think) the values for the add operations are not wrapped in SyftCrypTensor. The solution (there might be another) was to trace the ```add``` operation in the ```SyftCryptensor``` class.Q: Should we move this to a separate requirement file?Changed the target to ```master```I think we should now follow the master, no?Should this be available to the user to import from syft?we doesn't have a get_plain_text hereSeems like this is all duplicated 
```suggestion
```This is a deuplicate as well
```suggestion
```Will doFWIW we've retargeted to a crypten branch after lots of discussion becuse we need torch nightly
We'll make sure this Openmined:crypten branch is updated regularlyIs the `.tolist()` the single difference with crypten/mpc/context.py ?Should we remove it from this PR if we don't use it?When can this happen?Agreed@gmuraru shouldn't get_plain_text be removed?No, we use a different initialization, ours initialize over tcp which makes it possible to have parties on different machinesThe trace for the error was this:
```

    @pytest.fixture(scope=""session"", autouse=True)
    def hook():
>       hook = TorchHook(torch)

test/conftest.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
syft/frameworks/torch/hook/hook.py:145: in __init__
    self._hook_native_tensor(crypten.mpc.MPCTensor, TorchTensor)
syft/frameworks/torch/hook/hook.py:277: in _hook_native_tensor
    self._hook_native_methods(tensor_type)
syft/generic/frameworks/hook/hook.py:102: in _hook_native_methods
    native_method = getattr(tensor_type, attr)
../../../miniconda3/envs/pysyft/lib/python3.7/site-packages/crypten/cryptensor.py:39: in __getattribute__
    dummy = cls(None)
../../../miniconda3/envs/pysyft/lib/python3.7/site-packages/crypten/mpc/mpc.py:73: in __init__
    super().__init__(requires_grad=requires_grad)
../../../miniconda3/envs/pysyft/lib/python3.7/site-packages/crypten/cryptensor.py:132: in __init__
    self._reset_gradients()
```

What (I think is happening) -- when we hook we use the ```getattr```  -- but that method is changed in the CrypTensorMetaclass (metaclass for the MPCTensor). The problem is when we hook a method and that method is called in the ```getattr``` (in getattr we call an already hooked method and we do not have the syft.hook initialized -- syft.hook is initialized at the end of the TorchHook init method)Yes, DoneAmazing, thanks for the through explanation!",3,True,2020-03-26 00:12:36,2020-04-13 21:42:15,2020-04-13 21:42:15
https://github.com/OpenMined/PySyft/pull/3251,['documentation '],Fixed arguments documentation of federated_avg method,"Fixed arguments documentation of federated_avg methodThe method 'utils.federated_avg' actually accepts a **dictionary** of models as input, and not a list of models. In fact, the list of models (the dictionary's values) are extracted from the dictionary via models.values().

Following are examples of the actual usage of this method. Look for 'utils.federated_avg'. You'll notice that the variable ""models"" (a dictionary) is actually being passed to the method 'utils.federated_avg'

https://github.com/OpenMined/PySyft/blob/5998f68e6f51d9ec72ba32fe944e8971a598da24/examples/tutorials/advanced/websockets_mnist/run_websocket_client.py

https://github.com/OpenMined/PySyft/blob/a0e888ac4b4da7488e03a8d401c6a198a85ef74c/examples/tutorials/advanced/Federated%20Recurrent%20Neural%20Network.ipynb",1,True,2020-03-24 07:30:03,2020-03-26 17:59:36,2020-03-26 17:59:36
https://github.com/OpenMined/PySyft/pull/3241,['bug '],Fix relative imports in example notebooks,Fix relative imports in example notebooks,2,True,2020-03-22 16:34:52,2020-03-22 20:17:15,2020-03-22 20:17:14
https://github.com/OpenMined/PySyft/pull/3234,[],Add a `protocol` kwarg to `encrypt()` method to select between MPC and Paillier HE,"Add a `protocol` kwarg to `encrypt()` method to select between MPC and Paillier HEFixes [#3212](https://github.com/OpenMined/PySyft/issues/3212)

> Modify .encrypt() to add a kwarg and be used not only for Paillier encryption but only MPC (default should be MPC, and it should call for mpc .fix_prec(...).share(...) under the hood)Could you also allow fix_precision kwargs to be given alongside the share ones (workers & crypto_provider)? Would be simpler for the end user :) (I think we would provide kwargs and not args for fix_precision)Use .get() when the parameter is not compulsory (like in fix prec)with my suggestion, then you will had to remove kwargs here I guessExtract public_key like you did for mpc (it's nicer :) ) 
and typo: blacHomomorphicI have a slight preference for `protocol` instead of `encryption_method` (mostly because its shorter and because we often speak of crypto protocols)Check other examples you can improt workers directly:

ex: in test_additive_sharing.py
```
def test_eq(workers):
    me, alice, bob, crypto_provider = (
        workers[""me""],
        workers[""alice""],
        workers[""bob""],
        workers[""james""],
    )
``` Oh we've forgotten to add a generic decrypt method!!
Would you be willing to do also this one? :) 
for MPC .decrypt() would call `.get().float_prec()` 
and for paillier, it would'nt changeSure. Do I submit it in another PR?
I've incorporated the suggestions mentioned. I don't think this case is different from the one above, is it?
Doesn't `self.fix_prec(**kwargs_fix_prec)` work with an empty dict?Yes. You are correct. 
That style was necessary for an older version of code where the kwargs were ""None"" rather than an empty dict.In this PR would be more coherent if you can do it !Do you need  copy() here?@LaRiffle It served a purpose of not making it an inplace operationOh I see 
Ok I got your point, let's keep it your way",2,True,2020-03-21 14:28:45,2020-04-01 05:11:04,2020-03-30 11:37:24
https://github.com/OpenMined/PySyft/pull/3221,[],included websockets-example-MNIST-parallel again,included websockets-example-MNIST-parallel againshould be tested due to some errors with **kwargs and woker.evaluate(),1,False,2020-03-18 21:24:37,2020-03-18 21:52:26,2020-03-18 21:52:26
https://github.com/OpenMined/PySyft/pull/3205,[],Sigmoid aprox with tanh + fix sigmoid,"Sigmoid aprox with tanh + fix sigmoidFixes #3131 
Also adds the approximation for sigmoid using the tanh.So basically you're suggesting .inverse() doesn't work ? :(
(The benefit of .inverse is that it's supposed to be faster)But yeah you're right it's failiing apparently sometimes, I was discussing of this two weeks ago on the call with Andr√©typo this should be here :) To improve test speed could we remove the case precision=5 everywhere where it occurs? From what I see =4 is already compelling enough :) Detail a bit the method for people which don't know about numerical stability etcLet's keep this like this, until I come back to trying my approx inverse trickHmm..this is for ```_tanh_chebyshev``` (forgot to put it in the previous PR)",1,True,2020-03-16 22:45:25,2020-03-22 13:49:23,2020-03-22 13:49:23
https://github.com/OpenMined/PySyft/pull/3199,[],Improve Plans flexibility,"Improve Plans flexibilityThis PR fixes #3184 

Solved:
* added support when input tensor is also output tensor.
* added a test case when input tensor is also output tensor.

To do:
* check for the number of parameters
* create support for nested structures",1,True,2020-03-15 17:29:16,2020-03-16 12:39:20,2020-03-16 12:39:20
https://github.com/OpenMined/PySyft/pull/3196,[],Websocket worker start Error,"Websocket worker start ErrorChanged the tornado version

Closes #3019",1,True,2020-03-15 09:04:03,2020-03-17 15:47:45,2020-03-15 11:31:48
https://github.com/OpenMined/PySyft/pull/3190,['bug '],Removing grad tensors creation if tensor does not require grad.,"Removing grad tensors creation if tensor does not require grad.I observed that syft is creating grad tensors for all tensors, even if freezed and calling backwards on them. The grad tensors are useful with unfreezed layers only, right? The lead was from the autograd[0] documentation. If I remove the grad_tensor creation for the freezed layers and leave it None, it seems to work fine. Any advices?

This PR fixes #3180.
[0]: https://pytorch.org/docs/stable/autograd.html
Can we keep the f-strings. They compile faster",2,False,2020-03-13 15:14:37,2020-03-15 22:20:21,2020-03-15 22:20:21
https://github.com/OpenMined/PySyft/pull/3185,[],FL Training Plan fix & PyGrid hosting example,"FL Training Plan fix & PyGrid hosting exampleChanges:
 * Fix errors in experimental FL Training notebooks, due to recent Plan/etc. updates
 * Add new ""Host Plan"" notebook that hosts training plan & model params to PyGrid

These notebooks test e2e interactions between PySyft, syft.js, PyGrid.",1,True,2020-03-12 18:43:17,2020-03-13 09:17:39,2020-03-13 09:17:38
https://github.com/OpenMined/PySyft/pull/3165,[],Plans for CrypTen,"Plans for CrypTen@LaRiffle this one and the one below are the fixes for the issues you observedWe may not know the size of loaded tensors so we shouldn't have `size` as a required parameter, instead, it should be optional, and we either exchange messages in case size is not provided, or doesn't exchange messages at all if it's not.typo?maybe use `hasattr` instead?Why do you need to run the function? I mean is it required or it could just be always a random response ?I think you can use the same strategy than for the imports or PT & TF (but this is not important)This line can be removed I guess scenarii is ok - it's when you want to look as a star, you put the italian plural for works that have italian originsWhy not using tags to retrieve the plan?Not sure I understand the role of those 3 lines for the moment!Removed them",2,False,2020-03-09 09:12:03,2020-03-27 00:42:46,2020-03-27 00:42:46
https://github.com/OpenMined/PySyft/pull/3162,[],README.md update: specify python=3.7,"README.md update: specify python=3.7As of now, `conda create -n pysyft python=3` will create an environment with python 3.8, which will not be able to run PySift, due to its Tensorflow dependencies. By specifying `python=3.7`, we solve this issue.",2,True,2020-03-08 20:25:43,2020-03-17 23:15:12,2020-03-09 12:40:20
https://github.com/OpenMined/PySyft/pull/3148,[],Modified securenn to work with >3 parties,"Modified securenn to work with >3 partiesI modified the code to work with N parties instead of just 3. Most of it is straightforward; the only care is in the Share Convert protocol, since wrap(a0, a1... an, L) can be any number from 0 to n-1 instead of 0 or 1 (as is assumed in the paper).

Partial fix for #2631Don't call get() but move() instead, and move to the first worker for example. The local worker should see this values as you commented, but the other workers can: it's a public value that they all own.I don't think we usually link the author's name in the codebase, this PR already shows you were the one who implemented this change :)!I don't think this is correct, what you want instead is something like:
`children=[torch.tensor([worker_indx]).send(worker, **no_wrap) for worker_indx, worker in enumerate(workers)]`sameInstead of adding comments on top of the function follow the google style guide for python: http://google.github.io/styleguide/pyguide.htmluse worker instead of w, it makes it easier to understand the code  :)samesamesameI guess this is fine, `j` is here to make sure you add some value only once. But maybe I'm wrong

<img width=""452"" alt=""Capture d‚ÄôeÃÅcran 2020-03-08 aÃÄ 17 02 41"" src=""https://user-images.githubusercontent.com/12446521/76166409-a158e880-615e-11ea-95d6-70817d1c68a4.png"">
Where and how should I add the test?Add a test close to the one for secure_nn (I think it's the comparison tests in precision.py or additive_sharing.py)
Where you put more than 2 share holders (maybe 3 and 4 is fine)Looking at 'test_additive_shared.py', the tests have the form 'def test_stuff(workers): alice, bob = (workers['alice'], workers['bob']).
Should I add new tests without these assumptions or change the existing ones? Also, where are these tests called?As you want, take the cleanest option :)

To run you tests:
`pytest <path>/test_additive_shared.py` Done! Do you see anything else to add/fix, or can we proceed with the merge?",15,True,2020-03-06 12:50:32,2020-05-06 10:22:37,2020-05-05 12:42:18
https://github.com/OpenMined/PySyft/pull/3144,[],Added german language translated tutorial notebooks for 1-10 parts,"Added german language translated tutorial notebooks for 1-10 partsAdded examples/tutorials/translations/german/Part 01-10 
Update translation, Fixes #2781 Not sure if I would call a python notebook ""Notizbuch"" or just leave it as ""notebook""Missing the comma: ""befinden, gespeichert""That sounds very literal and not really matching the context to me. How about going more in the direction of ""Was kann ich zus√§tzlich tun?""- ""Ein wettbewerbsf√§higer Karrierevorteil"" sounds constructed to me. How about just ""Ein (starker) Karrierevorteil""
- ""Mit neuen Regelungen wie z GDPR,Unternehmen stehen unter dem Druck...."" -> ""Mit neuen Regelungen wie zB. GDPR, stehen Unternehmen unter dem Druck...""
- ""Wettbewerbsvorteil erzielen Deine Karriere"" -> ""Wettbewerbsvorteil erzielen in Ihrer Karriere""- no space between ""[README] (https://github.com/OpenMined/PySyft.git)""
- no space between ""[slack.openmined.org] (http://slack.openmined.org/)""Don't translate ""torch"" ;)Don't translate ""torch"" to ""Fackeln"" - it is a nameI think we can do both. But, if you say I will change all of ""Notizbuch"" to ""notebook"".I translated it using a part-1 English tutorial. So that's why I used ""Wie bekomme ich zus√§tzliches Guthaben?"", instead of ""Was kann ich zus√§tzlich tun?""- I had translated English tutorials as it is.

- I don't think ""z"" will be changed to ""zB."" because of ""such as."" in English doesn't exist.

- I had done a mistake in this one. As I am a beginner in German.the issue has been fixedOkokFixed it in all notebooks.",3,False,2020-03-05 21:01:03,2020-03-11 12:48:14,2020-03-11 12:48:14
https://github.com/OpenMined/PySyft/pull/3142,[],Training models in PySyft using CrypTen,"Training models in PySyft using CrypTenCrypTen assume that each party will be running the same function as all the other ones, so we need a way to have Syft workers exchange functions.

Here I experiment making Syft workers exchange function as source code, workers will make sure that the source code is a function definition and execute it in a secure environment. Built-in functions aren't accessible and someone can't import arbitrary module but is only limited to what CrypTen provide.

I was able to replicate [tutorial 7 of CrypTen](https://github.com/facebookresearch/CrypTen/blob/master/tutorials/Tutorial_7_Training_an_Encrypted_Neural_Network.ipynb) in  examples/experimental/secure_exec/test.py, some changes might be applied first to CrypTen, be sure to apply the patch found at  examples/experimental/secure_exec/update_data_attr.patch using `git apply`, this changes are related to issue regarding the `data` attribute of torch tensors.

This will allow us to experiment use-cases and see what should be implemented next.Quick suggestion to have recursivity to more complexe responses:
```python
def _pack_values(values):
    """"""Pack return values to be passed into a queue then sent over the wire.
    The main goal here is to be able to return torch tensors.""""""

    packed_values = []
    # multiple value
    if isinstance(values, (tuple, list)):
        for value in values:
            packed_values.append(_pack_values(values))
    # single values
    else:
        packed_values.append(_pack_value(value))
    return packed_values
```some small changes also would be required here for more complex nested responses I think you can do:
```
with torch.no_grad():
    p.set_(torch.tensor(p_val))
```Much better !What I was checking here is either the function is returning multiple value (which then should be a tuple) or a single value (think `return model, loss` and `return tensor`). When 2 values gets packed, 2 should be unpacked, I'm thinking of this process more like a pipe allowing for returning remote objects, but the returned values should remain the same. Correct me if I'm wrong but this will flatten the returned values, right?",1,False,2020-03-05 18:15:46,2020-04-05 09:48:03,2020-04-05 09:48:02
https://github.com/OpenMined/PySyft/pull/3130,[],Add documentation,"Add documentationPartially solves #2510
> Added docstrings to test_callable_pointer.py",1,True,2020-03-02 09:32:32,2020-03-11 13:48:42,2020-03-06 14:18:46
https://github.com/OpenMined/PySyft/pull/3120,[],Add and refine document,"Add and refine documentThis PR makes a contribution to #2510 

1. Refine tutorial.
2. Remove unnecessary dependencies. 
3. Add document.
Could you use 'R' in place of 'r' such that it follows the same pattern as the above comment (""Create a dataset"")Could you use 'A' in place of 'a' such that it follows the same pattern as the above comment (""Create a dataset"")```Raise an error```Could you use 'A' in place of 'a' such that it is the same as above?Could you use 'A' in place of 'a' such that it is the same as above (with big ""C"")?Q: Maybe remove this comment - I think the attribute name is self-explanatory.",3,True,2020-02-28 22:37:42,2020-03-02 10:41:16,2020-03-02 10:41:16
https://github.com/OpenMined/PySyft/pull/3115,[],Fix failing tests on master,"Fix failing tests on masterRNG state coupling between tests still needs a more thorough fix, but
this will make the tests pass for now.",1,True,2020-02-27 19:03:44,2020-02-27 19:22:22,2020-02-27 19:22:22
https://github.com/OpenMined/PySyft/pull/3103,[],Update Part 10 - Federated Learning with Secure Aggregation.ipynb,"Update Part 10 - Federated Learning with Secure Aggregation.ipynbUpdate translation, Fixes #2778",2,True,2020-02-26 10:16:54,2020-02-28 18:42:03,2020-02-26 16:30:03
https://github.com/OpenMined/PySyft/pull/3076,[],[WIP] Crypten POC for plans,"[WIP] Crypten POC for plansNeed to rethink thisShouldn't the plan be transparent here? Like the user would only decorate his function and we build a plan out of his functionThe load function shouldn't fail and it would return a real `cryptensor`, right? Then it's not the fake `get_plain_text` that's called here.Yep, it should. I added it like this to have a fast testing mechanism if the plans work,Hmm, when the plan is building the  ```load``` should go on the Exception branch from here:
```
try:
    response = func(*args, **kwargs)
except Exception as e:
    response = th.rand(kwargs[""size""])
```
And it should be a dummy response (only to have the shape of the response) and the result should also be a ```        th.randn(self.shape)``` from ```syft/generic/tensor```",1,False,2020-02-20 11:53:32,2020-04-02 14:54:08,2020-04-02 14:54:08
https://github.com/OpenMined/PySyft/pull/3074,[],Syft load based ONLY on rank,"Syft load based ONLY on rankFixes #3043 
Send the party information in the init message such that we need to keep track only of the worker id.If the worker is the owner of the object - then simply return the objectThis plays a bit fast and loose with types (tensors vs pointers), which works fine in Python but becomes kind of a pain to encode in the Syft protocol and deal with in other languages. If this is a convenience change that isn't required for the main purpose of the PR, I'd probably rather leave part alone.This PR does not depend necessary on this (I can call ```get``` in the ```load function```).
I think the problem was more ""semantics"" wise - like if you have the tensor  - why you should get a pointer to that tensor and not the effective tensor?To make the return type consistent. If sometimes I get one thing and sometimes I get another thing, then I have to check what was returned in order to know how to process it, which proliferates `if` statements through the code base.Yep, you are right. I will update this.we should keep crypten's paradigm of checking `src` value for differentiating the operations to be done, some issues may if the `get_worker` return a worker for a non-expected `worker_id`spacesDoneI modified to use the rank :)Can't we simply check either `src == comm.get().get_rank()` ?Yep...you are right. I overcomplicated this :(",3,False,2020-02-20 00:56:56,2020-03-19 10:36:19,2020-03-12 09:30:03
https://github.com/OpenMined/PySyft/pull/3067,[],Remove decode instruction from tags/description,"Remove decode instruction from tags/descriptionTags and descriptions are no longer received as binary values during the `pointer_tensor.detail` method.
Therefore, we can remove the decoding instructions.
These changes solve  the [PyGrid issue](https://github.com/OpenMined/PyGrid/issues/473)",2,True,2020-02-19 08:51:26,2020-02-19 15:50:26,2020-02-19 15:50:25
https://github.com/OpenMined/PySyft/pull/3057,[],Function Secret Sharing,"Function Secret Sharing### About
This PR proposes an implementation and some refinements of Function Secret Sharing.

Function Secret Sharing (FSS) (introduced with great details [here](https://eprint.iacr.org/2018/707.pdf) and [here](https://eprint.iacr.org/2019/1095) is a technique which allows to compute over secret shared data. Details of this cool method will come in a future blog post.

Most interestingly, this is the first internal use of Plans to improve efficiency of crypto protocols by reducing the number of interactions, which is made necessary because the great benefit of FSS over other MPC protocols like SPDZ is the small number of communication rounds.

What's the type of `return_value` here and what is it for? Should be added to the docstring.Does this mean I can set `return_value` to `False` and still get a response? That seems surprising.How does the sender decide whether or not to request a return value?üëç 

We should make `ObjectStorage` work like this too (eventually.)The PEP8 standard for avoiding name collisions with reserved words like `class` to append an underscore like `class_`.@karlhigley This is a way to do a combined remote op + get in a single communication round
Classical workflow in PySyft requires two separate calls, and because in work on low interaction crypto protocols I suggested this optimization.
However, this should only be valid when .get() is valid ie it shouldn't provide extra rights
I have probably left a security issue here... But this idea is not supposed to allow people to do weird stuff!Could you add a line to test that no keys were removed when setting remove to False?Sorry if it's just me but I find using filter with a lambda function less clear than:
`{id_: obj for id_, obj in self._objects.items() if isinstance(obj, FrameworkTensor)}`Could you add some docstring to say how and when it's used?Why are we changing that? Maybe we already answered this question but what is the difference between remote_send and move now?
Because I see written
```
def move():
     return self.remote_send()
```Can't we do `torch.tensor([1.0, 2, 3, 4, 5]).send(bob)` anymore?Why is this translation needed? Couldn't we use directly ""fss_eq"" everywhere?Could you specify the form of the primitive_stacks somewhere in the docstring?
Because I don't know if it could be simpler or if I just miss something ^^What about having a dict attribute instead of using getattr?DoneI agree!Because remote_get is a remote operation which shouldn't change the object you're working on; it's still the same pointerOh no we're doing the opposite!Will fix thisThere isn't much difference actually, except the one you just mention: it's the same remote operation but you get as an answer not the same pointerYou can but I need to keep a ref to `t` :) I've added it :) Not sure it makes a big difference right?Oops, indeed üëç Not at all, it's just that I don't like `getattr` very much ^^
But after having read it again, I'm fine with it""very multi-dimensional"" -> ""high-dimensional"" or something like that?nit: Could this comment be between `"""""" """"""` and not prefixed with `#`?Oh oh, a `#TODO`What about the poor Beaver? üôÅ 
typo `in` -> `is`
Plus, I don't get the end of the comment but maybe it's because I don't know enough about these keysThe `torch.narrow()` method didn't do the job?Would len(primitive_stack) be always the same for each type? (For instance always 3 for beaver)
In this case, we could init self.beaver to ([], [], []), what do you think?Not sure I followed everything but this method never uses already created primitives, does it?
Do you think it could call `get_keys()` and `get_keys()` would return or create keys if there are not enough of them?Could you add a bit of doc here?What is the reason for this to have changed?Will come in a later PR :)
First i'd like to have automatic generation of the preprocessing, and much more (including my FSS numpy PR which is almost pushed)I've added details here!<3 So much better! <3It might vary depending on the implementation. For example depending of the bit precision, (now 32), the number of elements in the FSS keys would change with the current implemThis method is here to feed the different crypto stores, so shat get-keys will work.
It's here to decorrelate preprocessing and online phase: provide_primitives should be called way before the computation really happens, to have a fats online phaseJust a small optimization nothing essential hereRandom thought here.
Isn't it possible to use pointer to plans to request a run?Yes you can, but you need an extra round of communication to get your pointer",6,True,2020-02-16 16:27:27,2020-05-01 12:07:42,2020-05-01 12:07:40
https://github.com/OpenMined/PySyft/pull/3048,[],Fix broken asynchronous federated learning tutorial,"Fix broken asynchronous federated learning tutorialThe methods federated_client.evalute() and federated_client.async_fit() are now called via the new Baseworker.execute_worker_function() function. Before it relied on Baseworker.execute_command() whose behaviour changed and is meant to be used for Tensor functions only. 

The pullrequest needs a modification in syft-proto. You can put isinstance syft.FrameworkTensor I think here and not have to import torchThis method name is a bit confusing, it's very generic, while we are performing a specific ExecuteWorkerFunctionMessage order.
Maybe it could be renamed `create_message_execute_worker_function` or smthg like this?",1,True,2020-02-11 13:15:55,2020-03-11 11:45:57,2020-03-11 11:45:56
https://github.com/OpenMined/PySyft/pull/3037,[],Fix potential medium/high severity security holes,"Fix potential medium/high severity security holesIn order to # #3036 to merge on master, some minor edits need to be made and remove the few warnings that show up.",4,True,2020-02-09 05:29:54,2020-02-18 00:52:30,2020-02-18 00:52:30
https://github.com/OpenMined/PySyft/pull/3024,[],[Minor fix] badge for notebooks test at README is broken,"[Minor fix] badge for notebooks test at README is brokenI just removed a ""\n"" 

[look](https://github.com/OpenMined/PySyft/workflows/Run%20notebook%20tests/badge.svg) :carousel_horse:",1,False,2020-02-07 20:32:17,2020-02-07 21:32:28,2020-02-07 21:10:43
https://github.com/OpenMined/PySyft/pull/3022,[],Tutorial Notebook 10 note on numpy hook,"Tutorial Notebook 10 note on numpy hookHi,

I have seen a lot of questions in the community forum about errors when using Part 10 notebook because numpy is not hooked ""yet"". 
I thought it made sense to put a small disclaimer at the top so people are aware. It can be removed once the issue is fixed.",1,True,2020-02-07 19:03:45,2020-02-18 14:24:01,2020-02-18 14:24:00
https://github.com/OpenMined/PySyft/pull/3021,[],Remove a stray line break to fix notebook test badge,Remove a stray line break to fix notebook test badge,1,False,2020-02-07 15:28:02,2020-02-09 15:02:54,2020-02-09 15:02:53
https://github.com/OpenMined/PySyft/pull/3020,[],Translate PySyft Tutorials to Portuguese,"Translate PySyft Tutorials to PortugueseTranslated notebooks:

- [x] Part 12 - Train an Encrypted Neural Network on Encrypted Data.ipynb
- [x] Part 13b - Secure Classification with Syft Keras and TFE - Secure Model Serving.ipynb

Issue #2776

**Note: The build is failing because it cannot find the `short-conv-mnist.h5` file on Notebook _Part 13b_, but the file is a result of a `model.save` operation on the Notebook _Part 13a_, so it needs to be run first.**Part - Parteadd a space beteween ' ` ' and 'e'Oh, I forgot about this ""Part"", hahaDone!",2,True,2020-02-06 15:59:34,2020-02-10 20:32:28,2020-02-10 20:32:28
https://github.com/OpenMined/PySyft/pull/3013,[],Remove travis,"Remove travisFrom a discussion with @karlhigley , now that the project has migrated to github actions, travis still takes a long time to run. Leading to errors on checks even if everything passes, which prevents us from merging good pull requests.We'll probably want to replace the Travis build status badge with a [GH Actions build status badge](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/configuring-a-workflow#adding-a-workflow-status-badge-to-your-repository).Might be worth creating an issue for this.Done, Also added a second badge for notebooks, currently its broken, but should work once my other pull request is merged to master.",2,True,2020-02-05 03:18:35,2020-02-07 15:26:12,2020-02-07 15:26:12
https://github.com/OpenMined/PySyft/pull/3004,[],Add tanh Chebyshev approx,"Add tanh Chebyshev approxThis implementation is taken from the FacebookResearch Crypten project[1].
The PR that adds this functionality in Crypten is here[2].

[1] https://github.com/facebookresearch/CrypTen
[2] https://github.com/facebookresearch/CrypTen/pull/45I'm curious, `2 * sigmoid(2 * tensor) - 1`  wasn't precise enough or too slow?In the PR, they say this:
```
Implemented improved tanh approximation that's ~43.5% more accurate (measured by total relative error) and ~33% faster (see n196073).
```
Before this, they also had the implementation using the ```sigmoid```Ok cool!",1,True,2020-02-03 20:59:50,2020-02-26 16:36:38,2020-02-26 16:36:38
https://github.com/OpenMined/PySyft/pull/2982,[],Increase field to 2**64,"Increase field to 2**64Issue: #2981
General:
- [x] Documentation fixes (remove my debugging notes)
- [x] Remove CRTTensor
- [x] Remove LargePrecisionTensor
- [x] Tutorial Part 9 fix in all languages
- [x] Optimisations

FPT:
- [x] Get FPT operations working for 64 bit and 32 bit field
- [x] Add dtype arg (long: 64bit field and int: 32bit field)
- [x] Typecasting and test for dtype arg
- [x] Add tests for ops on 32 bit field

AST:
- [x] support dtype arg
- [x] Add test for dtype and ops on 32 bit field ~(only div left due to snn issues)~
- [x] Get AST ops working for 64/32 bit field
- [x] Refactor cat and stack
~- spdz mul issues~
~- securenn div issues~
~- securenn relu_deriv issues for comparison ops~
Turns out there is some issue with `.send` that's causing above 3 to fail so just need to find it
Problem: 2**64 field arg was causing overflow during serialisation

Serialisation:
- [x] Msgpack hacky solution: convert field from int->str before serialization and str->int after deserialization 
- [x] Protobuf support for 2**64 values
- [x] Add dtype in protobuf serialisation
- [x] syft_proto changes to support above changes

Beaver:
- [x] Negative share support for triples

SecureNN:
- [x] Update Qbits and remove modulo that can be handled by native tensor
- [x] Investigate and handle behavior of shares with L-1,p field values

Investigation Report:
Problem: Need some way to support custom fields like old times. And for that need custom modulo function to handle negative share values properly.
Solution:
AST:
- [x] custom field support in AST using dtype
- [x] custom modulo function for negative custom field support in AST
- [x] AST test for dtype=""custom"" and field=67 and field=2**64-1 ~(private_mul spdz dtype mismatch issue)~
- [x] Tests for custom dtype(with field=67 and field=2**64-1)

SecureNN:
- [x] - 32 bit support (any operations between torch.IntTensor and torch.LongTensor give result as torch.LongTensor and that's a problem also initialise Q_bits based on field value instead of default 64)
- [x] Tests for 32 bit ops (need to resolve recursion error in share_convert and msb output shares always in 2**64 issue)

This actually should be imported from `syft.execution.placeholder`.Ok yeah, I messed it up while resolving merge conflict probably. I will fix it.Can you add a small explanation about this new arg in the docstring?nit: use `field is None`nit: `if isinstance(field, int) and field > 0` is clearerMaybe the user used dtype but didn't write one of the possible choices for this argument. Maybe you could check what dtype is and specify a bit the warningnit: this naming makes me think that this a function. I think `self.torch_dtype` or something like that would be sufficient.Are you forcing the type of x to be the same as the one of self?Maybe add a small comment to say that the modulo is performed automatically with torch.int32 and torch.int64 values (if I'm not wrong ^^)nit: maybe the part where the if is needed could be performed in `cat` and `stack` directly?Just a question: why can we have a custom field for AST but not for FPT?Could you explain what you're doing here?Maybe write `str(2^64)` instead of ""18446744073709551616"" so that it's clearer what is meansYes, for automatic modulo based on native tensor dtype.Initial plan was to not provide an option for custom field at all for both FPT and AST. But there are certain use cases in securenn that demand field to be 67 and 2^64-1(for dtype long) and 2^32-1(for dtype int). 
So had to introduce custom dtype option that should only be used internally. But we don't need anything of the sort for FPT.msgpack can't pack python integers larger than 2**64-1 and causes an error like: ""Python int too large and can't be converted to C unsigned long long"". So we need to convert all values larger than that into string for successful serialisation. Oh yeah, sure. I will change itI just find it a bit weird that this function is applied to x, takes a field and uses self for the dtype (which should be possible to infer from the field, am I wrong?).In the future, (long after this PR is merged), if we wanted to represent something bigger than ""long"" - how would we do so?

(Hint: please mention something about a separate BigIntTensor abstraction tensor type  - not extending FixedPrecisionTensor with the ability to represent even larger precisions.)Please write a github issue for future work here before mergingOkay, I will check and add appropriate message for that.Yeah that would be much better. I will rename itYes, that does seem like a roundabout way of doing it and it can cause if not issues then confusion in the future. I will modify it to derive torch_dtype from the field itself here.Yes you are right. I will add the comment.And then, if self is not used, you can make this a static method :)Sure, I am on it.Yes that will be for the best. As I can revert generate_shares to be static too then. Okay, I am on it.Yes, done.ok, doneYeah, doneWhat does `typeCode == 5` signify here? Which type does `5` mean?It's the syft-proto/proto.json defined code for type strDo you need to hardcode it? Isn't there a way to have the code from the type?I haven't explored that possibility yet. I will look into it.I think you can use something [like this](https://github.com/OpenMined/PySyft/blob/ba804732961722d9e9b41e19fd580691bf5ebf42/test/serde/msgpack/test_msgpack_serde.py#L51).Done :+1:```suggestion
from syft.serde import compression
from syft.serde import msgpack
```Any reasons for doing it this way? or it's just about being clear over concise.It makes the diffs of future PRs cleaner if one of those needs to change, which makes rebasing/cherry-picking/merging easier.field_to_dtype, n_bits, powers, moduli on depend on `field`
Can you make an auxiliary funciton to get those from field, and use the decorator @memorize i've created in this PR?
https://github.com/OpenMined/PySyft/pull/3057/files#diff-bbd4e6d1b15d378099f2332dbef12baeUse the same trick for all the times you use field_to_dtypesamesamesame (I let you check all the others ;) )can you create for additive shared tensors a .min_value (= -(L // 2)) and a max_value to avoid such calculations
Those should be properties computing it and caching the result when computed the first time in ._min_value.
If you're afraid that changing the field would break this, you can make the field be a property with a setter which updates min & max valueyou're computing twice here. Define it in a vairable to optimize
```
a_tilde_shares[workers[0].id].copy().get()
    + a_tilde_shares[workers[1].id].copy().get()
```
Also add a comment to explain why we do this double check :) what is this? Is this what we discussed?
I guess: if you don't use it, comment all the lines producing theta_sh, but explain why you do this, else you should use itsmall remainder to cache all thoseas wellis this one needed?can you set it during the switch juste above?Put `self.dtype = dtype` outside the switch to factorize  (and you overload only when need with the self.dtype = ""int"" / ""long"")this will be optimzied  with min_value and max_value
`torch_dtype = torch.int64 if field > 2 ** 32 else torch.int32` should be cached with some mechanismThis method generate up to 2 private comparisons -> this can be a new expensive piece right?
Unless x is not an additive sharing tensor? Can you type annotate and add an explicite type check as well?I'm confused that self.modulo is applied not on AST, so a type check will help in def modulouse  min_value and max_valueI wonder if you could refactor modulo to include this if self.dtype == ""custom"" check?
haha maybe we can rename this `share_combine`?same remarks than in ASTI wonder if all this logic if still required for signed sahres? If it's not, you get a big speed up here!cache `str(2 ** 64):` in a variableI left it there initially because we were using sign related info below in some cases. 
But now upon reconsideration it is clear that it is just redundant with signed shares and should be removed to save computation. DoneYes, I am still not entirely clear on the maths behind it but as we discussed it seems a simple add operation with zero shares in L-1 field gets the job done here. 
I will comment unused parts out and open a separate issue for further inspection into this.x can be either AST, PointerTensor or simple torch.Tensor so I don't think we can annotate type for the function.It is definitely expensive but we currently only need it for custom field ops that are in securenn.Done :+1:SureYes, I believe it solves the problem where spdz output was causing recursion depth limit exceeded issues in `modulo` function for custom field value ops.Done :+1:DoneCan't I just do this with memorize decorator on a field_max and field_min methods?We call this modulo method only for custom dtype which is only for internal use so I know it will only get 3 field values (2^32 -1, 2^64 -1, 67). Which can be memorized to speed it up .I made an auxilliary function for this alongside max and min with memorize decorator.Update: Removing this is causing shape mismatch assertion error when linalg calls securenn.division. Need to investigate further. Keeping this logic in place for now.I agree! But as the  `(L - 1) // 2`  isn't that slow, you won't get substantial gain using memorize like this. I would suggest keeping those as attributes for the AST (well property to be exact) and compute them them only when required (possibly using memrize this time). That's not the thing which will bring huge speed up but it will help :) Commented everything out. Now it's just `u_sh(in L-1) + a_sh(in L)` and it works.don't forget to cache this onedon't forget to cache this oneCache this :) ping here, this would make your code simpler :) > shape mismatch

That's intriguing!Oh, I missed this one. I am on it :+1: DoneRefactored modulo as non-static now we can use self.torch_dtype directly and for min/max using propertyI added the property for min_value and max_value but I am not sure how to cache it into private variable after computing first time from there. Need help.```suggestion
    # declare in the init  self._min_value  = None and same for max
    @property
    def min_value(self):
        if self._min_value is None:
            self._min_value = -(self.field // 2)
        return self._min_value

    @property
    def max_value(self):
        # same here
```Thanks, done :+1:Naive conversion doesn't work! This commit introduces subtle bugs in everything that depends on convert share and thus most of SecureNN

Simply try:
```
x = torch.tensor([-2, -1, 0, 1, 2]).share(alice, bob, dtype=""custom"", field=32, crypto_provider=crypto).child
relu_deriv(x).get() #gives garbage results 
```

Please read carefully the papers; if a mathematician has introduced tons of weird operations, it's probably for some reason.

EDIT: Rereading this, I realize I was unnecesarily rude. I'm sorry for my language.@knexator Could you create an issue for this?@knexator Well we did investigate it quite a bit. But you are right I missed out on it maybe tests I used weren't strict enough. Anyway let's open an issue and work to resolve it quickly.It was left as comments for precisely this reason so that it can be further investigatedAlso dtype=""custom"" is not intended for user. I think we need someway to make sure that user can't use custom dtype to share the tensors while at the same time it can be used internally for secureNN ops.#3389 @knexator It's alright, no worries :slightly_smiling_face: . Let me know if you need help wrt any changes made here.",12,True,2020-01-30 22:37:23,2020-06-01 11:26:44,2020-04-20 17:19:47
https://github.com/OpenMined/PySyft/pull/2963,[],Context of computation for crypten,"Context of computation for cryptenThe new context of computation can run parties that are distributed across syft workers. The communication of crypten parties remains the same, however, syft workers handle initialization and the serialization of return values.

The crypten computation done for the moment is a toy function defined at worker level. The exchange of function isn't yet supported for security reasons.

This PR introduces:

- A new message type CryptenInit that lets workers exchange information such as rank of crypten party to run, number of parties involved, and ip and port of the master party.
- A new router for the CryptenInit message type that runs crypten party according to the information received.
- A function decorator (that doesn't distribute the function yet) to distribute the function and run it as crypten parties across workers.

![PySyft-Crypten-context-of-execution](https://user-images.githubusercontent.com/21220087/73119691-cd7d2900-3f65-11ea-91aa-afd70c8f8aee.png)


#### TODO

- [x] Tests
- [x] Add CryptenInit message to protobuf OpenMined/syft-proto#30Is there any importance to join first the process and then the thread?Can you do stuff like this?
```python
alice_t = crypten.cryptensor([73, 81], src=0)
bob_t = crypten.cryptensor([90, 100], src=1)
out = (alice_t + bob_t).get_plain_text()
```No, we can even later try to make it non-blocking.Just tested this locally to make sure. It works perfectly.```python
>>> import torch
>>> import crypten
>>> import syft
>>> from syft import WebsocketClientWorker
>>> from syft.frameworks.crypten.context import run_multiworkers
>>> 
>>> 
>>> hook = syft.TorchHook(torch)
>>> alice = WebsocketClientWorker(hook=hook, id=""alice"", host='127.0.0.1', port=8777)
>>> bob = syft.VirtualWorker(hook=hook, id=""bob"")
>>> 
>>> @run_multiworkers([alice, bob,], master_addr=""127.0.0.1"")
... def test():
...     pass
... 
>>> test()
{0: [163.0, 181.0], 1: [163.0, 181.0], 2: [163.0, 181.0]}
Q: The Thread is done only for the message to arrive ""nearly"" at the same time to all parties?The _send_party_info is blocking, and won't return until all parties are initialized and return their results, we need to send all messages in parallel.",2,True,2020-01-25 10:32:39,2020-02-05 11:04:17,2020-01-31 17:14:28
https://github.com/OpenMined/PySyft/pull/2959,[],WIP: interim commit for feedback,"WIP: interim commit for feedbackPR for feedback:

current state of notebook as screenshot https://share.getcloudapp.com/z8un95GO

(don't worry ... I will rebase / squash / clean up before merge ... )

one weird thing is I get this error when specifying a shape in the float tensor like 
`r = test.promise_tensor().FloatTensor(shape = (3,3))` (see notebook or notebook screenshot for details)
https://share.getcloudapp.com/2NurQx57

if I don't specify shape it works.

If don't set https://github.com/OpenMined/PySyft/pull/2959/files#diff-c98d0a76eff05791989ddd31f5dcc7a6R13 ""shape=None"" by default ... even if I specify shape in the float tensor I get ""missing args for shape"" error.

I haven't seen this particular error before, but I can say that I've been wrestling with serialization a lot, so I'm not surprised that it doesn't work as expected right off the bat. It might be useful to print and/or use the debugger to examine the contents of `simple_objects` on line 280. It should only contain Python primitive like lists, tuples, integers etc, and if there's other stuff in there that might be what's causing the issue.<p>I think making the method <code>move</code>  do something other than moving the tensor is going to be a surprising and non-intuitive API design choice. Can we keep the existing semantics of <code>move</code>  and add a second method that extends the graph across workers?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2959/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='21'/><p>Could we use different example roles/people here?</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2959/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='24'/>good call, I will look into that!ack(acknowledged!) @iamtrask ^ your thoughts?ack(acknowledged!) => will change (although this notebook is mostly for my own debugging purposes... probably won't get actually merged)Sounds good to me (and should be an easy change).

This does beg the question though, what does .move() do in this context? I suppose it would move the PromiseTensor without extending the graph?I haven't seen this error either. Let's try Karl's idea.",4,False,2020-01-24 02:59:32,2020-02-09 15:11:28,2020-02-09 15:11:28
https://github.com/OpenMined/PySyft/pull/2940,[],fix 3 bugs in the tutorial Part06-07,"fix 3 bugs in the tutorial Part06-07in Part07: boston_data is a tuple. it cannot be index with str type as ['alice']. we should index with boston_data[0]['alice'] to avoid ""TypeError: tuple indices must be integers or slices, not str""

in Part06: 2 strange Runtime error about cuda. 
1) ""RuntimeError: Expected object of device type cuda but got device type cpu for argument #1  'self' in all to _th_set_"" can be fixed with ""torch.set_default_tensor_type(torch.cuda.FloatTensor)  # fix ERROR: Runtime error when move the model to device. ""
2) ""RuntimeError: Caught RuntimeError in DataLoader worker process 0. RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method"" can be fixed with ""torch.multiprocessing.set_start_method('spawn') ""If it's only for cuda I'd make it optional base on the boolean `use_cuda`Yes, I just tested on CPU. These errors are only for a cuda device.",2,False,2020-01-21 15:38:24,2020-02-01 02:34:07,2020-02-01 02:34:07
https://github.com/OpenMined/PySyft/pull/2936,[],Migrate CI tests to Github Actions,"Migrate CI tests to Github ActionsThis also splits up the test suite so that the notebook translation tests are only run when there's a change to the tutorials in order to speed up the builds. The (untranslated) basic and advanced tutorial notebook tests are run in every build, so we'll still know if changes to the rest of the code base break the notebooks. Making changes to the tutorials to fix issues caused by other parts of the code base will then trigger the translation tests.Not sure this will help to speed up things much, but what if we run linting tools before deps installation?
Turnaround for formatting errors would be faster, in theory :)`--omit=""syft/grid/*""` is removed in travis config, but left here - is that intentionally?
This should be also triggered on `pull_request` event so it works on PRs?
And `push` event should probably [exclude](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/workflow-syntax-for-github-actions#excluding-branches-and-tags) `master` branch - so we don't run tests on master after PR is merged? (Assuming nobody is committing directly to master :))

Keeping test when merging on master is good to make sure that nothing got wrong, and to have the ‚úîÔ∏ésign on our last commit which is a good security when someone reaches out saying master is broken ;)It already works on PRs using `push`, from what I experienced on another projectI agree!You might need to keep  `pip3 install -r pip-dep/requirements_dev.txt` however@LaRiffle hmm, that's strange. Technically this is a push to a forked repo.Oh yes on a forked repo it might not work :/
But on branches from OpenMined/Pysyft it should (but they are not a majority)Ah, forgot to remove it here. It‚Äôs been moved into setup.cfg.I was considering running linting as a completely separate action, since the first failure will cancel other actions in progress. Let's leave that for a future issue/PR though?",7,True,2020-01-20 17:15:47,2020-01-23 16:35:24,2020-01-23 16:35:23
https://github.com/OpenMined/PySyft/pull/2922,"['bug ', 'documentation ']",Update hook.py,"Update hook.pyBased on comment here: https://github.com/OpenMined/PySyft/pull/2913#discussion_r367878631 
`HookedTensor` is missing a `numpy_tensor` argument.
Submitting the fix.I think we actually want to make the reverse change here; the Numpy code got moved over to [NumpyTensor](https://github.com/OpenMined/PySyft/blob/be9e0d894e7fe09cf22f93a3a27ad42a3903a037/syft/frameworks/torch/tensors/interpreters/numpy.py), so we should probably remove both the `numpy_tensor` parameter and the docstring explanation of it.oh okay. fixed it now.",3,True,2020-01-18 01:29:19,2020-01-18 18:00:36,2020-01-18 18:00:19
https://github.com/OpenMined/PySyft/pull/2919,[],Split notebook tests into separate tests per notebook,"Split notebook tests into separate tests per notebookThis will give us a better idea of which notebooks are failing when we have problems, and should resolve #2880 in a way that doesn't run afoul of the test coverage checker.This notebook is now in the excluded list above.
Maybe test should be skipped then?Hmm, seems redundant subset of previous test.
It should always have the same result as `test_all_non_excluded_notebooks`, no?
The only case when a notebook is untested seems to be when it's outside of `tutorials`, `tutorials/advanced`, or `tutorials/translations` folders, but translated_notebooks are always inside `tutorials/translations`Yeah, this test is partially redundant, but on purpose. What I'm about to do is set up Github Actions so that the basic notebook tests get run on every PR, but the full notebook tests including the translations only get run when something in the examples directory changes. One or the other of these tests will be excluded from the command line in each check suite action.Ah, good catch. I think this test only runs successfully if you first change directories, so it fails if run via the standard parametrization, but passes here.",1,True,2020-01-17 19:11:24,2020-01-19 22:10:49,2020-01-19 22:10:48
https://github.com/OpenMined/PySyft/pull/2918,[],Split tutorial notebook translation test into separate tests per lang,"Split tutorial notebook translation test into separate tests per langThe single translation test was taking more than 10m to execute, which was leading Travis CI to infer that our tests were frozen and terminate the builds.",1,True,2020-01-17 18:02:27,2020-01-17 18:46:58,2020-01-17 18:46:57
https://github.com/OpenMined/PySyft/pull/2909,[],"Hindi translations for Part 5, 6, 13b and 13c","Hindi translations for Part 5, 6, 13b and 13c@Yugandhartripathi please review the translations for parts - 5, 6, 13b, 13c
<p>Here translator and editor names seems to be redundant as they are also mentioned below author names.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2909/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='0'/><p>English version of notebook doesn't contain any author names. Did you check the history for it? Also, if you can add link to your profile(s) as well.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2909/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='1'/><p>Remove white space between <code>]</code> and <code>(</code> in links following markdown syntax. Also translator messed up author's github link username part should be <code><a href=""https://github.com/LaRiffle"" target=""_blank"">LaRiffle</a></code></p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2909/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='1'/><p>Change <code>.‡§´‡•á‡§°‡§∏‡•á‡§ü</code> with <code>.federated</code>.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2909/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='8'/><p>Fix markdown between <code>alice</code> and <code>bob</code>.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2909/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='12'/><p>This is redundant(english ver).</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2909/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='16'/><p>Fix markdown syntax for bold it needs double asterisk on both sides.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2909/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='20'/><p>In first line change ‡§™‡§æ‡§á‡§ü‡•ã‡§∞‡•á‡§ï&nbsp; with PyTorch. In 4th <code>‡§è‡§®</code> to <code>n</code>. Fix markdown for all links, proper syntax is [visibleText](actualLink) without any space in between.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2909/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='21'/><p>Thanks Yugandhar!</p><p> </p><p>For some reason, my jupyter notebook doesn't show the 'Authors, ... ' cell. I  had earlier removed it, precisely because the English version doesn't have this information. But apparently, it still somehow lurks in there....  Would you know why?</p><p>Will add github link of my profile here :)</p><p> </p><p>Again, doesn't show up in my Jupyter notebook  :(</p><p>Done!</p><p>Done and done!</p>done!got it!<p>I wonder why it's showing up when it's clearly not in your branch. Anyway I guess it's resolved for now.</p>Then we don't need to worry about it. I got no idea why it's showing up there but as long as branch is clean that's all we want.<p>Change ‡§ü‡•á‡§®‡§∞ to ‡§ü‡•á‡§®‡•ç‡§∏‡§∞</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2909/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='4'/><p>done! :)</p>",8,True,2020-01-14 19:30:10,2020-02-07 16:03:10,2020-02-07 16:03:10
https://github.com/OpenMined/PySyft/pull/2902,[],"Hindi Translation part 11,12 and 12-bis","Hindi Translation part 11,12 and 12-bisMajor changes:
 - Added translated part 11 notebook in `examples/translations/hindi/`
 - Minor typo fix in hindi part 3
 - Added translated part 12 notebook in `examples/translations/hindi/`
 - Added translated part 12-bis notebook in `examples/translations/hindi/`
 - Minor typo fix in english part 12-bis",1,True,2020-01-13 14:58:26,2020-01-19 16:20:41,2020-01-17 14:50:36
https://github.com/OpenMined/PySyft/pull/2899,[],Translation to Indonesian Tutorial - Part 01,Translation to Indonesian Tutorial - Part 01Partial fix #2898,1,True,2020-01-12 09:09:22,2020-01-17 14:50:54,2020-01-17 14:50:54
https://github.com/OpenMined/PySyft/pull/2891,[],Update Part 11 - Secure Deep Learning Classification.ipynb,"Update Part 11 - Secure Deep Learning Classification.ipynb.fix_precisio‰Ω†ÔºàÔºâ ==„Äã.fix_precision()

there is a typo error.",1,True,2020-01-09 13:01:21,2020-01-11 14:41:01,2020-01-11 14:41:01
https://github.com/OpenMined/PySyft/pull/2890,[],Add tutorial 2 romanian,"Add tutorial 2 romanianPartially fixes #2792. 

Add the 2nd tutorial translation in Romanian.",1,True,2020-01-09 11:56:45,2020-01-14 09:32:47,2020-01-14 09:32:47
https://github.com/OpenMined/PySyft/pull/2880,[],Add useful error message for failing notebook,Add useful error message for failing notebook,3,False,2020-01-03 03:45:13,2020-01-17 19:31:17,2020-01-17 19:31:17
https://github.com/OpenMined/PySyft/pull/2875,[],Add sk-learn to notebook docker image,"Add sk-learn to notebook docker image[Tutorial 5](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2005%20-%20Welcome%20to%20the%20Sandbox.ipynb) and presumably some others require sk-learn in order to work correctly. This was causing some errors to run with the default notebook docker image.

This patch adds the `sandbox` option to installing syft for this the notebook docker image.I think it shouldn't be a space there",2,True,2019-12-31 16:35:13,2020-01-07 17:03:42,2020-01-07 17:03:42
https://github.com/OpenMined/PySyft/pull/2871,[],Make grad_fn serializable,"Make grad_fn serializableSolves #2797 
This PR makes grad_fn serializable.Does this depend on changes in `syft-proto`? If not, we should probably leave the requirement version specification.Yes. I have added GradFunc proto code in proto.json
https://github.com/sukhadj/syft-proto/blob/88b27455905264e9d3e11950936ab2b7e5493aed/proto.json#L159Ah, okay, cool. Could you open a `syft-proto` PR for that change?YesThis file shouldn't be changed I thinksmaeWould you be willing to fix the `AutogradTensor.__init__`  to fix this? Should be very quickMaybe this is not restrictive enough, can you add a check on the type (should be a Tensor I think)Why do you transmit param.child and not param?I think that you can all `syft.serde.msgpack.serde._detail` directly on the list, which would make your syntax lighter:
smthg like this
`cls_name, *grad_fn_params = syft.serde.msgpack.serde._detail(worker, gradfn_tuple)`That must have occurred while resolving the conflicts
It has been restored to the master. :)Fixed.So the problem here is there might be a case where attribute is not a Tensor (consider power in PowBackward function). 
Can you explain a case for which this might not be restrictive enough? My intuition was here we need to push every element of the chain in the tuple (like we do AutogradTensor). But that will be taken care of.
Hence the code is changed and now transmitting param instead of param.child.Hey, I have changed the functions. Please review :)Oh you're right!
My only concern is that if people add new internal attributes to GradFunc, then they will need to add them to this list. But maybe this is ok for the moment. Just change the list to be a set :)Done :+1: This `except` clause is a good idea but doesn't get exercised by the test suite, which causes a CI test failure due to insufficient coverage. Since it isn't used yet, probably okay to remove/comment it and leave a note that `compare` only works for tensors.This seems like a good addition, but doesn't look like it actually gets called anywhere yet. Did I miss it?It was used in how previously garbage_collect_data was handled. Tiny 
```python
if cls == ""GradFunc"":
     cls =  GradFunc
else:
     cls = getattr(gradients, cls)

return cls(*grad_fn_attrs)
```I think you can do 
`self.grad_fn = kwargs.get(""grad_fn"")`DoneYess :smile:",13,True,2019-12-30 13:46:17,2020-01-13 05:30:51,2020-01-12 18:25:50
https://github.com/OpenMined/PySyft/pull/2868,['bug '],fix: targets in makefile were faulty,fix: targets in makefile were faultyUpdate the makefile to install requirements specific to target's needs. Also added requirements_notebooks to setup tests_require because `python setup.py test` was failing.,1,True,2019-12-29 08:36:42,2020-01-16 00:47:48,2020-01-16 00:47:48
https://github.com/OpenMined/PySyft/pull/2867,[],Update Part 08 - Introduction to Plans.ipynb,"Update Part 08 - Introduction to Plans.ipynbMake code that shows error when non-built Plan is sent to be code cell.
To avoid notebook test fail, wrap this error with try/catch.",1,True,2019-12-28 04:29:15,2019-12-30 10:36:49,2019-12-30 10:36:49
https://github.com/OpenMined/PySyft/pull/2857,[],Update Pytorch version in installation/readme docs,"Update Pytorch version in installation/readme docs1. Some docs are still mentioning pytorch 1.1. Updated to 1.3.
2. Additionally, there were number of questions in Slack about pysyft refusing to install because of incorrect pytorch version in spite it was 1.3. The fix is specifying exact `1.3.0` version in pip or conda command when installing pytorch. Alternate fix for this issue might be relaxing pytorch version requirement in requirements.txt: https://github.com/OpenMined/PySyft/pull/2816#issuecomment-568608125",1,True,2019-12-25 23:01:54,2019-12-27 17:59:09,2019-12-27 17:59:08
https://github.com/OpenMined/PySyft/pull/2856,[],Added `String` type to `msgpack/serde.py` to allow serialization,"Added `String` type to `msgpack/serde.py` to allow serializationI added `String` to the `OBJ_SIMPLIER_AND_DETAILERS` list in `msgpack/serde.py` to allow serialization of `String`.

I also added some unit tests for serializing `String` objects and other `str` hooked methods.It seems to be more correct to use `.tag()` and `.describe()` methods to set tags and description.
And when using .tag(), it will be simplified as `set` instead of `list`.Thank you @vvmnnnkv for the review. Actually, `String` type inherits `syft.generic.object.AbstractObject`. That latter has `tags` and `description` as `__init__` arguments, so I followed the same `__init__` method prototype for uniformity. Do you suggest that  I remove `tags` and `description` for `__init__`? Well, there's some type inconsistency with tags.
`syft.generic.object.AbstractObject` constructor stores `tags` as is (in your case as a `list`).
However, `syft.generic.object.AbstractObject.tag()` assumes `self.tags` is a `set` (and makes it a `set` if it's not initialized yet).
So if you set tags to be a `list` from `__init__`, this will probably make further usage of `.tag()` method on that tensor impossible because `list` doesn't have `.add` method.

Also, simplified object will be different because `list` and `set` are simplified differently.
Given that tutorials use `.tag()` method, I'd rather use it instead of passing tags in constructor. Or update what you're passing to `set`. 

Additional issue you'll see is that `set` doesn't preserve order, so if you specify 2 tags (`{""tag1"", ""tag2""}`) it can be simplified as `(CODE[set], ((CODE[str], (b""tag1"",)), (CODE[str], (b""tag2"",))))` or `(CODE[list], ((CODE[str], (b""tag2"",)), (CODE[str], (b""tag1"",))))` and comparison of simplified objects may fail (flaky). See here https://github.com/OpenMined/PySyft/blob/master/test/test_serde_full.py#L104 how to tackle this (or just use 1 tag).
Might be worth sorting the results of set simplification to iron out the flakiness?@karlhigley in serde itself? :)@vvmnnnkv That's what I was thinking. Unless the list of tags is enormous, should be relatively cheap. I don't think there's a standard Python data structure that would move the cost of ordering to the write (rather than read) side, but there could be one I don't know about.Thank you @vvmnnnkv and @karlhigley for the helpful review.

I changed `tags` to a set instead of a list and used a custom comparison function as in `make_set`. This should be good nowWould you please review the new commit? Thank you",1,True,2019-12-25 12:36:44,2019-12-30 18:56:01,2019-12-30 18:56:01
https://github.com/OpenMined/PySyft/pull/2849,[],Cleanup Protocol,"Cleanup ProtocolCleans-up `protocol.py` a little bit. In particular:
* Arrange imports to PEP8
* Add doc strings and Args, Returns, Raises where missing
* Adds a couple of inline comments
* Adds missing typehints / corrects some type hints

Overall, errors found when running `mypy syft` have decreased

This PR makes a small contribution to #2510",1,True,2019-12-20 22:45:09,2019-12-20 23:22:44,2019-12-20 23:22:40
https://github.com/OpenMined/PySyft/pull/2838,[],fix: fix problem with notebooks tests due to outdated library,"fix: fix problem with notebooks tests due to outdated librarynbconvert seems to be causing problems due to it being an outdated
version.

See: https://github.com/nteract/papermill/issues/125",1,False,2019-12-18 14:37:28,2020-01-17 19:13:47,2019-12-19 00:07:44
https://github.com/OpenMined/PySyft/pull/2827,[],fix: fix test_fl_sms on test_notebooks.py,"fix: fix test_fl_sms on test_notebooks.pyCurrently it could fail with os.chdir since changing directory doesn't alter import paths, it changes the directory for opening files, according to [this SO answer](https://stackoverflow.com/questions/23619595/pythons-os-chdir-function-isnt-working). Instead, it should import the module with `sys.path.append` before using `os.chdir`.

The corresponding tests were failing on my machine (Linux) but works fine with this fix.

See original PR: https://github.com/OpenMined/PySyft/pull/2805I think the best solution would be not to call ```os.chdir``` at all. I see [here](https://stackoverflow.com/questions/23619595/pythons-os-chdir-function-isnt-working) - the link you provided - that it has different behaviors on different operating systems.
What I am thinking is to add the ```preprocess.py``` path to ```sys``` and then simply run the notebook (how it is done now).

This will mean that on line 137, notebook will be ```p_name / notebook```.

What do you think?Just for the record... Tried this with @gmuraru but it doesn't work so the best solution is the current one on the PR.",3,True,2019-12-16 21:50:50,2020-01-17 19:14:02,2019-12-21 00:05:08
https://github.com/OpenMined/PySyft/pull/2814,[],fix: Translation issues in hindi notebook 1 in example,"fix: Translation issues in hindi notebook 1 in exampleThe changes required in #2787 is done in a6610c0 is unfortunately reverted back in f9c18d1. Sorry for the inconsistencies. The file is now checked and OK.

Thank you  @Yugandhartripathi for pointing the error  

Ref: #2787 
fix: #2787 
See also: #2811 

Signed-off-by: Arkadip <in2arkadipb13@gmail.com>",1,True,2019-12-11 17:54:02,2019-12-12 15:20:59,2019-12-12 14:36:18
https://github.com/OpenMined/PySyft/pull/2807,[],Fixed installation command in Windows,"Fixed installation command in WindowsSimply removed the 'udacity' keyword in the installation, which is not currently recognized by the installation procedure.You noticed it good. I also thought udacity is not needed here.",1,True,2019-12-10 09:42:40,2019-12-13 21:46:34,2019-12-13 21:46:34
https://github.com/OpenMined/PySyft/pull/2803,[],Add prototype of parameters() for pointer plans,"Add prototype of parameters() for pointer plansThis requires to fix hook_args registration

Usa case targeted:
```python
import torch
from torch import nn
from torch import optim
import syft as sy
hook = sy.TorchHook(torch)
sy.local_worker.is_client_worker = False # To register objects locally
bob = sy.VirtualWorker(hook, id=""bob"")
alice = sy.VirtualWorker(hook, id=""alice"")
# A Toy Dataset
data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True).send(bob)
target = torch.tensor([[0],[0],[1],[1.]], requires_grad=True).send(bob)
# A Toy Model
lin = nn.Linear(2,1)
class Net(sy.Plan): 
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(2,1)
    def forward(self, x):
        x = self.fc1(x)
        return x
model = Net()
opt = optim.SGD(params=model.parameters(),lr=0.1)
model.build(torch.tensor([[0,0]]))
model = model.send(bob)
def train():
    # Training Logic
    for iter in range(20):
        # 1) erase previous gradients (if they exist)
        opt.zero_grad()
        # 2) make a prediction
        pred = model(data)
        # 3) calculate how much we missed
        loss = ((pred - target)**2).sum()
        # 4) figure out which weights caused us to miss
        loss.backward()
        # 5) change those weights
        opt.step()
        # 6) print our progress
        print(loss.get().data)
train() 
```Q: I have an idea, but I do not know if it is a good one or not. What do you think of linking issues directly in the code? (like create the issue and put the issue number as comment)Definitely a good idea :)I don‚Äòt think it is a good idea to link github issues in code, this should be separated. Not every part could be matched to one issue over time. And imagine e.g. moving to gitlab ;-).Yeah, I don't think every part could be linked to an issue (that's what the history and `git blame` are for anyway), but it might be helpful in `TODO` comments to be able to link to an issue with further info and discussion so that stuff doesn't have to live in the comments. I have no strong opinions here, just wanted to chime in to say that I can see potential value. ü§∑‚Äç‚ôÇ I 100% support TODO: <link to issue>",3,True,2019-12-10 00:04:50,2019-12-27 14:19:45,2019-12-27 14:19:44
https://github.com/OpenMined/PySyft/pull/2791,[],Deactivate plans in plans,"Deactivate plans in plans### Description
Solves #2707.

Building a plan that has already a built plan should directly call the function - take into consideration ```*args and **kwargs```.

For doing this, we keep in the owner a state, which would indicate if a plan is already building or not. If a plan is already building, then directly call the function.

### Later EDIT
Made this PR to support plans in plans - even if the inner plan is already built (with a state) - for that I had to add more information that is sent to the worker (the states for the first plans)I think this was not neededI can remove this - It was only a simple way that I thought of to show what each plan/function is doing",1,True,2019-12-07 13:43:45,2019-12-27 15:37:40,2019-12-27 15:37:40
https://github.com/OpenMined/PySyft/pull/2790,[],Fixed Docstring inconsistencies,Fixed Docstring inconsistenciesFixed docstring issues such as punctuation and typos.,3,False,2019-12-06 18:20:34,2019-12-20 15:10:36,2019-12-20 15:10:36
https://github.com/OpenMined/PySyft/pull/2787,[],feat: Added the Part 01 Tutorial in Hindi,"feat: Added the Part 01 Tutorial in HindiRef: #2774 

- Created a Hindi folder inside tutorial folder
- Translated Part 01 - The Basic Tools of Private Deep Learning in Hindi

Signed-off-by: Arkadip <in2arkadipb13@gmail.com>Like can be changed with star here for following through with proper terminology. Correct tensor translation and change ‡§Æ‡•Å‡§ù‡•á with ‡§Æ‡•Å‡§ù‡§™‡•áSpacing issue with x_ptr need to swap out space in backticks to the left. I think data outside parenthesis here(‡§°‡•á‡§ü‡§æ (‡§°‡•á‡§ü‡§æ ‡§ï‡•á ‡§∏‡§æ‡§•)) should be tensor instead. What do you think?Also in the last line here word pointers is replaced with ‡§∏‡§Ç‡§ï‡•á‡§§ need to fix that for it to make senseAgain change ‡§∏‡§Ç‡§ï‡•á‡§§ with pointers here.  It seems the translator interprets pointers as in tips or signals when it falls in the last part of a sentence.‡§ü‡•á‡§®‡§∞ to TensorTonsor to Tensor""‡§π‡§Æ (‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á) ‡§π‡§Æ‡•á‡§∂‡§æ ‡§π‡§Æ‡§æ‡§∞‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç"" does not seem to have the same meaning as ""we (by default) always have one for us as well""‡§ü‡•á‡§®‡§∞‡•ç‡§∏ to Tensor  
‡§™‡•â‡§á‡§Ç‡§ü‡§∞‡•ç (Pointers) ‡§∏ move last letter to left of parenthesisCommend to Command  
"" ‡§è‡§ï Tensor z ‡§¨‡§®‡§æ‡§Ø‡§æ, ‡§î‡§∞ ‡§´‡§ø‡§∞ ‡§™‡•â‡§á‡§Ç‡§ü‡§∞(Pointer) ‡§ï‡•ã ‡§π‡§Æ‡•á‡§Ç ‡§µ‡§æ‡§™‡§∏ z ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡§æ‡§™‡§∏ ‡§ï‡§∞ ‡§¶‡§ø‡§Ø‡§æ!"" needs a fixRemove ‡§∏‡§ø‡§∞‡•ç‡§´ that makes it sound restrictive.Like to star for proper terminologyChange PySyft GitHub to PySyft GitHub Issues",4,True,2019-12-06 02:45:54,2019-12-11 17:30:15,2019-12-08 17:21:32
https://github.com/OpenMined/PySyft/pull/2762,[],Complete serde unit tests,"Complete serde unit testsThis is for issue #2654
PR tries to fully cover serde with unit tests. As a byproduct it also kind of documents how serde simplifies values (by providing reference simplification samples) and reveals some inconsistencies in how simplification is done.

New `test/test_serde_full.py` contains 3 generic tests:
1. test_serde_coverage: check that all serde types (loaded in serde.py) are included in tests
2. test_serde_roundtrip: roundtrip a value through serde and compare it to itself after that
3. test_serde_simplify: simplify a value and compare it to reference value

`samples` dict that contains samples for each serde type is supplied to these tests.

Additionally, there're few fixes here to make tests pass:
 * Exceptions simplify/detail 
 * Autograd tensor init (to properly use values sent by detail fn)

I had to exclude this file from black formatting because it squashes reference simplification examples and makes them less clear.
Since these tests also function as documentation for consumers (like the JS and Android workers), they would be a little easier to read if the samples were dictionaries. Readers wouldn't have to keep the format in mind from this comment at the top. Might also be helpful in the case where a change breaks one or two tests and someone navigates directly to them without reading the whole file.:+1:I agree on this point, but more because of `test_serde_simplify` which is not 100% clear because it uses indices of the tuple.
By the way, I see that when you define a `compare` function, you always do for 1 (custom_detailed_values_comparison_function ?) and the other one (custom_simplified_values_comparison_function ?) is always None, is this last one needed?@karlhigley Please take a look now?Thanks, that helps! Turns out I was misreading the examples the first time. üòÖ",6,True,2019-11-30 20:40:48,2019-12-08 17:17:48,2019-12-08 17:17:48
https://github.com/OpenMined/PySyft/pull/2757,[],Issue regarding library path and tests,"Issue regarding library path and tests## Description
Rename ```federated``` to ```fl``` in the MNIST tutorial and solve indentation issue.
Add ```.gitignore``` file such that datasets from ```examples``` would not be taken into consideration by git.

## Testing
[x] Run the tutorials and validate that they work.Is this output expected?sameI think the indentation was correctIs this not needed?Nope. will remove themAlso, will remove this oneYes, it was ok :(...my badI moved at the top of the file - but rerun the examples and it seems it is ok how it is now.",1,True,2019-11-25 23:20:09,2019-12-09 20:24:56,2019-12-09 20:24:56
https://github.com/OpenMined/PySyft/pull/2756,[],Fix some problem regarding to {RuntimeError} (please see PR details},"Fix some problem regarding to {RuntimeError} (please see PR details}# PySyft problem and solution I encountered

My environment settings

* Linux
```$ uname -ar
<Machine_name> 4.15.0-65-generic 
74~16.04.1-Ubuntu SMP Wed Sep 18 09:51:44 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

```
* CUDA
```
$ /usr/local/cuda-9.2/bin/nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Tue_Jun_12_23:07:04_CDT_2018
Cuda compilation tools, release 9.2, V9.2.148
```
* Pysyft
```
$ pip show syft                    
Name: syft
Version: 0.2.0a2
Summary: A Library for Private, Secure Deep Learning
Home-page: https://github.com/OpenMined/PySyft
Author: Andrew Trask
Author-email: contact@openmined.org
License: Apache-2.0
Location: /home/alfons0329/.local/lib/python3.6/site-packages
Requires: msgpack, torchvision, tblib, torch, Flask, websocket-client, flask-socketio, websockets, numpy, lz4, zstd
Required-by:
```
* Pytorch and torchvision
```
$ pip show torch torchvision
Name: torch
Version: 1.3.0+cu92
Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Home-page: https://pytorch.org/
Author: PyTorch Team
Author-email: packages@pytorch.org
License: BSD-3
Location: /home/alfons0329/.local/lib/python3.6/site-packages
Requires: numpy
Required-by: torchvision, syft
---
Name: torchvision
Version: 0.4.1
Summary: image and video datasets and models for torch deep learning
Home-page: https://github.com/pytorch/vision
Author: PyTorch Core Team
Author-email: soumith@pytorch.org
License: BSD
Location: /home/alfons0329/.local/lib/python3.6/site-packages
Requires: numpy, pillow, six, torch
Required-by: syft
```

## 1. Runtime Error: {backend CUDA for arg...} in tutorial 6
* Problem be like
```
RuntimeErrorTraceback (most recent call last)
<timed exec> in <module>

~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)
    424             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
    425 
--> 426         return self._apply(convert)
    427 
    428     def register_backward_hook(self, hook):

~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    200     def _apply(self, fn):
    201         for module in self.children():
--> 202             module._apply(fn)
    203 
    204         def compute_should_use_set_data(tensor, tensor_applied):

~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    225                 should_use_set_data = compute_should_use_set_data(param, param_applied)
    226                 if should_use_set_data:
--> 227                     param.data = param_applied
    228                 else:
    229                     assert isinstance(param, Parameter)

~/.local/lib/python3.6/site-packages/syft/frameworks/torch/hook/hook.py in data(self, new_data)
    330 
    331                 with torch.no_grad():
--> 332                     self.set_(new_data)
    333             return self
    334 

RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_set_
```
In the traning session
![](https://i.imgur.com/PQPNRfR.png)

* Solution
Add `torch.set_default_tensor_type(torch.cuda.FloatTensor)`
in ![](https://i.imgur.com/LXyN9Rz.png)

## (Appear after 1.) Runtime Error: {RuntimeError: Cannot re-initialize CUDA in forked subprocess.} in tutorial 6
* Problem be like
```
File ""/home/alfons0329/.local/lib/python3.6/site-packages/syft/frameworks/torch/hook/hook.py"", line 486, in new_tensor
    current_tensor = hook_self.torch.native_tensor(*args, **kwargs)
  File ""/home/alfons0329/.local/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 191, in _lazy_init
    ""Cannot re-initialize CUDA in forked subprocess. "" + msg)
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
```
* Solution
Force the spawn method in the beginning part near arguments
![](https://i.imgur.com/qiqQn6e.png)",1,False,2019-11-25 13:37:06,2019-11-25 13:43:13,2019-11-25 13:43:13
https://github.com/OpenMined/PySyft/pull/2755,[],[MRG] Notebook test,"[MRG] Notebook testAutomated testing of Notebooks using Papermill. Reopened this after some Repo problems.shall we make it as a global variable to make a bit more transparent? this is great, that we can just pass such parameter! üëç Why do you need to separate these? besides the pathgood question, I did initially because the tests where hard to debug and taking long. But I was thinking to work with a more general approach soon because that way new notebooks are discovered and don't have to be added manually.I though it's because you wanna give different timeout or so, but I do not think it's currently the case :)",16,True,2019-11-24 17:01:41,2019-12-09 22:47:24,2019-12-07 19:20:24
https://github.com/OpenMined/PySyft/pull/2749,[],Fix import in worker Docker image,Fix import in worker Docker imageWebsocketServerWorker should now be imported from syft directly instead of syft.workers.,1,True,2019-11-20 18:08:24,2019-11-20 18:43:24,2019-11-20 18:43:23
https://github.com/OpenMined/PySyft/pull/2746,[],Fixed SMPC tutorial link,Fixed SMPC tutorial linkFixed SMPC tutorial link,1,True,2019-11-19 08:18:42,2019-11-19 14:45:21,2019-11-19 14:45:21
https://github.com/OpenMined/PySyft/pull/2741,[],"Create tensor directly on worker - zeros, tensor, rand, randn","Create tensor directly on worker - zeros, tensor, rand, randnFixes #2678

Added hooks for ```torch.zeros```, ```torch.rand``` and ```torch.tensor``` directly into the ```BaseWorker``` such that we could do:
```
alice = sy.VirtualWorker(id=""alice"", hook=hook)
x_ptr = alice.tensor([1,2,3]) <- create the tensor [1,2,3] at alice and return a pointer
```

### Tests
[x] Added unit tests for VirtualWorker, BaseWorker and WebsocketClient
[x] Unit tests for whitelisted and not whitelisted methods
[x] Add a simple way to extend the ```remote``` call with other frameworks (like ```tensorflow```)Q: @LaRiffle should I also copy those two tests in the ```test_websocket_worker```would be a good idea!
just an info:
prefer to do:
```
def test_send_command_whitelist(workers):
    bob  = workers[""bob""]
    ...
```Had to move those here because of the following scenario.
1. In the tests we spin up the ```Websocket Server``` and ```Websocket Client```
2. Create one client and get a ```ConnectionRefused```
3. Create the second client and for this one ```line 149``` will run, which will override the ```remote``` attribute.
4. ```Remote``` contains the ```torch``` attribute (is a ```Torch``` instance - class that can be found in the hooks for the torch framework)  and the ```Torch``` instance contains an attribute ```worker``` that is a weak reference to ```self```.

The problem is that remote would get copied into the new ```WebsocketClient``` and we will have a reference to an item that should be garbage collected + ```ws``` attribute from ```WebsocketClient``` would be ```None``` (since we got a ```Connection Refused``` with the first client).

In the end there will be generated an error caused by the function ```_forward_to_websocket_server_worker``` because ```self.remote.torch.worker.ws``` <--- the one that was refused is```None```",4,True,2019-11-17 02:04:48,2019-11-20 14:40:51,2019-11-20 14:40:51
https://github.com/OpenMined/PySyft/pull/2733,[],Fix test_websocket_worker_multiple_output_response unit test,Fix test_websocket_worker_multiple_output_response unit testAdded worker arg to exceptions' `simplify`,4,True,2019-11-14 00:22:36,2019-11-14 10:51:02,2019-11-14 10:50:35
https://github.com/OpenMined/PySyft/pull/2722,['bug '],Changed handle_command_function to use @overloaded.module for syft tensors,"Changed handle_command_function to use @overloaded.module for syft tensorsThis PR solves #2637 . 
We check in handle_func_command if the function is overloaded or not. (the check was already existed for PureFrameworkTensor).
Thanks @LaRiffle for the help!!I think you can remove the occurence of the same code below at line 316.This will be only for the test, when you're happy with the PR remove these linesOk!!Hey @LaRiffle, the code above is important because else it doesn't handle case where we pass tensor parameter as shift parameter.
For example,
```python
x = torch.tensor([1.0, 2.0, 3, 4, 5])
index = torch.tensor([-1.0])
result = torch.roll(x, index)
```
exits with
```python
TypeError: roll(): argument 'shifts' (position 2) must be tuple of ints, not Tensor
```
Do we have any alternative or this is fine?I think you should put that block out of the 
```
<here>
try:
...
except PureFrameworkTensorFoundError
```
it should work better!This way you can indeed remove the duplicate and only keep a single occurence Hey,
That too results in an error 
```python 
RuntimeError: Error, Please consider calling "".get"" method instead of "".item"" method, so you can be safely getting the item you need.
```
when we pass syft tensor (wrapper) as shift parameter.

This might be because the ```handle_command_function``` calls correct overloaded function but arguments needs to be resolved first (wrapper to be converted to the framework tensor) before passing to the overloaded function. The 2nd arg of `rgetattr` should be a string with a structure like so: `attr1.attr2.attr3...`
But here you are feeding the function with `new_command` which is a tuple.

Shouldn't you be feeding it with `cmd` instead?

With the current implementation, you will always get an `AttributeError`That's correct. Thanks for bringing this to notice :).
I have replaced ```new_command``` with ```cmd```. It was raising `AttributeError` everytime for previous code. 
Now it's not working for one case though- when we pass **wrapper tensor as shifts argument**. It's raising the exception `RuntimeError: Error, Please consider calling "".get"" method instead of "".item"" method, so you can be safely getting the item you need.`",4,False,2019-11-07 15:21:07,2020-01-26 12:23:16,2020-01-26 12:23:16
https://github.com/OpenMined/PySyft/pull/2713,[],Making changes to avoid errors on avgpool layer,"Making changes to avoid errors on avgpool layerThe error arises when a network with AvgPooling layer is used. I realized that the command created for this layer used the path _C_nn and not nn.functional as well as other similar layers, such as MaxPooling.

The problem was reported in this issue:

https://github.com/OpenMined/PySyft/issues/2562

My initial solution was to switch all commands that are using the _C_nn path to nn.functional.




Could we format this as a try except? I would prefer to avoid having the overhead of this on every cmd",4,True,2019-10-30 12:54:45,2019-12-20 15:23:36,2019-12-20 15:23:07
https://github.com/OpenMined/PySyft/pull/2709,[],Private Tensors,"Private Tensors- Create Private Tensors
  - [x] Convert Torch Tensor into Private Tensors
  - [x] Serialize Private Tensors
  - [x] Allow/Disallow access to tensor values.
  - [x] Forward tensor methods.
  - [x] Perform Arithmetic Operations.
  - [x] Unit tests.Thinking about this, .send() should have all the same permissions as .get(), causing this to fail. Otherwise I could just try to call .send(me) to get around .get() blocking me.what is the certificate?Why was this necessary to put here? Is this not the same as https://github.com/OpenMined/PySyft/blob/f39a93d085b74f16724633fd12a28c893a724c59/syft/generic/frameworks/hook/hook.py#L365In the final of `_get_hooked_private_method`, we're setting pointers to the result tensor parent(s) and saving the command/operation used to generate it.But thinking about this, It doesn't seem the best way to implement it. Do you have any suggestion?Yes, It makes sense! I'll fix it!It's about SSL/TLS Certificates. We can implement an interface to authenticate companies by their Certificates (like HTTPS/WSS protocols), receiving their encoded public keys (string).Should I wrap it into a mockup class?ah nvm - yours looks good. Yes we could probably compress the code some more but this is readable so I'm happyepic! wrapping in a mockup class might be nice but we can come back to it when the time comesDone!Done!You're a rockstar.",3,True,2019-10-28 17:15:31,2019-12-04 15:29:17,2019-12-04 15:29:16
https://github.com/OpenMined/PySyft/pull/2698,[],Added missing WebsocketClientWorker.ipynb ,"Added missing WebsocketClientWorker.ipynb The notebook ""WebsocketServerWorker"" states to be a two notebook tutorial. The seccond notebook ""WebsocketClientWorker"" was missing. I copied it from the official colab of the [opend minded homepage](https://www.openmined.org/).

Also I added a minor fix to the code snippet in the notebook. Can you clean the output?You have an error in the output here tooI will clean the output but it is [copied straight from this example from openmined.org ](https://colab.research.google.com/drive/1Je1rk7olA9uTWWaqvvt4_gXf7yX1rTBm) which also includes this lines. May the example there should also be fixed.Or is this output produced as soon as i open it with my colab? I have cleard the ouputs and also moved the Websocket Tutorials to a own folder",1,True,2019-10-24 18:12:18,2019-11-20 14:50:44,2019-11-20 14:50:44
https://github.com/OpenMined/PySyft/pull/2692,[],Add python context to base worker,"Add python context to base worker**Changes:**
* Add decorator such that we could have a python ctx by calling a method
* Changed one of the tests that were using the *is_client_worker* member explicitly.

LE: Fixes #2689Q: Should I add new tests to validate that the context manager is working correctly? (and leave the old tests how they were?)
Another way is the one I added here - have a **use_context** parameter and depending on that use the python context or not (I do not like this method that much because it is hard to read).
Third method would be to duplicate the tests and in one test use the parameter explicitly and in another to use the context.
I would like the first method :)

What do you think @LaRiffle?
Thanks!
So you can add a specific test to verify it works as expected, like this one
```
def <...>_test(...):
    assert hook.local_worker.is_client_worker == True
    with hook.local_worker.registration_enabled():
        hook.local_worker.is_client_worker == False
    assert hook.local_worker.is_client_worker == TrueAnd then change all the occurences of the tests where you have the old syntax `assert hook.local_worker.is_client_worker == True` to use context insteadI think it's `finally` right?Yes, my bad",3,True,2019-10-22 17:52:59,2019-10-29 15:23:16,2019-10-29 15:23:16
https://github.com/OpenMined/PySyft/pull/2691,[],Fix errors / warnings with test of encrypted linear regression,"Fix errors / warnings with test of encrypted linear regressionThis PR fixes #2680 .

As mentioned in the [Encrypted Linear Regression Tutorial](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/advanced/Encrypted%20Linear%20Regression.ipynb), the inputs to the Encrypted Linear Regression model should be standardized. The test I had implemented was simulating data where one of the covariates had a standard error of 10 which was causing overflow",3,True,2019-10-22 16:57:52,2019-10-22 17:41:14,2019-10-22 17:10:40
https://github.com/OpenMined/PySyft/pull/2690,[],WIP: enable cuda processing in PySyft,"WIP: enable cuda processing in PySyftUnhooking the tensor.data attributes enables .to(device) to work correctly. 
However, this seems to create some problems.
Thus, this pull request needs some more work to make the unit tests pass again. Any help would be greatly appreciated.I don't think you have to do this, you're already directly redefining `torch.nn.Parameter.data` ",1,False,2019-10-22 15:41:11,2019-11-15 11:51:52,2019-11-15 11:51:52
https://github.com/OpenMined/PySyft/pull/2684,[],Added `String` and `StringPointer` types,"Added `String` and `StringPointer` typesI added two new types, `String` and `StringPointer` types. They have have the same API as native 'str' python objects.

In order to use them, here is an example:

```
from syft.generic.string import String

string = String('hello world') # The 'owner' will now be automatically set to syft.local_worker

string_pointer = string.send(bob)
```

It  strikes me that this part should be integrated in the hook file, in order to perform the hook a single time and put all the hook mechanisms at the same place.You might also want to use what's done in hook_args as well which does unwrap the args (_args_adaptor) and wrap the response back (_wrap_return_value), although it might be complicated to adapt. Normally for tensors it's a one-line change in hook_args.pyUse google style for docstingsIf this is relevant, it might be more appropriate to call it `.child` to be consistent with the tensors api@LaRiffle Done
DonedoneHere is a short list of all str method in python, you might want to include more like title or capitalize maybe depending on your usecases: https://www.w3schools.com/python/python_ref_string.aspthis list is duplicate with the other one, try to only define it in one place for consistency purposeI think I already mentioned it but this may fit in the hook_args logicusually it is expected to put ""self"" and not ""self.id_at_location""Add a dosctring to explain the kwargs (like encoding, errors, etc) which is very useful!can't you do just `tags = sy.serde._simplify(string.tags)`?smae here
can't you do just `tags = sy.serde._detail(worker, string.tags)`?I added this list as a class set variable of `String`, so we can get the list by typing `String.methods_to_hook`DoneDoneDoneDoneThat is right, Done!",16,True,2019-10-19 17:58:53,2019-12-24 16:27:58,2019-12-24 16:27:57
https://github.com/OpenMined/PySyft/pull/2659,[],Add support for exp and log (with approx) on additive shared tensors,"Add support for exp and log (with approx) on additive shared tensorsThis PR extends the set of operations supported in Additive Sharing using approximations

- [x] Exponential 
- [x] Logarithm
- [x] Inverse
- [x] Sigmoid

Fix #2537Should this just be other - self?Should this live in polynomial tensor instead of here?Should this be an int or a long?Same with both below functionsThese should also have citations, since the inspiration comes from CrypTen. I'd say it's not substantial enough to require that the license be copied, so a simple online comment + link should doNot exactly because `__rsub__(self, other)` is called after `other - self` failed, typically because other was an integer. So we need to reverse arguments.float can be supported tooIndeed, but I think this is a good temporary place until polynomial is ready to receive them and we have a clear strategy to integrate Poly & FixedP.
Also, I've added references to the repo where I analyse, test and adapt those approximations suggested by crypten.Shouldn't we replace the sigmoid approximation by the mathematical definition using the approximated `exp`? Or do you think we lose precision by doing that?

I think we should at least compare the differences between both...Definitely! I'll make a comparison as part of my lib to see what's best, thansk for the suggestionYou're welcome!

Maybe also do a time performance comparison, to see what are the trade-offs between bothThank you for doing that @LaRiffle and @jvmancuso Why is this tested twice? (also tested in `test_add_method()`)This can be removed :)Are you doing this because the approximation results vary significantly depending on the sharing?
If so, do you know when ""bad"" cases occur and how bad they are?I think we might have talked about it at some point but should you use `.inverse()` here?I will remove itYes
No not really... :/Correct!",3,True,2019-10-11 16:55:00,2019-12-27 17:57:12,2019-12-27 17:57:11
https://github.com/OpenMined/PySyft/pull/2657,[],Fixed tutorial on Encrypted LR after changes in worker.search(),Fixed tutorial on Encrypted LR after changes in worker.search(),1,True,2019-10-11 13:48:42,2019-10-14 05:39:18,2019-10-14 05:39:18
https://github.com/OpenMined/PySyft/pull/2652,[],"Fix tutorial part 6, to fix issue #1893","Fix tutorial part 6, to fix issue #1893Added one line to change the default tensor type to fix ```RuntimeError: Expected object of backend CPU but got backend CUDA for argument #2 'source'``` 

Fixes #1893 for Tutorial part 6
Source : https://github.com/OpenMined/PySyft/issues/1893#issuecomment-478717757",3,True,2019-10-09 05:53:29,2019-10-28 09:24:09,2019-10-28 09:24:08
https://github.com/OpenMined/PySyft/pull/2648,[],Fix tutorial part 7,"Fix tutorial part 7I modified the optimizer to use Adam instead of SGD and the convergence is going nicely. 
Also increased the batch size as the estimation of the linear classifier will be more stable across batches. 
Loss calculation had a broadcasting problem as well.

fixes #2642Why do you have to do this?
You're giving the same references to model parameters to all optimizers so the result should be the same, or not? (I don't know how Adam works)I don't remember exactly the reason, but currently in PySyft having one joint optimizer across locations doesn't work with optimizers that use momentum (including Adam). 
And I switched to Adam as searching on internet I noticed that other people had problems getting the training to converge with SGD as well.Oh yes you're right, the pb is with momentum!",1,True,2019-10-08 15:21:55,2019-10-09 07:44:28,2019-10-09 07:44:27
https://github.com/OpenMined/PySyft/pull/2639,[],Currently virtualworker search requires a list,"Currently virtualworker search requires a listThis PR should fix the non-working notebook (Part 05 under the `/examples` folder).  The reason for it is such:

`VirtualWorker.search()` requires a list (i.e. `VirtualWorker.search(['#boston', '#target'])`)

... while `VirtualGrid.search()` receives params as *args (i.e. `VirtualGrid.search('#boston', '#target')`)

I figure this may or may not be an intentional API change.  But it is a bit confusing.  Until the API standardizes between a list or *args, I've at least corrected the notebook.",1,True,2019-10-03 11:40:40,2019-10-03 12:03:51,2019-10-03 12:03:50
https://github.com/OpenMined/PySyft/pull/2636,[],Make installation friendlier for alt framework users,"Make installation friendlier for alt framework usersRight now, it's really easy to install PySyft if you're following along with the Udacity work & dependencies (PyTorch & TF Encrypted Keras), but it's not so great if you're experimenting with other frameworks (i.e. TensorFlow).  This PR fixes that, but breaks the old pattern of installing syft.

- `pip install syft` no longer installs scikit-learn, torch, or tf-encrypted.  It now installs a less-than-minimal set of dependencies (less than because currently tests will not pass if pytorch is not installed, see #2632).
- To reproduce the current dependency setup, use `pip install syft[udacity]` (or `pip install -r pip-dep/requirements_udacity.txt; python setup.py install`)
- Moved all pip requirements to a `pip-dep/` directory.
- Split torch and tf-encrypted dependencies out into `pip-dep/requirements_udacity.txt`.
- Added a DependencyError exception to the sandbox & `syft.frameworks.torch.linalg` for when people try to load sklearn/scipy without the dependency.I like that udacity is the default (That being said could we make this a parameter which gets passed to the make target since I think I may actually be the only person who uses this target and would like to be able to use other configurations)",1,True,2019-10-01 16:50:05,2019-10-24 04:56:09,2019-10-24 04:56:09
https://github.com/OpenMined/PySyft/pull/2630,[],Added tutorial for encrypted linear regression and fixed some errors in the LR implementation,"Added tutorial for encrypted linear regression and fixed some errors in the LR implementationWith this PR I added a tutorial to show the correct usage of the BloomRegressor (Linear Regression with SMPC).

I also fixed some errors in the computation of standard errors and p-values.",2,True,2019-09-26 15:17:11,2019-10-03 11:24:12,2019-10-03 11:24:12
https://github.com/OpenMined/PySyft/pull/2614,[],[WIP] Fix nested structure in Serde output,"[WIP] Fix nested structure in Serde outputFixes #2424. 

To try to either get rid of duplicated PySyft `serde` output details expressed in the form of a nested structure, or to simplify the `serde` output by supplying some ""globally verbose=False"" option to turn off the printout of the unnecessary info. 

The problem is as laid out in https://github.com/OpenMined/PySyft/issues/2424#issue-475676662.",5,False,2019-09-11 17:26:18,2019-11-20 15:05:07,2019-11-20 15:05:06
https://github.com/OpenMined/PySyft/pull/2613,[],Improvements to the Federated Recurrent Neural Network notebook,"Improvements to the Federated Recurrent Neural Network notebook**Changelog:**
- Implemented dataset automatic download from the PyTorch official repository to the current directory and detection of the dataset to be already present in the current directory (in that case, the dataset is not downloaded anew).
- Slightly modified a few explanations
- Removed some obsolete unused code for predictions that was just being used by me for debugging purposes
- Suppressed PySyft's warnings related to TensorFlow
- Removed errors related to the first line of code, to be run in the shell and not in the Jupyter Notebook.
<p>you could change this to: <code>!pip install -r ""../../../requirements.txt""</code> which runs on the notebook ;)</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2613/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='5'/>Done in the latest commit!",1,True,2019-09-11 08:18:53,2019-09-12 13:11:10,2019-09-12 13:11:09
https://github.com/OpenMined/PySyft/pull/2611,[],"TypeError with storage=""large""","TypeError with storage=""large""When performing remainder operation with floats and a large field `numpy` was throwing a `TypeError` exception.

This issue explains how to fix it

https://github.com/numpy/numpy/issues/6464 

Closes #2604",1,False,2019-09-11 06:18:19,2019-09-11 06:21:02,2019-09-11 06:19:09
https://github.com/OpenMined/PySyft/pull/2610,[],Continue the work on Promises and PromiseTensors,"Continue the work on Promises and PromiseTensorsContinue the work done here: https://github.com/OpenMined/PySyft/pull/2516

TODO:
- [x] Operations with scalars
- [x] Promises that can be kept several times
- [x] Bufferize output of Plans with promises
- [x] Plans with promises
- [x] Operations between promises and non-promises
- [x] Promises on sent plans
- [x] Operations on sent promises
- [x] Protocols with promises
- [x] TestsbasicConfig needs to be called before importing syft as only its first invocation is used by the logging package.
This is (was) a bug in tf-encrypted which is already resolved. The next release of tf-encrypted will enable to call basicConfig in the main function where it belongs.why hard-coded?This file is outdated. Could you please go ""back"" to the newer version?It is unfortunate that you have to wrap this. We previously haven't had good support when two tensors are interacting with each other where one has a different class hierarchy than the other. @LaRiffle any ideas?Can we also check that .move() works with Promises as well, meaning you could chain promises across a long series of workers and they would work?WOAH!!! This is incredible!!! Well done!Is there a way we can test the asynchronous execution of these promises? Perhaps we can create a ""ThreadWorker"" which is like a VirtualWorker but it runs in its own thread (might have to communicate with sockets... in fact this could just be a wrapper around SocketWorker)? In theory the execution time for doing promises on a series of ThreadWorkers should be much faster?

Also it would be really amazing if we could check/test that our MPC stuff can be used to create a graph of promises. Meaning we want the ability to call .share() on a promise, then do some executions, and then call .get() (which is still a promise holding the results). Then call .keep() on the input and the whole thing should run automatically.
Hum, that's right, I didn't think about .move(), it does not work for now.
Can you give a small example of what you mean by chaining promises across a series of workers?I think it still needs some work before we can share Promises but I'll think about that!

And for the first part, I'm not sure I got what you mean... Do you want to compare with ""classic"" plans for which (I guess) the results are waited for each time the plan is called?Sure thing!

The first approach is via .send() which sends the pointers around.
```
x = sy.Promises.FloatTensor([2,2])
x_bob = x.send(bob)
x_john = x_bob.send(john)
x_bill = x_john.send(bill)
```
So now we have a promise pointing to bill, which is a promise pointing to john, which is a promise pointing to bob, and if we keep the first one it'll send the data all the way down the chain.
```x_bill.keep(th.zeros([2,2])```

We can do something identical using .move()
```
x = sy.Promises.FloatTensor([2,2])
x_bill = x.send(bill)
x_john = x_bob.move(john)
x_bob = x_john.move(bob)
```
This is identical, but as you can see we can construct it in reverse. Often the order matters. I hope this is useful.


Not quite. I'd like to compare asynchronous promises against doing everything synchronously. Aka... if you .keep() a promise on VirtualWorkers, it should be slower than if you .keep() a promise on ThreadWorkers.

However, if you execute a command on a VirtualWorker or a ThreadWorker normally (without promises) they should run in the same amount of time (because your executing thread blocks and forces them to be synchronous).Why is this?I'm surprised that this is needed: you can usually use scalars with torch operation:
A cmd like `th.tensor([1.]) + 1` will (unless I'm wrong) be serialized and store the 1 as a scalarDoes it work with .shape ?I understand it will make your next call to .shape fail, but you can test there if it has a shapeCall `AbstractTensor.__init__(self, ...)` for more clarity indeedWhy exactly is it a property?
Does it need to be ""un-changeable"" ?Agreed, we're a bit too early for this stuff^^That's the same piece of code than for AbstractTensor? If so we can remove it :)Use .shape insteadIs this needed? I'm not 100% sure it doesIs this needed? Looks a bit confusing to me when and why I should use this aliasThis looks like a duplicate code right?Add a docstring to tell what it doeswhat's the benfit of casting it into a list?That's not allowed ;)
But as discussed it's out of scope for this PR, maybe juste remove itThis won't work - and is not allowed ;)not sure you can infer the type of the res from the input type: maybe just let it empty for now and replaced by a default ""th.floattensor""Maybe just remove this caseit's ok because the owner is support to be local, but use instead .get_obj()
`_objects` should only be used for debug purposeagain, use .get_obj()
and also pop(0) to be consistent with the next lines.get_obj().get_obj()So, we have done specific stuff to allow AST to operate with FPT in specific cases, but nothing general
I don't really get why calling .wrap() here make the thing work :/I don't know, it was part of the branch I continued the work on. @iamtrask?This line is added compared to AbstractTensor:
`self.obj_type = tensor.type()`You can safely revert it :)It seems to workI guess it's because it is a property for AbstractObject, which is a parent class of PromiseIt was to be able to use plans with scalar arguments.
I don't think it works to call a plan for which some arguments are Python integers, for instance.I think it's more common this way, to avoid using `.wrap()` and setting an attribute to a string representing a type each time we need a promise.
But it's @iamtrask's idea, maybe he can be more precise?I don't see any, I think that the only difference is that the list could be modified but it's not done anywhere.
@iamtrask, can I remove the conversion?Should this class be renamed to promise tensor instead of create promise tensor?or if not what is the difference bettween it and the normal promise tensorwhy was this check split up? Alternatively, I think it might make sense to move both of these checks to a seperate function since it seems we write the combination alotWhat does keep do? Could you add a docstring/explain why this is needed for native tensorSame with valueThis should absolutely be a property. What a property means in python is that it returns the value when you do not add parenthesis to itCould you add that line then call super.on to avoid rewriting the same codeWhy are these here and should they live in their own file?This is in the generic foldersamesameShould this live here or in a more generic pointer?They are the interface to create promised tensors, the user can do something like `a = sy.Promises.FloatTensor(shape=torch.Size((3, 3)))`.
But I agree, maybe they should be in another file, and we may want to think again about the naming (almost the same as the class Promise). Do you propose something?
@iamtrask?CreatePromiseTensor is a helper function to build PromiseTensor, I do agree that this is a bit weird, I'll try to make it betterWould be a good idea!
Can someone just remind me what .on() is used for? I'm not sure I understand when it would be needed for promises (actually, it's never used at this point). If it's never useful, I'll remove thatIt keeps the child promise with the argument given.
I need it to be able to call the method on wrapped promise but thinking about it, I'm not 100% sure it's the right way to do it.
(This is the same for value.)I think it does now :)It's for ""manual"" use of Syft Tensor
If you want to include a PromiseTensor in a Chain x you'll do x = PromiseTensor(...).on(x)
You'll get `(wrapper)>PromiseTensor>...`For example you could do this FixedprecisionTensio but in practice you will prefer to call x.fix_prec() insteadI'll agree with bobby though: if you think that method can be used and if it's tested, then ""Could you add that line then call super.on to avoid rewriting the same code""
If not, just remove it then :)You can do a 1 liner:
` if hasattr(obj, ""child"") and  hasattr(obj.child, ""garbage_collect_data"")`I also think we should remove this, it is very confusing
You can still manually wrap all the promises.FloatTensor etc in `class Promises`, but this renaming CreatePromiseTensor -> PromiseTensor is maybe not the best optionHow often will we use plans with scalar arguments? If this this an edge case we can remove this an leave it for another PR with a TODO
""operation"" -> ""method/plan/etc"" ?
operation means smthg precise in a plan so renaming a plan an operation is confusingAs I said somewhere else, let's remove it it's more  confuing than helpfulWhat about putting this in _hook_promise_tensor ?Because this looks a  bit of a hookThis sounds indeed a bit hacky
A solution would be to have a syntax:
`with worked.registration_enabled():` (which would also be super useful for the tests, to be sure the registration is turned off at the end of the test even in case fo failure)
and to hook torch.zeros, torch.one, and every torch.<smth> which creates a tensor to have it registered (it should be done but hasn't been so far)
This way your code would be much cleaner:
```python
with self.owner.registration_enabled():
     args = self._create_placeholders(self.input_shapes)
     output = self(*args)
self._output_shape = output.shape
```would it make sens to create a method for `any([hasattr(arg, ""child"") and isinstance(arg.child, PromiseTensor) for arg in args])`?use get_obj() insteadyou don't need prom but prom.owner
in the for loop behind you can remove `if prom is None:`
`p` -> `arg`can't we find another strategy to get the type which is torch agnostic?Maybe yesyou can simplify None so sy.serde._simplify(plan._output_shape) should be sufficientwhat is the role of ABC?use get_objHacky
use get_obj and except a ObjectNotFoundErrorWhat is the exact motivation of putting this in messaging and not in promiseTensor?
Do we have reason to believe this will be reused by other types?That's what I thought but in this case, I don't think we would ever need it in the case of Promises, am I wrong?
I'll remove itI think we could indeed have promises for objects that are not tensors, maybe for instance strings like we plan to have with SyferTextI just removed the if/else, I think things should just break if we move a promised object away from the plan to which it has been promised. Do you agree?I think it depends on the use case: for NN stuff maybe it's not needed but for crypto protocols, it might be more often (I'm not a crypto expert though).
And the problem is that we need the arg to have an id so that we can find it back. Or we can just store the scalar value and use it instead of an id but this needs some changesShould this be another PR?I think it was to indicate that this class is an abstract class but as no abstract method is declared, I don't think it's usefulMaybe simplify and detail should be abstract, what do you think?Yeah sounds good!Definitely I'll open an Issue  for the context ""with ..."" and one for ""hook torch.zeros, torch.one, and every torch. which creates a tensor to have it registered""Please add a description of all arguments to the docstringI think you can ""just store the scalar value"" in the plan, it looks like it works from what I've seenbreak this line:
args = (destination,)
kwargs = {""inplace"": True}I still think it's easier to change the scalars to tensors because this way, I can store and retrieve them from the worker when needed (when the promises are fulfilled). I can't see how to do that with scalars (but maybe it's just me :))I forgot this comment, @LaRiffle !
I'm not sure if this is a problem here because PromiseTensors are torch specificWhy do we need to hook the promise tensor? Why can't we just define all the methods directly in the PromiseTensor class? The class resides within PySyft, so why don't we define the methods as for example `DoubleTensor` there? It doesn't seem to rely on the DoubleTensor being hooked before defining the method.Methods like `DoubleTensor` were in the file where the class is defined before but I was asked in some comments to move them here ^^
For the other methods, I think this file was supposed to be where this kind of method generation happen but maybe not?Ok, that sounds like a contradiction. I'd like to hear the opinion of @LaRiffle and @robert-wagner :smile:status on this?status on this?a protocol should be a list of VirtualWorkers not a list of plansCurrently, when you use `.send()` on a Promise, you don't get a promise but a pointer pointing to a promise.
Was it a typo on your side or do we have a different understanding on what should happen when sending promises?
And currently, using `.move()`moves the promise to another worker but shouldn't create a chain as with consecutive sends, am I wrong?I think ThreadWorkers could indeed be useful for benchmarking!
But maybe it should be another PR?Historically PromiseTensor has always been a little bit of an exception because of the way it works. I'm ok with this for now.agreedI implemented a basic version of getting things to interact which dont' have the exact same class path. Not sure if it would work in this case but just FYII implemented a basic version of getting things to interact which dont' have the exact same class path. Not sure if it would work in this case but just FYIAhh, I see what you mean by consecutive sends. I think that this can work. I'm happy for now but we might modify this in the future.",2,True,2019-09-10 22:54:18,2019-12-03 16:05:27,2019-12-03 16:05:27
https://github.com/OpenMined/PySyft/pull/2593,[],Version,VersionThis PR is prototype of solution of issue #2592 that I had in mind. Please suggest changes and provide reviews.This all lives in the requirements.txtPip does all of this automatically (assuming there is an error),3,False,2019-09-06 05:34:59,2019-11-20 15:07:24,2019-11-20 15:07:23
https://github.com/OpenMined/PySyft/pull/2591,[],Generalize and improve hook_args.py implementation,"Generalize and improve hook_args.py implementationNote to reviewer: code review will be easiest here if taken commit-by-commit. Care was taken to make sure that the commit messages are comprehensive and descriptive.

The main purpose of this PR is to generalize hook_args to non-torch frameworks. This was actually a very small amount of work and can be seen in the first and third commits in this PR, however I noticed several problems across the code base that I'm also addressing here:

1. Importing was a struggle after moving files around, because we were often using absolute-ish paths (non-relative paths but importing a lot of things in various __init__.py files throughout the repo).  I've standardized imports to be completely absolute across the repo.  This means that you should _never_ import things from an __init__.py other than the main one in the `syft/` package (which should mainly consist of the user-facing API). If there happen to be any leftover exceptions to this, they are only in packages with very low connectivity with their submodules and other modules, and they were left untouched by accident.
2. After debugging all the import issues related to the above, I realized that the `syft.frameworks.torch.hook_args` module had quite a few circular dependencies with modules that define their own tensor types.  The main reason for this was that hook_args needs these tensor types in its `type_rule`, `forward_func`, and `backward_func` registries, however hook_args was also being used in many of these tensor types' definitions.  To fix this, registering tensor types should now happen in the same module as their definition using [these new functions](https://github.com/OpenMined/PySyft/commit/5f305a538b810e88fea6d2ce2005abd0a647f341#diff-fb445e63f5c3a9effbdbea9651ff3496R93).Can we remove this file?
It fails and Autograd now is very far more this initial notebookI notice it just because you made a change, but you can remove these 2 lines here, they are not usedCan you keep the `/` instead of  `.`?Much better! üôè You did! üéâ You can remove the TODO :)Same :)Same :) (can you just check for all occurences?)This is not generic, I believe at least part of the funcs in ambiguous_methods & ambiguous_functions should in the torch hook_args(torch or syft) -> (framework or syft)
There are some occurrences of torch in this file, maybe juste change them all from ""torch"" to ""framework""?typo ""case""Can we now remove the `if TYPE_CHECKING`?We don't need anymore in this file to put type annotation in strings right?Can you add an import line to make those 3 lines more readable?I understand why you did this, but I wonder if there is not a more pythonic way to trigger this registration?Can you add all main workers as well (socket, etc)?I would prefer to follow the google style guide for import statements: https://github.com/google/styleguide/blob/gh-pages/pyguide.md, saying (section 2.2):

> Use import statements for packages and modules only, not for individual classes or functions. Note that there is an explicit exemption for imports from the typing module.

The advantage is that you always know, from which module your classes are coming from. agreed, totally forgot about theseI recall it breaking several versions back when I added this code, will try againI wasn't too sure about python's scoping when you import objects from modules vs. modules -- will give it a shotopen to ideas!I'm not sure how I feel about it, but I suggest moving this discussion into an issue or the changes into a separate PR. This one is already quite large, and it should be easier to do once imports are standardized after this merges.In certain cases I think we do, e.g. when we need to avoid circular imports or when referring to BaseWorker in its own definitionbut this line can be changed I believe!no longer need it for these pointer classes, but we do need it for FrameworkHook still, so gonna make that changeIt is definitely a discussion that is out of the scope of this pull request. However you are also changing the import statements that fulfilled the google style guide. So we cannot just postpone the discussion.There were very few existing import statements that fulfilled the requirement you quoted, and the existing import statements are clearly not yet standardized, so I don't think that should be a blocker to this PR. I will make an issue and link this thread to it.Why is it need to call global?Why is this one needed? just calling syft. AdditiveSharingTensor doesn't work?or, are you preparing the future? :)yes, this change was from when I was hoping to remove tensors from the main `syft` namespace. I can change it back if we want!Scoping in Python functions is one-way from global --> local, so modifications made to this variable will not be retained outside of this function unless we call `global` (or at least `nonlocal`).But to the larger question of why we need to do this whole registration business, it's so that we can register classes from outside of this module in these dictionaries/sets without importing such classes.Let's keep it for now :)> But to the larger question of why we need to do this whole registration business, it's so that we can register classes from outside of this module in these dictionaries/sets without importing such classes.

I totally understand and agree with this



> Scoping in Python functions is one-way from global --> local, so modifications made to this variable will not be retained outside of this function unless we call `global` (or at least `nonlocal`).

Have you tested it without global? I tried a simplified version of it, and it didn't seem necessary to turn this globalYes, it should work if you call them all in the same module, but `global` is necessary when calling these registration functions from a different module, which is the intended usage.",4,True,2019-09-05 23:12:58,2019-09-09 11:16:40,2019-09-09 11:16:39
https://github.com/OpenMined/PySyft/pull/2584,[],Pulling out generic frameworks logic into syft/generic,"Pulling out generic frameworks logic into syft/genericPreviously, generic code that different frameworks shared was living in the syft/frameworks code. This meant we had generic code in more than one place (both `syft/generic` and `syft/frameworks`), which was a bit strange, but also introduced some problems around importing from/inside external packages framework packages. This PR fixes those issues by moving all framework-related generic logic into a `syft/generic/frameworks` subdirectory, leaving `syft/frameworks` purely as an entry point into the frameworks themselves (which will eventually be migrated out of the main PySyft codebase per OpenMined/rfcs#3).",1,True,2019-09-03 19:24:31,2019-09-04 15:13:34,2019-09-04 15:13:33
https://github.com/OpenMined/PySyft/pull/2578,[],Partial fix for #2573,"Partial fix for #2573First basic implementation of a working maxpool2d for CNNs + tests.

**This is not ready to be used with real modules yet, too slow :cry: .**

Related to #2573 I think this function belongs more to the nn module that we have created and filled with rnn.
It's close to conv2d, which is currently not in nn but will be moved there as soon as someone does it :D 
Our nn module is kind of a place to see how to build complex DL objects from basic pytorch ops. It is used by precision and AST tensors/I wasn't sure where to put it, thanks!",2,True,2019-08-30 10:50:16,2019-11-20 15:09:40,2019-11-20 15:09:14
https://github.com/OpenMined/PySyft/pull/2575,[],Fixed performance issue due to mul_and_div,"Fixed performance issue due to mul_and_divThis fix solves the problem of performance of RNNs and activation functions such as `sigmoid` and `tanh` in MPC.

Time execution of a `sigmoid` on a (2, 1) tensor decreased from 15s to 0.18sWould be nice to modularize time monitoring. I suggest to create a function `assert_time_is_less_than(func, time=1)`, that implements this logic. This should be stored in another file at the efficiency-tests folders named `efficiency_assertions.py` (or something similar).Great idea! It would help benchmark different parts of our codebase and see how various optimization would help. Yeah, good idea! What do you think about a decorator for that?niceYes!! Decorators are great for this.",1,True,2019-08-29 09:33:34,2019-09-03 11:35:41,2019-09-03 11:35:40
https://github.com/OpenMined/PySyft/pull/2545,[],Fixing RNN tests,"Fixing RNN testsAfter a recent merge, the tests for RNNs take a _very_ long time to complete, slowing down CI and local dev testing by nearly an order of magnitude. This PR patches it by moving the tests to a separate integration folder that are only run by CI on merge. I've also reduced the hyperparameters all to 2 to try to improve speed as much as possible.

As a side note, the length of time it takes to complete each test leads me to believe that RNNs are generally not suitable to be computed fully with MPC, and I'd suggest users search for alternatives as often as possible :)Could you add pytest integration after the coverage checking? We want to make sure that integration tests pass before merging (but as a separate stage)I was thinking having integration as a top level folderthe tests rely on shared fixtures with other tests. I was thinking of pulling those fixtures into a common module elsewhere but this seemed simplerthis coverage run includes the integration tests since the `integration/` folder is a subdirectory, so they will block merging if they failThat a good point. That being said could you also add a target which only runs the integration tests?I was saying to run them seperately. The idea being that if unit tests fail, we don't want to bother running the integration tests since they're slowwill dogotcha, will do!",1,True,2019-08-22 15:08:14,2019-08-24 15:08:24,2019-08-22 18:27:28
https://github.com/OpenMined/PySyft/pull/2543,[],fix: spdz_mul fails when values are shared between more than 2 workers,"fix: spdz_mul fails when values are shared between more than 2 workersTo reproduce:
```python
import torch
import syft as sy
hook = sy.TorchHook(torch)

# Create > 2 workers
bob = sy.VirtualWorker(hook, id=""bob"")
alice = sy.VirtualWorker(hook, id=""alice"")
bill = sy.VirtualWorker(hook, id=""bill"")

# And share between them all
x = torch.tensor([25]).share(alice, bob, bill)
y = torch.tensor([5]).share(alice, bob, bill)

# Multiply tensors
(x * y).get()
```
Get TypeError
![image](https://user-images.githubusercontent.com/24773652/63514239-594fdc80-c512-11e9-94db-13492c25ff0e.png)",6,True,2019-08-22 12:26:33,2019-11-20 15:27:18,2019-11-20 15:16:39
https://github.com/OpenMined/PySyft/pull/2526,[],Modified Gradient Clipping to work with real remote/local model parameters,"Modified Gradient Clipping to work with real remote/local model parametersFixed the clip_grad_remote method to work with real-life model parameters, so as to work with the SMS prediction example by @andrelmfarias and other production RNN/LSTM/GRU models. 

- Fixed the` clip_grad_remote` method to work with a generator of parameters
- Replaced the hooked method from   `self.torch.nn.utils.clip_grad` to `self.torch.nn.utils.clip_grad_remote`, which is the actual method's name
- Modified the test case for remote/local gradient clipping to resemble a more fitting real-life usage scenario with a model's parameters for training an RNN. what is parameters[0]? Is better to use a better naming convention.Move this comment to docstringDone!Created a method to create whether a parameter is remote and refactored the whole clip_grad_norm_remote_ method so as to check whether a list of remote parameters was being passed to it. The code should be much cleaner now.why is this only sent to one locationis there a reason this was removed? it isn't inherently a bad testyou could change this to greater_tensor_check.all()Because the location of all parameters in one model is the same. So, I could have picked the location any random parameter within the model. Because in a real scenario, this method is supposed to be applied to a generator of parameters of a model and not to one simple tensor. The new tests checks pretty much the same things as the previous one anyway. Next time I create one such test, I'll use your syntax, thanks! It's much more compact than time. However, the test case using where this line is present was removed. There is a difference between unit and integration tests. The deleted test seems a bit closer to a unit test but it seems you replaced it with an integration testYou don't need the torch.eq or ones tensor. .all returns a boolean (not a tensor)Well, it looks to me like it actually returns a tensor containing a scalar being 0 for False and 1 for True.

![image](https://user-images.githubusercontent.com/4907418/63527563-a2e3fb80-c501-11e9-93bf-1f7201858a5f.png)

So I just changed the check into an equality among integers and removed the torch.ones",5,True,2019-08-18 10:52:16,2019-08-26 15:09:23,2019-08-26 15:09:22
https://github.com/OpenMined/PySyft/pull/2502,[],Modularizing frameworks,"Modularizing frameworksCloses #2517.

Per OpenMined/rfcs#3, this PR introduces a separation between the torch framework and core components of PySyft.  Here, I've focused on generalizing TorchHook and torch.Tensor throughout the code, including in the definitions for BaseWorker, AbstractTensor, PointerTensor, and related functionality.

I've also created a `BaseHook` that should allow for calling framework-common functions in a framework-agnostic way (e.g. `create_shape` that calls either `torch.Size()` or `tensorflow.TensorShape` under the hood depending on what's in use).

Similarly, for when type checks are necessary, I've created `FrameworkTensor`, `FrameworkTensorType`, and `FrameworkTensorShape`.  These last abstractions dynamically determine their own type definitions based on what dependencies are installed: for example, if both torch and syft-tensorflow are installed, `FrameworkTensor = (torch.Tensor, tf.Tensor)` and `FrameworkTensorType = Union[torch.Tensor, tf.Tensor]`.  This is built to be extended with further framework types as they're added.

Finally, I've improved the dependency_check to be more robust to various dependency combinations. Note I have NOT yet changed the packaging to allow for different installation structures as planned in the connected RFC.Why do we use this method here but actually import tfwhy is weakref removed?Should this file live in abstract rather than frameworks? We're actually only checking to see if a package is available here. Actually importing the package is unnecessary to complete that task, so it should be handled elsewhere if needed.I wanted to keep it separate, since `abstract.py` is meant for tensor abstraction. I think that file deserves to be renamed!This seems like an accident!in the most recent series of commits, I split abstract.py into object.py and a tensor.py, and put both under the syft/generic directoryfixedSurely this class is still PyTorch specific?I don't think so! I've generalized most torch-specific functionality here already and removed the `import torch` from this file's header and all tests still pass on CI. The only things left to handle are the `data` and `grad` properties, which should be pretty easy to handle on their ownCould we make this an absolute import?Could we make this an absolute import?Following up on this, should this file live in generic then instead of frameworks?absolute?While we're here could we make these absolute as well üòâ What is the behavior if I call both Torch Hook and Tensorflow hook? Should there be a check to see if this value is already setThis does not interact with the childwhy is cls a parameter to this method?Just tensor instead of is_tensor (is tensor makes me think its a boolean)Can we git rid of this comment. We don't support python 2 (or any python below 3.6)Is there a better way to do this by operating on the list?Same as aboveWhat does this do?Is this done?Should this be capitalized?Should this be done (or should an issue be made for this)Should this be refactored to use the new message classPlease create an issue for this and link it herePlease create an issue for this and link heresame as aboveSamedoes randint behave the same in torch and tf?Should this be replaced with the new message object?Please add a comment explaining exactly what this doe and why it is hereThis could be simplified to `if name in list`Could you make an issue for having a better way to no cover these?sure, was keeping it consistent with what was there before.as aboveoverrides a classmethod in the base classCoolFor the TF side, we will need to create `tf.Tensor` and `tf.Variable` wrappers depending on what chain we're wrapping, so this method needs that parameter, it just happens to be unused for torch.as above!as above üë®‚Äçüé® It is a Boolean! üò± sure thing, nice catchAllows for typechecks. Doing `isinstance(tensor, FrameworkTensor)` will dynamically do `isinstance(tensor, (torch.Tensor,))` or `isinstance(tensor, (tf.Tensor, torch.Tensor,))` or `isinstance(tensor, (tf.Tensor,)` depending on which combination of frameworks are installed.nope, will be in a soon-to-be PRit's importing a module, so nope!see abovehm, probably! thanks for the pingno, there should be a TODO here.In my opinion no, because it's part of the framework integration spec outlined in OpenMined/rfcs#3, and I was hoping to keep those points of integration self-contained. Hooking is logically and functionally separate from the rest of the generics.  In particular, it's only useful in tandem with a framework, so I think it makes sense to keep in the frameworks subdirectory since it would never be used outside of there.I was hoping to figure out a way to make swapping between various hooks more well-defined and programmatic -- likely deserves an issuedon't you only need to check for `tf_ecnrypted` in the case when `tensorflow_available` from above?the same here if we do not care about python2 I couldn't find any -- the `Union` object is pretty rigidsame as abovewill make an issue for it thoughwould it make sense to define it above as

and extend within `if dependency_check.torch_available:`
```py
FrameworkTensorType = Union[FrameworkTensorType, torch.Tensor]
```

the same for sizeshttps://github.com/OpenMined/PySyft/issues/2530https://github.com/OpenMined/PySyft/issues/2530https://github.com/OpenMined/PySyft/issues/2530actually, this should've been done elsewhere and is beyond the scope of this PR, so I'll just open an issue for it https://github.com/OpenMined/PySyft/issues/2531https://github.com/OpenMined/PySyft/issues/2530https://github.com/OpenMined/PySyft/issues/2530https://github.com/OpenMined/PySyft/issues/2530https://github.com/OpenMined/PySyft/issues/2530actually, a new PR moved this to the crypto folder, so it's no longer necessary after updating from devsee #2531 #2532No -- I'm working on https://github.com/OpenMined/PySyft-Tensorflow which uses TF2, so the tensorflow versions are not the same and we need to check to see which one is installed.true, fixed, thanks!>would it make sense to define it above as

I would end up repeating that code for each framework -- this way you just create an if block for each new framework that adds its own FrameworkTensorType(s) and FrameworkShapeType and the rest of the code doesn't have to change.Following up on this, we could do the check inside the if block above since tfe available and tf available can never both be truemakes sense to meAh okay. Could you add a comment about that here so people know this isn't a bugCan we change the name of the parameter in the surronding function then?@jvmancuso we can do this as FrameworkTensorType = Union[*framework_tensors]can you link it in the codeThat's what I tried originally, but that assignment will fail with a SyntaxError. I suspected it has to do with the `*` operator only being interpretable at runtime, although I didn't actually check to verify that.Synced in Slack, gonna leave as isFunny enough it work with 
```python
Union[(*framework_tensors)]
```![image](https://user-images.githubusercontent.com/775466/63290723-5c548e00-c2c2-11e9-85ef-db983c9d4722.png)
or 
```python
Union[tuple(t)]
```
![image](https://user-images.githubusercontent.com/775466/63290873-a89fce00-c2c2-11e9-8ee6-5996d8e05173.png)
nice one! will fix that up now",1,True,2019-08-13 20:03:01,2019-08-19 19:49:38,2019-08-19 19:49:37
https://github.com/OpenMined/PySyft/pull/2497,[],Add error message when passing internal_type if not needed,"Add error message when passing internal_type if not neededJust a small modification to send a more informative error message when the user sets `internal_type` using `.fix_prec()` on a tensor that does not need large precision.


When doing, for instance, 
`torch.tensor([1.5, 2.0, 3.0]).fix_prec(internal_type=torch.int16, precision_fractional=3)`

The error message was:
TypeError: __init__() got an unexpected keyword argument 'internal_type'

Now, it is:
TypeError: internal_type not needed if data does not need LargePrecisionTensor to be storedDoes this need to be a TypeError? Or is a warning sufficient? 
What happens if you just ignore the internal_type argument? Would the code would work correctly?The error originally sent was a TypeError so what I did is just to catch it just before and send it with another message.
But I agree, we could remove this attribute from the kwargs and raise a warning, it would work the same. I also thought about this and wasn't sure what was the best. Do you prefer this solution?My preference would be on removing the needless argument and emit a warning. And maybe change the message to make it clear that internal_type should not be provided (not just not needed) in this case.Fine, I'll do that :)Done
It is now clear that the argument should not be provided but I find it weird because the user might not know that his tensor does not need large precisionLooks good to me. @robert-wagner Any comments on the usage of warnings?",1,True,2019-08-12 22:18:21,2019-08-21 12:49:36,2019-08-21 12:49:36
https://github.com/OpenMined/PySyft/pull/2492,[],Remote Gradient Clipping for PySyft Tensors,"Remote Gradient Clipping for PySyft TensorsCurrently, as noticed by @andrelmfarias in his tutorial, the plain PyTorch `torch.nn.utils.clip_grad.py` method does not work for remote tensors, like the parameters of a remote RNN. This method is essential for preventing the exploding / vanishing gradient problem from occuring with RNNs and their LSTMs or GRU versions. 

I modified the in-place `torch.nn.utils.clip_grad.py` method from the PyTorch 1.1.0 version and adapted it to work in a remote manner, then hooked it to the torch.nn.utils.clip_grad method for PySyft remote tensors.

In the current implementation, I'm overriding the `torch.nn.utils.clip_grad` method, so the plain PyTorch `torch.nn.utils.clip_grad` method is no longer available when hooking PySyft. 

Should I use a different name or hooking strategy for this method, so that both the plain `torch.nn.utils.clip_grad` and the clip_grad_remote can live side-by-side? (one method would operate on plain PyTorch tensors and the other one on PySyft tensors) 

I tested it with the Federated SMS Spam Detection tutorial #2288  and it does indeed seem to be working.

![image](https://user-images.githubusercontent.com/4907418/62819061-1d02a080-bb50-11e9-813f-73d76f1a8e00.png)

P.S: Why are there so many commits being shown below??
Could you replace torch.Tensor(1) by torch.zeros(1)?Done!remove `r`Can you make more explicit in the comment why do we need a specific function for clipping remote tensors? maybe add comments to the lines that were changed and why/how they were changed? Is there a better place to import inf from? six is mostly geared for 2:3 compatabilityCan you add a test to make sure that gradient clipping works locally as wellI replaced it with `from math import inf`. I suppose that should work too?Added! And modified the method to support gradient clipping of local tensors as well.Made the comment at the beginning of the function more explicit about the motivation for having this extra function. Actually, the only two lines changed are the following ones:

```
total_norm = torch.zeros(1)
#Let's send the total norm over to
total_norm = total_norm.send(worker)
```Done!",2,True,2019-08-10 07:20:52,2019-08-16 16:29:15,2019-08-16 16:29:15
https://github.com/OpenMined/PySyft/pull/2491,[],Fix build issues by downgrading torch,"Fix build issues by downgrading torchFor now, we had to downgrade torch (force version 1.1) and torchvision (force version 0.3)Looks good to me",1,True,2019-08-10 00:16:43,2019-08-16 12:48:03,2019-08-12 09:16:54
https://github.com/OpenMined/PySyft/pull/2479,[],fix test_loader to load train data instead of test,fix test_loader to load train data instead of testThis PR fixes issue #2478 :),2,True,2019-08-09 01:58:32,2019-08-12 10:06:35,2019-08-12 10:06:35
https://github.com/OpenMined/PySyft/pull/2445,[],Add disable_garbage_collection  flags to native.py  ,"Add disable_garbage_collection  flags to native.py  This always returns truewhat is the difference between this and gc?This string is wrongremove this print statementIt should be the same - I thought it might be nice to have a shorthand version. Open to comments there.My point for this is that repoducing the logic in 2 places is bad. If we change one of them and forget to change the other, that leads to unexpected behavior@robert-wagner there's no difference, gc just shorthand form. It was mentioned in the description https://github.com/OpenMined/PySyft/issues/2444

@iamtrask I agree with @robert-wagner, I would suggest we keep the garbage_collection only as it explains its purpose although variable gc is also quite implicit about its usage. 
So which one should we keep?removedFixedfixedgo with Bobby's suggestion - he's usually right :)For clarification, I think we should keep both, just abstract them so that they call the same thing üôÇ emote -> remote. Also could you split this up into 2 lines (its a bit wide on my screen)",2,True,2019-08-06 20:47:24,2019-08-12 20:11:40,2019-08-12 20:11:40
https://github.com/OpenMined/PySyft/pull/2443,[],Add support for float_prec() on pointer tensors,"Add support for float_prec() on pointer tensorsCurrently `float_precision()` is not supported for pointer tensors. This code adds support and includes testing for both `fix_precision()` and `float_precision()` on pointer tensors.

Had to manually wrap the response to the `send_command()` as it returned an unwrapped pointer. Might cause issues in the future if later on any change makes the response return a wrapped pointer.Hmm, I don't like that you have to wrap the response for this specific case but not for the others... I think it's worth exploring why this is the case...can you import PointerTensor and use it here?same about importing.can you import FixedPrecisionTensor and use it here?Can you add the same test but for a pointer of a pointer?Can you add the same test but for a pointer of a pointer?I've found why, but I'm not sure how to fix it. the definition of `float_prec()` in `interpreters/native.py` is 
```
def float_prec(self):
    return self.child.float_precision()
```
which differs to `fix_prec()` here
```
if self.is_wrapper:
    self.child = self.child.fix_prec(*args, **kwargs)
    return self
```
What I can see is that it is assumed that a float_precision tensor is wrapped, and must return an unwrapped torch tensor. I see two ways of solving this:

1. check if `self.child` is a pointer, in which case return the wrapped tensor
```
def float_prec(self):
    if self.child is PointerTensor:
        self.child = self.child.float_prec()
        return self
    return self.child.float_precision()
```
2. Check that if a torch tensor has a wrapper, then remove the wrapper, as it is useless. 

The first option is the easiest, but I feel is somewhat dirty to have to manually check for type and specify different behavior. The second one might be out of my knowledge on how to implement it.

What do you think?I agree that 2. looks like a better option. @LaRiffle @robert-wagner do you know if 2. is possible/worth implementing?This should be a FixedPrecisionTensor, right?This should be a FloatPrecisionTensor, right?- Call fix_precision() on the ptr before calling float_precision() (otherwise it doesn't make sense even if it doesn't fail)
- You can use bob._objects[ptr.id_at_location] to inspect a remote object (if you want to check types for example)
- Prefer maybe isinstance(..., ...) instead of type() == ..., it is more robust for sub classesPrefer isinstance(...) to type(...) ==",4,True,2019-08-06 16:52:32,2019-09-10 08:14:01,2019-09-10 08:14:01
https://github.com/OpenMined/PySyft/pull/2441,[],Docker image for the WebsocketServerWorker,"Docker image for the WebsocketServerWorkerWe can now run server workers with Docker, this will enable automating the deployment of workers across different nodes.what do we need gcc for?Rather than having this as a shell script can we just set a default and do this in the docker file?some python modules require gcc to be built and that's the case for some of the PySyft's depsDoing this in the Dockerfile will require rebuilding the image if we need to use another id, however, being able to provide the id at runtime give more flexibility.
What also led me to oblige the user to specify the id is that if we use a default one then the user may be experiencing some issues due to id duplications without knowing what's really happening.+1+1Nice work. 
The port can stay fixed, as we can modify the port exposed by docker, right? (e.g. docker run -p 8770:8777 etc.)
As a next step it would be interesting to preset datasets on the websocket server worker. But this is out of the scope of this pull request.One way to get rid of gcc is to use a docker multi-stage build and copy the created python installation (environment) to an environment without gcc.yeah, you can expose any available port using -p exposed_port:8777.

I will be making an example on how to run workers using a docker-compose file next. 

And yeah, the idea of the dataset is awesome, making this image a fully ready to use workerthat's right, but I'm not sure about all the files that I must copy (especially with pytorch) so I took image size against complexity in this case> I will be making an example on how to run workers using a docker-compose file next.

Can you make this in this PR? I think it would be very valuable. The instructions could be a new section in the CONTRIBUTING.md fileI was making some examples on a separate draft PR, but yeah, I will merge that work here.Use `exec python ...` to pass python PID to docker, not shell PID.
Otherwise docker will not be able to terminate the process normally.This layer will be cached with whatever syft version is available at the time of the first build and won't change until you reset docker build cache.
It is better to specify syft version explicitly via ARG or ENV
mkdir is redundant, WORKDIR creates the folder automatically :)entrypoint.sh should be made executable in git?In seems these 2 COPY's can be replaced with 1:
COPY [""./entrypoint.sh"", ""./worker-server.py"", ""./""]Consider adding `&& rm -r /var/lib/apt/lists/*`
To cleanup apt files not needed in the imageI'm doing it this way cause I think of automating the build process, and whenever you pull the image from the docker hub you have that last stable version available on pip, adding the version here will add an extra management, to keep the Dockerfile updated, right?thanks for the info. doneI prefer to keep it as is, it's not necessary to have it as executable (no one is gonna use it) as long as we have that `chmod`I was thinking that worker-server.py is more prone for updates and so ending up building only the last layer, what's your thoughts about that?As far as I know, deleting those files will not reduce the image size in anyway and only delete those files in the last layer but keeping them in previous onesIt won't keep files if you add `rm` in the same `RUN` command where apt-get is executed, e.g.:
`RUN apt-get update && apt-get install -y gcc && rm -rf /var/lib/apt/lists/*`This makes sense :) However, you'll want to disable caching anyway if you want `pip install syft` to pull the latest version.done
right, I will update that one.Then you'll probably have to disable caching :). Oh, and somehow make sure the latest syft is available for pip before docker build is triggered?I think this will make the directive too long and harder to read, the packages are now only 17mb, I will switch to this if it gets way bigger than thisI'm not yet sure about the tool, but I think that the build process is generally made on a fresh instance and the image shouldn't be cached their, I will just make sure to setup the trigger correctly like you said",8,True,2019-08-03 20:15:44,2019-11-20 18:15:15,2019-11-20 17:22:39
https://github.com/OpenMined/PySyft/pull/2439,[],Move serde type constants to a separate file,"Move serde type constants to a separate fileHi,

While working on js worker for syft I've noticed that syft serialization protocol is quite unstable because type codes in simplified objects (e.g. `5` in `'string' => (5, (b'string',))`) are just indices in detailers list dynamically generated by `serde._generate_simplifiers_and_detailers`.
Any insertion or deletion to `MAP_NATIVE_SIMPLIFIERS_AND_DETAILERS`, `MAP_TORCH_SIMPLIFIERS_AND_DETAILERS`, `OBJ_SIMPLIFIER_AND_DETAILERS`, `EXCEPTION_SIMPLIFIER_AND_DETAILERS` (like https://github.com/OpenMined/PySyft/pull/2436) may shift indices and hence change some type codes.
This will be a problem for other projects that want to communicate using syft serialization protocol like Android Worker and syft.js. For example, type constants defined in Android Worker are already out of date.

I've discussed this concern with @cereallarceny and he suggested that type constants can be moved to a file in separate repository that can be used by PySyft, syft.js, Android Worker and any other projects. Such repository could be also good place to keep serialization protocol documentation (if any).

This PR is to start discussion whether such approach is useful and refine details.
Changes here:
 * constants are moved to proto.json file
 * serde is updated to use constants from there (based on class name)
 * unit tests updated

Open questions:
 * JSON file format and types naming, e.g. maybe it makes sense to drop module name, at least for ""syft.*"" classes?
 * if proto.json file will be in a separate repo, how it should be linked in other repos, including PySyft? I'd suggest using git submodule, but people often don't like that
 * Approach to versioning the proto.json file?

ThanksRather than releasing this on git can we just put it on pypi/npm so we can do a normal importSure, but there's not really a need to have this be a public package.  It's not a project that's useful in any open-source context and there's no problems with installing via git.  For me, I'd be in favor of doing things the way they are currently in this PR, it's also the approach we took in syft.js for the above reason.The other issue here @robert-wagner is that we will ideally be publishing updates to proto according to PySyft deployments.  Whenever a new PySyft is deployed, it should kick off a new proto deployment.  This is going to be done in the future.

However, adding this package to PyPi and NPM would only complicate the matter further.  It would also make it harder for syft.js and other libraries to stay up to date.  Can we avoid doing this for now until a better situation presents itself in the future?  @robert-wagner But what if i wanted an old version of proto? Dos this allow for that?All for it assuming we can reference old versions of proto. Working on automating as much of the deployment as possible so we should not worry about the pypi/npm problems going forwardWe can create tags in proto's git and reference specific tag in PySyft's requirements.txt. There can be extra step of publishing these tags to pypi/npm but it's not as important as figuring out the versioning process itself (i.e. which version to assign and how to sync pysyft and proto).
@robert-wagner what is PySyft publishing procedure currently?
Perhaps the simplest plan could be PySyft@dev using proto@master and then cut proto versions in sync with PySyft. Is it possible to have `proto@master` in requirements.txt of PySyft@dev, but freeze proto version in requirements.txt when publishing PySyft?
For example:
1. bumpversion on pysyft@dev (0.1.28a1 -> 0.1.29a1)
2. make pysyft branch `release-0.1.29a1`
3. [in release-0.1.29a1 branch] update requirements.txt: proto@master -> proto@0.1.29a1
4. tag pysyft@release-0.1.29a1 HEAD as pysyft@0.1.29a1
5. tag proto@master HEAD as proto@0.1.29a1
6. publish pysyft@0.1.29a1
@iamtrask brings up a good point.  I think it would be wise to do some sort of versioning process.  @vvmnnnkv and I will determine some sort of solution and report back.We absolutely could do something like described. I will be working on automating a lot of the release processes this week. As of right now as long as you document the processes that you want, I can come back to it later and get it standardized (it should be pretty similar to much of the other work needed for syft-tf)Awesome @robert-wagner - could you and @vvmnnnkv work together to implement some sort of versioning and release strategy for Proto?  I'd love to build that into this PR and get it merged soon.  üòÅ @robert-wagner Please see suggested dev workflow regarding updates that involve proto:
https://github.com/OpenMined/PySyft/pull/2439/commits/7a19d7cd33ea647e9a518edbfe075077a070eeb6
That way PySyft@dev can continue using github dependency of proto@master (to avoid publishing proto every time somebody makes a change between pysyft releases). The only concern is who will do the last step (maybe some automation is possible?).

For PySyft release, I think it can be same as described above:
1. At the time of PySyft release, proto@master is tagged with PySyft version
2. PySyft's requirements.txt is updated with tagged proto version
3. PySyft is released/published

There may be a step to publish proto to pypi between 1 and 2. In this case requirements.txt is updated with reference to this version instead of reference to github tag.",21,True,2019-08-03 05:12:50,2019-11-12 21:08:51,2019-11-12 21:08:51
https://github.com/OpenMined/PySyft/pull/2437,[],explicitly call get_obj() from BaseWorker,explicitly call get_obj() from BaseWorkercomplement this fix #2434 looks good to me,2,True,2019-08-02 13:37:31,2019-08-12 09:56:14,2019-08-12 09:56:14
https://github.com/OpenMined/PySyft/pull/2436,[],Created an actual Message type and moved Plan out of federated,"Created an actual Message type and moved Plan out of federatedAll needs to change as wellPut these on separate linesWhat is this parameter for?Use absolute imports pleaseShould plans inherit from message? Also this class seems like it might be a security risk, what prevents me from creating an arbitrary message?This would look much cleaner if you imported message. That being said, what is this point if this change, why not just keep it a tupleSame as above for my question about tuplesWhy does this method that does nothing exist?Again, what advantage does this give us over a tupleWhy are there no other tests for messages?for not yet implemented functions better use 
```
def save_promise(self):
    pass
```doneDoneoops - bit early for that - this was originally a bigger PR but i had to stop because it was already getting quite large (better to merge along the way)doneNot any more of a security risk than it was before, messages are still whitelisted based on the msg_type.  This is literally just a class where the tuple used to be which carried messages. Starting small.Could we limit this pull request to only contain the introduction of the message type?
It is not clear to me, why Plan should be in a message folder.It's a myriad of reasons, mostly organization. I'm working on some more advanced async/protocol/promise functionality and it's become quite annoying that messages aren't an actual type with actual to-strings which help with introspection. is this within the scope of this pull request?eventually Message objects will be passed to send_msg but I didn't want to do too much in one PR. This is an incremental PR for sure.same answer as aboveremovedi'm working toward something which will give us more advantages - but i didn't want to make too many changes for one PR - so this is an incremental one.There is no new functionality for Message yet. I literally just replaced the tuple with a class with two attributes. Zillions of other tests already use it.removed it instead The message folder will contain all messages, groups of messages, and graphs of messages. Plan was in the federated folder which implied that it's only useful for Federated Learning. removedThere might be tests that are using it, but all of them are integration tests (meaning, that they use something that uses the class Message). It would be good to have a basic unit test as well. fair - i'll make a test :)i'll test serde - which is newStill would look cleaner importing messageAlso we should be testing message construction - which we are not.Still should import message thoughThis will also allow us to add functionality inside the messages to make sure they're well formed. At present we only find out if they end up causing an error on the other side.I agree that Plan can be used for more than federated learning. But I am not sure that it should be in a message folder neither.Also this will make the code easier to write - at present it's totally undocumented what goes into a message (aka - our protocol is undocumented). A good first step here is setting out messages types and what information goes in each message.This will also allow us to do smarter simplification/compression in the futureSo yeah - lots of good reasons I think. :)groovy - added a test for message serdeDonealso - changed it to import Message directlyShould this comment be here?Why are all these imports here?My prefered style is to use:
from sy import msg
and then use msg.Message(...)
That way you always have the source referenced. 
And additionally you follow the google style guide, see http://google.github.io/styleguide/pyguide.html, section 2.2a plan is a cache of messages - seemed pretty close. Other ideas for what to name the folder?maybe rename the folder ""messaging""?lol - ok - will fixfixedremovedremovedCould you unify the way the import is done? (sy.Message vs sy.msg.Message vs sy.msg.Plan)
sy.msg.Plan => msg.Planmissing newline@mari-linhares what do you think about the naming of the folder and the location of Plan?donedoneI agree that we should rename it eventually. Not sure if ""message"" is the best option but I don't have a good naming in mind neither.msg, cache,... something. 

It's relatively easy to change in the future with modern IDEs. But I think all are better than having it in federated.",1,True,2019-08-02 12:38:10,2019-08-02 14:15:58,2019-08-02 14:15:58
https://github.com/OpenMined/PySyft/pull/2434,[],fix private tensor disclosure via execute_command,"fix private tensor disclosure via execute_commandPrivate tensors aren't meant to be accessible from a remote client, however, execute_command was getting any object using his id, this fix get the object using the get_obj method that doesn't return private tensors
#2432",7,True,2019-08-02 08:32:47,2019-08-02 12:05:44,2019-08-02 09:06:25
https://github.com/OpenMined/PySyft/pull/2431,[],Make the local worker aware of itself on TorchHook creation.,"Make the local worker aware of itself on TorchHook creation.This PR solves issue #2430 and issue #2426. When the local_worker is being created, `hook.local_worker` is `None`. Because of this, the local_worker is not added to the `hook.local_worker._known_workers` dictionary, making himself and any subsequent worker unaware of local_worker's existence.",3,True,2019-08-01 20:09:54,2019-08-02 07:43:09,2019-08-02 07:43:09
https://github.com/OpenMined/PySyft/pull/2415,[],make use of secureNN div for AdditiveSharingTensor,"make use of secureNN div for AdditiveSharingTensorSome problems occurred:
- [x] In the secureNN division, we multiply the divisor by `2**Q_BITS` at some point, which overflows.
- [x] When dividing a FPT>AST by another FPT>AST, the scaling (`base ** precision_fractional`) disappears. We need to override the FPT division to do the inverse of a truncation after the division.

Also secureNN division is super slow""Division of a FPT by an AST not implemented"" would be more clear
Also let's not use FPT & AST abbreviations in our debug messages, people might not know them",2,True,2019-07-30 22:21:04,2019-08-16 19:05:50,2019-08-16 19:05:50
https://github.com/OpenMined/PySyft/pull/2411,[],Add encrypted training on MNIST demo,"Add encrypted training on MNIST demoUpdate Part 12 (no wrapper version)
Add a new demo Part 12 bis on MNIST

I would love advice and corrections on the Part 12 bis!Why do you need to use `.refresh()` here? Can't it be hidden?I don't see an easy way of doing it currently, unfortunately :/<p>Is there an existing diagram explaining the third paragraph? I feel that I might have seen it in a tutorial... anyway, a diagram would be nice :)</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2411/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='0'/><p>&nbsp;et -&gt; and (using all my vast knowledge in french I inferred that)</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2411/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='6'/><p>Here we're using a utility function which simultate -&gt; Here we're using a utility function which simulates</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2411/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='8'/><p>Can you comment more about <code>requires_grad</code>, <code>fix_precision</code> and <code>share</code>? Maybe a paragraph highlighting why they are needed and a intuition on what they are doing.</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2411/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='9'/><p>Not sure if this python snippet is formatted correctly (it doesn't seems so at app.reviewnb.com)</p>
<br/>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2411/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='25'/>What does refresh() do?So... Division by integers C (or truncation) is a real pain with additive shared tensor:
For fixed precision, values x live in Zq so if x < q/2 you just do x/C and if x > q/2 you do (x - q) / C + q.
But now if you use additive shared tensors, now you have x1 and x2 in Zq, not observable at the same time, with x = x1 + x2 allegedly in Zq. When dividing by C you can't know if x > q/2 and no worker with one share can also decide this...
So we implement a rule of thumb of a paper which works in most cases, provided that the shares are fresh: ie x1 and x2 look like random values in Zq. That's why I need to **refresh the shares by adding shares of 0** because otherwise the division fails.

To the question: should be automatize refreshing under the hood? I really don't know, I need to better understand when it's needed and when it's not, etc.

Anyway thanks for spotting this, I agree it needs further explanations.I vote always refresh() - better to be slow and not scary for new users than the other way around :)New User: ""WTF! Why doesn't division work?!?!?!"" 

is worse than

New User: Hmm - this encryption thing is kindof slow - but i guess it's worth it and will get faster as technology advances.<p>I have the figure you need, I'll add it right now :)</p><p>Haha</p><p>I will, should I also remove this part of the demo, like just call a utility file? I have the feeling that many people asked why sending the data to workers at the beginning (in the FL demos especially), so maybe keeping this magic and moving this to a .py file would be better, what do you think?</p><p>Looks really nice on jupyter notebook (I used `x3 and ""python"" keyword)</p>Yes, but for some mysterious reason if I refresh everytime it doesn't work neither! I need to find when refreshing should automatically be performed. Sounds good - no need to block for that feature as it doesn't pertain to this tutorial directly.",1,True,2019-07-29 17:20:56,2020-01-23 08:42:33,2019-07-31 22:40:19
https://github.com/OpenMined/PySyft/pull/2407,[],One worker bug,"One worker bugIn reference to issue #2392, Single iterator does need to shift worker IMHO.  Thus, for case of 1 worker we can have 1 iterator insetead of general (worker - 1) iterators.",4,True,2019-07-28 23:29:26,2019-08-01 17:40:49,2019-08-01 17:40:49
https://github.com/OpenMined/PySyft/pull/2406,[],notebook with more instruction and some changes in scripts,"notebook with more instruction and some changes in scriptsstart_websocket_servers have some unnecessary lines to provide python versions and is causing errors on windows 10 but since in subprocess.Popen we provide the name of exe that will execute certain file with certain arguments. In run_websocket_client logging.debug is replaced by print because logging is not showing losses and errors. Notebook has more instructions and possible solutions for possible errors and removed some extra codeShould be markdown not code: have a look at your notebook in jupyter to make sure it prints nicely, here is how it looks like currently: https://github.com/OpenMined/PySyft/blob/039798359758a5f02d841c5d2c15fe676318a0c4/examples/tutorials/advanced/websockets-example-MNIST/Federated%20learning%20with%20websockets%20and%20federated%20averaging.ipynbDon't put windows path
or make it clear when this is for windows users (you can use citations with '>' . in markdown)

Example:
> **For Windows users**
> Use `cd my\path`Remove this printIt doesn't print nicelyWhy not keeping logger.debug?Why changing this?logger was not printing anything on the notebook output, I don't know what was causing that maybe it was because we were getting functions from script instead of running script directly from terminal so I tried print and loss after LOG_INTERVAL start showing in notebookI might have accidently did that while on pycharm changing it asap Install the markdown module for pycharm :)
Put you should prefer jupyter notebook I'd say :)done made it more generic It was giving error no such file exists ... so I tested what arguments in subprocess.Popen are doing by using these arguments ['notepad','README.md'] so it opened README.md so what that line what doing is opening that README.md file with notepad.exe so in windows there is only one python.exe file in each envirenment there is no python3.exe or python3.6.exe so in windows 10 if I activate some conda environment to execute this script it will use python.exe of that environment for that subprocess.Popen  ... I dont know if it will behave the same on linux I havn't tested it on that if you want I can make an if for when using windows and when using linux but I will try to  do few more test on logging ok so I tried writing !python3 and !python2 and !python3.6 on google colab and its working there so it means these will work on Linux so I am adding an if statement for people using Linux logging working now had to add steam handlerswhy the use of .format()?so in short we need:
```
if os.name == ""nt"":
    python = ""python""
else:
    python = ""python"" + sys.version[0:3]
```I think we can remove the comment concerning the BrokenPipe issue. Should be resolved by now (improved startup parameters of the websocket connection).@midokura-silvia no particular reason to me it was more pythonic approach also these in .format approach you don't need to specify any type I can change it back if you think there is no need for changetrue will make that changethis error happened to me 2 days ago when I was trying to run that box. When was the issue resolved?I don't really use the notebooks, so it seems that I confused it with another problem. If it persists, leave it in.Maybe keep this information even if you remove the train function description, because it is important to understand how the training works.sure@LaRiffle done",2,True,2019-07-28 22:14:12,2019-07-31 10:33:23,2019-07-31 10:33:23
https://github.com/OpenMined/PySyft/pull/2405,[],notebook with more instruction and some changes in scripts,notebook with more instruction and some changes in scriptsstart_websocket_servers have some unnecessary lines to provide python versions and is causing errors on windows 10 but since in subprocess.Popen we provide the name of exe that will execute certain file with certain arguments. In run_websocket_client logging.debug is replaced by print because logging is not showing losses and errors. Notebook has more instructions and possible solutions for possible errors and removed some extra code,1,False,2019-07-28 21:09:24,2019-07-28 22:11:11,2019-07-28 22:11:04
https://github.com/OpenMined/PySyft/pull/2402,[],[WIP] Refactor Polynomial Tensor ,"[WIP] Refactor Polynomial Tensor Polynomial tensor was previously commented out due to some test errors due to it.  This PR aims to resolve it. Also this pr aims to make polynomial tensor part of syft chain with end goal being passing additive and fixed precision tensors passing tests. This is incorrect. PolynomialTensor should be a part of the class hierarchy of ""x"", not a function on which x is called.this method is running sigmoid inline. This is incorrect given that sigmoid() does not have an underscore (also this means it will not work with autograd). It should be returning a new tensor with the results. self.child should be unchanged. 

If the other methods are this way as well they will also need to be modified.This doesn't look like it's properly wrapped. 

On line 185: 
x = self.child

On line 190:
val += (x ** i) + .....

which means val is the same type as x

Thus, val is the same type as self.child which means it's the wrong type. 

Relatedly, shouldn't there be a hooking decorator on this method?Are you filling a dict with the coefficients for all the functions here?
If so couldn't you wait to use a function to create the appropriate tensor?

And also, why are `self.exp_coeffs`, etc. lists and not directly tensorsThanks for the suggestion, Jason. This is a refactor of the old PR. I realized its inefficient considering how only the required coefficients could be used when required. It works fine now :)",3,False,2019-07-28 10:49:11,2019-11-21 01:16:32,2019-11-21 01:16:32
https://github.com/OpenMined/PySyft/pull/2395,[],reduced the timeout interval on windows due to C timeval overflowerror,"reduced the timeout interval on windows due to C timeval overflowerrorWebsocketClientWorker call to connect() method fails on Windows due to ""OverflowError: timeout doesn't fit into C timeval"". Reduced the TIMEOUT_INTERVAL in syft/workers/websocket_client.py. After this change WebsocketClientWorker connect() call is successful on Windows.",3,True,2019-07-25 16:21:09,2019-07-26 20:50:33,2019-07-26 20:50:33
https://github.com/OpenMined/PySyft/pull/2387,[],Fix broken Openmined.org demo,"Fix broken Openmined.org demoFixes #1905.

Addresses issues brought on by changes that have replaced  the `SocketWorker` class with the `WebsocketServerWorker` and `WebsocketClientWorker` classes. Aim to get the notebook functional in the Google Colab environment.The print statement should rather be: print(""Websocket server stopped.""). Because after hitting ctrl-c the websocket server won't respond any more.

Could we use logging instead of print statements?Yup, sure. Will make a commit to change the print statement into logging instead. Done.Could you explain me the purpose of this notebook? It only starts a WebsocketServerWorker.
In the text it says this is a 2 notebook tutorial. But in this pull request there is only one. Is there a file missing in git?

Additionally, there is a naming convention of the notebook files in this folder. Could you please copy the style from the other notebooks available.

Concerning the setup. No need to install everything again, when using the notebook server started as provided in the pysyft-repo. I would prefer: `logging.info(""Websocket server stopped."")`Actually I only modified a tiny bit of the `Server` notebook downloaded from the Openmined.org website (the same one given by Trask in the Issue #1905) to make it work. So much of this version retains what he did previously. I do agree that it looks kind of rudimentary in its nature and can expand of it if requested. It is to be used on Google Colab hence the weird setup code block... But I need more feedback as to what new things to be included in the new Colab version to continue work. The intension is for the Openmined.org website maintainer to use this version instead of the old non-functional one. There is indeed a companion `Client` notebook. I have opened another issue #2396 to deal with that one and am actively following up on it.",5,True,2019-07-23 22:25:03,2019-08-01 17:32:44,2019-08-01 17:32:43
https://github.com/OpenMined/PySyft/pull/2367,[],Fix typos,Fix typos,1,True,2019-07-18 22:27:32,2019-07-19 15:11:40,2019-07-19 15:11:40
https://github.com/OpenMined/PySyft/pull/2364,[],Added fix_prec for Linear Object,"Added fix_prec for Linear ObjectI encountered the issue
""AttributeError: 'Linear' object has no attribute 'fix_prec'""

while I replaced fix_precision with fix_prec as shown in this tutorial https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%2012%20-%20Train%20an%20Encrypted%20Neural%20Network%20on%20Encrypted%20Data.ipynb",2,True,2019-07-17 11:52:30,2019-07-18 15:08:59,2019-07-17 19:06:46
https://github.com/OpenMined/PySyft/pull/2360,[],Monitor network usage with pyshark,"Monitor network usage with pysharkFixes #2256. 

Implementing the monitoring of network usage with pyshark, by adding two static methods `get_packets` and `read_packet` to `metrics.py` in `PySyft.syft.generic`. The `get_packets` function takes in the network `interface` as the basic argument and returns the LiveCapture object with the packet info being sniffed on, plus the number of packets captured in the event. On the other hand, the `read_packet` is a convenience function for showing payload details regarding a single packet in a LiveCapture object. This is a great work. It looks very good and it will be very helpful.
My only concern is whether this utility could be somewhere else and not within this class. Yeah, I could move them to somewhere more appropriate... Any suggestion(s)?I could move the `pyshark` utility elsewhere, say further up like at [syft/workers/base.py](https://github.com/OpenMined/PySyft/blob/1d216ae362bc35a2238cbaf12a9c02f047debefd/syft/workers/base.py)? Would that be more appropriate? I checked `torch` is imported there hence my suggestion. I would move it to its own file/module and let the users that want to have those metrics import it in their projects.
What about `metrics.py`?
Second opinions are welcomed!It depends a bit of how it is used I would say, if you have an example to share it would be super cool!
I would say, either this is very related to workers is that case we can have it as part of the abstract.py worker file, or if it is more agnostic of the concept of worker, then a `metrics.py` file maybe in syft/generic or syft/federated would be more appropriate!Thanks for the suggestions! I have moved the utility to a spot called `metrics.py` in the `PySyft/syft/generic` directory. I think this would be a lot more fitting than its former location. Also, I will add examples and explanations for the newly added methods soon, say within the next few days. wheere->whereThanks, corrected along with other typos.",13,True,2019-07-16 16:04:05,2019-07-28 15:02:38,2019-07-27 15:33:52
https://github.com/OpenMined/PySyft/pull/2358,[],Added serialization and deserialization methods for AutogradTensor,"Added serialization and deserialization methods for AutogradTensorIn response to issues raised in #2318 , adding methods to serialize and deserialize an AutogradTensor.why were these tests renamed?Can you add tests for this?

It would be great to have simple tests for serialize / deserialize and also a test where you:

- train a model (with some toy data)
- run inference in the model (with another batch of toy data)
- serialize and deserialize the model
- check that the evaluation has the same result as the original model
- continue training the model and check that the weights are changing
Thanks for the comments. I tried to perform training of an RNN using Autogradtensors just yesterday, and I bumped into some issues related to backpropagation yielding errors, because tensors were wrapped into AutogradTensors. I fixed this issue by extracting the tensors via an ""hotfix"" into the Pytorch's library code at `torch/autograd/__init__.py` in the `def backward` method.

At line 87, we just had:

```python grad_tensors = list(grad_tensors)```

I replaced that line  with:

```python
 try:
        grad_tensors = list(grad_tensors)
 except:
        print(""Gradients extracted!"")
        #extract the tensor from the autogradTensor in every tensor
        grad_tensors = extract_tensors_from_autograd_tensors(grad_tensors)

def extract_tensors_from_autograd_tensors(auto_grad_tensors):
    grad_tensors = []
    for i in range(len(auto_grad_tensors)):        
        tensor_grad = auto_grad_tensors[i].child
        grad_tensors.append(tensor_grad)
        
    return(grad_tensors)
```
Any suggestions on how this hotfix could be introduced into PySyft for autograd_training with Autogradtensors?

Nevetheless, the gradients are still not being produced in my specific use case #2318, so no real training can be performed =|This is just some leftover from #2343 Would you mind undoing the changes in this file if possible? so we can keep the PR as simple and narrow as possibleWhat's the exception you get when you try to execute `list(grad_tensors)`?Probably adding these changes to hook should do the job, and in my opinion, is not that hacky.Yeah, I'm going to do that tomorrow morning for sureYeah, hooking these changes should probably be sufficient. I just didn't have much of a clue as to where they should be placed.The error I was getting is the following one, BTW:

![image](https://user-images.githubusercontent.com/4907418/61272661-ee3f1780-a7a7-11e9-87f9-44ebed405875.png)
the files in the .spyproject folder should not be part of this pull request.why do you need to copy the weights before calling get()?There is no explicit reason here, but usually calling .copy().get() allows the weights to remain in the remote model for further operations, whereas .get() would just get them locally and remove them from the remote model.Let's see how we can improve this second assert statement.
A loss < 500 seems very lax to me. Does the fitting work on this dataset work with 10 epochs? I guess it does, given that the classifier just needs to look at the first coordinate (linearly separable problem, much easier than xor). 
If the training is stable, we could ask for both remote and local loss to decrease through training. 
I just saw that you send the local model after training to the remote location to train again. That makes it a bit more tricky to check the loss. Any propositions?The comment ( # Training procedure) should be attached to the nested train() functionmodel_local and model_local_trained point to the same model, right?Yeah, it is indeed a bit more tricky because it looks like the local model is being affected too, following the remote training. I tried to detach all the parameters of the local model, but that didn't make a difference.

As you suggested, we could probably check for a monotonic decrease of the loss during the training procedure both for the local and the remote training procedure. Yeah, I'll fix it.Yep, they actually point to the same model, so we could possibly get rid of the model_local_trained variable.If you calculate the loss before training you could see a decrease. But probably not for the second training round (the remote one), as there are only 4 data points and you already trained the 3 parameters of the model for 10 steps. Can you undo this change? Just so we don't track in unedeed file.You can just have a boolean assertion here: `assert torch.all(...)` instead of asserting  that is equal to 1.Same comment about boolean assertionAgree.Could be contracted into a ternary operator:
chain = syft.serde._simplify(tensor.child) if hasattr(tensor, ""child"") else NoneDone!Done!Should be done nowThis tests doesn't work: there are no autogradtensor created here because:
- you need to set requires_grad to true on tensors
- send(..., local_autograd=True) creates an autogradtensor locally, you would need to send this object to another worker to have the expected behaviour (so 2 send are called)

I would suggest check types of your local and remote objects in the tests to avoid this and to improve readability :)Same here!So there is a bug here: Autograd should be created because model parameters have require_grad to True but it's not currently working :/So, I need to add a requires_grad=True on these tensors like:

`data_local = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1.0]])
data_local.requires_grad=True
target_local = torch.tensor([[0], [0], [1], [1.0]])
target_local.requires_grad=True`Alright, so would it be fine  to have:

`random_tensor = torch.randn(5, 3)

random_tensor.requires_grad=True` ?
Better to not use randn I think to be more elegant, try `tensor = torch.tensor([[3.0, 2], [-1, 2]], requires_grad=True)` for exampletry `data_local = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1.0]], requires_grad=True) ` instead",8,True,2019-07-14 13:58:17,2019-08-08 13:13:41,2019-08-08 13:13:41
https://github.com/OpenMined/PySyft/pull/2356,[],Minor Typo fixes in Tutorials,Minor Typo fixes in Tutorials,2,True,2019-07-13 16:23:22,2019-07-13 18:58:24,2019-07-13 18:58:07
https://github.com/OpenMined/PySyft/pull/2353,[],Fix operations on pointers to additive shared tensors #2352,Fix operations on pointers to additive shared tensors #2352Fix #2352Can you put the two tests here as separate tests? Having them separate will help with debugging down the line (right now if the first one fails the second one doesn't run)Done!,1,True,2019-07-12 09:19:03,2019-10-26 18:07:05,2019-07-12 18:30:48
https://github.com/OpenMined/PySyft/pull/2349,[],Implementation of RNNs in python with linear layers for MPC,"Implementation of RNNs in python with linear layers for MPCImplemented all RNN modules available in PyTorch in Python with linear layers for MPC.

Tests of these modules were fine in pure Pytorch. However, I still get errors when I hook torch and run tests under syft.Import rnn as `from syft.frameworks.torch.nn.rnn import ...`Add docstring for this method.raise ValueError instead.Add docstring.I would prefer this to be a (perhaps static) method in `RnnCellBase`.shouldn't we be using `num_chunks`?shouldn't we be using `num_chunks`?shouldn't we be using `num_chunks`?shouldn't we be using `num_chunks`?`# TO DO: implement a nn.Dropout class for PySyft` -> `#TODO: implement a nn.Dropout class for PySyft`

I've just created an issue for this https://github.com/OpenMined/PySyft/issues/2500, please link here as well.Won't this change with `self.batch_first`?same.This part is a bit confusing. Comments could be helpful.

Also, there's a lot of ""repeated"" code, this method could be more modularized.Just checking... shouldn't this be input.shape[1] instead of input.shape[0]?Add docstring.`max_degree = degrees[-1]`I don't think these two variables (selected_weights, selected_degrees) are needed. You can just use weights[1:max_idx] and degrees[1:max_idx] in the for loop.Is this test exactly like `test_simple_rnn` but for GRU? If so, it would be better to add the cell type as a parameter at @pytest.mark.parametrizesame comment.I think a test showing that a RNN trains properly (aka loss decreasing) is a good idea.Will doWill doWill doIndeed it is more coherentOkThis doesn't need to change, as discussed on SlackI agree. I will do itWill doOK, will dohere input.shape[0] is the batch_size and input.shape[1] is the embedding size (exactly as in the PyTorch original RNNCell module: https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell)

We need to initialize h with shape (batch_size, hidden_size)Will doAgreeOKNot exactly equal, as GRU and LSTM don't have the arg `nonlinearity` that RNN has. I prefer to keep them separated or we will have to iterate twice for GRU and LSTM (by using two values for `nonlinearity`) with no need to do so.I can do only one test for GRU and LSTM and add the pytest decorator, but it might be a little bit messy as I will have some `if`s to handle it, since we have two states (hidden and cell) with LSTM and only one with GRU.

What do you think?Agreehmmm, I think you could just use one argument (something like 'model-type') and you could set it to: gru, lstm, rnn-tanh or rnn-relu. Internally you could deal with the specificity of each model (aka setting the nonlinearity when needed) What do you think?

I think this is better than having 3 different tests that are not actually that different.I see. Ok, I will do like thatI just found out the RNNs are not working with AutogradTensors. It will take a bit of time to debug and implement a test for RNN training.@mari-linhares I implemented a test with federated learning instead. I will work on AutogradTensors in another PR@mari-linhares I have modularized some chunks of code in methods. What do you think about this new version of the code?great!shouldn't it be -1 instead of 2?It's temporally.
After merges from last week, these activation functions got too slow (probably due to several multiplications with degrees higher than 3), so I needed to reduce the max_degree to keep it still doable. The precision we lose by doing this is not that significant and I plan to get back to max_degree=9 after I investigate further the problem with this increase in time execution.",3,True,2019-07-10 20:58:40,2019-08-20 18:24:22,2019-08-20 18:24:22
https://github.com/OpenMined/PySyft/pull/2345,[],Modularize creation of a websocket worker for tests,"Modularize creation of a websocket worker for testsClean up tests.

Replace `start_proc` to `start_remote_worker` when possible to avoid redundant code.Could we use remote_proxy instead of remote_worker? Because the object previously called local_worker is actually local and not remote. Yet it represents a proxy of the remote worker.Please take a look at the changes in #2386 and #2388. They address another aspect of the websocket connection setup. I wasn't aware (forgot) about your changes to these files. Do we want a default value for port? Or should it always be a required argument?I would prefer another name for remote_worker (as remote_proxy). As this remote_worker instance is the local proxy of the remote websocket server.We can leave the comment, but the syntax is no longer correct. start_remote_worker returns a tuple of values.could you check whether the two lines:
del remote_proxy
and 
time.sleep(0.1)
are necessary?
The main issue with the websocket connections in the tests are connection requests after closing the connections.
These are mostly due to:
 - pointer objects that refer to remote objects (and are in charge of garbage collection) that are deleted at the end of the testport not specified. Is this wanted?
Is there a way to make sure that the process is terminated even when the test fails? I don't see an easy way to implement that right now. It would be easier if the pytest fixture returned a server instance and not the function. (Then you could put it in the teardown code after yield).

If I remember correctly this is one of the reasons for having different port numbers for each test.why did you remove this test?I think we can keep a default value port for the sake of simplicity.indeed, thank you!will do> Is there a way to make sure that the process is terminated even when the test fails?

I think this is out of the scope of this change. But I agree this would be great to have.

> If I remember correctly this is one of the reasons for having different port numbers for each test.

Yes, indeed. As I said before I'm okay with keeping a default port for tests for now, but in a different PR we can deal with automatic different port allocation and handling fails. What do you think?good catch! that was me failing to merge the two files, will add it back.I don't think we need them, getting rid of them.I agree that it is out of scope to catch port issues. 
However there were no port conflicts possible before this pull request, as we used a different port for each test. So I would propose to stick with separate port numbers.",2,True,2019-07-09 20:48:37,2019-08-19 14:59:18,2019-08-19 14:59:17
https://github.com/OpenMined/PySyft/pull/2343,[],Added size() method for remote tensors,"Added size() method for remote tensorsAdded a basic version of the size() method for remote tensors.

A lot of tests are still not passing though. Suggestions on what could be fixed so as to make all tests pass. If the size() method is successfully implemented, we would also ""unlock"" the usage of LSTMs in PySyft.

Implementation for issue #2201",7,False,2019-07-09 11:39:13,2020-01-23 12:01:20,2019-08-01 17:24:15
https://github.com/OpenMined/PySyft/pull/2342,[],Fixing __mul__ error for int multiplication issue 2341,"Fixing __mul__ error for int multiplication issue 2341When multiplying by integers, the expansion on other * self.base ** self.precision_fractional is not necessary. The values in self.child are already on fixed precision and the integer is also fixed precision, asl the previous implementation had some serious instabilities with float precision resulting in the errors found here https://github.com/OpenMined/PySyft/issues/2341.",2,False,2019-07-09 08:42:39,2019-07-09 11:07:48,2019-07-09 11:07:48
https://github.com/OpenMined/PySyft/pull/2339,[],Multiple workers with same id issues #2333,"Multiple workers with same id issues #2333The cause of this behavior is due to the way ""_objects"" dictionary is reset inside clear_objects() method using the statement ""self._objects = {}""

When we create a worker with the same id as a pre-existing worker then its ""__dict__"" object is shared with the pre-existing worker using ""self.__dict__.update(known_workers[self.id].__dict__)""

Due to this when we do ""self._objects = {}"" inside clear_object(), ""__objects"" now points to a new dictionary object and hence ""known_workers"" dictionary and ""__object"" dictionary are not in sync anymore.

To fix this we would need to call the clear() method rather than assigning {} inside clear_objects() method
self._objects.clear()

After the above change the sample code output is shown below-

```
import torch
import syft as sy
hook = sy.TorchHook(torch)

worker = sy.VirtualWorker(hook, id=""worker"")
worker.clear_objects()
ptr = torch.tensor([1,2,3]).send(worker)
print(worker._objects) # >>> Has 1 object!

worker = sy.VirtualWorker(hook, id=""worker"")
worker.clear_objects()
ptr = torch.tensor([1,2,3]).send(worker)
print(worker._objects) # >>> Empty!

```
Output-
{48603286308: tensor([1, 2, 3])}
{13171397894: tensor([1, 2, 3])}Could you add a test for this?Done. 
However, Travis fails due to 1 test case failing test/federated/test_federated_client.py::test_fit[gaussian_mixture-1]

the failed assertion is -
assert loss_after < loss_before
assert tensor(0.6931, grad_fn=<NllLossBackward>) < tensor(0.6931, grad_fn=<NllLossBackward>)

The above test case passes on my local development setup though. 
Should we trigger Travis build again? 
Or, I think the assert should be changed to ""assert loss_after <= loss_before"" we shouldn't assert if the loss remains the same.https://gitlab.com/pycqa/flake8-docstrings/merge_requests/15remove print statementremove print statementDone.",5,True,2019-07-05 18:54:17,2020-05-06 11:38:59,2019-07-09 19:34:12
https://github.com/OpenMined/PySyft/pull/2328,[],Fix typos,"Fix typosFixes #2322, Fixes #2319",1,True,2019-07-03 00:38:48,2019-07-03 15:50:55,2019-07-03 13:38:34
https://github.com/OpenMined/PySyft/pull/2321,[],"run static type checker in CI, fix annotations in syft/grid.py","run static type checker in CI, fix annotations in syft/grid.pyTowards #1456

While there are many static type hints in the codebase, it looks like the checker isn't currently ran anywhere, so there are many typing issues on the `dev` branch, which you can see via:

```
$ mypy --python-version 3.6 --ignore-missing-imports syft
syft/grid.py:16: error: Need type annotation for 'tag_counter'
syft/grid.py:22: error: Need type annotation for 'worker_tag_ctr'
syft/grid.py:56: error: Incompatible return value type (got ""Dict[Any, Any]"", expected ""Tuple[Dict[Any, Any], Counter[Any]]"")
syft/frameworks/torch/torch_attributes.py:36: error: Module has no attribute ""functional""
syft/frameworks/torch/torch_attributes.py:37: error: Module has no attribute ""nn""
syft/frameworks/torch/torch_attributes.py:111: error: Module has no attribute ""Tensor""
syft/frameworks/torch/torch_attributes.py:123: error: Module has no attribute ""FloatTensor""
syft/frameworks/torch/torch_attributes.py:124: error: Module has no attribute ""DoubleTensor""
[...snipped due to very long output...]
```

One way to incrementally address this would be to add a mypy call in CI, and gradually fix the typing issues, at which point you can cause type checking errors to fail the Travis build (like is done for other linters). This PR does that and then fixes the first few typing issues seen above in `syft/grid.py`. Let me know if another approach is preferred here. Oh, I've just noticed you've used typing in the code itself, for pysyft codebase we use typing just for docstrings. Could you remove this before we merge it?So there will be times where a static type checker will need additional information about the types of variables beyond the annotations in function/method argument and return types (e.g. one of the issues in `syft/grid.py` was `""error: Need type annotation for 'tag_counter'""`). If we want to be able to run mypy without errors on the codebase, there are two paths: 

* Add an annotation in variable definitions where needed such that mypy can infer the types of variables it doesn't yet understand (the approach I took in this diff), or

* Add the same type information in a comment e.g. `tag_counter = Counter()  # type: CounterType[int]`

Either is a fine solution, though the latter might be a bit more approachable for a contributor who is familiar with Python but not yet type annotations. I didn't take this path as there is [a linting check](https://github.com/OpenMined/PySyft/blob/dev/.pre-commit-config.yaml#L21) disallowing type information in comments. We _could_ disable that if it's preferable - what do you think?Hey @redshiftzero
What changes need to be made on a repository wide scale to get mypy happy with the typing in the code itself (ie do we need to add these comments/annotations all over the place or will it just be isolated.)

I personally am biased towards not having them as comments as I think it looks a bit cleaner and is more in line with how we do it for the method definitions. That being said I would love to hear @iamtrask @LaRiffle @mari-linhares and @midokura-silvia 's opinions on this as wellI agree with @robert-wagner. 

If these are isolated changes I'm pro supporting mypy by the cost of having some files with comments/static notations.

To be honest I don't have a strong preference for comments or static typing.",1,True,2019-06-29 19:28:28,2019-11-20 15:29:44,2019-11-20 15:29:44
https://github.com/OpenMined/PySyft/pull/2320,[],Fixed typo in instructions about the part number,Fixed typo in instructions about the part number,2,True,2019-06-29 14:17:26,2019-06-29 16:26:52,2019-06-29 16:26:47
https://github.com/OpenMined/PySyft/pull/2309,[],CRT tensor,"CRT tensorCan someone tell me what I did wrong to be unable to use .on() as with other types?what is ai in this case?what does this do?It's the residue modulo ni, I'll update the docI wanted to allow the usage of torch.operation(a, b) with CRT tensors.on() allows you to create a empty node before and then to insert it in the syft chain of a tensor, between the head wrapper and the first node. Here you already provide the syft chain as an argument of init so it works in a different way. This is standard for overloading torch functions for special syft tensorsBut (if that's what you meant) `CRTTensor(**kwargs).on(i, wrap=False)` was not working, that's why I chose this. I'll try to see what's wrong with thatWhy are these commented out?It's commented out because the __ne__ method is not hooked for any other syft tensor, and also because I think some upstream work (in native.py or something like that) needs to be done to enable the use of `!=`.
Maybe hooking `__ne__` for all syft tensors can be another PR? The result should be in the same precision fractional as the original tensornot sure where this is being called, so may already be the case, but most of this can be precomputed and reused. reconstruction in that case then boils down to a weighted sum.I see what you mean. Should I have the coefficients of this reconstruction as attributes computed when the tensor is instantiated and used when needed?do you have an example of where/how the CRT tensors are created? the ideal point to compute and cache these is where the moduli are provided (eg along these lines: https://github.com/tf-encrypted/tf-encrypted/blob/master/tf_encrypted/tensor/int100.py#L757-L760)btw, i've looked quite a lot at CRT tensors and happy to discuss!We still don't use CRT tensors so I don't have any real life example to show you... In test_chinese_remainder.py (in this PR), you can see how I create these tensors for the moment (I create the tensors of residues and build a CRTTensor instance with them).
Actually, it's still a bit blurry to me how and when this kind of representation is used, how it is created in the first place, etc. So I might come to ask you some questions if you don't mind :)`assert isinstance(r.child, FixedPrecisionTensor)` comes a biut too late, you already assume at line 49 for example that you're using FixedPrecisionTensorsSpecify that you execute here the steps you detailed in the docstring on the previous function solve_system which actually assumes the computation of the systems is already made --> maybe you should rename solve_system with smthg like `reconstruct` because it's already solved in a way, as you wantmaybe add `div` / `__truediv__` / etc. as methods which raise a notimplemented error so that we don't get silent unexpected behaviorsFrom what I see, having `precision_fractional != 0` for the residues doesn't make sense here, right?
You use FixPrecision Tensors because of the .field attribute exclusively (actually you would like to have a FieldTensor or smthg like this). If this is true, then you might want to check the precision_fractional is indeed set to zero.
However, `self.precision_fractional` still make sense I think, but is now completely leveraged: one thing I'd like to do maybe is take `th.tensor([1.54])`, put it in precision_fractional=3 in CRT Tensor which would compute the representation of 1540.I also think keeping the precision makes sense if the user wants to use float values.
Actually, CRT tensors are just another representation of FixedPrecision tensors.Good idea, the name `solve_system` was indeed an artefact from my previous implementationYes, so there si a distinction to make between the precision of the CRTTensor which is good to use float values and the precision of the residues which should always be 0 right?I'm not sure I see what you mean.
In what I did for the moment, the precision of the CRT is just the same as the precision of all the residues tensors.
I think this precision (which the precision of the residues) can be different from 0. It should be equal to the precision of the FPT we want to represent. I was thinking it was not necessary to use it in any operation but just when we want reconstruct the FPT but I just thought about multiplication where we might want to use it...Oh no, multiplication has to be taken care of in FPT so it's finetypo: this should be module.div = divCan we add a flag to skip this test, if the list of keys gets long, this test takes a long time and if we could verify it offline, that would a ton of time (also after the initial spin up) (or if we could cache the results of the function)Can you add a human readable message for this assertion so that people know what needs to be passsedsame as above for the assertionsDittodittoThis test will occur only when the user gives residues, not when the ""usual"" way of converting to CRT tensors is used (by calling .fix_prec(storage=""crt""). So this should not be that often...
But I can still only warn the user that the check is not done if the list is too long. What would you consider a long list in this case?fyi there are alternative reconstruction algorithms to avoid this issue (given that the final number is small)what is the size of the underlying types holding the residues? int32? int64?Could you point me to the method, please?FixedPrecisionTensors are used to hold the residue and they always are int64, I think.
I indeed think that it would make more sense to use int32 tensors behind CRTTensors, which means choosing the type of tensor when creating a FPTDefinitelyI think we need a bit of rebranding here: someone who doesn't know would be confused between the theorem itself and the tensor object: we don't really get the sens that its to store larger precision number
Regarding the file I'm thinking of (crt_precision, modular_precision, residue_precision, chinese_precision, etc)
For the class (CRTPrecisionTensor, CRTLargeTensor, etc)I think it's not necessary given this explanation :)Can you factorize the 2 tests in the type like:
`assert (isinstance(r.child, FPT) for r in residues.values()).all(), ""MSG ERROR""` ?rename maybe b -> base_residue for more clairityinplace operation can't be equivalent to out of place ops, maybe remove this line or make sure you don't create inconsistenciesI do agree but I didn't find any good names... Thanks for the name propositions :)Is it more correct to overload `__imul__` as I did it now?Nit, give the name to this tensor",4,True,2019-06-25 07:46:55,2019-07-31 22:52:26,2019-07-31 22:52:26
https://github.com/OpenMined/PySyft/pull/2300,[],Addition of missing SecureNN protocols,"Addition of missing SecureNN protocolsSolves #2261Have you checked that type(int / int) == int ?Where is this one used?It's used in the maxpool_deriv SecureNN protocolIt's not the case with native Python types but `/`does not change the type of torch tensorsI assume it should also workOk goodWhy 3 here ? The theorem 1 of https://eprint.iacr.org/2017/396.pdf is for 2 parties, have you tried with more? Can you add a docstring to explain the behaviour of this function for people who don't know the original torch.roll?the local client (so P2 in the paper implem) learns here shifts[worker].get().item() which is `i - g mod n` right? Can P2 learn `g` then, and then `r` and then deduce `ind_n`?Can I leave it as it is?I didn't follow you. `shifts[worker].get().item()` is `-(r % n)` (see 4) in the max pool_deriv function).
Thinking again about it, I think it is not secure to have P2 learning the shift because it could retrieve the output of the function...It's a bit hard to explain and I'm not 100% sure it's true but I think that for more shares, one third of them should be transformed with `Q - (Q - val) / div`. It's because it's needed as many times as the sum of shares crosses `Q / 2` when wrapped around the field. As values are concentrated around 0 for a big field and shares are generated randomly, I think that the number of times `Q / 2` is crossed is indeed one third of the number of shares.
Tell me if this is not clear, or even not true. Oh, I remember now!
If we consider the max pool_deriv protocol, `share` and `shifts[worker]` are PointerTensors here, so there should be a way not to get them. But as `shifts[worker]` is a tensor and the roll function takes an int here, I need to use the .item() method which raises an error when called instead of get() on a PointerTensor.
It works if I remove this error raising and if I use shifts[worker].item() but as I don't know why the error was there in the first place, I don't want to break anythingThe sum should be able to exceed 1 right? It should be only 0 or 1The reason why I had sent the mask to another worker is the following: in our construction, the local worker knows the randomness shared by P0 and P1 (because we didn't managed to hide it from him, but maybe we should). In that case, we can't allow the local worker to get and see permuted_mask because otherwise it can inverse the permutation and remove s to get c. So opening the permuted_mask should be made by a worker which don't have access to the randomness, that's why I chose the crypto_provider, but I agree this is not very elegant.I think you're right!It's a good reason, I'll change this back",4,True,2019-06-18 20:14:13,2019-07-12 11:47:57,2019-07-12 11:47:57
https://github.com/OpenMined/PySyft/pull/2289,[],POC: asynchronous federated training on MNIST using websockets and jit ,"POC: asynchronous federated training on MNIST using websockets and jit Original PR author is @midokura-silvia #2162 Marianne, there was a reason to have the call to basicConfig at the beginning of the file. It is ugly there, but it does not work if you put it later.
I didn't identify the source yet, but when doing import syft basicConfig seems to be set somewhere already. And any subsequent calls to basicConfig are ignored by the logging library.The spaces are voluntary to align the output of target hist and prediction hist for easier comparison.My bad, I imagined there was a good reason but I forgot to undo the change. Feel free to undo this.Got it, can you add a comment for this?Also, @midokura-silvia can you add a comment to it, something like:

> There is a reason to have the call to basicConfig at the beginning of the file. It does not work if you put it later.Why is only this line no covered?I'm wondering if we shouldn't just define start_proc somewhere in syft as a helper function. It seems like it is defined all over the place>Note The above module-level convenience functions, which delegate to the root logger, call basicConfig() to ensure that at least one handler is available. Because of this, they should not be used in threads, in versions of Python earlier than 2.7.1 and 3.2, unless at least one handler has been added to the root logger before the threads are started. In earlier versions of Python, due to a thread safety shortcoming in basicConfig(), this can (under rare circumstances) lead to handlers being added multiple times to the root logger, which can in turn lead to multiple messages for the same event.

This may be for threading related reasons. I don't thing we should worry much about itThis ovewrites if secure is truewe should not have print statements in testsShould the order be changed here (I prefer the new ordering of pred and target but want to be sure this does not break anything)Same as aboveremove print statements in testsdittoditto for print statementssameWe had similar issues in jupyter since jupyter uses asyncio under the hood. This might be the same issueIs there a reason these are commented outsamesamesamesameSkipif?samesameWhy the port change?I although thought of it. However we now put the setup of the remote datasets inside, so it's not so generic anymore. We have a similar function defined in conftest.py for the unittests.I removed the outdated comment.Is this a problem? Which pattern shall we prefer?
```
a = 1
if condition:
    a = 2
```
or 
```
if not condition:
    a = 1
else:
    a = 2
```8777 was conflicting with the port we use in the tutorials.Sorry let me clarify this. We are setting args['uri'] (in line 68) and never using it. I am fine with this pattern If it is no longer generic this is fineyes, that's a bug. I never tried the secure setup and there are no tests for it. It's the whole function. But probably the examples folder is excluded from the coverage anyhow (need to check).We should have tests for itShould this be here?@midokura-silvia yes, the code using torch.jit does not work with torch version 1.1. While we are waiting for the fix to be released, we need to keep the fixes for torch 1.0.1.Can we update the comment to be in relation to PyTorch 1.1.1 rather than PyTorch 1.0.2 then?Let's add a note, that this is currently only available as insecure connection.+1 to changing it to 1.1.1.",1,True,2019-06-15 19:04:58,2019-06-19 15:19:28,2019-06-19 15:19:24
https://github.com/OpenMined/PySyft/pull/2288,[],Added Advanced Tutorial Example - SMS spam prediction with handcrafted GRU,"Added Advanced Tutorial Example - SMS spam prediction with handcrafted GRU<p>&gt; In order to address this issue, there are two solutions: Federated Learning with Multi-Party Computation and Encrypted Computation.</p><p><br></p><p>Differential Privacy would be used to make sure the model doesn't give access to some private information.</p><p><br></p><p>Multi-Party Computation (one kind of Encrypted Computation) in return allows you do send the model privately so that the remote workers which have the data can't see the weights you're using</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2288/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='27'/>",1,True,2019-06-15 00:12:47,2019-06-15 14:53:38,2019-06-15 14:53:38
https://github.com/OpenMined/PySyft/pull/2263,[],[WIP] Fixed Precision in Zq,"[WIP] Fixed Precision in Zqfixed precision tensor only represented by positive int
fixes #2261 I think it makes a lot of sense to raise an warning here. I would argue that we should raise an warning if |upscaled| > field/2 since every data point that meets that condition will be embedded incorrectly. In the past we have considered raising an error if base**precision_fractional > field but this is a much more reasonable place to checkTheoretically we want to do this wrapping automatically. I am curious if it makes sense to add this automatically to every method (this is good for now)Do you think I should put this somewhere in the method hooking process?I'm not sure it would be easy with the current functionality we have :/I will open an issue for this and we can address it laterYes looks like a good idea, basically it would be an extension of overload_torch.py I believe to execute a class specific transformation at the `def hook_args(`You should keep `r` (ref: Algo 4 line 6)Are you sure? I see `r-1` written...In have `r` in https://www.microsoft.com/en-us/research/uploads/prod/2018/09/securenneprint.pdf
But https://eprint.iacr.org/2018/442.pdf says `r-1` haha that's right

So... let's see. I believe you're right `r-1` should be used. The eprint has been updated in 08-Mar-2019 and `r-1` has been put instead of `r`. And it seems to make sense from the math point of view",2,False,2019-06-10 21:53:58,2019-06-26 07:06:29,2019-06-26 07:06:29
https://github.com/OpenMined/PySyft/pull/2262,[],Add special case for torch function which dont have tensors as input,"Add special case for torch function which dont have tensors as inputFix #2260 
Basically the error was for torchvision >0.2.2
We had a special case to handle",2,True,2019-06-10 17:43:18,2019-06-10 20:56:17,2019-06-10 18:02:27
https://github.com/OpenMined/PySyft/pull/2254,[],Fix TFEWorker Windows bug,Fix TFEWorker Windows bugFixes #2250.,3,True,2019-06-07 17:37:45,2019-06-10 19:14:04,2019-06-10 19:14:04
https://github.com/OpenMined/PySyft/pull/2247,[],introduce and use TFECluster,"introduce and use TFEClusterThis PR solves a concurrency issue in the notebooks where sharing of a model happens before the servers are ready, resulting in an error. The solution here is to introduce the notion of a cluster that binds a set of workers together and is started before a model can be served.

Closes #2242.",6,True,2019-06-07 13:30:41,2019-07-13 00:09:05,2019-07-13 00:09:05
https://github.com/OpenMined/PySyft/pull/2244,[],Plans should be built by the blueprint owner,"Plans should be built by the blueprint ownerFixes #2241, Fixes #2264, Fixes #2003 
Rather than building plans at call time, could we build them at send time?That's how is currently done, but it will not work for remote workers (aka WebSocket workers)What in particular makes them break at send time for websocket workers?The main thing is that virtual workers are able to build a plan remotely because they can run the model blueprint (original function / method) this is not true for remote workers.What conditions result in the changing of the values of these checksWhat is the api for building a plan? Does anything change about how it is built when it is sent to a location?

One problem I am currently running into is if the parameters for a plan are multipointers it sends the entire multipointer to a workerI just had to fix this for multipointer tensor lolCould we add support for variably shaped args? ie a tensor with one set dimension and the other unset (for things like batching) why are these commented outditto> What is the api for building a plan? 

The way I see it is that building should be dealt with internally as a consequence of a call.

This means that the user should not call `plan.build`, but the first time they execute a plan (aka `plan(<some data>`) the plan is built.

> Does anything change about how it is built when it is sent to a location?

The plan should be built locally by the worker that knowns the blueprint (the actual function/method that the plan should execute).

So the build operation shouldn't be related to sending the plan to a location.

> One problem I am currently running into is if the parameters for a plan are multipointers it sends the entire multipointer to a worker

Yes, I think the way is currently implemented that's the expected behaviour, maybe @LaRiffle knows more about it.


I saw it and fixed here as well, thanks btw! :1st_place_medal: Yes, I've added support for -1, which usually means ""unknown"" or ""any shape"" tensors. Since for building the function we need to define the shape of the mock data, I map -1 to 1 in the mock data.nice catch, these should actually be removed since they aren't needed anymore. Before these fixes this line was needed. thanks!This hack is needed to make a method (aka a module) be executed locally in a transparent way. Mainly this will not be true if the plan was sent to some location.So for clarification. Self.locations is set when send is called. Self.owner is set when?It's a detail but maybe we don't want to do the  -1 => 1 mapping in general. Maybe you could just add a TODO to mention that a more complex strategy could be used?The proxy of the websocket worker is not doing anything at the moment. Just transmitting the message to the remote counterpart. And the remote part does not have access to the blueprint.
So if the proxy got intelligent and built the plan before sending it, it could work.I never understood why self is added as first argument. It will not be used as self argument anyhow. When is it really used?self.owner never changes and is initialized with the worker creating the objects.
With virtual workers, `self.owner == sy.hook.local_worker` is an indicator that the worker is the _real_ local worker, with socketworkers it doesn't work anymoreYes I think that when you call a Plan it only checks if the args provided are tensors are pointers, multipointers are seen as tensors and therefore the args are sent to the remote worker selected. `find_location` in plan.py is partially responsible for this.
However maybe this is slightly beyond the scope of this PR.will do@robert-wagner @midokura-silvia Actually, I've just noticed what I've said previously was not true.

> That's how is currently done, but it will not work for remote workers (aka WebSocket workers)

[An unbuilt plan can be sent successfully to a remote worker](https://gist.github.com/mari-linhares/d1feeb45903dc15c76dde1c5874b69c8) (if you add this test to `test_plan.py` in dev branch it works).

[But the current version can't be executed locally successfully](https://gist.github.com/mari-linhares/a30ea03066a9dc5c45c613624595447b). This PR solves this.

@LaRiffle do you think is good that we force the model to be built before being sent or since this is not particularly a problem we shouldn't?This is needed for building only, not used for executing the plan. It can be changed here and explicitly added in the building method if you think it would be better (I do!)I think so yes, otherwise we will use the similar tricks as before with lazy .send() which don't really send the plan until a call is made. This, which was previously used, is complicated to understand, use and debug. Does it make sense?Is the self.owner part of this check necessary then?Sounds good. That was more a question in line with my current work that I wanted to see if this addressedIt‚Äôs a useful hack but it only works for virtual workers 
We need a better way to know if the worker is the local worker (aka client)Would it work if it was sent? does it keep the same reference to the local worker after it is sent over the wire?Yes, I agree. > Would it work if it was sent? does it keep the same reference to the local worker after it is sent over the wire?

Considering that it was already built. Yes, it will work.

No, it doesn't keep the same reference.

But as @LaRiffle said we consider that the blueprint owner is a virtual worker which is usually true since when a model is instantiated it belongs to hook.local_worker which is a virtual worker. So I don't think we need to worry about this condition right now.",1,True,2019-06-06 20:06:12,2019-06-20 20:09:33,2019-06-14 18:14:35
https://github.com/OpenMined/PySyft/pull/2239,[],Raising error if PointerTensor got called with .item,"Raising error if PointerTensor got called with .itemAn error will be raised if any instance of ""PointerTensor"" got accessed its method ""item"" with an error message telling to consider accessing ""get"" method instead so you can get the item you want safely.",2,True,2019-06-05 17:54:52,2019-06-06 21:59:40,2019-06-06 21:59:40
https://github.com/OpenMined/PySyft/pull/2238,[],TrainConfig basic implementation,"TrainConfig basic implementationIs there a reason for storing this information? In the end this is information contained in self.model_ptr. 
We already have some duplication as model_id is for the case that the model has already been sent and model_ptr is needed, when TrainConfig sends the model.should we document somewhere that serializing a TrainConfig object does not serialize the model and loss_fn code/pointers? update docstring, function does not return anything any moreremove as function in FederatedClient does not exist any moreThis change introduces a mismatch between the signature of the fit method in WebsocketClientWorker and in FederatedClient. Will probably cause errors.why do you want the targets to be float?Can we leave this test as it was, please?
The goal of this test was to have a unit test, that does not need to be skipped (does not depend on jit). 
Also, no workers are used and it was a UNIT test for the FederatedClient only.
I added these tests to get the coverage above 95%.Do you really want to never GC collect an object wrapper when calling create_pointer?
If not you can add the parameter `garbage_collect_data` to the function `create_pointer` as for tensorsWe should avoid avoid if/else condition in the `_simplify` which is really used a lot. I would suggest keep the type as an entry of the dict: `torch.jit.TopLevelTracedModule` or other: it is not a pb to have several keys with the same value in the simplify dict, but adding a if statement slower all the ser/deser processI'm not sure to understand why you need only to keep the tensor args, would you like to detail a bit more the dosctring? That is weird, this code was already removed. I will take it out again.This is due to a weird behavior of plans that add self as first argument in the method case. Maybe we should check whether this code (plan.py, line 341)
```
        # Support for method hooked in plans
        if self.self is not None:
            args = [self.self] + list(args)
        return self.execute_plan(args, result_ids)
```
 is really necessary. I've just pulled the most recent version of this branch and is still here. I'll try to remove it now.doneWhich one? dataset_key parameter? I've changed there as well.I'll add more details to the docstring, I think the step Silvia pointed (plan.py, line 341) is needed to build the plan, is this right @LaRiffle ?I've removed this function and changed the behaviour to make more clear why do we need itWe need to store this in order to send it.

We can undo these changes and just pass the model along with the loss for sending train_config the same way it was being done before.

I like this wrapper idea better, makes more clear that train_config depends on the model and loss function and it's not just a wrapper for training hyper-parameters.Yes, nice catch.when it was an int I got an error.What error do you get? requires_grad=True requires a float tensor, but we don't need the gradient with respect to the target.  Ok thanks I'll look at the changes. That's right we needed to include the .self as we call methods , but maybe there is a work around to make it more transparent - will check your latest changesThe signature is not the same on both sides. In FederatedClient the argument dataset_key is allowed to be provided as positional argument, and in WebsocketClientWorker it is not. 
Thus code that provides dataset_key as positional argument will work for VirtualWorker but not for WebsocketClientWorker.I see I'll fix it now. Although the notebook demo seems to be working correctly :thinking: .All right, let's leave it like this for the moment. 
Maybe later on, we should add some logic to clearly separate between TrainConfig with model and loss_fn and TrainConfig with model_id and loss_fn_id (=> this is the case either when model and loss_fn have already been sent, or if it is a TrainConfig instance after serialization).",4,True,2019-06-05 15:21:11,2019-06-07 09:08:08,2019-06-07 09:08:08
https://github.com/OpenMined/PySyft/pull/2237,[],Raising error if PointerTensor got called with .item,Raising error if PointerTensor got called with .item- Raising error if PointerTensor got called with .item with an error message to call .get instead,3,False,2019-06-05 14:20:59,2019-06-05 17:55:50,2019-06-05 17:55:50
https://github.com/OpenMined/PySyft/pull/2230,[],"Create a serde folder with serde.py, native_serde.py and torch_serde.py","Create a serde folder with serde.py, native_serde.py and torch_serde.pyHello,
I split  _simplify_<object> and _detail_<object>  methods into 3 files:

- one for native python object
- one for torch object
- one for high level serializer

Please let me know what do you think. Thanks
It is a follow up of #2157 and #2110 By importing the serde.py file in the `__init__.py` of the folder we can avoid this long name.
Best practice is to have one identifier associated with the method. In the current case it would be:
serde.deserialize(...)
The initial sy.serde.deserialize(..) was already not optimal in my view.I'm not a big fan of `serde.serde` just `serde` would be cleaner, you can probably get rid of this by doing:

```
from syft.serde.serde import serialize
from syft.serde.serde import deserialize
....
```

at serde/__init__.py
This looks good :)same about serde.serdeI'm not sure if @robert-wagner @LaRiffle agree, but I would love to get rid of this dict, or maybe store simplifiers and detailers in the same dict.Do we need to keep this for efficiency reasons?_simplify_<syft_object> or _detail_<syft_object> should be replaced by sy.<syft_object>.simplify or sy.<syft_object>.detail.

So we would use sy.Plan.simplify instead of _simplify_plan, sy.PointerTensor.simplify instead of _simplify_pointer_tensorthis looks good.given that we really need it, my vote is: make a list of pairs containing (simplifier, detailer) pairs, a dict can later be built automatically using this list during initI made that change. Thanks for the tipssame updated it.I changed all serde.serde to just serde.
I am having trouble for the next step you are suggesting. 
I try the following without success
I added to __init__ of syft
```
# Import serialization tools
from syft.serde import serde
from syft.serde import torch_serde
from syft.serde import native_serde
```
Then I added the following into serde/init.py
```
from syft.serde.serde import serialize
```
Finally in plan.py added the following, it seems that it creates some circular import which create an error with pytest
```
from syft.serde import serde
```
I am working on it, if you have a work around, please let me know. Thanks :)One tiny question :
For me It is not possible to => create a method simplify within Plan class for example where the method simplify take as an object a Plan object type.

```
class Plan
      @staticmethod
      def simplify(plan: Plan) -> tuple:
           ....
```
Should i just use a basic object but we will lost the strong Type of Plan
```
class Plan
      @staticmethod
      def simplify(plan: object) -> tuple:
           ....
```
After that it might still have some circular import issue because of recursion call within methods simplify and detailI hate this dict but it does have some advantages for performance. That being said, if we could get rid of it I would be very happychange to `from syft import serde`can we change this back. I don't want to know that three files exist for these unless I have to. In syft.serde.__init__.py if you add `from torch_serde import *` it should fix thisdone thanksI would suggest doing something like in hook_args: the dict is built dynamically during execution: so each time you add an entry you have a tiny overhead and then you have the dict look-up speed. This would use a list of pairs as suggested by @mari-linhares to build on thisMaybe this is a detail, but is there a way to make it transparent? I mean: I'd like to still call sy.serde.detail_<mytype> no matter if detail_<mytype>  is in serde/torch_serde or serde/syft_serde etc
It would require to change the way the imports are done. If this is not too complicated I think it would make it easier to use in non-standard cases like this one.What if it is not static?done it by adding to serde/init.py
```from syft.serde.serde import *
from syft.serde.torch_serde import *
```It looks like it is possible to do it with this notation:

```
class Plan
      @staticmethod
      def simplify(plan: ""Plan"") -> tuple:
           ....
```
I ll try this wayI can do it if I change the detail method from protected to public. (from _detail_plan to detail_plan)
Is that ok?Are you forced to do this? I believe it works also with the ""_"" but maybe I'm wrongGood!

@mari-linhares 
> _simplify_<syft_object> or _detail_<syft_object> should be replaced by sy.<syft_object>.simplify or sy.<syft_object>.detail.
> 
> So we would use sy.Plan.simplify instead of _simplify_plan, sy.PointerTensor.simplify instead of _simplify_pointer_tensor

I believe I can achieve your requirement.
For that i need to put all functions from protected to public (mainly for _detail and _simplify to detail and simplify).
Is it an issue?I don't think so :)sgtmYou are right. At least after moving _detail_plan in Plan class, it does workYes in practice python doesn't really care about ""protected"", it's more like a convention for users and developersTo conclude on that, I believe the following happened
serde/init.py :
```
from syft.serde.serde import *
from syft.serde.torch_serde import *
```
In this case it seems that it does not import methods with ""_"" , 
so I couldn't use sy.serde._simplify() in plan.py for example

So I added in the serde.init.py
```
from syft.serde.serde import _simplify
```
Then it does workAh ok, good to know! ThanksShould I move more  _detail and _simplify methods? 
For example for the following objects: Exceptions, workers, torch.device, torch.jit ?
Hi @mari-linhares and @LaRiffle, 

I am trying to implement the hook_args @LaRiffle suggested. A bit lost to where to start.
I checked the file frameworks/torch/hook/hook_args.py.
Is it similar?
Still confused, would love to get some guidance on it? Thanks
Is the args would be same at each execution?So the mechanism would be the following:
Instead of 
```
simplifiers = {
    torch.Tensor: [0, _simplify_torch_tensor],
    torch.nn.Parameter: [1, _simplify_torch_parameter],
    tuple: [2, _simplify_collection],
    list: [3, _simplify_collection],
```
Have
```
simplifiers = {}
map_simplifiers = {
    torch.Tensor: (_simplify_torch_tensor, _detail_torch_tensor),
    torch.nn.Parameter: (_simplify_torch_parameter, _detail_torch_parameter),
    etc
detailers = []
```
Then in def _simplify, check if simplifiers[type] succeeds, in case of key Error,  create
simplifiers[type]= [index, func] where func = map_simplifiers[key][0] and append map_simplifiers[key][1] in a detailer list ; also increment index

This is the first step, does it make sense?I think we can do this change in a new PR :relaxed: My opinion: exceptions and workers should live in their own files (like you did for plans); torch.device, torch.jit should live in torch_serde.pyI do agree :)
Ok I ll open another one for that right after this current PR is closed I guess.
Thanks @LaRiffle , I do get it and working on it.there are different __init__.py files.
In the  `__init__.py`  in the _serde folder_ you can add
`from syft.serde import serde`
 
And in plan.py you add
`from syft import serde`

Then you can use serde.serialize throughout the file plan.pyBy doing so,
It creates many import errors. (circular imports?)
Basically I can't import TrainConfig, Plan, AdditiveSharingTensor and MultiPointerTensor in serde.py and torch_serde.py anymore@midokura-silvia , I have been working on it, unfortunately I believe we have too many circular imports. I don't see any clean solution for now. Let me know. ThanksAll right, I can't look into it myself at the moment. If the others are okay with it, let's go with it as it is for the moment.",1,True,2019-06-04 15:33:34,2019-06-12 15:55:38,2019-06-12 15:55:38
https://github.com/OpenMined/PySyft/pull/2226,[],Fixed predict method in RNN jupyter notebook example,Fixed predict method in RNN jupyter notebook exampleFix for issue #2218 by moving the position of the torch.no_grad() call,1,True,2019-06-03 10:09:03,2019-06-03 13:17:50,2019-06-03 13:17:50
https://github.com/OpenMined/PySyft/pull/2220,[],Update tutorials,Update tutorialsA import error was reported with Pointer in tuto 1,1,True,2019-06-01 21:46:56,2019-06-03 19:07:43,2019-06-01 22:02:09
https://github.com/OpenMined/PySyft/pull/2197,[],corrected import error for PointerTensor,corrected import error for PointerTensormodified the import statement for PointerTensor to syft.frameworks.torch.pointers from syft.frameworks.torch.tensors.interpreters,2,True,2019-05-30 18:10:25,2019-05-30 18:33:59,2019-05-30 18:33:58
https://github.com/OpenMined/PySyft/pull/2192,[],fixed notebook ordering,fixed notebook ordering,1,True,2019-05-30 07:15:52,2019-05-30 07:16:26,2019-05-30 07:16:21
https://github.com/OpenMined/PySyft/pull/2188,[],Modification to have operations between FPT>AST and FPT>torch,"Modification to have operations between FPT>AST and FPT>torchThis seems to be a bit of a hack to me
But it can at least be the start of a discussion about how to enable scalar-FixedPrecisionTensor multiplicationsActually there was an error in .share(): we should add the crypto_provider which can be found in self.crypto_providerSame hereyou can change the signature to be just add(self, _self,  other)samesameHave you added tests for those ""redirections""?I didSome thing here, you can't swap like this because it's a sub op ;)Just wondering, why  `.share(alice, bob, crypto_provider=james)` was removed?Because if some settings we don't want to hide the config used, to the learning rate should be public, ie just a normal fixed precision tensor",10,True,2019-05-29 22:09:47,2019-06-11 06:39:38,2019-06-11 06:23:25
https://github.com/OpenMined/PySyft/pull/2178,[],Add tuto on secure aggegration,"Add tuto on secure aggegrationUpdate
https://github.com/OpenMined/PySyft/blob/torch_031/examples/tutorials/Part%208%20-%20Federated%20Learning%20-%20Encrypted%20Gradient%20Aggregation.ipynb<p>This should be a subtitle with the same font size as ""training function""</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2178/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='3'/><p>This should be a subtitle with the same font size as ""training function""</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2178/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='5'/><p>is mometum supported? If not: better to remove this option from the notebook otherwise people will try to use it and get errors.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2178/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='6'/><p>This should be a subtitle with the same font size as ""training function""</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2178/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='7'/><p>This should be a subtitle with the same font size as ""training function""</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2178/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='13'/><p>This should be a subtitle with the same font size as ""training function""</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2178/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='15'/><p>This should be a subtitle with the same font size as ""training function""</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2178/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='18'/><p>Nice catch!</p>",1,True,2019-05-29 11:31:34,2019-06-03 19:08:07,2019-05-29 12:54:09
https://github.com/OpenMined/PySyft/pull/2170,[],[wip] TFE Keras integration,"[wip] TFE Keras integration- [x] new release of TFE
- [ ] tests for `tfe.py` to increase coverage (see [CONTRIBUTING](https://github.com/OpenMined/PySyft/blob/master/CONTRIBUTING.md))
- [x] last run through notebooks
- [x] rename notebook directory?Suggest removing the `.git``TensorflowServerWorker`?is `registered_cls` used anywhere?naming seems backwards since filter normally keeps elements where the predicate (`is_layer`) is truedocstring?Couldn't the default argument here lead to confusion? There's also a type mismatch since `get_protocol` returns an object while the others return classes.`tf` and `tfe` are probably in the same group of imports here (system, third-party, project):Dnot sure what the semantics are here. does only the first layer have the attribute? is it guaranteed to always have it?we should use the workers here if specified, otherwise the protocol will try to load by default names which might not match the names of the supplied workersmaybe let them have a role as well, say `crypto_producer`, that will be passed to the protocolnot true in general since queue construction and local sharing has to match (but this is done identically between Pond and SecureNN)I have no idea why the firewall gave me trouble here the other day, but great that it works now üíÉ Still really don't like the terms client and server worker. Any ideas on this front?client vs worker? we chose these to mirror the existing terminology, so could also be beyond the scope of this PRnope! leftover from a previous version, will fixwhen the workers are specified, the config is set just above, and so tfe will construct the protocol accordingly.  when the workers are not specified, we assume the user has created their own config outside of the scope of the `share` method. this seemed to me to be most flexible behavior for the user while conforming more closely to the `.share()` signature.  do you think there's a better way?`batch_input_shape` or `input_shape` must be provided as a kwarg to the first layer in a Sequential model.  usually it's `input_shape` and the batch dim is None, but here we stick to `batch_input_shape` due to https://github.com/tf-encrypted/tf-encrypted/issues/499#issuecomment-495412270this would likely change the semantics of `model.share()`, so I'm not sure it's worth it unless we can figure out a way around thatwe should rethink this with the rest of the config questions (cc @yanndupis)What's the benefit of saving this config file now? Do we want call it `shutdown` or `workers_shutdown`? Because the workers launched is abstracted in share, it's not very clear what's the intent of shutdown. change this to multiple imports instead of just onewhy do we need keras but not torch in this file?change it to single line importssingle line importsdocstringsany particular reason why model.get_weights is not being used currently?Create global variables or store player names on a list, this part of the code ('server0', 'server1', 'server2') is replicated every time players need to be addressed.

Also, do we want to control the naming internally or should be there a way for users to specify workers name?docstringI think this notebook could be merged with step 2.I did this way in order to load the right weights from the `tf.keras.layer` into the `tfe.keras.layer` based on the layer name.  The transition between `tf.keras.layer` to `tfe.keras.layer`is happening in ` _instantiate_tfe_layer`. Also in tfe currently we don't have a `mode.set_weights()`to load the weights from the entire model into a tfe model. Maybe there a better way to do this :) this should be outdated -- we now have a single TFEWorker, the comment needs to be updatedyup, `shutdown_workers()` seems appropriatewe no longer do, this was an artifact of a previous commit that I missed during cleanupI think this makes the code neater, and also the optimization we'd get is negligible in the context of the rest of the functionthese are player names, not quite the same as worker names. in TFE, a player represents a specific role in the MPC game, which means they're generally not symmetric.  in theory we could disambiguate worker names and these player names, but currently TFE expects specific names in order to distribute computation according to a given protocol, and that seems like unnecessary complexity for this first gobut yes, the method _config_from_workers can be converted to a classmethod and used here to avoid code duplicationagreed, there's an incoming commit that fixes this flowI think we might want to remove this notebook, so we present only one approach (server-client). the script launched below needs itnot sure, by the same logic we could also have used `share_to_workers` and `serve_on_workers` yet `workers` seem implicit in those actions@iamtrask what's your take on this naming? what fits best with the terminology you guys are using elsewhere?I think we should use `model.get_weights` eventually -- it'll be useful for loading in from e.g. a file as wellactually, there's an issue in TFE blocking this currently (https://github.com/tf-encrypted/tf-encrypted/issues/499#issuecomment-495412270). suggest keeping as is until that issue is resolved<p>- extra comma ""...define your Syft Keras model, like we did...""</p><p><br></p><p>- ""share"" doesn't tell us what is happening in the description (aside from plugging TFE). It should be talking about protocols (SPDZ, SecureNN, etc.), not libraries.</p><p><br></p><p>- ""serve"" also doesn't say what's happening. It says ""what you'll be ready for"" but it needs to say ""this opens up an endpoint on X ports/hostnames... etc.""</p><p><br></p><p>- ""shutdown_workers"" - this is the first time we mention a ""worker"". This should be mentioned earlier.</p><p><br></p><p>Some of these can be addressed by moving the bottom paragraph up to the top and then referencing it.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2170/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='1'/><p>""For this demo will send input data with the shape of (1, 28, 28, 1)."" is not grammatical.</p><p><br></p><p>""We also return the logit instead of softmax because this operation has not yet been implemented in TF Encrypted."" could use a bit more explanation but I generally get it.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2170/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='4'/><p>Explain the difference between manual and automatic use. </p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2170/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='11'/><p>num_steps is a poorly named variable. I get what it means but the first time user probably won't. We need to specify that the server is expecting to serve only a limited number of predictions.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2170/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='17'/><p>We should change ""Served"" to be much more informative. Ideally ""produced encrypted prediction to &lt;client name&gt;"" or at least a list of the servers involved.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2170/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='18'/><p>grammatical error</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2170/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='4'/><p>Before querying the model, you have to connect to it. To do so, you first create a client. Then, you define the exact same three TFEWorkers that we defined in the last notebook (<code>alice</code>,&nbsp;<code>bob</code>, and&nbsp;<code>carol</code>). Finally call&nbsp;<code>connect_to_model</code>.</p><p><br></p><p>we also need more explanation about what ""connect_to_model"" is doing.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2170/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='7'/><p>Replace with:</p><p><br></p><p>This is great. You are able to classify these three images correctly! But what's special about these predictions is that you haven't revealed any private information to get this service. The model host never saw your input data or your predictions, and you never downloaded the model. You were able to get private predictions on encrypted data with an encrypted model!</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2170/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='14'/><p>changed, couldn't get a client name unfortunately but it's more informative now</p>",1,True,2019-05-28 08:13:07,2019-05-29 20:05:43,2019-05-29 20:05:43
https://github.com/OpenMined/PySyft/pull/2166,[],small fix so that the tutorial works with torch 1.1.0,"small fix so that the tutorial works with torch 1.1.0This fixes the exception:
`RuntimeError: set_storage is not allowed on Tensor created from .data or .detach()`
that occurs when running tutorial 4 with `torch 1.1.0`.

Following the fix from [here](https://discuss.pytorch.org/t/api-change-for-tensor-data-set-in-torch-nightly/33310/2), we:

1. just remove the reference to `.data`,
2. wrap the code with `with torch.no_grad()`",1,True,2019-05-23 23:26:19,2019-05-24 15:59:59,2019-05-24 15:59:59
https://github.com/OpenMined/PySyft/pull/2165,[],Solving #2117 Conv2d not working in MPC,"Solving #2117 Conv2d not working in MPCfirst commit: implementation without bias, padding, dilationThanks for the addition
Having to add ""torch.stack"" and ""stack"" in exclude_functions is a bug that I will fix in a later PRüëç Much better indeedAgreed, we'll create a kind of dispatch function, but this is also for another PR I believeYes you can use `torch.nn.modules.utils._pair`You have `0 <= c < nb_cols_kernel ` so you shouldn't need the modulo here you might prefer to use .append()Same here I don't think the modulo is necessaryYou can do a bit simpler with the .child stuff here:
```
                res = torch.cat(res, dim=2)
            else:
                res = im_reshaped.matmul(weight_reshaped)

            # Add a bias if needed
            if bias is not None:
                res += bias

            # ... And reshape it back to an image
            res = (
                res.permute(0, 2, 1)
                .view(batch_size, nb_channels_out, nb_rows_out, nb_cols_out)
                .contiguous()
            )
            return res.child
```You should use the decorator `@overloaded.function` which helps you to have a simpler code:
```
                @overloaded.function
                def pad(input_shares, pad, mode=""constant"", value=0):
                    padded_shares = {}
                    for location, shares in input_shares.items():
                        padded_shares[location] = torch.nn.functional.pad(shares, pad, mode, value)

                    return padded_shares

                module.pad = pad
```",2,True,2019-05-23 21:21:57,2019-05-29 20:27:08,2019-05-29 09:41:01
https://github.com/OpenMined/PySyft/pull/2160,[],Added support for LSTM and GRU Cells in PySyft and merged PR #1995,"Added support for LSTM and GRU Cells in PySyft and merged PR #1995- Merged changes of Pull Request #1995  with the latest 'dev' branch
- Added support for GRUs, GRU cells and LSTM cells in PySyft
- Added a workaround to fix the forward pass in  LSTMs with LSTM cells by returning a single tensor instead of a tuple of tensors (which seems to lead to errors in case of websockets). 
- Added a workaround to fix the torch.sort method when being executed over websockets by returning a tensor instead of a tuple of tensors.

I found out that returning multiple tensors in a tuple on remote websockets currently leads to a ""ResponseSignatureError"":

https://files.slack.com/files-pri/T6963A864-FJRTUCPSQ/image.pngA line has vanishedThanks for re-using my hack, but actually I'm very unhappy with this hack, I don't think we should use it. I would either try to modify the PyTorch implement to have them use shape instead of size, or try another techniqueSame thing, this is a bit hacky and should not be merged in the code base. I understand that there is a pb, but let's open an Issue about this case when we use function with > 1 output and websocketsWould like to try implementing .dim() for remote tensors ? Could be  a cool contribution
You could rely on the .shape attribute which works for remote tensorsWhich line is no longer there exactly?So should we discard this fix for .size() completely?Okay. I guess this would better fit an issue too.Would it be sufficient to just hook the .dim() method from the original PyTorch, or what exactly would it take to implement .dim() for remote tensors?844: tensor_type and torch_type minus those in the exception listYes, because we can't modify the hook at runtime it will be far too slow and bad practice.If you hook dim() to use .shape, then it will also work for remote tensor: namely instead of returning the shape of the pointer itself it sends the call remotely to get the remote shapeImplemented dim() for remote tensors@mari-linhares Thank you!Removed the flip_hook_native_sizeüëç You can remove this part, we won't hook size() that easily as it is needed all to learn about the real size of the pointer objectüëç",10,True,2019-05-21 16:54:25,2019-06-05 06:50:19,2019-05-31 12:45:11
https://github.com/OpenMined/PySyft/pull/2159,[],"Added support for LSTM and GRU Cells in PySyft, ","Added support for LSTM and GRU Cells in PySyft, - Merged changes of Pull Request #1995  with the latest 'dev' branch
- Added support for GRUs, GRU cells and LSTM cells in PySyft
- Added a workaround to fix the forward pass in  LSTMs with LSTM cells by returning a single tensor instead of a tuple of tensors (which seems to lead to errors in case of websockets). 
- Added a workaround to fix the torch.sort method when being executed by returning a tensor instead of a tuple of tensors.


I found out that returning multiple tensors in a tuple on remote websockets currently leads to a ""ResponseSignatureError"":

https://files.slack.com/files-pri/T6963A864-FJRTUCPSQ/image.png",1,False,2019-05-21 16:44:24,2019-05-21 16:48:10,2019-05-21 16:48:10
https://github.com/OpenMined/PySyft/pull/2155,[],Plans jit experimental,"Plans jit experimentalTesting serializing/deserializing capabilities of pytorch jit tracing.<p>Oh actually I was expecting that we would consider the model as a plan (or any other forward function as a plan), and that the send would be done after serialisation is made (= plan is built): it comes for free with @torchi.jit.script and you have to build it with tracing, </p><p>Does it make sense?</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2155/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='15'/>I propose to first focus on sending the object to the worker and loading it from the buffer. I don't see the need of a run_script_module() function as this would be covered by the TrainConfig developments. (Ok for testing, though :-) )<p>Yes, I thought the same thing. This was just for testing to see what error we would get.</p>This was for testing only :), I was checking if we were able to train a model remotely using the serialized model.@LaRiffle I think the only thing is not totally clear is if we're going to use IoBuffer as I used in this PR, are we? I considered we were so this is more of a proof of concept that serializing a torch script module we can train a model remotely :100: Ok cool! Yes I'm not completely sure but so far I don't see another easy way to do so, except perhaps maybe sending together this buffer + the torchscript.graph for non-torch models which can't use the buffer, but this is not very important at the moment I believe.Yeah, I agree it makes sense to restrain plans to torch operations for now, also using @jit.script instead of tracing can deal with if statements and for loops.I agree with @LaRiffle last suggestion of adding complementary information when using syft tensors/operations along with jit.

For us to have plans changed completely to use jit I think we need a proof of concept of jit using at least one syft's specific operation.",1,False,2019-05-21 03:04:48,2019-05-29 19:26:21,2019-05-22 12:20:49
https://github.com/OpenMined/PySyft/pull/2153,[],[WIP] Paillier Tensor,"[WIP] Paillier TensorThis PR adds supports for Paillier Tensors (#2060 ). Currently it only supports addition, subtraction, scalar multiplication. Addition, subtraction, multiplication and division as there is a problem with decryption of `fixed precision` tensors. If you have any ideas, comments or suggestions please let me know.It would be ideal if we could simply add a ""paillier"" function to native.py such that you can take any LongTensor (such as ""x_"") and call x_.paillier(key) and it would automatically return a wrapped PaillierTensor.That's actually a great idea, I will look into it.Let's wait to resolve this conversation until it's in - helps me keep track :) can we get a unit test for this?One question this change raises is what tensor types should have convenience methods in native.py for their creation. Theoretically going forward the api for native tensors gets incredibly cluttered if we add these connivence methods for every tensor typecan we get a test for .paillier()",5,False,2019-05-20 06:17:38,2019-11-20 15:13:34,2019-11-20 15:13:34
https://github.com/OpenMined/PySyft/pull/2152,[],Fix typo in tuto 9,"Fix typo in tuto 9<p>Mai /June -&gt; May / June</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2152/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='0'/>",2,True,2019-05-18 20:58:14,2019-05-26 17:57:14,2019-05-26 17:57:10
https://github.com/OpenMined/PySyft/pull/2147,[],Large Precision Tensor,"Large Precision TensorThis PR contains the first steps to have very large integers in the system.
It works by transforming the input into an a torch tensor that contains the big number split in components.

This PR also includes a first operation `add` that allows addition of two LPTsrename file to large_precision.pywhy were these chosen as defaultsalso add typingformat this as an if elif elseChange to google stylegoogle styledo we need a method to explicitly convert this or should this happen automatically when calling something like fix prec why do we need this as a separate function on native tensorsame as for enc_fix_large_precwhat exactly is the difference between this and virtual precshould this not be a long tensor for more storageI used `16` bits as the precision that would be needed when moving this tensor to other systems (I had JVM in mind).
It would be good to have this value set in a global configuration of the setup.
I am also thinking that instead of using a number we could use a `dtype`. I will give it a try and see how it looks likeI wanted this to be separated until we had a full working solution.
Eventually `fix_prec()` will check the precision and use `FixedPrecisionTensor` or `LargePrecisionTensor`.
I would change the naming as follows:
`precision` -> `internal_type`
`virtual_prec` -> `precision`
I'll see if by using `dtype` as a param this can be done differently.The @overloaded.method decorator will automatically put LargePrecisionTensor on top of your result, so you can remove it, otherwise you have it twice (as cell 17 of the NB you shared shows)
Also, you can simplify the function signature `def add(self, self_, *args, **kwargs)` -> `def add(self, self_, other)` directlyUsing `dtype` nowUsing `dtype` now to create the tensor.Maybe add a short dosctring to explain what's happeningDo you need to specify `def __eq__`?Add a short docstringSame, instead of commentsMaybe even further clairfy `internal_type` : you could say that the storage of the large tensor is using small tensors of size `internal_type` bits, or smthg equivalentCan you also add negative values in the test and decimal numbers?Also, internal_type in now a type, you should modify the type infoI would make the case then that this should be a class method on large_prec tensor which we can either call manually or add to fix_prec later. The code now decided whether to use LPT or FPT depending on the requested precision.Leftovers. RemovedI'm not 100% sure about this solution though it is a way of allowing different setups. If you can think on a better alternative, happy to discuss itYes maybe this can be a bit more flexible. What I experienced is that dealing with integers > 2**62-1 is not fun with precision tensors, so 62 could be the total limit which sum the space allocated to the fractional_precision *and* the space allocated to the integer part. I think we should find a way to take this into account.
Another point to consider: we shouldn't mix precision in bits and decimal precision: from the user interface, people usually keep the `base` at 10 so the precision they ask for (3 by default) means 3 digits after the `.` which roughly corresponds to a 10 bits precisionWith respect to my comment below maybe we should consider add a `base` attribute just like in the precision tensorYou need to be more precise:
I would suggest doing a check as follows:
if log2(int(value)) + log2(base**precision_fractional) > 62
then go_to_large_precision

But because value can be < 1 or <0
if log2(int(abs(value))+1) + log2(base**precision_fractional) > 62
then go_to_large_precisionWhat happens when overflow occurs in one chunk of the data?Did you mean `filler = fill_value * torch.ones(...)`?What is exactly the use case for fill_value != 0?@Jasopaum Good catch
@LaRiffle I can only think of filling the tensor with 0s (for additions) or 1s (for multiplications) We're dead here :) I was expecting to address the overflow when implementing the multiplication but it's true this can already be a problem with the addition.

In fact, this is probably the *main issue* with this representation of large numbers.

1) One possible solution could be to halve the storage precision. This way there will always be enough room to avoid an overflow.
This approach, however, will force to recheck the result in case the new number needs to be split again as soon a we concatenate more additions or multiplications.

2) Another option is to deal with these numbers the same way that a processor deals with numbers greater than the word they work with. This approach is slower but we know it works and we know there exists an implementation.

3) The third option would be to work directly with NumPy objects when operating. This would require splitting and restoring back the tensor that represents the large number with every operation. Probably this option is the most reasonable in terms of coding effort.

What do you think? 
Why is it half and not just -1?Maybe we need a check to be sure that we do not try to put a negative number in a LPT made of uintAgreed on the 3rd, we shouldn't resent to use numpy when appropriate.
How does this PR relates with #2257 ? Is it just a fork of this one? #2257 was for showing how it could be with np. I will cancel it and keep working on this PR.Ok thanks for the update!Maybe you can add sub and div?As __add__ is the method called, I would still put this test in test_add and say that you test it when values are negativeNo longer appliesAdded `sub`make a @property in the class to allow `self.internal_precision` instead of `internal_precision[self.internal_type]`, it's more pythonic (and will be useful)You can make this method a @staticmethod like _internal_representation_to_large_ints, and change the signature `_create_tensor_from_numpy(self, ndarray):` -> `_create_tensor_from_numpy(ndarray, internal_type):`. I believe you can get `self.child.shape[:-1] ` for the ndarray directly (am i right?)
Also make this one static and ask that internal type be provided in the signature if neededIs this one needed?I think _internal_representation_to_large_ints could be non-static, as it needs a LargePrecisionTensor object to run, this way you will have smthg like `self._internal_representation_to_large_ints()`  (don't need to provide internal_precision enymore as you have self to get it)This is not prioritary but at least would deserver a #TODO: _create_internal_representation loops on all elements of the tensor while there is perhaps a way to do this on the whole tensor with vectorized operationsSame here, at least add a TODO to mention this should be vectorizedHere is a solution to have this one vectorized easily:
```
n_parts = len(number_parts)
base = 2 ** bits
powers = base ** np.arange(n_parts)
return np.sum(number_parts * powers)
```
smthg like this should work I hopeMaybe add a comment again to explain why you needed to do -1It's needed in the hooksDifficult with vectorisation as every element uses the result of the previous (maybe it can be done but I didn't find a solution).
I created a recursive function instead as it is cleaner and a better fit than the loop I had beforeChanged the type sizes to be what the platform provides. The -1 is applied when computing the internal size (comments added to the source to explain it)Can you add a test for mod? Hi @robert-wagner,
`mod` is not yet implemented or do you mean something else?Hey @mccorby That was a big part of why I asked for a test for it. By far the biggest thing I see my self using lpt tensor for is for converting to something like a Chinese remainder theorem tensor (since it is significantly more space efficient). However, that conversion requires mod. Theoretically we could add that as a separate pr thoughI don't think this is still true now that you use NumPyAvoiding the loop over all the elements is not too hard, you can just change _split_number so that it takes an array instead of a single value (I did it but I'm not sure I can push to this PR, I can send you the small changes though)I think it still applies since the internal type is still a PyTorch type. NumPy is only used to perform the operationsOk but we don't have to worry about overflows anymore, do we?The overflow can happen when doing `divmod`. A way of seeing it happening is to remove the -1 in `def internal_precision(self)`I think the code is ok, I was just saying that for me, it's not half the size that is used but all of it, and that the `-1` is not here to avoid overflow but because we need to keep a bit for the sign in each chunk Haha... I see it now. You're right. Fixing!Added `mod`Did you declare `add_` just for the tests?
Actually I think that you can write directly `x. __iadd__(y)` or `x += y`NB actually add_ is a valid torch function",3,True,2019-05-17 15:04:00,2019-06-25 18:33:00,2019-06-25 18:33:00
https://github.com/OpenMined/PySyft/pull/2129,[],Fix broken cell at tutorial part 10,"Fix broken cell at tutorial part 10Tutorial part 10 is working correctly but there was a broken cell on it. This PR just re-runs the entire notebook to get rid of the cell with the error message.<p>We don't call the conv ever so is it needed?</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2129/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='12'/><p>Sorry, debugging code for some other stuff I was trying.</p>",1,True,2019-05-08 20:34:05,2019-05-11 23:38:48,2019-05-11 23:38:48
https://github.com/OpenMined/PySyft/pull/2128,[],Recurrent Neural Network for Text Classification - Advanced Example,"Recurrent Neural Network for Text Classification - Advanced ExampleAdded Jupyter Notebook code for a Recurrent Neural Network based on two models being learned in a federated way. The RNN is used for text classification as belonging to a certain category. 

The code was also tested with Raspberry Pis using remote worker websockets. 

Federated implementation of: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html<p>&gt; Following distributed training, the resulting model is going to be able to perform operations like: ?</p><p><br></p><p>Missing some text here.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2128/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='1'/><p>Recommend: to pip install -r PySyft/requirements.txt</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2128/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='4'/><p>can you add a space before and after all the ""="" at variables declaration?</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2128/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='10'/><p>Make the first comment in this cell a text cell.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2128/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='12'/><p>Make the first long comment in this cell a text cell.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2128/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='13'/><p>Make the first comment in this cell a text cell.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2128/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='14'/><p>Make the first comment in this cell a text cell.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2128/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='19'/><p>Make the first comment in this cell a text cell.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2128/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='26'/><p>Make the first comment in this cell a text cell.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2128/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='27'/><p>Make the first comment in this cell a text cell.</p>

 _Reply via <a href='https://app.reviewnb.com/OpenMined/PySyft/pull/2128/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='OPEN' data-cellIndex='31'/><p>It was actually referring to the next cell's predictions. I added some more introductory text and a brief explanation about RNNs' training BTW.</p><p>To have an actual executable command (assuming the Jupyter Notebook was not moved), I added:</p><p><br></p><p><code>pip install -r ""../../../requirements.txt""</code></p><p>Done!</p><p>Done!</p><p>Done!</p><p>Done, and re-rephrased the explanation about word embeddings.</p><p>Done!</p>So is there already a federated averaging step included? It is commented out here and I didn't see the implementation of the function fed_avg_every_n_iters(). 

For information, there is code to average models already available: see syft.frameworks.torch.federated.utils.federated_avg()Actually, the function fed_avg_every_n_iters performing federated averaging is there in the last commit. It is indeed based on the `utils.federated_avg()` function,  however I commented the federated averaging part because the model loss increases in correspondence with the averaging function call, causing the overall model loss to diverge over time. By discussing this issue with a professor of mine and with Andrew Trask, the reason for that is probably the small batch size = 1 being used in the present example. I don't know why/how, but there is text leaving the first cell:

![image](https://user-images.githubusercontent.com/8157164/57474748-b937b580-728a-11e9-8aa5-516802c08b83.png)

I think is because this cell has type ""raw"" instead of ""markdown"", can you change it to markdown?
Remove # before example. In markdown this # -> title, which is probably unintended in this case.Can you use a single pattern for section titles? You started using <number> . <title>, in the end you're using <number> - <title>I have this same issueDone, made it all more coherentFixed",4,True,2019-05-08 17:22:22,2019-05-09 19:07:22,2019-05-09 19:07:22
https://github.com/OpenMined/PySyft/pull/2127,[],Fixed backpropagation for models using parameters.grad,"Fixed backpropagation for models using parameters.gradFix for issue #2122 reported while trying to implement a Recurrent Neural Network in a federated way. 

Fixed the loss.backward() function not generating gradient updates accumulated in the model parameters.

If trying to access the .grad.data attribute of a model's parameters, not all the gradient parameters were actually set, leading to the model weights not being updated. The de-commented lines actually allow the gradients to be stored in the model parameters and the weights to be updated, hence the loss function to converge. 

You don't need this importYou can get Alice by including workers as a parameter and writing Alice = workers[""Alice""]. The tests for most tensors have good examples for how to do thisWhat does this line do?This is the most important line, in which the` param.grad.data` was previously yielding a `None` that my PR intends to fix. It basically updates the model's parameters (e.g.: layers' weights) based on the gradients computed following the backpropagation phase. Done!Removed importWhy is this called after the assertion thoughBasically with this `assert param.grad.data is not None`, we would like to make sure that every single gradient among the model's parameters is indeed there.

 By executing `param.data.add_(-learning_rate, param.grad.data)` without an an assert prior to it, we may already have an exception in case a grad.data is None, meaning that the code is broken again and we had no way to check the effective presence/absence of the gradient beforehand",2,True,2019-05-07 16:10:04,2019-05-08 17:33:04,2019-05-08 17:33:04
https://github.com/OpenMined/PySyft/pull/2125,[],"added testing of plan functions, small bugfix","added testing of plan functions, small bugfixSmall bugfix in function Plan.replace_worker_ids to support integer ids
Added testsCan you cover all 4 possibilities when dealing with strings?

from_worker, to_worker
from_worker, to_worker.encode()
from_worker.encode(), to_worker
from_worker.encode(), to_worker.encode()There's a lot going on here. Break this into multiple tests and add some comments about what each test is covering.Break this into two tests: test__call__fn, test__call__methodWe don't need all four possibilities. The code in train-config contains the four possibilities. In the dev code it considers two cases (from_worker, to_worker) and (from_worker.encode(), to_worker.encode()) but without taking care of the integers that don't have a encode() function.

As we are replacing, it doesn't make sense to replace from_worker twice. Therefore only two outcomes, not four.Hmm, I would say we need the 4 possibilities (as in train-config) to cover all possible cases, don't we?

My question then is: can we guarantee that with only these two cases all the other cases will work?Let's think of an example doing string replacements:
```
 ""The quick brown b'brown' fox jumps over the lazy b'brown' dog.""
```

If we want to replace brown by black, we would call the replace method with from_worker=brown and to_worker=black.
First call would replace all brown instances with black:
```
""The quick black b'brown' fox jumps over the lazy b'brown' dog.""
```
The second call would replace all b'brown' instances with b'black':
```
""The quick black b'black' fox jumps over the lazy b'black' dog.""
```
And we are done.Got it, sorry, I got confused, but it's clear now!",2,True,2019-05-07 09:21:17,2019-05-07 15:29:24,2019-05-07 15:29:24
https://github.com/OpenMined/PySyft/pull/2113,[],FederatedClient draft,"FederatedClient draftWork in progress.

- [x] Question: Do we need to share the actual model to another worker or just the parameters? If we need to, how do we do this?

> Answer: we only want to share the parameters... parameters + plan == everything needed.

- [x] How to build the forward method locally in the blueprint owner? Currently, I get the error: `TypeError: __bool__ should return bool, returned Tensor`

> Answer: right now you need to explicitly send the model to the local worker. This is still work in progress since is not clear how to do this correctly.

- [x] Support remote method execution: implemented by @midokura-silvia :100:.

- [ ] Support remote method execution that returns a tensor.

- [ ] Build plans automatically: placeholders / input_data_args. This fixes #2062.

- [ ] Why do we need to call model.send(me) and model.get(), we should not need to do this.

- [ ] Test remote execution.

- [ ] Add experimental notebook with federated training loop working.

- [ ] Implement TrainConfig datasets.

@midokura-silvia",13,True,2019-04-29 20:56:25,2019-06-11 20:49:41,2019-05-17 16:34:01
https://github.com/OpenMined/PySyft/pull/2102,[],Fix minor bug in class FederatedDataset,Fix minor bug in class FederatedDatasetTakes care of workers with integer ids.,1,True,2019-04-25 14:17:59,2019-04-25 15:12:04,2019-04-25 14:32:37
https://github.com/OpenMined/PySyft/pull/2098,[],Plans should not be workers,"Plans should not be workersAs mentioned by @midokura-silvia at #2068. Plans should not be workers. This change addresses this and also:

* Create a new abstraction called: ObjectStorage, which consist of a wrapper class to _objects. Plans now inherit from ObjectStorage.

* **BaseWorkers inherit from ObjectStorage**.

* plan.plan_blueprint -> plan.blueprint.

* Fixes docstring style of some methods + typing.

**Important**

> *3 test cases were removed, why???*

Since plans are not workers anymore they are not being hooked, and so the blueprint attribute does not exist and a plan cannot be re-built locally after being built remotely (which was the case for the last tests)

Maybe this class deserves its own file",2,False,2019-04-25 01:39:02,2019-04-25 10:30:20,2019-04-25 10:30:20
https://github.com/OpenMined/PySyft/pull/2095,[],Continuing Mcleonard autograd tensor,"Continuing Mcleonard autograd tensorThis PR is a continuation of https://github.com/OpenMined/PySyft/pull/1942The AutogradTensor seems to be missing a get() function. Could be something like:
```
    def get(self, deregister_ptr: bool = True):
        if self.grad is not None and hasattr(self.grad, 'child'):
            self.grad.get(deregister_ptr)

        self.child.get(deregister_ptr)
        return self
```test_sqrt_backwards fails with:
E   TypeError: can not serialize 'AutogradTensor' object
msgpack/_packer.pyx:279: TypeErrorI've added this in the last commits, thanks Silvia!",1,True,2019-04-23 21:56:07,2019-06-03 19:08:18,2019-05-17 12:39:59
https://github.com/OpenMined/PySyft/pull/2087,[],Add WebSocket over TLS,"Add WebSocket over TLSImprove Secure level of data flow between remote workers adding WebSocket over TLS.
Benefits:
- Authenticate component identity by certificates.
- Provides end-to-end encryption

PS: These changes do not modify or render infeasible the old workers' behavior.This line should only be used in development mode.
It allows to work with self-signed certificates.Overall I think this PR looks ok. This boolean is a bit confusing. since we have `secure` in the top, can we use that down here too and `raise` if the settings aren't right. Reading this I get the sense that a user could inadvertently run the server in an insecure mode without realizing it. The secure flag doesn't exist on the server side, but can be implemented if this makes the idea of ‚Äã‚Äãa secure server explicit.

**kwargs at client side**
`
kwargs_websocket = {""host"": ""localhost"",
                    ""hook"": hook,
                    ""verbose"": True,
                    ""secure"": True }
`
**kwargs at server side**
`
kwargs = {
    ""id"": args.id,
    ""host"": args.host,
    ""port"": args.port,
    ""hook"": hook,
    ""verbose"": True,
    ""cert_path"" : ""localhost.cert"",
    ""key_path"" : ""localhost.key""
}
`


Should I add a secure flag to websocket server too?If the server side has the secure flag can we make it reject all non secure connections?By default, the websocket library already rejects connections that do not implement the pre-established security protocol (if it is established). The security protocol is defined here: `ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)`FYI - over the last little bit I've been working on a Heroku based flask app worker - when the time comes I'd love to pick your brain on how to integrate these - especially the security pieces.I would be happy to help. :smiley: Excellent! ",1,True,2019-04-23 08:37:59,2019-04-25 18:53:54,2019-04-25 18:53:54
https://github.com/OpenMined/PySyft/pull/2084,[], Fixed a timeout issue occuring on raspberry PIs or low-power devices,"Fixed a timeout issue occuring on raspberry PIs or low-power devicesWhen wanting to train a model via websockets on low-power devices with a slow CPU (e.g.:Raspberry PIs or mobile device), a timeout exception occurrs, as shown in the following pictures.

![error_broken_pipe_client](https://user-images.githubusercontent.com/4907418/56466422-53d27080-6412-11e9-85b5-0062cab3b554.png)

![issue_pysyft](https://user-images.githubusercontent.com/4907418/56466468-d9562080-6412-11e9-8403-8bf9b57610ed.png)



I fixed the issue by removing the default ping_timeout and close_timeout timers set to 20 seconds for the workers.@DanyEle can you remove the author name from the line? It's not a code pattern in syft to keep the author's name in a comment.create a TIMEOUT constant instead of using 9999999use constantuse constantremove author's name from commentremove commentDone!DoneDoneDonewhat is the default for this if nothing is passed (also could you add a justification for why this was chosen)This should not be here (if you see the message below this method should never be called)This value was just picked to avoid sockets from timing out. 9999999 seconds looked like a sufficiently large amount of time to me. If nothing is passed or no timeout is set, then you can refer to the default values listed in the respective documentation for sockets:

https://docs.python.org/3/library/socket.html
If no timeout is supplied, the global default timeout setting returned by getdefaulttimeout() is used.

And for websockets:

https://websockets.readthedocs.io/en/stable/api.html

ping_timeout=20, close_timeout=10 are the default values.If it's never called, then it may as well  be removed: i just added this statement to address any possible calls to websockets with no timeout set properly.",5,True,2019-04-21 06:51:35,2019-04-23 20:26:02,2019-04-23 20:26:01
https://github.com/OpenMined/PySyft/pull/2068,[],Plans Design Doc discussion,"Plans Design Doc discussion**this PR should not be merged**

## Summary

There is a lot of discussion going on about how the design and implementation of the Plan abstraction, this draft PR is a single place where we can keep track of all the plans we have for plans :sweat_smile:.

The goal is to avoid multiple changes in the code and interface and come up with an abstraction that works well for the intended use cases.

Ps: if you think there's a better way for us to keep a public log of this discussion feel free to suggest in this thread.

**Relevant PRs and link**:

* Plans drat #1938
* Search for plans #2044 (plans can not be searched anymore)
* fetch_plan #2064 
* [Theo's excelent introduction to Plans](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%208%20-%20Introduction%20to%20Plans.ipynb)
* Plans should not be workers #2099 

## Plan Definition

### Original Definition

A plan is a concept defined in the paper [Towards Federated Learning at Scale: System Design](https://arxiv.org/pdf/1902.01046.pdf)...

> *The server tells the selected devices what computation to run
with an FL plan, a data structure that includes a TensorFlow
graph and instructions for how to execute it.*

> *As already mentioned, FL plans are not specialized to training,
but can also encode evaluation tasks - computing quality
metrics from held out data that wasn‚Äôt used for training,
analogous to the validation step in data center training.*

> *An FL plan consists of two parts: one for the device and one
for the server. The device portion of the FL plan contains,
among other things: the TensorFlow graph itself, selection
criteria for training data in the example store, instructions
on how to batch data and how many epochs to run on the
device, labels for the nodes in the graph which represent
certain computations like loading and saving weights, and
so on. The server part contains the aggregation logic, which
is encoded in a similar way. Our libraries automatically split
the part of a provided model‚Äôs computation which runs on
device from the part that runs on the server (the aggregation).*

### Pysyft Definition of a Plan

From the first draft of Plans...

> The idea is to send a ""plan"" ie all the instructions at once to a remote worker, while you still execute your code line by line like in PyTorch.
It uses the fact that now the worker that sends the command fix the id where the result is stored and therefore and can already create a pointer to a result which doesn't exist so far and go on with instructions.

## Plan use cases

* How are plans going to be used? Do we expect plans to be executed remotely or locally?

    * Plans should be built only once. Usually locally by their original owner (the worker who has access to the plans blueprint).
    * After plans are built they can be served to multiple workers.


```python
# run script to start client workers....

## -------------- SERVER  (Scheduler) ------------------

# start a locak worker
me = local_worker()

# model definition
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(2, 3)
        self.fc2 = nn.Linear(3, 2)

    @sy.method2plan
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=0)

    @sy.method2plan
    def forward_no_softmax(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

net = Net()
workers = [alice, bob]

def update_model_callback_fn(res):
    net.set_weights(res.model.weights)
    pass

train_config = sy.TrainConfig(model=net, batch_size=16, epochs=1, loss='mse', optimizer='adam')

for worker in workers:
   train_config.send(worker)
   worker.run_training(update_model_callback_fn)
```

## How workers interact with Plans?

There seems to be a little bit of confusion about how plans are defined: are plans workers or tensors? Although plans are BaseWorkers there are a lot of operations performed on Plans named the same aa tensor operations: search, PlanPointers (recently removed #2050), ... but with different behavior.

Here are some ideas of possible changes in the current interface:

#### worker.fetch_plan(plan_id)

#2064 - Already implemented

Instead of searching for plans as we do for plans, a user can fetch a plan from another worker (they will get a copy of the plan).

#### worker.send(plan)

> The interest is that you can call .send() but actually this doesn't trigger a call to a remote worker, just ""a promise"" that it will be sent (with _send() ) as soon as the plan is called (and built)

This is good because we just build the pƒ∫an when we need to, and potentially this will be a remote operation.

This is bad because is not an intuitive behavior and can be very confusing for the user.

Being discussed #2062.

## Plan serialization

It seems to be done: #2067

## Other questions

* Currently only non-client workers can execute plans, client workers get errors when trying to run plans since they can't register objects of their own. How do we want to deal with this? This could be a potential pain for beginners.

* What should happen if a worker wants to run locally a plan that was already built on another worker? Currently, the plan is being rebuilt.

* If the plan is executed locally should it return a pointer to the result or the result itself (tensor)? Currently .get() is being called on the pointer, so a local client would get the result itself.
* The Pysyft concept of Plan will make it possible to isolate the remote workers from the server to a certain extent. But what degree of separation is the project aiming for? Will the control over the batch selection and all these training details move to the remote workers? Or be part of the plan? - [Silvia](midokura-silvia)",9,False,2019-04-13 02:39:57,2019-06-15 17:17:35,2019-06-15 17:17:34
https://github.com/OpenMined/PySyft/pull/2050,[],Remove PlanPointer,Remove PlanPointerIf the description is 0 or an empty string this will return incorrectly. The previous check was correctThanks for the careful review! Done.,2,True,2019-04-09 12:16:16,2019-04-10 12:52:22,2019-04-10 12:52:21
https://github.com/OpenMined/PySyft/pull/2028,[],Websocket worker example,"Websocket worker exampleThis pull request contains an example implementation of a federated training using websockets and federated averaging. The training on the workers is currently run sequentially (no scheduler implemented). Any way that we could create a version of this in a notebook so that people trying to use it can iterate step by step?Could we do this step by intitializing each of alice, bob and charlie with the data parameter. That seems to be much more in line with how fl will work in the wildThis probably shouldn't have a default as that can lead to conflictsThis should probably not have a default as that can lead to conflictswhy is this commented outdoes this need to be a raw string literal?This will fail or hang if the websocket is not connected. What is the check after this actually doing?Format this in pytest styleThis is handled by the conftest workersthis is handled by the conftest hookI thought of it, but debugging a notebook is more difficult than putting breakpoints in an editor as pycharm. 
The script starting the websocket servers will not go away, as starting the processes from a notebook does not work (see discussion in the slack channel).
I will see what makes sense in a notebook, as it shouldn't be too long neither.That is a left-over of some tests. I will un-comment it.No, I don't think so. I will remove the r-prefix from all the docstrings in the file._receive_action will not fail, if the websocket is not connected. but the response will be empty. 
The check on self.ws.connected has to be after a self.ws.recv() call as otherwise the variable is not updated. 
So the steps are: 1) try to send/receive, 2) check if connection is ok, if not 3) reconnect and send/receive again. 4) check if connection is ok now, if not fail.will doI added a notebook version of the example.This could be a separate example, where the focus is on setting up the remote data in a realistic way. My suggestion is to do a separate pull request for this.Could you make the name of this folder more command line friendly (ie no spacesWould the name ""websockets-example-MNIST"" be good?
Shall we change the ""Advanced"" folder to ""advanced"" to align it as well?",2,True,2019-04-01 10:20:50,2019-04-06 04:07:27,2019-04-06 04:07:08
https://github.com/OpenMined/PySyft/pull/2021,[],Preparation for SecureNN,"Preparation for SecureNNSecureNN uses alot MultiPointer and AdditiveShared Tensor,
I'm doing here many changes to theses tensors to fix them -- so that the secureNN can work properly. It will be done in a second PR.
This is really to address errors we have in MultiPointer and AdditiveShared Tensor, and joint interactions between them.Why are the children added back?The way I'm reasoning is that all objects a, b or c should be AdditiveSharedTensor or MultiPointerTensoir, (all without wrappers)
I'm pretty confident this is a good way to proceed, I just made sure I had the good object in hands, but this line should not be there and crypto_provider.generate_triple shoudl return the correct typeSo the setup earlier was that everything in this method was actually a dictionary of pointer tensors. Has that changed?Yes I think it was not the best choice -- well it was good to have it run quickly; but it forces to always iterate manually while we have protocols to handle classic operations transparentlyI think we can re-rewrite the spdz protocol making the hypothesis we only handle AdditiveSharedTensor and MultiPointerTensor. That's what I did for snn, but I added functionalities so that they would play together nicely.I've just fixed the snn thing (so relu, eq, comp) works, I'm planning on opening another PR with code cleaned and correctly committedSounds good to me. I have an open pr with decompose inplemented. Just need to write tests for itMaybe you can compare with the version of decompose implemented here also, it looks like it's also working :)",1,False,2019-03-28 22:38:18,2019-04-09 14:20:56,2019-04-01 07:34:30
https://github.com/OpenMined/PySyft/pull/1994,[],[Issue 1456] add type annotations and address pylint errors in some files,"[Issue 1456] add type annotations and address pylint errors in some filesThis is work related to issue #1456  and also adds a few small changes that I uncovered while working on this:
    1) Adds type annotations to a few methods. Not 100% sure that the selected types are the most accurage ones, please advice if there is an issue.
    2) Data argument that gets passed to workers has as default value of {} but seems like in reality it's a list. Also it's not a good practice to use mutalbe object as a default value to a methof so changing it to None and handling this in the actual methot that loads the data.
    3) Addresses some small pylint errors.
    4) Reorders the imports to follow the PEP 8 guidelines.Decided to handle a `None` value here and avoid using a {} or [] as default argument value, which is not a good practice overall.Assumed this type looking at other places in the code where this argument is passed, let me know if I'm missing something.I hope this is the right type to specify here, although looks a bit too specific. Please suggest if there is a better way to handle that.I think this is correct",2,True,2019-03-14 14:25:12,2019-03-18 18:00:26,2019-03-18 18:00:26
https://github.com/OpenMined/PySyft/pull/1993,[],A few more type annotations,"A few more type annotationsI just added some few type annotations as I studied the codeYou can probably also add a type info for tags like :
```
from typing import List

def my_func(l: List[str]):
    pass
```same you can for tensor put  ~ `tensor: sy...AbstractTEnsor`maybe you want to add annotation for all arguments (whe it's not too painful) or for the whole file, this way people continuing the type annotation know they don't have to check everything
https://docs.python.org/3/library/typing.html can be a good resourcesame as above",3,True,2019-03-14 08:18:37,2019-03-27 08:57:50,2019-03-27 08:57:50
https://github.com/OpenMined/PySyft/pull/1983,[],Handling remote commands with complex response; fix #1975,"Handling remote commands with complex response; fix #1975Add a method to handle registration and sending back resp to client when a command is sent.
Support response with multi tensors; like `torch.split(ptr, 2)` which returns a tuple of pointers

fix #1975Is object really the best we can do for these?Switch to other doc style for consistencyI think tensor in this case should be a union of abstract tensor and torch tensorCan be tuple, tensor, tuple of int, str, tensor very variousrightSounds good",2,True,2019-03-09 15:18:05,2019-03-12 15:38:02,2019-03-12 15:17:31
https://github.com/OpenMined/PySyft/pull/1979,[],Removing **kwargs from FederatedDataLoader func,"Removing **kwargs from FederatedDataLoader funcIn the FederatedDataLoader function, `**kwargs` is passed as a parameter. 
> kwargs = {'num_workers': 1, 'pin_memory': True}

However, the DataLoader function implementation in Pysyft doesn't take `num_workers` and `pin_memory` as arguments as in pytorch. 

> **Hence when I tried running this below snippet **

> `federated_train_loader = sy.FederatedDataLoader( 
    datasets.MNIST('../data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ]))
    .federate((bob, alice)), 
    batch_size=args.batch_size, shuffle=True, **kwargs)`

> **I got the following error:**


`>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-29-f419dd3295b1> in <module>()
      5                        transforms.Normalize((0.1307,), (0.3081,))
      6                    ]))
----> 7     .federate((bob, alice)), batch_size=args.batch_size, shuffle=True, **kwargs)
      8 
      9 test_loader = torch.utils.data.DataLoader(
      TypeError: __init__() got an unexpected keyword argument 'num_workers'  `

Shouldn't `**kwargs` be removed from this function?",1,False,2019-03-06 19:54:42,2019-03-08 13:59:50,2019-03-08 12:43:59
https://github.com/OpenMined/PySyft/pull/1977,[],Type annotation,"Type annotationHere are some more type annotations. I was not very sure how to annotation Syft Tensors that are not PointerTensors. I ended up using `AbstractTensor`. 

For torch tensors I used `torch.Tensor`.

For annotating self type classes I used strings ""BaseWorker"" when annotating this type from inside the BaseWorker class. It seems to be the solution for python<=3.6. For python 3.7 and above, there is another solution by importing

`from __future__ import annotations` then self type annotation is supposed to work out of the box

Thanks for the reviewThis method does not return binary data (it raises an exception). For specific implentions (ie virtual worker) it returns either a pointer tensor or NoneCould you split these imports on to separate linesLike _recv_msg this does not return binary dataThis is clever since all of the syft tensor types subclass abstract tensor. Good job@robert-wagner The doc string for this method `recv_msg`  actually says:

`Returns:
A binary message response
`

Is that an error?
Nevermind you are correctIgnore this comment",4,True,2019-03-06 10:35:59,2019-03-08 09:30:14,2019-03-08 09:30:03
https://github.com/OpenMined/PySyft/pull/1965,[],Fix moving a module to cuda,"Fix moving a module to cudanew_data and native_param_data are not different device.
moving new_data to cpu before passing to native_param_data's set method

Fixes: #1893",1,False,2019-03-03 21:35:30,2019-03-04 10:32:52,2019-03-04 10:32:52
https://github.com/OpenMined/PySyft/pull/1964,[],Fix error in tutorial 2,"Fix error in tutorial 2I found two places to change while running the tutorial today. Please correct me if I am wrong.

+ It seems that `syft` does not have `optim`?

+ `opt.step` would not take `data.shape[0]` as input because `opt.step` should take some closure.
Doc: https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.step",1,True,2019-03-03 21:07:43,2019-03-04 13:56:21,2019-03-03 22:35:40
https://github.com/OpenMined/PySyft/pull/1938,[],Add draft version of a plan,"Add draft version of a planA Plan is a specific kind of worker, to which you give your commands, but which doesn't execute them until you specifically ask a response (through a .get() for instance), and then it execute all the commands in order and return the response. This is a we call a plan.
The idea is to send a ""plan"" ie all the instructions at once to a remote worker, while you still execute your code line by line like in PyTorch.
It uses the fact that now the worker that sends the command fix the id where the result is stored and therefore and can already create a pointer to a result which doesn't exist so far and go on with instructions. 


Here is how you make and use a plan
```
import syft as sy
import torch as th

hook = sy.TorchHook(th)
bob = sy.VirtualWorker(hook, id=""bob"")

#build ""locally""
@sy.func2plan()
def plan_a(data):
    return data.abs()

x = th.tensor([-1,2,3])
y = plan_a(x)

#build ""remotely""
@sy.func2plan()
def plan_b(data):
    return -data.abs()

plan_b.send(bob)
x_ptr = th.tensor([-1,7,3]).send(bob)
p = plan_b(x_ptr)
print(p.get())

plan_b.get()
plan_b.send(alice)

x_ptr = th.tensor([-1,2,3]).send(alice)
p = plan_b(x_ptr)
print(p.get())


```
From syft import serde?I really like this implementation. Maybe put a comment to make it clear that what this is doing/whyCould we split these into separate linesCould we change these to absolute imports while we are hereIs there ever a case where this will be guaranteed to be simplified ahead of time?I love this commentCan you add a docstring comment for detailPlease add a docstring for compressCan we do this absolute?Same for all below and above as wellDo we want to make this yield rather than return since we say it is a generatorWhy are these tests necessary?Good use of gender neutral languageAbsolute importIs there a good reason that plans are a subclass of workers? What functionality do they inherit?Should this just be a private class method? Is it ever called from outside this fileThis is a good interface. I like that these are callables as it is very intuitiveShould send return  a plan pointer?Why is this a mock function?Why is the real send private but the mock send public?You can write code that functions as either a decorator or just a normal function. We should do that here rather than having two functionsI agree with the session tag. I had some tests that were being weird without it. We should keep this in mindWe should delete these linesWhy are these commented outAhahah Andrew's poetry!you're right, I changed the nameWe don't want to add several times know workers -> generate warning and the debug of pytest gets really dirty with all the warning messagesThey catch the call to function to build plans which is crucial to the plan functionality
Right!Because the send is virtual, it is not a necessity but I agree this leads to confusionThe interest is that you can call .send() but actually this doesn't trigger a call to a remote worker, just ""a promise"" that it will be sent (with _send() ) as soon as the plan is called (and built)They are no longer valid behaviour we want to testI've removed themdone",2,True,2019-02-22 17:04:56,2019-04-02 14:18:53,2019-03-19 15:17:27
https://github.com/OpenMined/PySyft/pull/1914,[],Add serialization of torch.device to support hook of .to(device),Add serialization of torch.device to support hook of .to(device)Just a serialization of torch device. Issues with Gpu not solved,3,True,2019-02-18 09:03:43,2019-03-06 08:45:37,2019-02-19 14:55:14
https://github.com/OpenMined/PySyft/pull/1856,[],create_sandbox convenience functin and tutorial,create_sandbox convenience functin and tutorialglobal jon?change to jonjon?fixedfixedfixedThis isnt fixedadded,1,True,2019-02-03 17:51:25,2019-02-03 23:26:53,2019-02-03 23:26:53
https://github.com/OpenMined/PySyft/pull/1848,[],federated learning working but there are still some strange bugs about,federated learning working but there are still some strange bugs about,1,True,2019-02-02 00:53:15,2019-02-02 01:34:58,2019-02-02 01:03:47
https://github.com/OpenMined/PySyft/pull/1843,[],adds support for pointers to attributes on remote tensors,adds support for pointers to attributes on remote tensorsAlso adds automatic support for .grad pointermaybe you want to do `'::'.join(self.point_to_attr.split('.')`or replacetype(y) : wrapper>logging>tensorfixedüëç fixedRemove comments or uncommentfixed,1,True,2019-02-01 16:56:42,2019-02-01 17:19:50,2019-02-01 17:18:43
https://github.com/OpenMined/PySyft/pull/1841,[],change tests to not use class structure and fix some typos,"change tests to not use class structure and fix some typosin reference to https://github.com/OpenMined/PySyft/issues/1835This is a typo I assumeThere are too many quotes hereI'm not sure this is what we want to do: this involves massive duplicate code that self.setUp() did avoid previously.@robert-wagner what code design where you thinking of?In general we can use the hook from conftest and don't need to rehook for each test. This can be done by passing hook as a parameter to each method. Also it should make the tests run faster@LaRiffle we don't need to call hook, we should be passing as arguments to the tests the fixtures in conftest as neededPerfect that works for me then :)The general idea was to reduce code duplicate across the project by moving the functionality of setup to a common fixtureThanks for the feedback. Changes have been made.You can also use the same mecanism to load the workers (see in conftest the workers fixture)
This way it will be even more compact :)Oh yeah. On it!Wondering same can be done to other tests (files) ?Alot of them already have been. Feel free to update any that you seeWould be super cool to have this done in all the files! üíØ Will do it! :D",3,False,2019-02-01 13:30:20,2019-02-04 22:54:23,2019-02-04 22:54:23
https://github.com/OpenMined/PySyft/pull/1834,[],Polynomial Tensor Torch_1,"Polynomial Tensor Torch_1Starting Polynomial Tensor PR for Torch_1. Currently a work in progress with this being the first version. write this in functional syntax rather than class syntax since it makes cleaner codeis this method a part of the original tensor api? if not what is the purpose of this methodthis should not take another value and should just be called on self. Alternatively this could be made a class method and take a tensor instead of a floatsame as above, this should either operate on self or be a class method. Additionaly should other functions have the option to choose order or should we pick a standard order for the entire class?same as above as to operating on self or making a class methodsame as above as to operating on self or making a class methodThe error of polynomial approximations grows without bound the further you go from zero. It seems a bit missleading to say we have a maximum permissible error when that is not the casepolynomial tensor should be a subclass of AbstractTensor. Furthermore we probably should have a constructor which takes an order parameter which specifies the order of the polynomial approximation at definition time (with a sensible default of 4 or 5)Each test should get its own tensor to avoid side effectswe probably should be comparing to the torch.nn.functional definitions of these functions rather than the math ones as we are trying to preserve feature parity with torchwhy does test range vary between the tests (is there a reason we are testing this function over a smaller rangeLooking at it further, it seems to me that this should be a helper function that lives in the test fileditto as to test range question from aboveditto as to test range question from aboveditto as to test range question from aboveIt might give clearer error messages if we parameterize these rather than putting a for loop in the testNope Could we test the same range for each then?",1,False,2019-01-29 12:53:12,2019-02-15 16:27:31,2019-02-15 16:27:31
https://github.com/OpenMined/PySyft/pull/1830,[],Minor Changes,"Minor ChangesI made a few minor changes while playing around with Garbage Collection and getting to know some of the next pieces of functionality.

- added LoggingTensor to syft. classpath (so you can now do syft.LoggingTensor). Added a few comments there as well.
- modified tensor __str__ and __repr__ to be more informative when printing tensors with wrappers
- added an experimental notebooks folder with a README describing its purpose and the lifecycle of notebooks it containsShould this file be in this pr
",1,True,2019-01-28 19:33:26,2019-01-28 22:25:00,2019-01-28 22:24:36
https://github.com/OpenMined/PySyft/pull/1829,[],Improve Garbage Collection verifications,"Improve Garbage Collection verificationsReorganise test files and add one specific to GC: test.gc.py

At the end of this file, one can find test for the Logging Test. One is failing:

```
x = torch.Tensor([1, 2])

x_ptr = x.send(bob)
x_log = LoggingTensor().on(x_ptr)

assert x.id in bob._objects

del x_log
assert x.id not in bob._objects
```
However this works:
```
x = torch.Tensor([1, 2])

x_log = LoggingTensor().on(x.send(bob))

assert x.id in bob._objects

del x_log
assert x.id not in bob._objects
```
This is due to x_ptr init, which increases the reference of x_log to 3  (should be 2 if you use `sys.getrefcount(x_log)`).
Is this something that we want as a behavior ?",3,True,2019-01-25 10:41:00,2019-01-29 16:16:07,2019-01-28 16:32:11
https://github.com/OpenMined/PySyft/pull/1821,[],Add x.max() for fixp shared tensor,Add x.max() for fixp shared tensor,1,True,2019-01-17 16:52:47,2019-01-29 16:16:19,2019-01-17 17:26:03
https://github.com/OpenMined/PySyft/pull/1814,[],LogTensor and decentralized handling of commands,"LogTensor and decentralized handling of commandsAdd a LogTensor and change the way commands are handled:

Add a handle_command method in syft/torch tensor classes for methods:
Move the call handling from the hook to each tensor class to allow for specific behaviour depending of the class.
Add a symmetric architecture to rebuild the chain on top of the response once the chain has been completely un-wrapped (like a forward and backward logic)


Add a LogTensor

```
# build a long chain tensor
wrapper = torch.Tensor()
wrapper.child = LogTensor().on(torch.Tensor([1, 2, 3]))
x = wrapper
print(x)
# operate on it
y = x.add(x)
print(y)
```

TODO:
 - resolve some todos on importsWe should make these absolute imports (I think we can just delete each of the .)This should call __repr__ or vice versa to reuse codeThis functionality is in abstract tensorThis functionality is covered by abstract tensorWhy is this commented out?Do this in the hookI agree!Not exactly, the code is a bit different because here you call `native_`Again code is a bit different, that's why we overwrite the method `__str__`Because it's not working at the moment, as it's function not method and it's a bit more tricky in the new setting, but it will get back as the PR gets more mature.Yup as discussed with AndrewWill fix",1,True,2019-01-10 18:54:49,2019-01-15 12:49:14,2019-01-15 12:49:14
https://github.com/OpenMined/PySyft/pull/1808,[],[WIP] Async operations,"[WIP] Async operationsFixes #1659

- [x] Add async decorator implementation
- [ ] Make send operations async (c = a + b)  - see Issue #1809 for details
- [ ] Raise error when y = a + c is executed
- [ ] Turn off async mode - #1810
- [ ] Save y = a + b in key-value (c.id, serialized_command) command queue - #1811
- [ ] If new tensor is registered into worker._objects
- [ ] Check if tensor.id is blocking some operation, and release blocked commands",4,False,2019-01-06 16:34:43,2019-01-15 08:08:38,2019-01-13 21:54:03
https://github.com/OpenMined/PySyft/pull/1805,[],Torch 1 and (not) headless tensor chains - send/get - GC - ptr.method() - etc.,"Torch 1 and (not) headless tensor chains - send/get - GC - ptr.method() - etc.Continue addressing #1657
- Improve ownership on send / get tensors #1697 #1698
- Add GC from #1796* to solve #1701 (see also #1658)
- Add hooking for torch methods on pointers, following up on #1729

PR was first intended to explore chains without torch wrappers. This solution is simpler to write but makes PySyft less flexible for libraries using PyTorch. We're currently putting back the wrapper while keeping the features developed in the first setting.


### Typical examples
```
import syft as sy
import torch
import torch.nn.functional as F
hook = sy.TorchHook(torch)
me = hook.local_worker
alice = sy.VirtualWorker(hook, id=""alice"")
bob = sy.VirtualWorker(hook, id=""bob"")
```
**Fast remote op on pointers**
```
x = torch.Tensor([1, 2, 3, 4])
x2 = torch.Tensor([4, 3, 2, 1])
x_ptr = x.send(bob)
x2_ptr = x2.send(bob)
```
(Run it twice to see the speed-up)
```
%%time
y_ptr = x_ptr + x2_ptr
```
**torch module functional**
(Run it twice to see the speed-up)
```
%%time
F.relu(x_ptr)
```
**Garbage collection**
```
# Clean registries
bob._objects = {}
alice._objects = {}
x = torch.Tensor([1, 2, 3, 4])
x_ptr = x.send(bob)
x_ptr_ptr = x_ptr.send(alice)
print(bob._objects, alice._objects)
del x_ptr_ptr
print(bob._objects, alice._objects)
```

**Todo checklist:**
- [x] Handling duplicate send from same tensor, and chained send (`x.send(bob).send(alice)`)
- [x] Tests for the method hooking
- [x] module Functional hooking (like `torch.nn.functional.relu(ptr1)`)

**For next PRs**
 - Change function name with integer when sending messages
- module Functional hooking (like `torch.nn.Linear(2, 3)(ptr1)`)
- Functional hooking (like `torch.add(ptr1, ptr2)`)

This error name is unclearShould this commented out code be deleted or uncommentedSince is not needed in the headless setting should this be deletedSince this is not needed should it be deletedWhat situations would we not want to delete the tensor when the pointer is deleted?
I can definitely see the case where there are multiple clients that want to access the same data on a worker but would that data be sent from a client or simply added on worker startup? I also think this parameter should be renamed to something like garbage_collect_dataSee above comments on this parameterComment for torch.nn should be deletednit: import syft as sy?This is because of the way we send data.
In the previous version of pysyft we had a `private` attribute in the serialisation process to desambiguate two scenarios:
- *local* send a tensor to *bob* which must receive the tensor and not a pointer to it (send with `private = false`)
- *local* performs an op on a pointer at *bob*, and must receive a pointer, so bob send back the result with `private = true` (and in practice then the tensor is serialized without its data)

What we do instead is now for the case non-private to just pickle/ser the tensor and send it, and for the private case to create locally the pointer and send it (see base.py execute_command), partly because the built-in method for ser torch tensor keep all the data.

But of course if *bob* creates a pointer to his own result, and sends it back to *local*, then this pointer gets garbage collected and the result is deleted, which we want to avoid. That's why we add a property `dont_garbage_collect_target` (which is not serialized and sent to local)I'll rename itAgreed on the `garbage_collect_data` name",9,True,2019-01-02 22:47:37,2019-01-29 16:16:38,2019-01-09 17:30:16
https://github.com/OpenMined/PySyft/pull/1796,[],Initial Garbage Collection Support,"Initial Garbage Collection SupportThis PR does several things:
- Fixes https://github.com/OpenMined/PySyft/issues/1701
- moves the MSG_TYPE codes from the top of base.py to it's own codes.py file (to eliminate circular dependency issue)
- removed objects.py which wasn't being used
I really like this solutionIt is good that we are testing thisWe should add a test for garbage collection of a pointer to a pointerCan you add a comment describing the file, and a brief comment defining the class?`from syft import codes` is preferable, but since the imports in other files do not follow this I suggest creating a bug and fix it later.Since we're only using msgtype, I feel like this is good for nowFYI: @robert-wagner and @mari-linhares: I'm working on getting GC for pointer->pointer->tensor working. It's been a lot more trouble than I expected.(I also didn't work on the 25th/26th of Dec... shameful I know) What problems are you having with pointer to pointer?",4,False,2018-12-24 20:34:10,2019-01-09 20:16:52,2019-01-08 15:31:53
https://github.com/OpenMined/PySyft/pull/1793,[],Added AbstractTensor.serlialize() convenience function w/ tests; added inline documentation,"Added AbstractTensor.serlialize() convenience function w/ tests; added inline documentationFixes https://github.com/OpenMined/PySyft/issues/1694

This PR does a few things.

1) it adds a new .serialize() convenience function to all tensors (by adding it to AbstractTensor). I added 2 unit tests for this function.
2) it removed the STRING based configuration of compression - opting instead for an integer based one (a general design decision we're optimizing for in Torch 1.0 to make things faster and more compressed). I modified previous unit tests to adjust (also fixed a style issue where compress_scheme was camel-caps for some reason)
3) it added a bit of inline documentation as I was going.
4) a few minor style fixes (removed unused imports)
5) added a new error class CompressionNotFoundException if you try to use a compression
code that doesn't exist (previously if you didn't specify one or you specified one that didn't
exist, it would just pick one at random).
Not a priority, but maybe make this an enum?Missing period.Empty lines between args are not needed.Empty lines between args are not needed.Missing period.Can you add a comment detailing what this is testing? What's the difference between this test and `test_hooked_tensor`?We've debated this a little bit - apparently enums are a bit less efficient for (more or less) the same interface (@LaRiffle did a few tests on the 0.3.1 version). Interesting, thanks for explaining.Put these on separate lines or the entire def on 1 lineDo these as absolute importsDo these as absolute inportsWhich one can't it find?
Also these type of comments belong in the slack or as an issue rather than a comment in the codefixedfixedfixedfixed i think - by ""dot"" you mean ""period"" right?good calldonefixeddonefixeddoneSorry, correct. Fixed in the other comments.",1,True,2018-12-24 04:59:18,2018-12-24 18:33:12,2018-12-24 18:33:12
https://github.com/OpenMined/PySyft/pull/1792,[],Fix #1785; Modify documentation generation code to use napoleon,Fix #1785; Modify documentation generation code to use napoleon,1,True,2018-12-23 14:27:37,2018-12-23 14:56:49,2018-12-23 14:56:49
https://github.com/OpenMined/PySyft/pull/1780,[],WIP: Serde Logic for PointerTensor,"WIP: Serde Logic for PointerTensorDocumentation for `fail_hard` argument is missing. I think we switched `TestCase` to object. Pytest does not require `TestCase`, but `TestCase` is disallowing the use of parametrizing methods. `TestCase`: see aboveIs this import really needed?Yeah because I want the local_worker and thus call `syft.local_worker.get_worker(val_str, fail_hard=True)`",2,True,2018-12-20 13:59:56,2018-12-22 13:57:45,2018-12-22 13:57:45
https://github.com/OpenMined/PySyft/pull/1752,[],Torch 1749 serde posthook,"Torch 1749 serde posthookThis is based on #1746. The test should be generally fine, but on current `torch_1`the test will fail as the torch functionality is broken.

Can be merged based on approval of #1746.",6,False,2018-12-06 15:41:23,2018-12-10 11:49:13,2018-12-10 11:49:13
https://github.com/OpenMined/PySyft/pull/1736,[],Fix the search function accordingly to ex/torch/toy sockets workers,"Fix the search function accordingly to ex/torch/toy sockets workersFix:
https://github.com/OpenMined/PySyft/blob/master/examples/torch/toy/SocketWorker%20Server.ipynb
https://github.com/OpenMined/PySyft/blob/master/examples/torch/toy/SocketWorker%20Client.ipynb

Which underlines that worker.search(#dataset)  is broken",1,True,2018-11-29 13:48:31,2018-11-29 17:33:54,2018-11-29 16:13:06
https://github.com/OpenMined/PySyft/pull/1728,[],Fix typos in introduction,Fix typos in introduction,1,True,2018-11-26 00:12:40,2018-12-02 15:36:54,2018-12-02 15:36:54
https://github.com/OpenMined/PySyft/pull/1702,[],Solved: Fix travis ci build process #1685,Solved: Fix travis ci build process #1685Adjusted some config files and reformatted the code to pass travis,1,True,2018-11-17 09:46:03,2018-11-18 18:59:30,2018-11-18 18:59:30
https://github.com/OpenMined/PySyft/pull/1692,[],"In `handle_call` for the ""end_get"" case need to call get() on a _SyftTensor","In `handle_call` for the ""end_get"" case need to call get() on a _SyftTensorCurrently it's called on a `dict` object and calling `end_get` in tutorial 3 generates an error because `dict` does not have a `get()` method with no arguments.

Adds a test that demonstrates a scenario in which the code currently fails.I realize this may not be the best solution but given my limited knowledge of the codebase this is what I came up with. Will be happy to modify, but at least I think we have an issue that needs resolving here, one way or another.I literally tried to solve this yesterday and if your solution works it's VASTLY superior! Well done! (will check in a bit)",2,True,2018-11-16 13:35:29,2018-11-19 18:51:15,2018-11-19 15:52:08
https://github.com/OpenMined/PySyft/pull/1684,[],Solved: Add serde logic for np.ndarray #1670,"Solved: Add serde logic for np.ndarray #1670 added logic for np.ndarray serialization and deserialization
 added unit test demonstrating proper serde of np.ndarray objects
 added docstrings for all methods describing their functionality including an example of how to serialize/deserialize it.",1,True,2018-11-15 15:59:59,2018-11-15 19:52:06,2018-11-15 19:51:51
https://github.com/OpenMined/PySyft/pull/1681,[],Solved: Add serde logic for range #1664,"Solved: Add serde logic for range #1664Added the logic (if necessary) for range serde.

Added a unit test

Added add docstrings for all methods describing their functionality including an example of how to serialize it",1,True,2018-11-15 09:24:30,2018-11-15 11:03:02,2018-11-15 11:03:02
https://github.com/OpenMined/PySyft/pull/1653,[],[WIP] Fix websocketworker,"[WIP] Fix websocketworkerI've created this pull request to bugfix branch, because I want to easily share code with Ogofo and Nivek92 who expressed interest in issue #1642. This code is not ready for master. Can someone create a separate branch for this maybe? I will then set it in this PR.

I believe the reason why there are so few tests is because this TorchHook solution (possibly combined with the way we write/run tests) makes adding new tests much harder for people. Running tests produce lots of  `Torch was already hooked... skipping hooking process` and `Replacing old worker which could cause unexpected behavior` warnings. In my case I'm adding a correctly failing test (TestWebSocketWorker) and when I run it with `python3 setup.py test` I see different error messages than when I test the same code manually (in a repl) or with
**`python -m unittest -v test.core.workers_test.TestWebSocketWorker`**

I understand the TorchHook solution is waiting for improvement. Meanwhile I tried to find a fix/workaround for tests. I experimented with setUp/tearDown and modules reloading, but wasn't successful.

So far I've put almost all effort into the test and I don't know how hard the complete fix may be.",2,True,2018-11-03 00:00:59,2018-11-14 13:46:18,2018-11-14 13:45:02
https://github.com/OpenMined/PySyft/pull/1650,[],Fix flake 8,Fix flake 8,1,True,2018-10-30 05:36:45,2018-11-09 22:17:44,2018-11-02 11:18:26
https://github.com/OpenMined/PySyft/pull/1646,[],Fix for issue #1645.,"Fix for issue #1645. Now correctly deregistering all objects that belong to a torch.Variable.

Example Code is attached to issue #1645. I additionally fixed some typos within the comments as well.",1,True,2018-10-29 12:56:08,2018-10-29 19:56:55,2018-10-29 19:56:55
https://github.com/OpenMined/PySyft/pull/1644,[],Update MNIST Example.ipynb,Update MNIST Example.ipynbfix an import problem caused by TorchHook which has been moved into frameworks.torch,1,True,2018-10-27 07:08:44,2018-10-27 11:09:48,2018-10-27 11:09:48
https://github.com/OpenMined/PySyft/pull/1643,[],Wrong import path fixed.,Wrong import path fixed.import syft.core fails as the torch folder is in the frameworks subfolder,1,True,2018-10-24 10:32:36,2018-10-25 13:19:19,2018-10-25 13:19:00
https://github.com/OpenMined/PySyft/pull/1640,[],Fix regexp for black exclude option in pyproject.toml,Fix regexp for black exclude option in pyproject.tomlWhitespaces in regexp inside pyproject.toml shouldn't be there. Maybe now travis will build fine? ,2,True,2018-10-19 18:03:22,2018-10-23 17:39:40,2018-10-23 17:39:40
https://github.com/OpenMined/PySyft/pull/1634,[],fixed the relative imports issue #1631,fixed the relative imports issue #1631#1631 all previous relative imports are now absolute,1,False,2018-10-14 19:17:43,2018-11-02 11:18:42,2018-11-02 11:18:42
https://github.com/OpenMined/PySyft/pull/1630,[],[WIP] Add profiling in BaseWorker.send_msg,"[WIP] Add profiling in BaseWorker.send_msgHello,

Here's what I have for now (#1602, first issue) and I need some feedback.
The method calls are profiled and basic info is stored in a log file.

I don't know how exactly you'd like to use the stats and what would be
most helpful.
Also I test my code by running PySyft tests (python3 setup.py test) which
may be giving me a wrong idea about how the stats will look in ""real use""
(e.g. I don't see any ""numpy_cmd"" type messages being sent during the tests).

1. I believe instead of logging individual calls like in this code,
   it would be better to have grouping and summaries. I don't know
   what grouping would be useful to you and when to start/stop collecting
   data for total stats. I could create functions to start and stop profiling
   with the idea to call them in a repl or notebook, but I don't know
   if it's good for you.
   Some total statistics can already be gathered from the log with
   grep, cut, wc etc. e.g.
   `less send_msg_profiling.log  | cut -f2,3 | sort |  uniq -c`
   One thing to remember is to rm or mv the log file whenever old entries
   become irrelevant.

2. I don't know what parts of message are relevant for the stats/logs.
   I currently save (message_type:mode) or (message_type:integer_val)
   when the message is not a dict e.g. ""req_obj:81697078720"". I think
   ""req_obj:int"" would be better(?).

3. The completion time is currently shown in milliseconds and I wonder if this
   is the right time unit. I see mostly fractions of milliseconds, but
   I imagine in ""real use"" the times will be much higher.

4. I have put a PROFILE_MODE flag in profiling.py file. It should probably
   go somewhere else if it's supposed to be manually set.

5. Is there an easy way to have something more similar to ""real use""
   than what I'm doing (running ""python3 setup.py test"")?",2,True,2018-10-14 10:17:18,2018-10-23 17:45:06,2018-10-23 17:44:05
https://github.com/OpenMined/PySyft/pull/1612,[],"Fixes #1592, should replace all modulos (%) with `torch.fmod`","Fixes #1592, should replace all modulos (%) with `torch.fmod`As spec'd in #1592, all modulos (%) need to be replaced with `torch.fmod` when seeking remainder on `torch.Tensor`s.

I believe I've covered these, but I'm not 100% sure on testing these, please advise. üôÇ",4,True,2018-10-10 02:45:39,2018-10-11 23:41:07,2018-10-11 18:01:58
https://github.com/OpenMined/PySyft/pull/1584,[],Pandas project,"Pandas project# Description

Made a pandas Series and DataFrame serializer to JSON.  

## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [x] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules
",2,False,2018-10-02 08:48:36,2018-10-11 18:02:13,2018-10-11 18:02:13
https://github.com/OpenMined/PySyft/pull/1583,[],Update ISSUE_TEMPLATE.md to fix issue #1549,"Update ISSUE_TEMPLATE.md to fix issue #1549# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules
This whole `Test Configuration` section can probably be replaced simply with operating system type/version (windows, linux distro, mac os) and PyTorch and/or TensorFlow versions.This could be rephrased -- maybe ""Please describe expected behavior and how it differs from current behavior.""This should probably be a separate issue/PR, but we should probably have two separate issue templates for bugs and features (since those are by far the two most common issue types).  Would you be able to open up an issue for this?Sure, I'll open an issue for this and perhaps work on it myself.",1,True,2018-10-01 02:41:03,2018-10-02 11:10:29,2018-10-01 12:20:56
https://github.com/OpenMined/PySyft/pull/1580,[],Math operations for fixedTensors added + Unit Tests,"Math operations for fixedTensors added + Unit Tests# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes #1532 

## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",5,True,2018-09-29 13:56:21,2018-10-05 02:42:57,2018-10-04 19:11:38
https://github.com/OpenMined/PySyft/pull/1578,[],Mathematical ops for FTensors added,"Mathematical ops for FTensors added# Description
This PR will add all the mathematical operations to Fixed Tensors.

Fixes # (issue)
#1532
## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update",2,False,2018-09-28 14:27:01,2018-09-29 09:04:14,2018-09-29 09:04:14
https://github.com/OpenMined/PySyft/pull/1570,[],Rename Pointer -> Pointer -> Data Example.ipynb to Pointer_Pointer_Da‚Ä¶,"Rename Pointer -> Pointer -> Data Example.ipynb to Pointer_Pointer_Da‚Ä¶‚Ä¶ta Example.ipynb

# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules
",2,False,2018-09-27 15:54:35,2018-09-27 15:57:43,2018-09-27 15:57:07
https://github.com/OpenMined/PySyft/pull/1568,[],Sigmoid,"SigmoidThis does not work because when multiplying tensors by themselves we get a ('All arguments should share the same child type.', [<class 'syft.core.frameworks.torch.tensor.LongTensor'>, <class 'syft.core.frameworks.torch.tensor._MPCTensor'>]) error. I have no clue how to debug this in the current pysyft. @iamtrask @LaRiffle If you could take a look at it that would be great. This is the last steplooks like a merge conflict in your ipynb üò¨ ",2,False,2018-09-27 03:37:39,2018-10-30 12:35:19,2018-10-30 12:34:55
https://github.com/OpenMined/PySyft/pull/1566,[],[WIP] Add Federated Learning and Differential Privacy demos,"[WIP] Add Federated Learning and Differential Privacy demos# Description

Adding demos combining Differential Privacy and Federated Learning 

Fixes #1556 

## Type of change

- [x] New feature (non-breaking change which adds functionality)
- [x] This change requires a documentation update

# How Has This Been Tested?

Not yet **(WIP)**

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",6,True,2018-09-26 10:53:35,2018-11-02 11:19:40,2018-11-02 11:19:40
https://github.com/OpenMined/PySyft/pull/1561,[],Original state issue 1532,"Original state issue 1532# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",1,False,2018-09-25 16:43:58,2018-09-28 10:33:16,2018-09-28 10:33:15
https://github.com/OpenMined/PySyft/pull/1560,[],adds getting workers from gpt,"adds getting workers from gpt# Description

Regarding the following issue: 
https://github.com/OpenMined/PySyft/issues/1548

Adding method to return list of workers from GeneralizedPointerTensor.

Fixes # (issue)

## Type of change
Adds method `workers` to `syft/core/frameworks/torch/tensor.py` L:749  

Please delete options that are not relevant.

- [ ] New feature (non-breaking change which adds functionality)

# How Has This Been Tested?

Adding unittest in `test/torch_test.py` L:1121

**Test Configuration**:
* CPU: Mac
* GPU: None
* PySyft Version: master
* Unity Version: None
* OpenMined Unity App Version: None

# Checklist:

- [ ] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",5,True,2018-09-25 09:09:58,2018-09-25 22:27:22,2018-09-25 14:38:59
https://github.com/OpenMined/PySyft/pull/1554,[],Issue1508 rename mpc tensor to spdz tensor,"Issue1508 rename mpc tensor to spdz tensor# Description

Refactoring of MPCTensor to SPDZTensor(including notebooks)

Fixes # (issue)
- https://github.com/OpenMined/PySyft/issues/1508
## Type of change

Please delete options that are not relevant.

- [x] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- python setup.py test
- executed updated notebooks

**Test Configuration**:
* CPU: intel i7 4790k
* GPU: titan xp
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",1,True,2018-09-24 02:37:22,2018-09-24 17:26:09,2018-09-24 12:15:31
https://github.com/OpenMined/PySyft/pull/1552,[],PATE with PyTorch ,"PATE with PyTorch Hey @iamtrask , 

#1547 

This PR add an implementation of PATE with PyTorch. I will have to clean up a bit the code later but everything seems to work fine. 

Thank you!


## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [x ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",1,True,2018-09-23 17:42:43,2018-09-23 17:56:39,2018-09-23 17:56:39
https://github.com/OpenMined/PySyft/pull/1551,[],Variable x MPC FixedPrecision (send / get / addition),"Variable x MPC FixedPrecision (send / get / addition)# Description

Support of send / get / addition on remote, shared, fix precision tensor.

Beware that valid (and tested) orders for composing are:
```
x = x.send(bob).fix_precision()
x = x.get().decode()
# or
x = x.fix_precision().share(alice, bob)
x.get().decode()
# or 
x = x.send(bob).fix_precision().share(alice, bob) 
 x.get().get().decode()
```
Next PR will address:
- Multiplication (and mat mul)
- Other composition schemes

## Type of change

- [x] New feature (non-breaking change which adds functionality)

# How Has This Been Tested?

<not tested yes>

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
",1,True,2018-09-23 12:43:46,2018-09-27 18:44:54,2018-09-27 18:44:54
https://github.com/OpenMined/PySyft/pull/1543,[],[WIP] Context session,"[WIP] Context sessionReferences Issue #1542

## Description

We introduce the notion of Session. A session is a global object which holds information about the context in which  computations should be performed. For example, it can say that the computations should be sent to worker Bob, or parallized across workers, secured using MPC, logged, etc.). As for now, we only support sending computation to a single worker.

This notion is introduced to move the context information which were previously enclosed in syft chains. For example, the PlusIsMinusTensor (a tensor informing that all `add` op touching it should become `sub`) could presumably be more clear as a PlusIsMinusSession, same for a LogSession in some contexts.

The other interesting benefits are that tensors involved in a remote session will be sent for the duration of this session et fetch back at the end automatically, and all remote pointers created in this processed are get() and destroyed after the session has ended (the pointers existing before the session are not altered), which makes echo to the issue of stale remote data.

This notion of Syft sessions complements the notion of Syft tensors on the notion of contexts of computation. Syft Tensor are still supposed to represent tensor transformation such as sending, or sharing for instance, while sessions represent transformations on computations.

## Examples

### Example 1 (working)
```
x = sy.FloatTensor([1.2])

#x.send(bob) #<- This is what you don't need to do

with sy.session(bob):
    # Here all the commands and ensors are sent to bob
    z = torch.add(x, x)

#z.get() or x.get() #<- This is what you don't need to do

print(z) # This is FloatTensor
```
Output:
```
INFO:Session started with bob, (secure: False)
INFO:bob:Executing cmd torch.add
INFO:Closing session

 2.4000
[syft.core.frameworks.torch.tensor.FloatTensor of size 1]
```

### Example 2 (working)

```
x = sy.FloatTensor([1.1])
x.fix_precision()
#x = x.eval() <- If you don't eval, no pb the fixed_precision will be set only in the session (try uncommenting)
chain_print(x)

with sy.session(bob):
    # Here all the commands and ensors are sent to bob
    z = torch.add(x, x)
    
chain_print(z)
print(z)
```

### Example 3 (dream)

```
# workers connected are (alice, bob, charlie) who compute things, 
# and health_corp who has awesome ML models

data = Dataset('my-3D-heart-image')

with sy.MultiPartySession(parties=(bob, alice, charlie)):
    model = health_corp.search('#heart-disease-model')  # model is searched at health_corp
    # which returns a MPCTensor shared between (bob, alice, charlie)

    pred = model(data) # data is automatically is shared with MPC, and the result 
    # pred is also a MPCTensor
    
    # End of session: try to fetch model and pred: succeeds depending of the 
    # privacy of the tensor.
    # For instance, model.get() fails as I don't own the model, but pred owner 
    # is the data owner, it's me, so pred.get() succeeds.
    
print(pred) # OMG I have no heart disease!
```

## Type of change




- [x] New feature (non-breaking change which adds functionality)
- [x] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [x] This change requires a documentation update


## Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",7,False,2018-09-22 09:04:49,2019-01-29 16:17:02,2018-11-20 10:50:27
https://github.com/OpenMined/PySyft/pull/1537,[],[WIP] Federated learning with tensorflow demo,"[WIP] Federated learning with tensorflow demoFederated learning demo with VirtualWorkers and tf estimator API

# Description

Added tensorflow demo with tensorflow's estimator API and VirtualWorkers.

Fixes   #1522 

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",6,False,2018-09-21 11:37:33,2018-11-20 10:49:54,2018-11-20 10:49:53
https://github.com/OpenMined/PySyft/pull/1523,[],Added keras examples and changed optimizer to work with keras,"Added keras examples and changed optimizer to work with keras# Description

Add keras tensorflow examples and correct custom optimizer to work with ResourceVariables used by keras.

Fixes [# 1521](https://github.com/OpenMined/PySyft/issues/1521)

# Checklist:

- [ ] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",2,True,2018-09-20 05:06:26,2018-09-20 10:08:12,2018-09-20 10:08:12
https://github.com/OpenMined/PySyft/pull/1476,[],"Format all html,css and js files in docs folder","Format all html,css and js files in docs folder# Description

Formated html, css, js files.
```
docs/_build/html/_autosummary/syft.core.frameworks.torch.html
docs/_build/html/_autosummary/syft.core.utils.html
docs/_build/html/_autosummary/syft.core.workers.html
docs/_build/html/_modules/index.html
docs/_build/html/_modules/syft.core.frameworks.html
docs/_build/html/_modules/syft.core.frameworks.torch.html
docs/_build/html/_modules/syft.core.html
docs/_build/html/_modules/syft.core.workers.html
docs/_build/html/_modules/syft.html
docs/_build/html/_modules/syft.mpc.html
docs/_build/html/_modules/syft.mpc.interface.html
docs/_build/html/_static/basic.css
docs/_build/html/_static/doctools.js
docs/_build/html/_static/searchtools.js
docs/_build/html/_static/websupport.js
docs/_static/css/PySyft_docs.css 
```
## Fixes # (issue)
1) remove multiple empty newlines.
2) Format script tag and code inside html files
3) indent all the files equally.
4) warp tag attributes.
5) preserve new lines whenever necessary . eg. [""head"", ""body"", ""/html""].
6) Add a newline between multiple selectors
7) increase readability and standards

# Type of change: Format Templates, stylesheets and javascript files.

# How Has This Been Tested?
- [y] Test A
open PySyft/docs/_build/html/index.html in browser,chrome prefered.

**Test Configuration**:
* CPU: intel i7
* Python version: 3.6 
* PyTorch version: 0.3.1

# Checklist:

- [y] My code follows the style guidelines of this project
- [y] I have performed a self-review of my own code
- [y] I have commented my code, particularly in hard-to-understand areas
- [n] I have made corresponding changes to the documentation
- [y] My changes generate no new warnings
- [n] I have added tests that prove my fix is effective or that my feature works
- [y] New and existing unit tests pass locally with my changes
- [n] Any dependent changes have been merged and published in downstream modules",1,True,2018-09-09 09:17:15,2018-09-17 19:06:48,2018-09-17 19:06:48
https://github.com/OpenMined/PySyft/pull/1471,[],Added custom optimizer for federated averaging,"Added custom optimizer for federated averaging# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Adding a custom optimizer that wraps the logic for federated averaging. Usage is very similar to that of [tf.train.SyncReplicasOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer)

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [ ] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules
",1,False,2018-09-07 21:14:15,2018-09-08 11:25:10,2018-09-08 11:25:10
https://github.com/OpenMined/PySyft/pull/1458,[],Mpc tensor,"Mpc tensor# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules
The attributes for the new _GeneralizedPointerTensor should be either created or pulled from result_dict. 

parent - Normally, the ""parent"" would come from the high level torch object wrapping the result that's returned to you, but since you'll have multiple, I suppose the only thing to do is to pick one at random. 

torch_type - you should be able to get this from any of the answers in result_dict, since they should all be identical (optional: a check to ensure that they are in fact identical)

id - this should be initialized by _GeneralizedPointerTensor automatically when it calls super().__init__ on line 596. Might be good to check that it is though.

skip_register - i think setting this to False makes sense... (which you have done)@LaRiffle feel free to commentJust one point regarding the ""parent"", the usual case would be, as far as I can imagine
```
x = sy.FloatTensor([..])
x = sy._MPCTensor().on(x)
x.send([alice, bob])
```
And _MPCTensor would use under the hood the _GeneralizedPointerTensor. Since the parent of _MPCTensor is clear, _GeneralizedPointerTensor should inherit the same one I think.
Agreed - although I think that in the specific line i was referring to... a method is being called generating a new _GPT class which doesn't necessarily have a parent yet.Perhaps polynomial approximations of function should go in its own tensor since it's highly likely that many tensors will share them? (other MPC tensors, any homomorphic encryption tensor we make, etc.)",2,True,2018-08-29 14:34:46,2018-09-18 19:54:40,2018-09-18 19:54:40
https://github.com/OpenMined/PySyft/pull/1457,[],[Pointers are tensors] Improve the get_pointer_to func: Add a pointer referencing system,"[Pointers are tensors] Improve the get_pointer_to func: Add a pointer referencing system# Description

Add a light pointer referencing system to be able th check very quickly if we already have a pointer to (location, id@loc)
Add minor fixes related to executing a fed learning task, and fixing the hook on backward 

## Type of change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)

# How Has This Been Tested?
Unittest
# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
",1,True,2018-08-28 17:48:03,2018-08-28 17:52:24,2018-08-28 17:52:17
https://github.com/OpenMined/PySyft/pull/1455,[],Strings id + Unittest ,"Strings id + Unittest # Description

I modify the custom_obj_hook method of PythonJSONDecoder in order to allow string id.

Fixes #
Before my change, remote operations on objects with string was throwing exception on server side and empty responses on client side.

## Type of change

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] Test FloatTensor with int id
- [x] Test FloatTensor with string id
- [x] Test Variable with int id
- [x] Test Variable with string id
- [x] Test Parameter with int id
- [x] Test Parameter with string id
- [x] Test DoubleTensor with int id
- [x] Test DoubleTensor with string id
- [x] Test HalfTensor with int id
- [x] Test HalfTensor with string id
- [x] Test ByteTensor with int id
- [x] Test ByteTensor with string id
- [x] Test CharTensor with int id
- [x] Test CharTensor with string id
- [x] Test ShortTensor with int id
- [x] Test ShortTensor with string id
- [x] Test IntTensor with int id
- [x] Test IntTensor with string id
- [x] Test LongTensor with int id
- [x] Test LongTensor with string id
- [ ] Test tuple
- [ ] Test set
- [ ] Test bytearray
- [ ] Test range
- [ ] Test slice

**Test Configuration**:
Docker container of the project

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
",1,True,2018-08-28 09:58:15,2018-08-28 13:00:03,2018-08-28 12:59:38
https://github.com/OpenMined/PySyft/pull/1453,[],set up precommit and fixed flake8 compliance issues,"set up precommit and fixed flake8 compliance issues# Description

Gets precommit to work (by entering a venv and running pre-commit install)
Enforces many style decisions before commits can be run

I am sorry for how many lines in this change there are. Feel free to take your time. This change should make these types of pull requests unnecessary in the futurehm, why this change? guessing it's a flake8 thing but seems curiouswish flake8 handled situations like this better üòï Should this be an AttributeError?  Not sure what other exception this could besee abovethis should be made consistent with the one below (`syft/core/workers/base.py` L7)hmmm thinking this should either be an AttributeError, RuntimeError, or bothOne import per line is pep8 actuallyThat is the best way to do it and still be valid python. RipMeant to add comments asking for reviews on those. I'm not sure what they throw but it should be more specific. There are 3 that I fixedAttributeError works hereWill replace all with attribute error in the morningCool, in that case, the file mentioned above should be changed as well.Oh, actually pep8 says otherwise:
```
Imports should usually be on separate lines:

Yes: import os
     import sys

No:  import sys, os

It's okay to say this though:

from subprocess import Popen, PIPE
```

Since we're importing these from the same module, looks like we're fine with how it was before.Will fix. My import fixer hook was having issues so I will try to debug and add back in the futureSeparate lines is better for merge conflicts thoughIf my only merge conflicts are in import statements, I'm a happy man :)  Happy to hear what the others think about it.

Also worth noting that TF/PyTorch/most open-source python projects I've seen do one line per module.Probably RuntimeError here now that I think about it...I don't have a problem with changing it back would love to hear other opinionsWill fix@jvmancuso could you write a test for this to make sure we are handling it properly when we would go down the exception pathRuntimeError here would just mean that `obj` is a Tensor.  In that case we don't need to worry about the data attribute being garbage collected, so I'm not sure if there would be anything to test",2,True,2018-08-15 01:45:33,2018-10-10 14:47:48,2018-08-15 16:20:33
https://github.com/OpenMined/PySyft/pull/1437,[],Style guide adopted,"Style guide adopted# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules
",1,True,2018-07-30 17:11:51,2018-07-30 19:29:30,2018-07-30 19:29:14
https://github.com/OpenMined/PySyft/pull/1432,[],[Pointers are tensors] Playing with PlusIsMinus Tensor : full support on Torch Tensors,"[Pointers are tensors] Playing with PlusIsMinus Tensor : full support on Torch Tensors# Description
Enable support on functions and methods on local and remote tensor non-trivial chains (ie including a PlusIsMinusTensor) 

The process chain is a bit changed:
The idea that we should always send a wrapper on even a partial chain is relativised: we still do this when sending commands / objects to another worker, because it is useful for a worker receiving an object for instance to have directly the wrapper available (just as it is convenient wor the local_worker, i.e. us); but within a local chain (ie Float > _PlusIsMinus > _Pointer), the intermediate nodes don't need the wrapper to execute correctly, and this enable to more easily overload methods as well, for the head element (of e.g. command['self']) is of the same type than the class in which handle_call is performed.

## Type of change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [x] This change requires a documentation update

# How Has This Been Tested?

Unittest

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
",1,True,2018-07-30 08:21:35,2018-07-30 09:20:45,2018-07-30 09:20:45
https://github.com/OpenMined/PySyft/pull/1424,[],Created a basic class for pandas,"Created a basic class for pandas# Description
This tries to implement the same thing that the pytorch tensors in PySyft do but with pandas data frames. This right now is a very early example of the classes. The Pandas functions are yet to be hooked and the worker class too needs a lot of work.

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [x] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration


# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",1,True,2018-07-24 14:57:27,2018-07-24 20:18:44,2018-07-24 20:18:25
https://github.com/OpenMined/PySyft/pull/1423,[],Add Docker Container,"Add Docker Container# Description

Add a Docker container which allow to run a notebook PySyft ready in one line.
The container is based on ubuntu:18.04, python3.6, torch 0.3.1 CPU (See Dockerfile for more details).

## Type of change

- [x] New feature (non-breaking change which adds functionality)
- [x] This change requires a documentation update

# Checklist:

- [ ] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",1,True,2018-07-24 14:41:19,2018-07-25 11:43:47,2018-07-25 11:43:47
https://github.com/OpenMined/PySyft/pull/1408,[],Update guard.py,"Update guard.py# Description

Was a #TODO in syft/core/hooks/torch
Receives a list of json objects and checks if it is able to create a torch tensor. If it is not it throws an AssertionError.

It throws error when
> The consecutive tensors that are meant to be stacked are not of equal length
> When the variable passed is not a list or a tuple
> When the lost contains datatypes other than integer and float

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

# How Has This Been Tested?
syft was reinstalled with setup.py with my core in it.
##Example
>>> import json 
>>> import syft
>>> from syft.core.hooks.torch.guard import TorchGuard
>>> Guard = TorchGuard()
>>> a = json.dumps({'Tensor':[[1,2,3,4],[1,2,2,2]]})
>>> b = json.dumps({'Tensor':[[1,2,3,4],[1,2,3,4]]})
>>> c = [a,b]
>>> Guard.tensor_contents_guard(c)
['{""Tensor"": [[1, 2, 3, 4], [1, 2, 2, 2]]}', '{""Tensor"": [[1, 2, 3, 4], [1, 2, 3, 4]]}']

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

https://imgur.com/a/NSV12w4

- [x] Test A
- [x] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules
Can you add a space before the word ""Throws"" here please?Can we add `bool` here?  It's possible to construct a binary tensor of integers of various bitwidth, e.g. `torch.ByteTensor([[True, False],[False, True]])` should have the same behavior as `torch.ByteTensor([[1, 0],[0, 1]])`, and similar for IntTensor/LongTensor.Please get rid of this before merging -- better to raise the exception if someone feeds it a bad json object.  If this is meant to be used for duck-typing, it's probably best to figure out a way to differentiate the specific cases you're trying to handle.  Maybe this means catching specific Exception types -- for example, if it's not a JSON object, pass and continue to the contents guard logic below, but if it _is_ a JSON object and it's just not loadable with `json.loads`, propagate that exception up.  This way, the function won't silently fail, or perhaps will just fail earlier (which is preferable).

You can also switch out the try/except for an if/elif/else block.  If that's more readable, that's likely best, although sometimes the try/except duck-type style is easier to read/understand.I believe this `break` is extraneous, and should probably be removed.Yup, trying this.Oh, also, can you fix `ojects` to `objects`?",4,False,2018-07-19 12:49:15,2018-08-30 15:07:59,2018-08-30 15:07:50
https://github.com/OpenMined/PySyft/pull/1400,[],Unit tests for multi output methods were added,"Unit tests for multi output methods were added# Description
Methods for multi-output methods were added
Issue #1385 

## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules
Maybe you can now remove the test `test_torch_function_with_multiple_output_on_remote_var` which is not as complete as yours :)
Also, I think `test_torch_function_with_multiple_input_on_remote_var` is also no longer needed tooHaha, I'm sorry. I was on an older branch and when I merged to submit a PR I didn't notice your unit tests. I will actually keep those untouched and add some more tests for Variables since you're testing the variables.",1,True,2018-07-14 17:29:22,2018-07-16 11:20:55,2018-07-16 11:20:30
https://github.com/OpenMined/PySyft/pull/1399,[],Seeded test_federated_learning and added test for remote optim.step(),"Seeded test_federated_learning and added test for remote optim.step()# Description
Seeded test_federated_learning and added unittest for remote optim.step()

Fixes #1398

## Type of change

Added unittests.


# How Has This Been Tested?

All tests pass.


# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
Hey, I think the point of this test is to make sure that the parameters change after doing all this. In particular, unless I'm missing something, I think it's best to remove this line and make sure that the parameters from the second iteration _don't_ equal those from the first.Other than that, looks good!Actually i was testing that two models with same initial weights have the same updates.
but now i have changed it as you have specified.Could these assertions be specific values as well as opposed to simply checking for equality. It could break in all sorts of terrible ways (param[2] could equal -inf, NaN, or None for example) and this test would still pass. Requiring it to equal a specific value, however, makes the test much more precise",4,True,2018-07-13 19:46:18,2018-07-14 11:50:09,2018-07-14 11:50:05
https://github.com/OpenMined/PySyft/pull/1395,[],Handling more sophisticated args and kwargs when calling torch func,"Handling more sophisticated args and kwargs when calling torch func# Description

The goal is to provide a fully working demo of federated learning on MNIST.
As for now, it is not possible to handle convolution (`F.conv2d`, `nn.conv2d`), because the args and kwargs required get lost during the hook.
Hence, we provide a stronger handling of arks and kwargs in the transmission process of commands to workers, with a special focus on the conservation of the types not natively supported in JSON.

See the **working example on MNIST** : https://colab.research.google.com/drive/1DxjiUjLUenFaVBrz3QMuJ665Z2HsfmgA

## Type of change

- [X] Bug fix (non-breaking change which fixes an issue)
- [X] New feature (non-breaking change which adds functionality)
- [X] This change requires a documentation update

# How Has This Been Tested?

Unit tests added on convolution

**Test Configuration**:
Colab config

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
golden - love thisone thing we want to ensure is that people can't accidentally inject their own dependencies or arbitrary python code by sending something special through JSON.(not saying your code doesn't... just as a heads up while you're working)Thank you for your feedback !
I'm trying to find a case where there would be a security problem, I have this one:
```
encoder = utils.PythonEncoder(retrieve_tensorvar=True)
decoder = utils.PythonJSONDecoder(alice)
enc = [{'__eval__': 'any([exec(\'self.leak = [str(obj) for k, obj in self.worker._objects.items()]\')])'}]
enc = json.dumps(enc)
dec = decoder.decode(enc)
print(decoder.leak)
```

But as this example points out, it can get quite tricky to detect malicious functions that are hidden behind *acceptable* ones.Indeed - if we can avoid giving the sender any ability to send raw python code (or references to python objects), that would likely be the level of safety required. Previously, this was accomplished by having dictionaries mapping strings to python objects so that we never call exec.This also means that the python decoder would be intentionally non-generic, that we'll have to add support for each new message type as we want it (which I think is acceptable)Well, the basic functions I would like to preserve are the iterables `enumerate, filter, list_iterator, map, range, zip` and also, `slice` (many other functions such as `any` are not useful as they should be evaluated directly so they can't be jsonfied). So what I can do it is specifically consider them, but then all other functions will be discarded.Ok I've enforced the checks on the json decoder so that it won't evaluate anything unsafe (for now it only eval `slice` which is used many times in training CNN)We could probably write a for loop which iterates through all of the functions in PyTorch and adds them to this kind of lookup table. (if the issue is you had to hand-encode ""slice"")(I'm very hesitant to merge anything with ""eval()"" directly when we can avoid it with a lookup table)Yes I agree this is not optimal. Either we could make a special case for ""slice"" and say ok for this one we do ""eval"" but only this one, or we figure an other way to json-ify it and we don't need to use ""eval"" for now.My heart wants to merge this - but i'm just fearful we'll forget about the eval() and it'll come back to bite us later. Do we think we have a fair workaround? (@jvmancuso )Is it ok if I hardcode that eval can only be used with `slice` ?(I can do it even more strongly than in my last commit 037d65)This line is a big problem -- eval on eval can for sure execute arbitrary codeWe should not call eval ever. It's a security riskI think we can have a workaround. Basically... whatever code you would ""eval""... cache that python object in a local dictionary where the key is the string version.

aka... if you wanted to call ""print""

guard = {}
guard['print'] = print

....

def receive_python_code_to_execute(function, params):
      return guard\[code\](params)

receive_python_code_to_execute(""print"", ""hello world"")
see how that can allow us to execute arbitrary python code without ever calling exec?Originally I'd used `eval` on all torch commands because, after thinking it through, there was no way to inject arbitrary code through it if you check to make sure everything's a legit command under the `torch` module.  (This is because you'd have to sneak that code into their PyTorch install, which means you've already got the ability to execute arbitrary code.)But it _is_ slow, so even if you can guarantee security, using a lookup is preferable .And also I'd add that the plan was alway to remove eval when we could, and it was originally only in there for convenience while prototyping :grin: Also, this could all be handled by switching from JSON to something more extensible like protobuf or some other format more suitable for byte streaming.Good idea ! I'll get rid of `eval` using your suggestions then.
Regarding protobuf or equivalent, sure it could be better, though for now I just wanted to fix the things with the args/kwargs so I tried to do as close as possible to the initial implementation. But this is definitefy something to explore.I just came across this, which is part of the unit test `def test_federated_learning(self):`
```
model = nn.Linear(2, 1)
p = model.parameters() # is a generator
opt = optim.SGD(params=p, lr=0.1)
```
`optim.SGD` calls at a certain point `torch.is_tensor(params)` (torch/optim/optimizer.py `l.24`) which we have hooked. The Json encoding in the hook implies that either we discard the generator (which we don't want even if in this case it would work), either we iterate on the `params` generator which will be empty at the time `is_tensor` has returned, so the next lines of `optim.SGD` (torch/optim/optimizer.py `l.32-34`) trigger an error.
```
param_groups = list(params)
if len(param_groups) == 0:
     raise ValueError(""optimizer got an empty parameter list"") 
```
We could have called `opt = optim.SGD(params=list(p), lr=0.1)` at the beginning but this is painful for users.
Do you have some suggestion on how we could deal with this ?Can you send the full stacktrace?  I'm thinking a specialized hook for `is_tensor` is in order, although I'm not 100% on the error you're describing.Yes sure ! But this is not the last commit, it is the previous one. On the last commit, there is a strong focus on limiting the things we evalIt's a short one, that's why I took time to figure out what's under the hood
```
Traceback (most recent call last):
  File ""/Users/ryffel/Documents/Code/PySyft/test/torch_test.py"", line 328, in test_federated_learning
    opt = optim.SGD(params=model.parameters(), lr=0.1)
  File ""/anaconda3/lib/python3.6/site-packages/torch/optim/sgd.py"", line 57, in __init__
    super(SGD, self).__init__(params, defaults)
  File ""/anaconda3/lib/python3.6/site-packages/torch/optim/optimizer.py"", line 34, in __init__
    raise ValueError(""optimizer got an empty parameter list"")
ValueError: optimizer got an empty parameter list

```Maybe the issue of pickling generator is related. (see https://stackoverflow.com/questions/7180212/why-cant-generators-be-pickled)>We could have called opt = optim.SGD(params=list(p), lr=0.1) at the beginning but this is painful for users.
Do you have some suggestion on how we could deal with this ?

I don't think there's a clean way to do this. It's strange that is_tensor is called on a whole generator and not something like 
`torch.is_tensor(p) for p in params`.

One possibility is to provide a special hook for is_tensor that checks for a generator arg and casts the generator to a list or something else we can handle before sending off the RPC, although that still seems like a dirty solution. Yes, as I've been thinking about it, maybe it's beyond the scope of this PR.
But we have a real issue with generators, because we can think of many examples where they could be used and where we would be blocked just as in the last example, because the real problem is that `params` is provided to `is_tensor()` but there is no return, and the code doesn't expect `is_tensor()` to iterate/exploit `params`. So even if we cast the generator args of `is_tensor()`in a list, we will empty `params` for the functions using it after `is_tensor()`.
And in this case it's not very well implemented in pytorch because at the end params is indeed casted into a list, but a bit to late.In general, we can worry about generators containing tensors/variables on a case by case basis, since right now I don't think there are any instances of places where we've overloaded functions that are called on those.

I'm still not sure why `params` comes back empty.  `is_tensor` just checks for `isinstance(x, torch.Tensor)` under the hood, which should return False, so that line should just continue.  We're initializing the Optimizer on the client, so that should be an iterable of Parameters or pointers to Parameters -- it should never be empty.  Do you have a specific test that's failing?

CC @iamtrask wasn't the optimizer working during one of your twitch streams?  is this a new bug?Actually,  maybe this is not a desirable behaviour, but at a certain point the call to `is_tensor` is serialized, so `params`is iterated over. Hence, when `is_tensor` performs his check `isinstance(x, torch.Tensor)` it already has a list.
This behaviour is linked to the fact that we inspect args and kwargs to retrieve all the tensors. So we inspect the generator, namely we iterated over it, and therefore it is then empty.I suspect there was previously a bug when transmitting the arguments, leading to `torch.is_tensor(params)` returning `False` as expected when calling the optimizer, but because it was given empty or incorrect argumentsThat sounds likely.  Can we verify it?  If that's the case, there should be a way to restart the generator (e.g. by catching the StopIteration exception), or better yet to fix up the code so that we avoid iterating over it before it's cast to list in `Optimizer.__init__`.Oh just saw your latest comment.  Now I'm a bit confused, is there currently a bug here?  If we can get a concrete test showing that this breaks or doesn't break that'd be perfect.It seems like the build has been passing, so I think if this were a problem, the unit test you were showing would have broken.I'm trying to find an appropriate test.
Which unit test are you referring to ?
This guy: https://github.com/OpenMined/PySyft/blob/a5b462eba1533974e79682ffb8d7d071a3f6dda8/test/torch_test.py#L459Ahah I'm completely confused now! I can't find any example relevant. Still searchingthat test fails from time to time. trask said that the random numbers havent been seeded.Yes @bartimaeus12, also one could extend the number of iterations (originally 3) to lower the probability of fail (I've done this in this PR)Why don't we just use a seed? There's a function called torch.manual_seed for thatYes right, but I'm not familiar with Pytorch, wasn't my test, so you know, I took the easy way^^ 
(Btw I run successfully 100/100 tests of test_federated_learning with this dirty fix, compared to 97/100 before, but yeah it's dirty !)Hmm, I suppose if we're testing for overall loss reduction, that's an okay fix.  (I'd be happier if it had a 1e-6 probability of failing but oh well...)  I do think that ultimately we want several subtests for this test, as when it fails we'd want to know why it failed and right now it could pretty much be anything in the stack.  So we'd want a test for sending modules, a test for remote execution of a model, a test for doing `backward`, a test for Optimizer.step on a remote model.  I'll put an issue up.Yes this would be more appropriate !#1398 üôÇ",2,True,2018-07-10 08:06:42,2018-07-13 16:05:47,2018-07-13 16:05:47
https://github.com/OpenMined/PySyft/pull/1393,[],Unit tests for binary methods were added,"Unit tests for binary methods were added# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Unit tests for binary methods were added.

 #1385 

## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

`python setup.py test`

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",1,True,2018-07-07 09:54:55,2018-07-10 14:42:37,2018-07-10 14:42:37
https://github.com/OpenMined/PySyft/pull/1392,[],Unit test for _add method was added,"Unit test for _add method was added# Description
Unit tests for add_ method were added.  

Fixes  #1385 

## Type of change
 New feature (non-breaking change which adds functionality)

# How Has This Been Tested?
python setup.py test

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",3,True,2018-07-07 09:34:33,2018-07-10 14:44:10,2018-07-10 14:44:09
https://github.com/OpenMined/PySyft/pull/1388,[],added whoami to SocketWorker and removed redundant paramter from Base‚Ä¶,"added whoami to SocketWorker and removed redundant paramter from Base‚Ä¶added whoami to SocketWorker and removed redundant paramter from BaseWorker.register_object

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
Style: put this on one line or have one line for each parameterStyle: put on one lineThis is really useful for sharing your id with other worker. Do we want to have the ability to call this on any known worker so that workers can share the id of workers that they know with other workers they know? Or more specifically the client can tell the workers who the other workers arestyle: one param per line would have missed owners hereYeah! Seems like we should. I'll add a default to BaseWorker.",2,True,2018-07-05 15:23:03,2018-07-06 13:41:33,2018-07-06 13:41:33
https://github.com/OpenMined/PySyft/pull/1383,[],"Fixes Issue #1382 : Call of torch functions on remote Variable (ex: torch.matmul(x,y))","Fixes Issue #1382 : Call of torch functions on remote Variable (ex: torch.matmul(x,y))# Description

Fixes Issue #1382

## Type of change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Pysyft test pass.

**Test Configuration**:
(See Issue #1382)

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",2,True,2018-07-03 14:35:46,2018-07-04 16:06:04,2018-07-04 16:06:04
https://github.com/OpenMined/PySyft/pull/1371,[],Allow socket connection to remote machines,"Allow socket connection to remote machines# Description

This pull request just replace the hardcoded 'localhost' host in socket connections with the object's attribute.
It allows to connect to remote computers (not only localhost)


## Type of change

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] Test with localhost (basically run this notebook: https://github.com/OpenMined/PySyft/blob/master/examples/SocketWorker%20Client.ipynb
- [x] Test with 2 computers on a local network and initalize SocketWorkers with real IP address

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules",1,True,2018-06-27 09:42:04,2018-06-27 11:49:45,2018-06-27 11:33:27
https://github.com/OpenMined/PySyft/pull/1366,[],cleaned up syft/core/workers and added documentation to SocketWorker,"cleaned up syft/core/workers and added documentation to SocketWorker# Description

cleaned up syft/core/workers and added documentation to SocketWorker

# How Has This Been Tested?

Using the existing Unit testing suite

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
",1,True,2018-06-25 17:27:16,2018-06-26 15:47:21,2018-06-26 15:47:21
https://github.com/OpenMined/PySyft/pull/1360,[],Fix broken links to Virtual Worker Tutorial notebook,"Fix broken links to Virtual Worker Tutorial notebook# Description

""SockerWorker Server.ipynb"" and ""SockerWorker Client.ipynb"" notebooks contains broken links to ""Basic Virtual Worker Tutorial"" notebook.

# Checklist:

- [x] My changes generate no new warnings
",1,True,2018-06-23 06:20:15,2018-06-23 16:28:15,2018-06-23 16:28:15
https://github.com/OpenMined/PySyft/pull/1357,[],Adding binder support for repository,"Adding binder support for repository# Description

Add the links and code to make Binder (http://repo2docker.readthedocs.io) work with the repo. Basically, it has the dependencies in a conda standard environment. It makes running and testing the code as easy as clicking on a link (https://mybinder.org/v2/gh/kmader/PySyft/patch-2?filepath=examples%2FFederated%20Learning%20Example.ipynb) to run and start the Federated Learning Example

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [x] New feature (non-breaking change which adds functionality)

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules",1,True,2018-06-22 12:48:52,2018-06-22 13:29:01,2018-06-22 13:29:01
https://github.com/OpenMined/PySyft/pull/1356,[],Adding notebook testing [WIP],"Adding notebook testing [WIP]# Description

First attempt to execute and test notebooks as part of travis testing

Fixes # (issue)

https://github.com/OpenMined/PySyft/issues/1355

## Type of change

Improved testing (didn't see a checkbox for that)

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules",4,True,2018-06-22 08:07:51,2020-02-01 12:55:14,2018-06-22 17:32:58
https://github.com/OpenMined/PySyft/pull/1336,[],Wrong variable reference,"Wrong variable reference# Description

Variable type `torch.autograd.Variable` was referenced incorrectly.

Fixes # (issue)

Ran unit test and got this error:

```
File ""/home/jaison/PySyft/syft/core/torch_/utils.py"", line 97, in <module>
    'torch.autograd.variable.Variable': torch.autograd.variable.Variable,
AttributeError: 'function' object has no attribute 'Variable'

```

Changed  `torch.autograd.variable.Variable` to  `torch.autograd.Variable` 


## Type of change

- [x] Bug fix (non-breaking change which fixes an issue)

# How Has This Been Tested?

- [x] Test A: Ran unit tests without errors after the fix.

- [x] Test B: Went through PyTorch documentation to verify the syntax [here](https://pytorch.org/docs/0.3.1/autograd.html#variable)

**Test Configuration**:
* CPU: i5
* GPU: none
* PySyft Version: current master branch
* PyTorch Version: 0.4.0
* Python Version: 3.5
* Unity Version: none
* OpenMined Unity App Version: none


# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",4,False,2018-06-09 06:22:51,2018-06-09 19:54:08,2018-06-09 07:16:49
https://github.com/OpenMined/PySyft/pull/1329,[],Grid to syft merge,"Grid to syft merge# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules
Probably best to .gitignore the .ipynb_checkpoints directory",1,True,2018-05-22 16:38:11,2018-06-02 20:02:59,2018-06-02 20:02:59
https://github.com/OpenMined/PySyft/pull/1326,[],Activating Open Collective,"Activating Open CollectiveHi, I'm making updates for Open Collective. Either you or a supporter signed this repo up for Open Collective. This pull request adds backers and sponsors from your Open Collective https://opencollective.com/PySyft ‚ù§Ô∏è
  
  It adds two badges at the top to show the latest number of backers and sponsors. It also adds placeholders so that the avatar/logo of new backers/sponsors can automatically be shown without having to update your README.md. [[more info](https://github.com/opencollective/opencollective/wiki/Github-banner)]. See how it looks on [this repo](https://github.com/apex/apex#backers).
You can also add a ""Donate"" button to your website and automatically show your backers and sponsors there with our widgets. Have a look here: https://opencollective.com/widgets

  P.S: As with any pull request, feel free to comment or suggest changes. The only thing ""required"" are the placeholders on the README because we believe it's important to acknowledge the people in your community that are contributing (financially or with code!).

  Thank you for your great contribution to the open source community. You are awesome! üôå
  And welcome to the open collective community! üòä

  Come chat with us in the #opensource channel on https://slack.opencollective.com - great place to ask questions and share best practices with other open source sustainers!
  ",1,False,2018-04-05 07:07:44,2018-04-07 21:13:54,2018-04-07 21:13:54
https://github.com/OpenMined/PySyft/pull/1322,[],Adresses #1162 add rsqrt functionality,"Adresses #1162 add rsqrt functionality# Description
Adds the Reciprocal Square Root functionality to int tensors

Addresses # (issue) 1162 in PySyft

## Type of change

- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update
- [x] Added tests for an existing feature

# How Has This Been Tested?
```
I added tests in  IntTensorTest.cs

            int[] data1 = {1, 2, 3, 4};
            int[] shape1 = {2, 2};

            int[] data2 = {1, 1, 1, 0};
            int[] shape2 = {2, 2};
```
# Checklist:

- [ ] My code follows the style guidelines of this project
- [X] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules


Link to Issue:
https://github.com/OpenMined/PySyft/issues/1162

Confusion/Problem:
```
What is the expected behaviour for IntTensor Rsqrt

`x => 1 / (int) Math.Sqrt(x)`

or

`x => (int)(1 /  Math.Sqrt(x))` 

```

Link to OpenMined PR: https://github.com/OpenMined/OpenMined/pull/427",2,True,2018-02-18 19:28:56,2018-02-19 15:49:25,2018-02-19 10:48:33
https://github.com/OpenMined/PySyft/pull/1315,[],Applied fix for issue #1314,"Applied fix for issue #1314# Description
The bug was fixed by resolving a missing import. 

## Type of change
- [x] Bug fix (non-breaking change which fixes an issue)

## How Has This Been Tested?
- [x] Ran existing unit tests for the IntTensor functions sin() and cos() 
- [x] Added 2 integration tests for the IntTensorsfunction sin() and cos() (the PR will follow-up) 


# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules",1,True,2018-01-31 22:24:32,2018-02-01 18:46:28,2018-02-01 18:46:28
https://github.com/OpenMined/PySyft/pull/1312,[],MPC Tensor Addition,"MPC Tensor Addition# Description

This is a work in progress. Goal is to do addition using mpc and Grid. 

Currently has not been tested

## Type of change

Please delete options that are not relevant.

- [x] New feature (non-breaking change which adds functionality)

- [x] This change requires a documentation update (working on a notebook example)

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Test A
- [ ] Test B

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules
",1,False,2018-01-29 22:35:27,2018-04-07 21:04:40,2018-04-07 21:04:40
https://github.com/OpenMined/PySyft/pull/1309,[],Feature/view  int tensor,"Feature/view  int tensor# Description

Implements view feature for int tensor with -1 parameter support .

Fixes # (1298)

## Type of change

Please delete options that are not relevant.

- [x] New feature (non-breaking change which adds functionality)
- [x] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] Test to check -1 parameter support.
- [x] Test to convert 3D tensor to 2D tensor

**Test Configuration**:
* CPU: i7 7700
* GPU: GTX 1070
* PySyft Version: latest
* Unity Version: latest
* OpenMined Unity App Version: latest

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules",2,True,2018-01-28 17:05:39,2018-01-28 18:35:36,2018-01-28 17:12:46
https://github.com/OpenMined/PySyft/pull/1306,[],feat: keras interface predict function update,"feat: keras interface predict function updateHello,

Sorry for the small PR.  For the Keras interface, the predict function returns now a numpy array like in the Keras library, instead of a FloatTensor. 

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [X] New feature (non-breaking change which adds functionality)

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] With notebook

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- x ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
",1,True,2018-01-25 17:40:01,2018-01-27 00:17:48,2018-01-27 00:17:37
https://github.com/OpenMined/PySyft/pull/1301,[],Feature/keras,"Feature/kerasHello,

This PR add Adam and RMSprop optimizer to Keras interface.

Fixes # (issue)

## Type of change

Please delete options that are not relevant.

- [x] New feature (non-breaking change which adds functionality)

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] Tested with notebook

**Test Configuration**:
* CPU:
* GPU:
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
",1,True,2018-01-21 07:15:02,2018-01-25 16:18:26,2018-01-21 09:06:23
https://github.com/OpenMined/PySyft/pull/1296,[],added sin cpu and gpu functionality for Int tensors.,"added sin cpu and gpu functionality for Int tensors.# Description

Added sin functionality for Int Tensors, that accepts an Int tensor and returns a Float Tensor containing the sin of each element.

Fixes #1178, #1180 

## Type of change

Please delete options that are not relevant.

- [x] New feature (non-breaking change which adds functionality)
- [x] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] Test : The function passed all the test on TestRunner on Unity.

**Test Configuration**:
* CPU: Windows 10 Intel(R) Core i5-4200U CPU@ 1.6GHz 2.30GHz
* GPU: AMD Radeon HD 8670M Intel(R) HD Graphics 4400
* PySyft Version: 0.1
* Unity Version: latest
* OpenMined Unity App Version: latest

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules",3,True,2018-01-17 12:26:35,2018-01-18 07:45:00,2018-01-18 03:32:03
https://github.com/OpenMined/PySyft/pull/1295,[],Feature/transpose  int tensor,"Feature/transpose  int tensor# Description

Added T() method to IntTensor with CPU support to calculate the transpose of an IntTensor.

Fixes # (1222)

## Type of change

Please delete options that are not relevant.

- [x] New feature (non-breaking change which adds functionality)
- [x] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration
- [x] Verifying transpose of 2X2 IntTensor
- [x] Verifying transpose of 2X3 IntTensor
- [x] Verifying transpose of 2X2X2 IntTensor

**Test Configuration**:
* CPU: Windows Core-i7
* GPU: GTX 1070
* PySyft Version: 0.1
* Unity Version: 2017.03.0f3
* OpenMined Unity App Version:

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules",1,True,2018-01-16 18:12:25,2018-01-18 10:57:59,2018-01-18 10:43:38
https://github.com/OpenMined/PySyft/pull/1288,[],Implemented IntTensor Lt and inline Lt on CPU,"Implemented IntTensor Lt and inline Lt on CPUFixes #1039 and #1040 

## Type of change

Please delete options that are not relevant.

- [x] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

# How Has This Been Tested?

- [x] Passing OpenMined Unity tests
- [x] Passing Notebook integration tests (examples)


# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules",1,True,2018-01-14 19:09:50,2018-01-15 01:53:18,2018-01-15 01:53:18
https://github.com/OpenMined/PySyft/pull/1287,[],added sign function for Int Tensor,"added sign function for Int Tensor# Description

Implements the sign function for Int Tensors as stated in issue Pysyft OpenMined/PySyft#1174

Corresponding OpenMined PR:  https://github.com/OpenMined/OpenMined/pull/349

Fixes # (issue)  OpenMined/PySyft#1174

## Type of change

Please delete options that are not relevant.

- [x] New feature (non-breaking change which adds functionality)

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- I added a unit test and tested that the functionality of the sign function.

- I ran a jupyter notebook tests of the form, to ensure functionality is accurate.

`data = np.array([[-1,-2,3],[4,5,-6]]).astype('int')`
`m = IntTensor(data)`
`m.sign()`



**Test Configuration**:
* CPU: 2.7 GHz Intel Core i5
* GPU: 
* PySyft Version:
* Unity Version:
* OpenMined Unity App Version:

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modulesreturns a float?",2,True,2018-01-14 15:34:07,2018-01-15 22:50:25,2018-01-15 22:50:25
https://github.com/OpenMined/PySyft/pull/1280,[],Add trace function to intTensor,"Add trace function to intTensor# Description

Fixes #1246
OpenMined PR: https://github.com/OpenMined/OpenMined/pull/331

## Type of change

- [x] New feature (non-breaking change which adds functionality)

# How Has This Been Tested?

- [x] Added CPU test in Unity

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules",1,True,2018-01-13 21:09:22,2018-01-13 21:31:12,2018-01-13 21:31:05
https://github.com/OpenMined/PySyft/pull/1132,[],Add instructions for Anaconda installation to README,"Add instructions for Anaconda installation to READMELink to tutorials instead, I think. https://github.com/OpenMined/tutorailsyour link is broken! https://github.com/OpenMined/tutorialsWhoops!",2,False,2018-01-13 14:26:55,2018-07-22 16:44:45,2018-01-16 11:15:15
https://github.com/OpenMined/PySyft/pull/758,[],__add__ method for IntTensor,"__add__ method for IntTensor# Description

Added the `__add__` method to `IntTensor`

Fixes OpenMined/PySyft#755

## Type of change

- [x] New feature (non-breaking change which adds functionality)

# How Has This Been Tested?

- [x] Created a Jupyter notebook example
- [x] Added CPU/GPU tests in Unity

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules",4,True,2018-01-11 23:35:00,2018-01-13 00:27:02,2018-01-12 22:56:57
https://github.com/OpenMined/PySyft/pull/756,[],"[WIP] Implementing Stricter NumPy, PEP8 and PEP257 Standards for Docs and code","[WIP] Implementing Stricter NumPy, PEP8 and PEP257 Standards for Docs and code# Description

I noticed that our doc strings were not following proper NumPy Standards and I had missed out on some minor things in my last PR #714 , This PR fixes and changes some things. Before we move on to the actual HTML generation using Sphinx during to Hackathon.

- Removed Empty lines after """"""
- Removed Empty Parameter docs
- Added Spaces According to PEP8
- Fixed DocString for contiguous

>     Before
        def cos_(self):
        """"""
        Performs the cosine of the input tensor inplace.
        Parameters
        ----------
        Returns
        -------
        FloatTensor
            Caller with values inplace
        """"""
        return self.no_params_func(""cos_"")

>     Now
        def cos_(self):
        """"""Performs the cosine of the input tensor inplace.

        Returns
        -------
        FloatTensor
            Caller with values inplace
        """"""
        return self.no_params_func(""cos_"")

## Type of change
Documentation

# Checklist:

- [X] My code follows the style guidelines of this project
- [X] I have performed a self-review of my own code
- [X] I have commented my code, particularly in hard-to-understand areas
- [X] I have made corresponding changes to the documentation
- [X] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published in downstream modules

Everything has been done till ~400 Line to allow members to review my work as I proceed to other files",1,False,2018-01-11 00:21:54,2018-02-13 15:15:14,2018-02-13 15:15:14
https://github.com/OpenMined/PySyft/pull/754,[],bugfix: do not delete results for inline aritmetic operations,"bugfix: do not delete results for inline aritmetic operations# Description

Fixes a bug: for inline arithmetic operations, no new python FloatTensor should be created. Otherwise, the first version of the tensor in python will be deleted, causing the Tensor in C# to be deleted aswell.

## Type of change

- Bug fix 

# How Has This Been Tested?

Tested in Syft Tensor Example Notebook.ipynb

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
",1,True,2018-01-09 15:45:14,2018-01-09 22:39:58,2018-01-09 22:37:56
https://github.com/OpenMined/PySyft/pull/752,[],bugfix no-params-func,"bugfix no-params-func# Description

Simple tensor ops (a-b) were broken

## Type of change

Please delete options that are not relevant.

- [ X ] Bug fix (non-breaking change which fixes an issue)

# How Has This Been Tested?

Tested some notebooks that weren't working for me before",1,True,2018-01-08 20:58:13,2018-01-08 21:06:25,2018-01-08 21:06:25
https://github.com/OpenMined/PySyft/pull/748,[],implement model.evaluate() function,"implement model.evaluate() function# Description
To make it easier to evaluate the performance of different models, it's nice to have a model.evaluate() function that returns certain metrics and loss.

- Implemented model.evaluate() that returns metrics you provide (this can be custom metrics as well).
- Return the test loss

Dependent upon: https://github.com/OpenMined/OpenMined/pull/316 !

## Type of change

- [ ] Bug fix (non-breaking change which fixes an issue)
- [X] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?
Tested in the Keras notebook.

# Checklist:

- [X] My code follows the style guidelines of this project
- [X] I have performed a self-review of my own code
- [X] I have commented my code, particularly in hard-to-understand areas
- [X] My changes generate no new warnings
- [X] I have added tests that prove my fix is effective or that my feature works
- [X] New and existing unit tests pass locally with my changes
- [X] Any dependent changes have been merged and published in downstream modules",4,True,2018-01-06 13:48:59,2018-01-15 08:21:17,2018-01-15 02:16:18
https://github.com/OpenMined/PySyft/pull/747,[],Implemented hyperparameter forwarding Keras,"Implemented hyperparameter forwarding KerasImplemented hyperparameter forwarding Keras
# Description
I noticed it wasn't possible to forward any hyperparameters like lr, momentum and decay to the optimizer in Keras. So I added keyword arguments to the optimizer and unpack them when calling the syft function.

## Type of change
- [ ] Bug fix (non-breaking change which fixes an issue)
- [X] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?
Tested in the Keras notebook.

# Checklist:
- [X] My code follows the style guidelines of this project
- [X] I have performed a self-review of my own code
- [X] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [X] New and existing unit tests pass locally with my changes
- [X] Any dependent changes have been merged and published in downstream modules
",1,True,2018-01-06 11:55:43,2018-01-06 13:50:41,2018-01-06 13:50:32
https://github.com/OpenMined/PySyft/pull/742,[],pysyft side of the openmined -> pysyft grid communication,"pysyft side of the openmined -> pysyft grid communication# Description

pysyft side of the openmined -> pysyft grid communication

## Type of change

- [x] New feature (non-breaking change which adds functionality)

# Checklist:

- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
",3,True,2018-01-05 19:17:51,2018-01-05 20:05:31,2018-01-05 20:05:25
https://github.com/OpenMined/PySyft/pull/740,[],Reducing default epsilon for RMSProp and Adam to fit in float,"Reducing default epsilon for RMSProp and Adam to fit in float# Description

1e-8  = 0.00000001 which is too many digits for a float so the value of epsilon becomes 0
which can result in nan errors with RMSProp and Adam.
Not sure if there's an easy way to use doubles in this scenario so setting epsilon to 1e-6 for now

## Type of change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update",1,True,2018-01-05 18:56:21,2018-01-05 19:55:00,2018-01-05 19:54:50
https://github.com/OpenMined/PySyft/pull/701,[],Messaging and typed values (Syft side),"Messaging and typed values (Syft side)(Lot of text, but it's better to read it)

Pursuing the idea in https://github.com/OpenMined/OpenMined/issues/166, I suggest that we upgrade the string messages returned by Unity to JSON messages like for instance

    {'ok': True, 'objectType': 'tensorID', 'id': 34}

You can read about this in the corresponding PR on the Unity side (https://github.com/OpenMined/OpenMined/pull/215)

In this PR, there are 3 closely related things. I really wanted a set of smaller PR, but I could not find my way, so I did go through all of it this past week.

The topics are
- reorganize methods in sections such as 'tensor operations', 'memory management', 'dim reduction', 'arithmetic ops', 'properties access'
- more importantly those sections use methods with clearer semantics (detailed below, so that you don't have to infer it from the code)
- and even more importantly, the strings used to transfer from Unity don't leak up to those method calls: everything is Python types 

The reordering was necessary to eliminate duplicates, and make sense out of the different category of operations. I saw @emregoynugur did the same in csharp code independently ;)

1. `_op(self, name, params)` returns a `FloatTensor` or a `float` (only in the case of `trace()`). The semantics are: ""computes new values: means no inline, no properties reading"". Corresponding operations are for instance `round`, `__add__`, `trace`

it is typically called like this
```
    def view_as(self, x):
        assert isinstance(x, FloatTensor)
        return self._op(""view_as"", x.id)
        # return self.params_func(""view_as"", [x.id], return_response=True)
```

2. `_op_inline(self, name, params)` returns actually nothing. But the underlying call to Unity had received a json OK status (or an json error message) so it raises an exception in case of failure. 

It is typically called like this:

```
    def view_as_(self, x):
        assert isinstance(x, FloatTensor)
        # self.params_func(""view_as_"", [x.id], return_response=False)
        self._op_inline(""view_as_"", x.id)
        return self
```

so it's obvious what is returned, and there is a guarantee that silence means success.

All inline methods use this call.

3. `_manage(self, name)` returns nothing, no parameters can be given. Technically, it is very close to `_op_inline` that true, but no computation is conducted.

Example call

```
    def cpu(self):
        self._manage(""cpu"")
        return self
        # return self.no_params_func(""cpu"")   <- left temporarily for clarity
```

so it's obvious what is returned, and silence = success also. Methods that use `_manage` are `cpu()`, `gpu()`, `delete_tensor()`

4. `_get_prop(self, name, params)` returns the Python typed equivalent of the Unity property (bool, float, list[int], etc). The semantics is no computation is conducted, only reading.

```
    def autograd(self, setter=None):
        if setter is None:
            return self._get_prop(""autograd"")

        assert isinstance(setter, bool)
        self._set_prop(""autograd"", setter)
        return self

        # if setter is None:
        #     if self.get(""autograd"") == ""1"":
        #         return True
        #     return False
        # else:
        #     if setter:
        #         out = self._set_prop(""autograd"", True)
        #         # out = self.set(""autograd"", [""1""])
        #     else:
        #         out = self._set_prop(""autograd"", False)
        #         # out = self.set(""autograd"", [""0""])

        #     if (out == ""1"" and setter) or (out == ""0"" and not setter):
        #         return self
        #     return False

```

5. `_set_prop` is the same semantics for writing a value. It returns nothing, as for inline ops, but the semantics are 'no computation'. Example call above in autograd.

6. `_build_arithmetic_operation(self, x, name)`

Example call:

```
    def __add__(self, x):
        name, param = self._build_arithmetic_op(x, ""add"")
        return self._op(name, param)
        # return self.arithmetic_operation(x, ""add"", False)
```

I hope there is more code reuse and clearer semantics also in this call: ""this is a computation, with a special operation name (I did *not* change the naming convention of arithmetic operations), it returns a tensor""

I'm eager to get your comments. I have ran the nice ""Test Tensor operations"" notebook without any problem and ran several experiments myself, but there may be some bugs.

Last thing, as on the Unity side, I temporarily left the commented previous calls for better readability in case you read the code.",3,False,2017-12-14 23:03:24,2017-12-15 21:36:15,2017-12-15 15:05:49
https://github.com/OpenMined/PySyft/pull/397,[],do not use hard coded type sizes,"do not use hard coded type sizes<!--
Thanks for submitting a PR. Please make sure  that you have read the contribution guidelines: https://github.com/OpenMined/Docs/blob/master/contributing/guidelines.md
-->


<!-- Example: Fixes #334 -->


#### What does your implementation fix? 

We should not use hard coded type sizes in the library. This fix uses the host architecture type size.



<!--
Please note that we are a small team of volunteers so patience is
essential; We value all the contributions, no matter how small they are. 
If we are slow to review, either the pull request needs 
some tweaking,convincing, etc. 
or probably the reviewers could be busy. In any
case, we hope for your understanding in the process.

However If you feel that  this needs to be fastened up.
Reachout to us on our slack #pysyft channel.

Thanks for the contribution!
-->",1,True,2017-11-10 14:06:08,2017-11-10 14:19:37,2017-11-10 14:19:37
https://github.com/OpenMined/PySyft/pull/396,[],Implement default set functionality resolves #93,"Implement default set functionality resolves #93<!--
Thanks for submitting a PR. Please make sure  that you have read the contribution guidelines: https://github.com/OpenMined/Docs/blob/master/contributing/guidelines.md
-->
#### Reference Issue
Fixes #93 

<!-- Example: Fixes #334 -->


#### What does your implementation fix? 

Following methods added on TensorBase :
- `storage_offset`
- `set_` 

#### Explain your changes.

Implements the same API as `torch.Tensor.set_()`, only it uses an `np.ndarray` in place of `torch.Storage`. 

So as a syft user instead of passing a `torch.Storage` to `set_` I need to pass a `np.ndarray`

#### include other comments?(if any)


<!--
Please note that we are a small team of volunteers so patience is
essential; We value all the contributions, no matter how small they are. 
If we are slow to review, either the pull request needs 
some tweaking,convincing, etc. 
or probably the reviewers could be busy. In any
case, we hope for your understanding in the process.

However If you feel that  this needs to be fastened up.
Reachout to us on our slack #pysyft channel.

Thanks for the contribution!
-->",1,True,2017-11-10 13:35:27,2017-11-10 13:52:49,2017-11-10 13:52:49
https://github.com/OpenMined/PySyft/pull/394,[],"rolled back to using phe's floating point, until we have a better solution","rolled back to using phe's floating point, until we have a better solution#### Reference Issue
#393 

#### Explain your changes.
It removes the fixed-point implementation that was contributed by yours truly!
The implementation was not very well thought out and buggy, and should not have been merged in the first place. I decided to remove it and fall back to `phe`'s built-in floating-point implementation, until we have a better solution.",1,True,2017-11-02 20:09:32,2017-11-03 14:47:26,2017-11-03 14:47:26
https://github.com/OpenMined/PySyft/pull/392,[],Add cross functionality for Base Tensor Type,"Add cross functionality for Base Tensor Type#### Reference Issue
- Fixes #382

#### What does your implementation fix? 
Adds default cross functionality for base tensor type

#### Explain your changes.
- [X] Implement cross functionality using numpy.cross
- [X] If the Base Tensor type's attribute ""encrypted"" is set to True, it should return a NotImplemented error.
- [X] corresponding unit tests demonstrating the correct operation on the Base Tensor type implemented over int and float Tensors.
- [X] Inline documentation.
could you add an encryption [check](https://github.com/OpenMined/PySyft/blob/34af4f778c2b2f1a16a5a8aa505c37b7d9b18009/syft/math.py#L330) for the inputs that are passed?Have made the requested changes and updated the PR.",2,True,2017-11-02 19:06:29,2017-11-03 07:02:26,2017-11-03 07:02:26
https://github.com/OpenMined/PySyft/pull/391,[],Split tensor,"Split tensorThis fixes #383 

#### What does your implementation fix? 
This add split functionality to default tensor type.

#### Explain your changes.
In syft.math it add a split function which uses numpy.array_split() for splitting the array into given axis into `split size` size of each chunk. The function returns a list containing divided sub-arrays. I had to some initial calculations to match the numpy api.",10,True,2017-10-31 09:08:36,2017-11-01 10:33:53,2017-11-01 10:33:53
https://github.com/OpenMined/PySyft/pull/389,[],first non linearity,"first non linearity<!--
Thanks for submitting a PR. Please make sure  that you have read the contribution guidelines: https://github.com/OpenMined/Docs/blob/master/contributing/guidelines.md
-->
#### Reference Issue

<!-- Example: Fixes #334 -->


#### What does your implementation fix? 

#### Explain your changes.


#### include other comments?(if any)


<!--
Please note that we are a small team of volunteers so patience is
essential; We value all the contributions, no matter how small they are. 
If we are slow to review, either the pull request needs 
some tweaking,convincing, etc. 
or probably the reviewers could be busy. In any
case, we hope for your understanding in the process.

However If you feel that  this needs to be fastened up.
Reachout to us on our slack #pysyft channel.

Thanks for the contribution!
-->",2,False,2017-10-31 02:02:33,2017-10-31 05:17:11,2017-10-31 05:17:11
https://github.com/OpenMined/PySyft/pull/388,[],Dense to sparse matrix,"Dense to sparse matrix#### Reference Issue
Implements #274 


#### What does your implementation fix? 
Implements dense to sparse matrix conversion

#### Explain your changes.
Implement sparse function in math.py

implemented function uses scipy sompressed sparse row matrix format to create sparse matrix
function returns TensorBase object
Implement spars functions in tensor.py
sparse() returns TensorBase object same as sparse() in math.py
sparse_() operates inline
Add unit tests for both sparse() functions and for sparse_() 

#### include other comments?(if any)
A few days ago I created original pull request (#376), but I got a bit confused trying to push my changes after rebase and changes according to comments from reviewer so I accidentally deleted branch from my fork which automatically closed that pull request.",4,True,2017-10-30 21:15:08,2017-11-04 02:43:52,2017-11-04 02:43:52
https://github.com/OpenMined/PySyft/pull/387,[],Remove capsule dependency from notebooks,"Remove capsule dependency from notebooks#### Reference Issue
- Addresses #384 
#### What does your implementation fix? 
- Removes capsule dependencies from the following notebooks
  - [Syft - Paillier Homographic Encrypted Linear Classification example ](https://github.com/OpenMined/PySyft/blob/master/notebooks/Syft%20-%20Paillier%20Homographic%20Encrypted%20Linear%20Classification%20example%20.ipynb)
  - [Syft - Paillier Encrypted Linear Classification](https://github.com/OpenMined/PySyft/blob/master/notebooks/Syft%20-%20Paillier%20Encrypted%20Linear%20Classification.ipynb)
#### Explain your changes.
Use `syft.he.keys.Paillier` instead of capsule to get the keys",3,True,2017-10-30 16:39:21,2017-11-03 14:47:59,2017-11-03 14:47:59
https://github.com/OpenMined/PySyft/pull/376,[],Dense to sparse matrix,"Dense to sparse matrix#### Reference Issue
Closes #274 

#### What does your implementation fix? 
Implements dense to sparse matrix conversion

#### Explain your changes.
Implement sparse function in math.py
   - implemented function uses scipy sompressed sparse row matrix format to create sparse matrix
   - function returns TensorBase object
Implement spars functions in tensor.py
  - sparse() returns TensorBase object same as sparse() in math.py
  - sparse_() operates inline
Add unit tests for both sparse() functions and for sparse_()it should be `tensor`.this as well (`tensor`)",3,False,2017-10-28 19:54:08,2017-10-30 21:06:39,2017-10-30 21:06:39
https://github.com/OpenMined/PySyft/pull/375,[],Initial BV API,"Initial BV API<!--
Thanks for submitting a PR. Please make sure  that you have read the contribution guidelines: https://github.com/OpenMined/Docs/blob/master/contributing/guidelines.md
-->
#### Reference Issue
Fixes #317 
<!-- Example: Fixes #334 -->


#### What does your implementation fix? 
The API has been re-written predominantly in Python, and will mainly serve as a mockup for faster implementations.

#### TODO
  * Ring addition function from PyPARI API has to be implemented purely in Python, for a faster 
implementation.
  * The interface file between PARI and Python needs to be changed, to allow better type conversion. The current interface file is just for the sake of completion.
  * Support for Full Homomorphism, along with development of PyPARI accordingly.
  


<!--
Please note that we are a small team of volunteers so patience is
essential; We value all the contributions, no matter how small they are. 
If we are slow to review, either the pull request needs 
some tweaking,convincing, etc. 
or probably the reviewers could be busy. In any
case, we hope for your understanding in the process.

However If you feel that  this needs to be fastened up.
Reachout to us on our slack #pysyft channel.

Thanks for the contribution!
-->",1,False,2017-10-27 17:34:32,2017-11-18 23:21:59,2017-11-18 23:21:59
https://github.com/OpenMined/PySyft/pull/374,[],changes multinomial to match PyTorch's API,"changes multinomial to match PyTorch's API<!--
Thanks for submitting a PR. Please make sure  that you have read the contribution guidelines: https://github.com/OpenMined/Docs/blob/master/contributing/guidelines.md
-->
#### Reference Issue

<!-- Example: Fixes #334 -->


#### What does your implementation fix? 
Addresses the issue discussed in PR #367 comments

#### Explain your changes.
It changes multinomial implementation to match PyTorch's API

#### include other comments?(if any)


<!--
Please note that we are a small team of volunteers so patience is
essential; We value all the contributions, no matter how small they are. 
If we are slow to review, either the pull request needs 
some tweaking,convincing, etc. 
or probably the reviewers could be busy. In any
case, we hope for your understanding in the process.

However If you feel that  this needs to be fastened up.
Reachout to us on our slack #pysyft channel.

Thanks for the contribution!
-->",1,True,2017-10-24 19:45:20,2017-10-25 01:51:05,2017-10-25 01:51:05
https://github.com/OpenMined/PySyft/pull/373,[],Added Swish Activation Function üêç,"Added Swish Activation Function üêç#### Reference Issue
Fixed #372 

#### What does your implementation fix? 
Swish Function often performs better than ReLu.

#### Changes.
- Implemented Swish() in math.py which returns a tensor.
- Implemented Swish_() in tensor.py which operates inline.
- Added Tests for the same.",4,False,2017-10-24 19:36:01,2017-10-29 12:50:45,2017-10-29 12:45:49
https://github.com/OpenMined/PySyft/pull/371,[],Added Negative Sampling module in word2vec for word2vec implementation with HE,"Added Negative Sampling module in word2vec for word2vec implementation with HE<!--
Thanks for submitting a PR. Please make sure  that you have read the contribution guidelines: https://github.com/OpenMined/Docs/blob/master/contributing/guidelines.md
-->
#### Reference Issue

<!-- Example: Fixes #334 -->


#### What does your implementation fix? 
Added Negative Sampling module in word2vec
#### Explain your changes.
Added Negative Sampling module in word2vec in word2vec implementation with HE

#### include other comments?(if any)


<!--
Please note that we are a small team of volunteers so patience is
essential; We value all the contributions, no matter how small they are. 
If we are slow to review, either the pull request needs 
some tweaking,convincing, etc. 
or probably the reviewers could be busy. In any
case, we hope for your understanding in the process.

However If you feel that  this needs to be fastened up.
Reachout to us on our slack #pysyft channel.

Thanks for the contribution!
-->",2,False,2017-10-24 14:49:13,2017-10-26 03:38:21,2017-10-26 03:38:21
https://github.com/OpenMined/PySyft/pull/370,[],Update abstract_model.py,"Update abstract_model.py#### Reference Issue

can't encrypt a model without a public key or decrypt one without a private key - the interface should reflect this.


#### What does your implementation fix? 


#### Explain your changes.


#### include other comments?(if any)


<!--
Please note that we are a small team of volunteers so patience is
essential; We value all the contributions, no matter how small they are. 
If we are slow to review, either the pull request needs 
some tweaking,convincing, etc. 
or probably the reviewers could be busy. In any
case, we hope for your understanding in the process.

However If you feel that  this needs to be fastened up.
Reachout to us on our slack #pysyft channel.

Thanks for the contribution!
-->",1,True,2017-10-24 09:00:39,2017-10-29 14:08:34,2017-10-24 15:57:17
https://github.com/OpenMined/PySyft/pull/369,[],chore/removes django installation,"chore/removes django installation<!--
Thanks for submitting a PR. Please make sure  that you have read the contribution guidelines: https://github.com/OpenMined/Docs/blob/master/contributing/guidelines.md
-->
#### Reference Issue: #368 
<!-- Example: Fixes #334 -->

Removes Django installation during Travis build

<!--
Please note that we are a small team of volunteers so patience is
essential; We value all the contributions, no matter how small they are. 
If we are slow to review, either the pull request needs 
some tweaking,convincing, etc. 
or probably the reviewers could be busy. In any
case, we hope for your understanding in the process.

However If you feel that  this needs to be fastened up.
Reachout to us on our slack #pysyft channel.

Thanks for the contribution!
-->",1,True,2017-10-24 05:15:17,2017-10-24 13:52:40,2017-10-24 08:58:28
https://github.com/OpenMined/PySyft/pull/367,[],Implement Default multinomial Functionality for Base Tensor Type,"Implement Default multinomial Functionality for Base Tensor Type<!--
Thanks for submitting a PR. Please make sure  that you have read the contribution guidelines: https://github.com/OpenMined/Docs/blob/master/contributing/guidelines.md
-->
#### Reference Issue

<!-- Example: Fixes #334 -->


#### What does your implementation fix? 

#### Explain your changes.


#### include other comments?(if any)


<!--
Please note that we are a small team of volunteers so patience is
essential; We value all the contributions, no matter how small they are. 
If we are slow to review, either the pull request needs 
some tweaking,convincing, etc. 
or probably the reviewers could be busy. In any
case, we hope for your understanding in the process.

However If you feel that  this needs to be fastened up.
Reachout to us on our slack #pysyft channel.

Thanks for the contribution!
-->",9,True,2017-10-23 07:02:55,2017-10-24 21:46:56,2017-10-23 18:40:23
https://github.com/OpenMined/PySyft/pull/366,[],Naming convention on test files solved #365,"Naming convention on test files solved #365#### Reference Issue #365

Test files follow naming convention",3,True,2017-10-22 16:49:04,2017-10-23 19:18:33,2017-10-23 17:43:17
https://github.com/OpenMined/PySyft/pull/360,[],Fix incorrect documentation,Fix incorrect documentation,1,True,2017-10-21 15:27:33,2017-10-22 06:08:36,2017-10-22 06:08:02
https://github.com/OpenMined/PySyft/pull/357,[],Fixed 'high is out of bounds for int32' which occurs in windows,"Fixed 'high is out of bounds for int32' which occurs in windowsadjusted the types to fix ""high is out of bounds for int32"" in windows",1,False,2017-10-19 15:14:47,2017-10-21 16:36:50,2017-10-21 16:36:50
https://github.com/OpenMined/PySyft/pull/344,[],ignore capsule style errors,ignore capsule style errors,1,True,2017-10-17 01:41:46,2017-10-17 09:02:26,2017-10-17 08:54:19
https://github.com/OpenMined/PySyft/pull/342,[],update iadd and isub methods,update iadd and isub methodsUpdated the __iadd__ and the __isub__ method in tensor.py and also fixed a typo in docs,1,True,2017-10-16 16:54:52,2017-10-19 17:09:21,2017-10-19 16:09:53
https://github.com/OpenMined/PySyft/pull/337,[],Updated LinearClassifier,"Updated LinearClassifierAdded some checks to encrypt() and decrypt(), fixed notebooks to use the right capsule. Also added an abstract interface for future implementations.",1,True,2017-10-15 21:56:19,2017-10-19 08:55:34,2017-10-19 08:55:34
https://github.com/OpenMined/PySyft/pull/320,[],Feature/paillier fixed point functionality,"Feature/paillier fixed point functionalityThis partially addresses issue #12 
This is my third solution for having a more abstract fixed-point implementation.
My previous solutions involved creating our own fixed-point data type. But that was awkward given that `phe` module has its own internal fixed-point representation. 
The next solution was to  have our own fixed-point data type, and only use ""raw"" encrypting functions of `phe`, but that seemed like a waste. Because `phe` already has alot of fixed-point functionality built-in which we had to replicate for our own data type.

The only problem with `phe`'s fixed-point implementation was that we can not control the precision through its API (It is calculated automatically).

This is the third solution that I came up with. It basically replaces some built-in functions in `phe` module (without altering the code in the module itself) to allow us to explicitly control fixed-point precision encoding. The major change in this pull request is at line 140 of `he/paillier/keys.py`.",7,True,2017-10-11 01:21:58,2017-10-23 08:16:27,2017-10-23 08:16:27
https://github.com/OpenMined/PySyft/pull/314,[],"handle slices on 1 dim tensors, resolves #311","handle slices on 1 dim tensors, resolves #311Trying new PR as I force pushed by error on #312 and couldn't push anymore. Sorry will not  force push anymore.",1,True,2017-10-10 10:13:12,2017-10-10 16:01:17,2017-10-10 16:00:57
https://github.com/OpenMined/PySyft/pull/306,[],remove cached pyc files before running tests,"remove cached pyc files before running testsIf I run tests locally then inside the container (or other way around) I get the following error. 

```
tests/test_integrations_locally.py _____________________________________________________________
import file mismatch:                                                                                                                                                         
imported module 'PySyft.tests.test_integrations_locally' has this __file__ attribute:                                                                                         
  /src/PySyft/tests/test_integrations_locally.py                                                                                                                              
which is not the same as the test file we want to collect:                                                                                                                    
  /home/spike/projects/PySyft/tests/test_integrations_locally.py                                                                                                              
HINT: remove __pycache__ / .pyc files and/or use a unique basename for your test file modules      
```                                                                        

This situation could easilly happen if someone installs and tests locally then switches to Docker or the opposite. 

The test rule in the Makefile should delete the pyc files. ",1,True,2017-10-08 18:08:45,2017-10-08 18:44:53,2017-10-08 18:44:53
https://github.com/OpenMined/PySyft/pull/288,[],Feature/paillier fixed point functionality  fp for paillier tensors,"Feature/paillier fixed point functionality  fp for paillier tensors‚Ä¶Modifies operation to work with the new `FixedPoint` class.

This party addresses issue #12. By changing the underlying datatype of `PaillierTensor` to `FixedPoint` and letting it accept different fixed point configurations upon instantiation. The goal is for all other Paillier tensor types (i.e. `PaillierFloatTensor`, `PaillierLongTensor`, etc) to derive from `PaillierTensor`

@iamtrask Let me know if this implementation (so far) is what you had in mind, and what needs to change. Definitely let me know if you think this is totally the wrong direction :)",2,False,2017-10-04 01:13:01,2017-10-07 22:51:18,2017-10-07 22:51:18
https://github.com/OpenMined/PySyft/pull/278,[],Git fix,"Git fix.DS_Store is a file added by some vs code extensions.

Ignoring this file so that someone doesn't accidentally add it",1,True,2017-10-02 04:48:33,2017-10-02 12:58:54,2017-10-02 12:24:04
https://github.com/OpenMined/PySyft/pull/275,[],Implements Default half Functionality #52,"Implements Default half Functionality #52spacing needs some fix here.ignore this. The issue was fixed. My lazy eye missed the last commit.There are more separators than required. please check that.
Or maybe less too. this is also similar to the numpy example. not sure why different lengths are used.To adhere to the docstring format in #252, it should say 
```
Parameters
----------
```
even when there are no parameters",4,True,2017-10-01 14:32:42,2017-10-17 15:22:31,2017-10-05 15:02:18
https://github.com/OpenMined/PySyft/pull/273,[],#252 Inline Documentation Consistency,"#252 Inline Documentation Consistency This is one of my first pull requests. So Please feel free to tell me where I am wrong; I would love to get some feedback

I followed the NumPy Style on inline documentation. http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_numpy.html

It's a bit rough around the edges but let me know if I am on the right trackn x p output tensor.Hey, @CT83 The changes look ok, However, I will leave this open so that we can hear what other folks think about it, and if they like it as well, I think we could move forward.

and you can learn the basic git workflow over [here](https://github.com/OpenMined/Docs/blob/master/tutorials/github_contribution_workflow_tutorial.md)Yes, Thank you! (Deleted my previous Reply as I figured it out myself; later)Done!",23,True,2017-10-01 09:52:49,2017-10-04 17:13:58,2017-10-04 02:10:21
https://github.com/OpenMined/PySyft/pull/267,[],Changed weights initialization & added stride functionality #100,"Changed weights initialization & added stride functionality #100Initialization of all weights to zero mathematically leads to either the neuron values are zero (for multi layers) or the delta would be zero.Please make the documentation consistent with #252

you can take a look at this link 
https://github.com/OpenMined/PySyft/blob/4e2aa9e34df985f4d8e0703d900eaa2558fd413a/syft/math.py#L259Runtime error!! I guess not.
We can do a simple `Error` or better `ValueError` which suits this situation.
Look into these. `RuntimeError` is definitely not the one we should be using.@AkashGutha Pytorch used Runtime error for this issue.This still hasn't resonated with the expected style of documentation.
use Parameters and Results and separators.

Take a look at this link.
https://github.com/OpenMined/PySyft/blob/4e2aa9e34df985f4d8e0703d900eaa2558fd413a/syft/math.py#L259A tuple is returned when there's no Argument, If not an int is returned.
so maybe you can change the doc to reflect that?",18,True,2017-09-30 13:12:02,2017-10-19 15:42:11,2017-10-19 15:41:56
https://github.com/OpenMined/PySyft/pull/264,[],BMM functionality - issue #28,"BMM functionality - issue #28issue #28 

Features looking at:
1. if any of the tensors is encrypted return NotImplemented error.
2. Unit tests to check shape returned tensor shape.
3. Unit tests to check the tensor value returned.
4. Inline documentation should take inspiration from pytorch but not copy it (which currently i did).",5,True,2017-09-30 11:47:35,2017-10-02 04:59:03,2017-10-02 04:37:39
https://github.com/OpenMined/PySyft/pull/225,[],syft.he.paillier ModuleNotFoundError misspelling fix.,syft.he.paillier ModuleNotFoundError misspelling fix.,1,True,2017-09-09 18:06:09,2017-09-10 11:50:03,2017-09-10 11:49:51
https://github.com/OpenMined/PySyft/pull/200,[],change Paillier to paillier to be consistant with PEP8 and fix case-s‚Ä¶,"change Paillier to paillier to be consistant with PEP8 and fix case-s‚Ä¶‚Ä¶ensitivity bug
fixes #194",1,True,2017-09-01 06:27:55,2017-09-30 23:03:43,2017-09-01 22:51:14
https://github.com/OpenMined/PySyft/pull/176,[],Fix for an indentation error on ceil,Fix for an indentation error on ceilThe ceiling fell in?,2,False,2017-08-22 22:35:34,2017-08-23 08:55:56,2017-08-23 08:55:35
https://github.com/OpenMined/PySyft/pull/163,[],fixed the CI test commands,fixed the CI test commands,2,False,2017-08-19 23:55:31,2017-08-20 00:19:30,2017-08-20 00:16:23
https://github.com/OpenMined/PySyft/pull/160,[],fix-pep8-tests,"fix-pep8-testsFixes #119

We got automatic pep8 style checks now üëç",8,True,2017-08-19 10:45:06,2017-08-20 09:47:17,2017-08-20 00:44:44
https://github.com/OpenMined/PySyft/pull/156,[],Fixes #142,Fixes #142 #142,2,True,2017-08-18 15:15:41,2017-08-18 16:04:46,2017-08-18 16:04:46
https://github.com/OpenMined/PySyft/pull/155,[],"CI setup: automated docker build, tests, and docker image hosting (issue #119)","CI setup: automated docker build, tests, and docker image hosting (issue #119)Fixes #119

Next steps:

- You'll need to go to https://circleci.com/vcs-authorize/ and sign in with an GitHub account that is admin on the PySyft repository, and follow instruction in the UI to enable CI for the PySyft project.

- Once done, you need to store the dockerhub environment variables into the circleci UI under the project settings.

- You can see a preview of what the result look like [here.](https://circleci.com/gh/cypherai/PySyft/9?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link). Tests are currently failing because of the indentation error (see PR #149).",1,True,2017-08-17 19:41:09,2017-08-18 22:11:19,2017-08-17 22:53:56
https://github.com/OpenMined/PySyft/pull/145,[],Add Cumprod & erros fix,Add Cumprod & erros fixI fixed few errors that @samsontmr & I discussed I also added cumulative product over axis I also noticed the test for addbm failed but since @bartimaeus12 has already noticed the last issue #142 I'll leave it to him .,1,True,2017-08-16 10:57:25,2017-08-16 14:22:22,2017-08-16 14:22:21
https://github.com/OpenMined/PySyft/pull/4,[],"Corrected Issue of missing math libraries: mpc, mpfr, gmp","Corrected Issue of missing math libraries: mpc, mpfr, gmpAll of them are required for installation of gmpy2
** This fix is for anaconda users**",1,True,2017-07-24 20:33:44,2017-07-24 22:21:08,2017-07-24 22:21:08